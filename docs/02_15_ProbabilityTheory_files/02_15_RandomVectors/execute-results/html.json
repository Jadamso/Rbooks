{
  "hash": "974ed954b9cfb9819837712294f1d463",
  "result": {
    "engine": "knitr",
    "markdown": "\n# Probability Theory\n***\n\n## Theoretical Distributions\n\nWe now consider a bivariate *random vector* $(X_{i}, Y_{i})$, which is a theoretical version of the bivariate observations $(\\hat{X}_{i}, \\hat{Y}_{i})$. E.g., If we are going to flip two coins, then $(X_{i}, Y_{i})$ corresponds to the unflipped coins and $(\\hat{X}_{i}, \\hat{Y}_{i})$ corresponds to concrete values after they are flipped.\n\n\n#### **Definitions for Discrete Data**. {-}\nThe *joint distribution* is defined as\n\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y)\n\\end{eqnarray}\nFor example, consider a bivariate random vector with three outcomes for $X_{i}$ and three outcomes for $Y_{i}$. The plot below shows each outcome $(x,y)$ with deeper colors reflecting higher probability events.\n\n::: {.cell}\n\n```{.r .cell-code}\nprob_table <- expand.grid(x=c(0,1,2), y=c(0,10,20))\nprob_table[,'probabilities'] <- c(\n    0.0, 0.1, 0.0,\n    0.1, 0.3, 0.1,\n    0.1, 0.2, 0.2)\nplot(prob_table[,'x'], prob_table[,'y'],\n     xlim=c(-0.5,2.5), ylim=c(-5, 25),\n     pch=21, cex=8, col='blue',\n     bg=rgb(0,0,1, prob_table[,'probabilities']),\n     xlab = \"X\", ylab = \"Y\")\n```\n\n::: {.cell-output-display}\n![](02_15_RandomVectors_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nNote that variables are *statistically independent* if $Prob(X_{i} = x, Y_{i} = y)= Prob(X_{i} = x) Prob(Y_{i} = y)$ for all $x, y$. Independence is sometimes assumed for mathematical simplicity, not because it generally fits data well.^[The same can be said about assuming normally distributed errors, although at least that can be motivated by the Central Limit Theorems.]\n\n\nThe *conditional distributions* are defined as\n\\begin{eqnarray}\nProb(X_{i} = x | Y_{i} = y) = \\frac{ Prob(X_{i} = x, Y_{i} = y)}{ Prob( Y_{i} = y )}\\\\\nProb(Y_{i} = y | X_{i} = x) = \\frac{ Prob(X_{i} = x, Y_{i} = y)}{ Prob( X_{i} = x )}\n\\end{eqnarray}\nThe *marginal distributions* are then defined as\n\\begin{eqnarray}\nProb(X_{i} = x) = \\sum_{y} Prob(X_{i} = x | Y_{i} = y) Prob( Y_{i} = y ) \\\\\nProb(Y_{i} = y) = \\sum_{x} Prob(Y_{i} = y | X_{i} = x) Prob( X_{i} = x ),\n\\end{eqnarray}\nwhich is also known as the *law of total probability*.\n\n\n\n#### **Coin Flips Example**. {-}\n\nFor one example, Consider flipping two coins, where we mark whether \"heads\" is face up with a $1$ and \"tail\" with a $0$. E.g., the first coin has a value of $x=1$ if it shows heads and $x=0$ if it shows tails.\nThis table shows both the joint distribution and also each marginal distribution.\n\n|    | $x=0$  | $x=1$   | Marginal    |\n|:------:|:---:|:---:|:---:|\n| $y=0$    |$Prob(X_{i}=0,Y_{i}=0)$|$Prob(X_{i}=1,Y_{i}=0)$|$Prob(Y_{i}=0)$|\n| $y=1$    |$Prob(X_{i}=0,Y_{i}=1)$|$Prob(X_{i}=1,Y_{i}=1)$|$Prob(Y_{i}=1)$|\n| **Marginal** |$Prob(X_{i}=0)$    |$Prob(X_{i}=1)$        | $1$           |\n\nNote that different joint distributions can have the same marginal distributions.\n\n:::{.callout-note icon=false collapse=\"true\"}\nSuppose both coins are \"fair\": $Prob(X_{i}=1)= 1/2$ and $Prob(Y_{i}=1|X_{i}=x)=1/2$ for either $x=1$ or $x=0$, then the four potential outcomes have equal probabilities. \n\\begin{eqnarray}\nProb(X_{i} = 0, Y_{i} = 0) &=& 1/2 \\times 1/2 = 1/4 \\\\\nProb(X_{i} = 0, Y_{i} = 1) &=& 1/4 \\\\\nProb(X_{i} = 1, Y_{i} = 0) &=& 1/4 \\\\\nProb(X_{i} = 1, Y_{i} = 1) &=& 1/4 .\n\\end{eqnarray}\nThe joint distribution is written generally as\n\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y) &=& Prob(X_{i} = x) Prob(Y_{i} = y).\n\\end{eqnarray}\n\nThe marginal distribution of the second coin is \n\\begin{eqnarray}\nProb(Y_{i} = 0) &=& Prob(Y_{i} = 0 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 0 | X_{i} = 1) Prob(X_{i}=1)\\\\\n&=& 1/2 (1/2) + 1/2 (1/2) = 1/2\\\\\nProb(Y_{i} = 1) &=& Prob(Y_{i} = 1 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 1 | X_{i} = 1) Prob(X_{i}=1)\\\\\n&=& 1/2 (1/2) + 1/2 (1/2) = 1/2\n\\end{eqnarray}\n\nThe marginal distribution of the first coin is found in the exact same way\n\\begin{eqnarray}\nProb(X_{i} = 0) &=& Prob(X_{i} = 0 | Y_{i} = 0) Prob(Y_{i}=0) + Prob(X_{i} = 0 | Y_{i} = 1) Prob(Y_{i}=1)\\\\\n&=& 1/2 (1/2) + 1/2 (1/2) = 1/2\\\\\nProb(X_{i} = 1) &=& Prob(X_{i} = 1 | Y_{i} = 0) Prob(Y_{i}=0) + Prob(X_{i} = 1 | Y_{i} = 1) Prob(Y_{i}=1)\\\\\n&=& 1/2 (1/2) + 1/2 (1/2) = 1/2\n\\end{eqnarray}\n\nAltogether, we find\n\n|    | $x=0$  | $x=1$   | Marginal    |\n|:------:|:---:|:---:|:---:|\n| $y=0$        |$1/4$|$1/4$|$1/2$|\n| $y=1$        |$1/4$|$1/4$|$1/2$|\n| **Marginal** |$1/2$|$1/2$| $1$ |\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a 2x2 matrix for the joint distribution.\n# Rows correspond to X1 (coin 1), and columns correspond to X2 (coin 2).\nP_fair <- matrix(1/4, nrow = 2, ncol = 2)\nrownames(P_fair) <- c(\"X1=0\", \"X1=1\")\ncolnames(P_fair) <- c(\"X2=0\", \"X2=1\")\nP_fair\n##      X2=0 X2=1\n## X1=0 0.25 0.25\n## X1=1 0.25 0.25\n\n# Compute the marginal distributions.\n# Marginal for X1: sum across columns.\nP_X1 <- rowSums(P_fair)\nP_X1\n## X1=0 X1=1 \n##  0.5  0.5\n# Marginal for X2: sum across rows.\nP_X2 <- colSums(P_fair)\nP_X2\n## X2=0 X2=1 \n##  0.5  0.5\n\n# Compute the conditional probabilities Prob(X2 | X1).\ncond_X2_given_X1 <- matrix(0, nrow = 2, ncol = 2)\nfor (j in c(1,2)) {\n  cond_X2_given_X1[, j] <- P_fair[, j] / P_X1[j]\n}\nrownames(cond_X2_given_X1) <- c(\"X2=0\", \"X2=1\")\ncolnames(cond_X2_given_X1) <- c(\"given X1=0\", \"given X1=1\")\ncond_X2_given_X1\n##      given X1=0 given X1=1\n## X2=0        0.5        0.5\n## X2=1        0.5        0.5\n```\n:::\n\n:::\n\n:::{.callout-note icon=false collapse=\"true\"}\nNow consider a second example, where the second coin is \"Completely Unfair\", so that it is always the same as the first. The outcomes generated with a Completely Unfair coin are the same as if we only flipped one coin.\n\\begin{eqnarray}\nProb(X_{i} = 0, Y_{i} = 0) &=& 1/2 \\\\\nProb(X_{i} = 0, Y_{i} = 1) &=& 0 \\\\\nProb(X_{i} = 1, Y_{i} = 0) &=& 0 \\\\\nProb(X_{i} = 1, Y_{i} = 1) &=& 1/2 .\n\\end{eqnarray}\nThe joint distribution is written generally as\n\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y) &=& Prob(X_{i} = x) \\mathbf{1}( x=y ),\n\\end{eqnarray}\nwhere $\\mathbf{1}(X_{i}=1)$ means $X_{i}= 1$ and $0$ if $X_{i}\\neq0$.\nThe marginal distribution of the second coin is \n\\begin{eqnarray}\nProb(Y_{i} = 0) \n&=& Prob(Y_{i} = 0 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 0 | X_{i} = 1) Prob(X_{i} = 1)\\\\\n&=& 1 (1/2) + 0(1/2) = 1/2 .\\\\\nProb(Y_{i} = 1)\n&=& Prob(Y_{i} = 1 | X_{i} =0) Prob( X_{i} = 0) + Prob(Y_{i} = 1 | X_{i} = 1) Prob( X_{i} = 1)\\\\\n&=& 0 (1/2) + 1 (1/2) = 1/2 .\n\\end{eqnarray}\nwhich is the same marginal as in the first example!\n\nThe marginal distribution of the first coin is found in the exact same way (show this yourself).\n\nAlltogether, we find\n\n|    | $x=0$  | $x=1$   | Marginal    |\n|:------:|:---:|:---:|:---:|\n| $y=0$        |$1/2$|$0$  |$1/2$|\n| $y=1$        |$0$  |$1/2$|$1/2$|\n| **Marginal** |$1/2$|$1/2$| $1$ |\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the joint distribution matrix for the unfair coin case.\nP_unfair <- matrix(c(0.5, 0, 0, 0.5), nrow = 2, ncol = 2, byrow = TRUE)\nrownames(P_unfair) <- c(\"X1=0\", \"X1=1\")\ncolnames(P_unfair) <- c(\"X2=0\", \"X2=1\")\nP_unfair\n##      X2=0 X2=1\n## X1=0  0.5  0.0\n## X1=1  0.0  0.5\n\n# Compute the marginal distribution for X2 in the unfair case.\nP_X2_unfair <- colSums(P_unfair)\nP_X1_unfair <- rowSums(P_unfair)\n\n# Compute the conditional probabilities Prob(X1 | X2) for the unfair coin.\ncond_X2_given_X1_unfair <- matrix(NA, nrow = 2, ncol = 2)\nfor (j in c(1,2)) {\n  if (P_X1_unfair[j] > 0) {\n    cond_X2_given_X1_unfair[, j] <- P_unfair[, j] / P_X1_unfair[j]\n  }\n}\nrownames(cond_X2_given_X1_unfair) <- c(\"X2=0\", \"X2=1\")\ncolnames(cond_X2_given_X1_unfair) <- c(\"given X1=0\", \"given X1=1\")\ncond_X2_given_X1_unfair\n##      given X1=0 given X1=1\n## X2=0          1          0\n## X2=1          0          1\n```\n:::\n\n:::\n\n\n\n\n#### **Definitions for Continuous Data**. {-}\nThe *joint distribution* is defined as\n\\begin{eqnarray}\nF(x, y) &=& Prob(X_{i} \\leq x, Y_{i} \\leq y)\n\\end{eqnarray}\nThe *marginal distributions* are then defined as\n\\begin{eqnarray}\n F_{X}(x) &=& F(x, \\infty)\\\\\n F_{Y}(y) &=& F(\\infty, y).\n\\end{eqnarray}\nwhich is also known as the *law of total probability*.\nVariables are statistically independent if $F(x, y) = F_{X}(x)F_{Y}(y)$ for all $x, y$.\n\n\nFor example, suppose $(X_{i},Y_{i})$ is bivariate normal with means $(\\mu_{X}, \\mu_{Y})$, variances $(\\sigma_{X}, \\sigma_{Y})$ and covariance $\\rho$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate Bivariate Data\nN <- 10000\nMu <- c(2,2) ## Means\n\nSigma1 <- matrix(c(2,-.8,-.8,1),2,2) ## CoVariance Matrix\nMVdat1 <- mvtnorm::rmvnorm(N, Mu, Sigma1)\ncolnames(MVdat1) <- c('X','Y')\n\nSigma2 <- matrix(c(2,.4,.4,1),2,2) ## CoVariance Matrix\nMVdat2 <- mvtnorm::rmvnorm(N, Mu, Sigma2)\ncolnames(MVdat2) <- c('X','Y')\n\npar(mfrow=c(1,2))\n## Different diagonals\nplot(MVdat2, col=rgb(1,0,0,0.02), pch=16,\n    main='Joint Distributions', font.main=1,\n    ylim=c(-4,8), xlim=c(-4,8),\n    xlab='X', ylab='Y')\npoints(MVdat1,col=rgb(0,0,1,0.02),pch=16)\n## Same marginal distributions\nxbks <- seq(-4,8,by=.2)\nhist(MVdat2[,2], col=rgb(1,0,0,0.5),\n    breaks=xbks, border=NA, \n    xlab='Y',\n    main='Marginal Distributions', font.main=1)\nhist(MVdat1[,2], col=rgb(0,0,1,0.5),\n    add=T, breaks=xbks, border=NA)\n```\n\n::: {.cell-output-display}\n![](02_15_RandomVectors_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# See that independent data are a special case\nn <- 2e4\n## 2 Indepenant RV\nXYiid <- cbind( rnorm(n),  rnorm(n))\n## As a single Joint Draw\nXYjoint <- mvtnorm::rmvnorm(n, c(0,0))\n## Plot\npar(mfrow=c(1,2))\nplot(XYiid, xlab=\n    col=grey(0,.05), pch=16, xlim=c(-5,5), ylim=c(-5,5))\nplot(XYjoint,\n    col=grey(0,.05), pch=16, xlim=c(-5,5), ylim=c(-5,5))\n\n# Compare densities\n#d1 <- dnorm(XYiid[,1],0)*dnorm(XYiid[,2],0)\n#d2 <- mvtnorm::dmvnorm(XYiid, c(0,0))\n#head(cbind(d1,d2))\n```\n:::\n\n\n\nThe multivariate normal is a workhorse for analytical work on multivariate random variables, but there are many more. See e.g., <https://cran.r-project.org/web/packages/NonNorMvtDist/NonNorMvtDist.pdf>\n\n\n#### **Bayes' Theorem**. {-}\n\nAlso note *Bayes' Theorem*:\n\\begin{eqnarray}\nProb(X_{i} = x | Y_{i} = y)  Prob( Y_{i} = y) \n    &=& Prob(X_{i} = x, Y_{i} = y) = Prob(Y_{i} = y | X_{i} = x) Prob(X_{i} = x).\\\\\nProb(X_{i} = x | Y_{i} = y)\n    &=& \\frac{ Prob(Y_{i} = y | X_{i} = x) Prob(X_{i}=x) }{ Prob( Y_{i} = y) }.\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Verify Bayes' theorem for the unfair coin case:\n# Compute Prob(X1=1 | X2=1) using the formula:\n#   Prob(X1=1 | X2=1) = [Prob(X2=1 | X1=1) * Prob(X1=1)] / Prob(X2=1)\n\nP_X1_1 <- 0.5\nP_X2_1_given_X1_1 <- 1  # Since coin 2 copies coin 1.\nP_X2_1 <- P_X2_unfair[\"X2=1\"]\n\nbayes_result <- (P_X2_1_given_X1_1 * P_X1_1) / P_X2_1\nbayes_result\n## X2=1 \n##    1\n```\n:::\n\n\n\n\n## Theoretical Statistics\n\n#### **Conditional Expectation**. {-}\n\n#### **Correlation**. {-}\n\nWe will now dig a little deeper theoretically into the statistics we compute. When we know how the data are generated theoretically, we can often compute the theoretical value of the most basic and often-used bivariate statistic: the Pearson correlation. To see this, we focus on two discrete random variables, first showing their covariance, $\\mathbb{C}[X_{i}, Y_{i}]$, and then their correlation $\\mathbb{R}[X_{i}, Y_{i}]$. Referring to and $\\mu_{X}=\\mathbb{E}[X_{i}]$ and $\\mu_{Y}=\\mathbb{E}[Y_{i}]$, we have\n\\begin{eqnarray}\n\\mathbb{C}[X_{i}, Y_{i}] \n&=& \\mathbb{E}[(X_{i} –  \\mu_{X})(Y_{i} – \\mu_{Y}])] \n= \\sum_{x}\\sum_{y} (x –  \\mu_{X})(y –  \\mu_{Y}) Prob(X_{i} = x, Y_{i} = y)\n\\\\\n\\mathbb{R}[X_{i}, Y_{i}] &=& \\frac{\\mathbb{C}[X_{i}, Y_{i}] }{ \\sqrt{\\mathbb{V}[X_{i}]} \\sqrt{\\mathbb{V}[Y_{i}]} }\n\\end{eqnarray}\n\nFor example, suppose we have discrete data with the following outcomes and probabilities. Note that cells reflect the probabilities of the outcomes depicted on the row and column labels, e.g. $Prob(X_{i}=1, Y_{i}=0)=0.1$.\n\n|    | $x=0$ | $x=1$ | $x=2$ |\n|:--:|:--:|:--:|:--:|\n| $y=0$  | $0.0$ | $0.1$ | $0.0$ |\n| $y=10$ | $0.1$ | $0.3$ | $0.1$ |\n| $y=20$ | $0.1$ | $0.1$ | $0.2$ |\n\n\nAfter verifying that the probabilities sum to $1$, we then compute the marginal distributions\n\\begin{eqnarray}\nProb(X_{i}=0)=0.2,\\quad Prob(X_{i}=1)=0.5,\\quad Prob(X_{i}=2) = 0.3 \\\\\nProb(Y_{i}=0)=0.1,\\quad Prob(Y_{i}=10)=0.5,\\quad Prob(Y_{i}=20) = 0.4\n\\end{eqnarray}\nwhich allows us to compute the means:\n\\begin{eqnarray}\n\\mathbb{E}[X_{i}] &=& 0(0.2)+1(0.5)+2(0.3) = 1.1 \\\\\n\\mathbb{E}[Y_{i}] &=& 0(0.1)+10(0.5)+20(0.4) = 13\n\\end{eqnarray}\nWe can then compute the cell-by-cell contributions: $Prob(X_{i} = x, Y_{i} = y) (x-\\mathbb{E}[X_{i}])(y-\\mathbb{E}[Y_{i}])$, which lead plug in to the covariance formula;\n\\begin{eqnarray}\n\\begin{array}{l l r r r r r}\n\\hline\nx & y & Prob(X_{i}=x, Y_{i}=y) & x-\\mathbb{E}[X_{i}] & y-\\mathbb{E}[Y_{i}] & (x-\\mathbb{E}[X_{i}])(y-\\mathbb{E}[Y_{i}]) & \\text{Contribution}\\\\\n\\hline\n0 & 0  & 0.0 & -1.1 & -13 & 14.3  & 0\\\\\n0 & 10 & 0.1 & -1.1 & -3  & 3.3   & 0.330\\\\\n0 & 20 & 0.1 & -1.1 & 7   & -7.7  & -0.770\\\\\n1 & 0  & 0.1 & -0.1 & -13 & 1.3   & 0.130\\\\\n1 & 10 & 0.3 & -0.1 & -3  & 0.3   & 0.090\\\\\n1 & 20 & 0.1 & -0.1 & 7   & -0.7  & -0.070\\\\\n2 & 0  & 0.0 & 0.9  & -13 & -11.7 & 0\\\\\n2 & 10 & 0.1 & 0.9  & -3  & -2.7  & -0.270\\\\\n2 & 20 & 0.2 & 0.9  & 7   & 6.3   & 1.260\\\\\n\\hline\n\\end{array}\n\\end{eqnarray}\n\\begin{eqnarray}\n\\mathbb{C}[X_{i},Y_{i}] &=& \\sum_{x} \\sum_{y} \\left(x-\\mathbb{E}[X_{i}]\\right)\\left(y-\\mathbb{E}[Y_{i}]\\right) Prob\\left(X_{i} = x, Y_{i} = y\\right) \\\\\n&=& 0 + 0.330 -0.770 + 0.130 + 0.090  -0.070 +0 -0.270 + 1.260\n= 0.7\n\\end{eqnarray}\n\nTo compute the correlation value, we first need the standard deviations\n\\begin{eqnarray}\n\\mathbb{V}[X_{i}] &=& \\sum_{x} (x-\\mathbb{E}[X_{i}])^2 Prob(X_{i} = x) \\\\\n&=& (0-1.1)^2(0.2)+(1-1.1)^2(0.5)+(2-1.1)^2(0.3)=0.49 \\\\\n\\mathbb{V}[Y_{i}] &=& \\sum_{y}  (y-\\mathbb{E}[Y_{i}])^2 Prob(Y_{i} = y) \\\\\n&=& (0-13)^2(0.1)+(10-13)^2(0.5)+(20-13)^2(0.4)=41 \\\\\n\\end{eqnarray}\nThen we can find the correlation as\n\\begin{eqnarray}\n\\frac{\\mathbb{C}[X_{i},Y_{i}]}{ \\sqrt{\\mathbb{V}[X_{i}]} \\sqrt{\\mathbb{V}[Y_{i}]} } \n&=& \\frac{0.7}{\\sqrt{0.49} \\sqrt{41}} \\approx 0.156,\n\\end{eqnarray}\nwhich suggests a weak positive association between the variables.\n\nNote that you can do all of the above calculations using the computer instead of by hand.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make a Probability Table\nx <- c(0,1,2)\ny <- c(0,10,20)\nxy_probs <- matrix(c(\n    0.0, 0.1, 0.0,\n    0.1, 0.3, 0.1,\n    0.1, 0.1, 0.2\n), nrow=3, ncol=3, byrow=TRUE)\nrownames(xy_probs) <- paste0('y=',y)\ncolnames(xy_probs) <-  paste0('x=',x)\nxy_probs\n##      x=0 x=1 x=2\n## y=0  0.0 0.1 0.0\n## y=10 0.1 0.3 0.1\n## y=20 0.1 0.1 0.2\n\n# Compute Marginals and Means\npX  <- colSums(xy_probs)\npY  <- rowSums(xy_probs)\nEX  <- sum(x * pX)\nEY  <- sum(y * pY)\n\n# Compute Covariance\ndxy_grid <- expand.grid(dy=y-EY, dx=x-EX)[,c(2,1)]\ndxy_grid[,'p'] <- as.vector(xy_probs)\ndxy_grid[,'contribution'] <- dxy_grid[,'dx'] * dxy_grid[,'dy'] * dxy_grid[,'p']\nCovXY <- sum(dxy_grid[,'contribution'])\nCovXY\n## [1] 0.7\n\n# Compute Variances\nVX <- sum( (x-EX)^2 * pX)\nSX <- sqrt(VX)\nVY <- sum( (y-EY)^2 * pY)\nSY <- sqrt(VY)\n\n# Compute Correlation\nCorXY <- CovXY / (SX * SY)\nCorXY\n## [1] 0.1561738\n```\n:::\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nCompute the correlation for bivariate data with these probabilities, using both math (first) and the computer (second)\n\n|    | $x=-10$ | $x=10$ |\n|:--:|:--:|:--:|\n| $y=0$ | $0.05$ | $0.20$ |\n| $y=1$ | $0.05$ | $0.20$ |\n| $y=2$ | $0.05$ | $0.20$ |\n| $y=3$ | $0.05$ | $0.20$ |\n\n:::\n\n:::{.callout-tip icon=false collapse=\"true\"}\n\nAlso compute the correlation for bivariate data with these probabilities\n\n|    | $x=-10$ | $x=10$ |\n|:--:|:--:|:--:|\n| $y=0$ | $0.05$ | $0.05$ |\n| $y=1$ | $0.10$ | $0.10$ |\n| $y=2$ | $0.15$ | $0.15$ |\n| $y=3$ | $0.20$ | $0.20$ |\n\nAlso compute the correlation for bivariate data with these probabilities\n\n|    | $x=-10$ | $x=10$ |\n|:--:|:--:|:--:|\n| $y=0$ | $0.05$ | $0.15$ |\n| $y=1$ | $0.05$ | $0.15$ |\n| $y=2$ | $0.10$ | $0.20$ |\n| $y=3$ | $0.10$ | $0.20$ |\n\nExplain intuitively when the correlation equals $0$ and when it does not.\n:::\n\n\n\n\n## Further Reading \n\nMany introductory econometrics textbooks have a good appendix on probability and statistics. There are many useful statistical texts online too\n\nSee the Further reading about Probability Theory in the Statistics chapter.\n\n* <https://www.r-bloggers.com/2024/03/calculating-conditional-probability-in-r/>\n\n",
    "supporting": [
      "02_15_RandomVectors_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}