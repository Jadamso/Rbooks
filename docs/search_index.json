[["index.html", " 1 Preface", " 1 Preface This Rbook introduces students to econometrics without parametric assumptions and formulas. In many ways, it is a modern version of “Introductory Econometrics: Using Monte Carlo Simulation with Microsoft Excel” by Barreto and Howland, updated to give econometrics students the best tools for their labor market and adhere to modern statistics teaching guidelines. The Rbook is organized into three parts Part I: Data Analysis in R Part II: Linear Regression in R Part III: Reproducible Research in R Part I introduces students to the basics of programming and statistical analysis of economic data using R. There are many practical examples, including on how to analyze data interactively and communicate results. I aimed to replace mathematics with simulations whenever possible. We also cover statistical reporting using R + markdown, which research suggests is a good combination 1 2. Part II refines material from several introductory econometrics textbooks and covers linear models only from a “minimum distance” perspective. (We operate under the maxim “All models are wrong” and do not prove unbiasedness.) Also included is a novel chapter on “Data scientism” that more clearly illustrates the ways that simplistic approaches can mislead rather than illuminate. (I stress “gun safety” instead of “pull to shoot”, which I feel is missing from many textbooks.) Overall, there is a more humble view towards what we can infer from linear regressions and more room for economic theory in model development and interpretation. Part III synthesize a lot of programming guidance and examples now available on the internet. This is useful for semester projects and beyond. Although any interested reader may find it useful, this Rbook is primarily developed for my students. It could serve as a technical prerequisite for more advanced courses (such as nonparametric statistics or structural econometrics.) Please report any errors or issues at https://github.com/Jadamso/Rbooks/issues. Last updated: 16.04.2025 "],["first-steps.html", " 2 First Steps 2.1 Why R 2.2 Install R 2.3 Interfacing with R 2.4 Introductions to R", " 2 First Steps 2.1 Why R We focus on R because it is good for complex stats, concise figures, and coherent organization. It is built and developed by applied statisticians for statistics, and used by many in academia and industry. For students, think about labor demand and what may be good for getting a job. Do some of your own research to best understand how much to invest. 2.2 Install R First Install R. Then Install Rstudio. For help setting up, see any of the following links https://learnr-examples.shinyapps.io/ex-setup-r/ https://rstudio-education.github.io/hopr/starting.html https://a-little-book-of-r-for-bioinformatics.readthedocs.io/en/latest/src/installr.html https://cran.r-project.org/doc/manuals/R-admin.html https://courses.edx.org/courses/UTAustinX/UT.7.01x/3T2014/56c5437b88fa43cf828bff5371c6a924/ https://owi.usgs.gov/R/training-curriculum/installr/ https://www.earthdatascience.org/courses/earth-analytics/document-your-science/setup-r-rstudio/ Make sure you have the latest version of R and Rstudio for class. If not, then reinstall. 2.3 Interfacing with R Rstudio is easiest to get going with. (There are other GUI’s.) There are 4 panes. The top left is where you write and save code Create and save a new R Script file My_First_Script.R could also use a plain .txt file. The pane below is where your code is executed. For all following examples, make sure to both execute and store your code. Note that the coded examples generally have objects, functions, and comments. 2.4 Introductions to R There are many good and free programming materials online. The most common tasks can be found https://github.com/rstudio/cheatsheets/blob/main/rstudio-ide.pdf Some of my programming examples originally come from https://r4ds.had.co.nz/ and I recommend https://intro2r.com. I have also used online material from many places over the years, including https://cran.r-project.org/doc/manuals/R-intro.html R Graphics Cookbook, 2nd edition. Winston Chang. 2021. https://r-graphics.org/ R for Data Science. H. Wickham and G. Grolemund. 2017. https://r4ds.had.co.nz/index.html An Introduction to R. W. N. Venables, D. M. Smith, R Core Team. 2017. https://colinfay.me/intro-to-r/ Introduction to R for Econometrics. Kieran Marray. https://bookdown.org/kieranmarray/intro_to_r_for_econometrics/ Wollschläger, D. (2020). Grundlagen der Datenanalyse mit R: eine anwendungsorientierte Einführung. http://www.dwoll.de/rexrepos/ Spatial Data Science with R: Introduction to R. Robert J. Hijmans. 2021. https://rspatial.org/intr/index.html What we cover in this primer should be enough to get you going. But there are also many good yet free-online tutorials and courses. https://www.econometrics-with-r.org/1.2-a-very-short-introduction-to-r-and-rstudio.html https://rafalab.github.io/dsbook/ https://moderndive.com/foreword.html https://rstudio.cloud/learn/primers/1.2 https://cran.r-project.org/manuals.html https://stats.idre.ucla.edu/stat/data/intro_r/intro_r_interactive_flat.html https://cswr.nrhstat.org/app-r "],["mathematics.html", " 3 Mathematics 3.1 Scalars 3.2 Vectors 3.3 Functions 3.4 Logic 3.5 Matrices 3.6 Arrays", " 3 Mathematics Scalars and vectors are probably your most common mathematical objects 3.1 Scalars xs &lt;- 2 # Your first scalar xs # Print the scalar ## [1] 2 (xs+1)^2 # Perform and print a simple calculation ## [1] 9 xs + NA # often used for missing values ## [1] NA xs*2 ## [1] 4 3.2 Vectors x &lt;- c(0,1,3,10,6) # Your First Vector x # Print the vector ## [1] 0 1 3 10 6 x[2] # Print the 2nd Element; 1 ## [1] 1 x+2 # Print simple calculation; 2,3,5,8,12 ## [1] 2 3 5 12 8 x*2 ## [1] 0 2 6 20 12 x^2 ## [1] 0 1 9 100 36 Mathematical operations apply elementwise x+x ## [1] 0 2 6 20 12 x*x ## [1] 0 1 9 100 36 x^x ## [1] 1.0000e+00 1.0000e+00 2.7000e+01 1.0000e+10 4.6656e+04 c(1) # scalars are vectors ## [1] 1 1:7 ## [1] 1 2 3 4 5 6 7 seq(0,1,by=.1) ## [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 3.3 Functions Functions are applied to objects # Define a function that adds two to any vector add_2 &lt;- function(input_vector) { output_vector &lt;- input_vector + 2 # new object defined locally return(output_vector) # return new object } # Apply that function to a vector x &lt;- c(0,1,3,10,6) add_2(x) ## [1] 2 3 5 12 8 # notice &#39;output_vector&#39; is not available here There are many many generalizations add_vec &lt;- function(input_vector1, input_vector2) { output_vector &lt;- input_vector1 + input_vector2 return(output_vector) } add_vec(x,3) ## [1] 3 4 6 13 9 add_vec(x,x) ## [1] 0 2 6 20 12 sum_squared &lt;- function(x1, x2) { y &lt;- (x1 + x2)^2 return(y) } sum_squared(1, 3) ## [1] 16 sum_squared(x, 2) ## [1] 4 9 25 144 64 sum_squared(x, NA) ## [1] NA NA NA NA NA sum_squared(x, x) ## [1] 0 4 36 400 144 sum_squared(x, 2*x) ## [1] 0 9 81 900 324 Applying the same function over and over again sapply(1:3, exp) ## [1] 2.718282 7.389056 20.085537 c( exp(1), exp(2), exp(3)) ## [1] 2.718282 7.389056 20.085537 # More complex example sapply(1:10, function(i){ x &lt;- i^(i-1) y &lt;- x + mean( 0:i ) z &lt;- log(y)/i return(z) }) ## [1] 0.4054651 0.5493061 0.7837918 1.0474137 1.2883487 1.4931972 1.6679272 ## [8] 1.8195116 1.9530885 2.0723266 # mapply takes multiple vectors # mapply(sum, 1:3, exp(1:3) ) Functions can take functions as arguments fun_of_seq &lt;- function(f){ x &lt;- seq(1,3, length.out=12) y &lt;- f(x) return(y) } fun_of_seq(mean) ## [1] 2 fun_of_seq(sd) ## [1] 0.6555548 3.4 Logic Basic. x &lt;- c(1,2,3,NA) x &gt; 2 ## [1] FALSE FALSE TRUE NA x==2 ## [1] FALSE TRUE FALSE NA any(x==2) ## [1] TRUE all(x==2) ## [1] FALSE 2 %in% x ## [1] TRUE is.numeric(x) ## [1] TRUE is.na(x) ## [1] FALSE FALSE FALSE TRUE The “&amp;” and “|” commands are logical operations that compare vectors to the left and right. x &lt;- 1:3 is.numeric(x) &amp; (x &lt; 2) ## [1] TRUE FALSE FALSE is.numeric(x) | (x &lt; 2) ## [1] TRUE TRUE TRUE if(length(x) &gt;= 5 &amp; x[5] &gt; 12) print(&quot;ok&quot;) see https://bookdown.org/rwnahhas/IntroToR/logical.html Advanced. x &lt;- 1:10 cut(x, 4) ## [1] (0.991,3.25] (0.991,3.25] (0.991,3.25] (3.25,5.5] (3.25,5.5] ## [6] (5.5,7.75] (5.5,7.75] (7.75,10] (7.75,10] (7.75,10] ## Levels: (0.991,3.25] (3.25,5.5] (5.5,7.75] (7.75,10] split(x, cut(x, 4)) ## $`(0.991,3.25]` ## [1] 1 2 3 ## ## $`(3.25,5.5]` ## [1] 4 5 ## ## $`(5.5,7.75]` ## [1] 6 7 ## ## $`(7.75,10]` ## [1] 8 9 10 xs &lt;- split(x, cut(x, 4)) sapply(xs, mean) ## (0.991,3.25] (3.25,5.5] (5.5,7.75] (7.75,10] ## 2.0 4.5 6.5 9.0 # shortcut aggregate(x, list(cut(x,4)), mean) ## Group.1 x ## 1 (0.991,3.25] 2.0 ## 2 (3.25,5.5] 4.5 ## 3 (5.5,7.75] 6.5 ## 4 (7.75,10] 9.0 3.5 Matrices Matrices are objects x1 &lt;- c(1,4,9) x2 &lt;- c(3,0,2) x_mat &lt;- rbind(x1, x2) x_mat # Print full matrix ## [,1] [,2] [,3] ## x1 1 4 9 ## x2 3 0 2 x_mat[2,] # Print Second Row ## [1] 3 0 2 x_mat[,2] # Print Second Column ## x1 x2 ## 4 0 x_mat[2,2] # Print Element in Second Column and Second Row ## x2 ## 0 There are elementwise operations x_mat+2 x_mat*2 x_mat^2 x_mat + x_mat x_mat*x_mat x_mat^x_mat Functions. You can apply functions to matrices sum_squared(x_mat, x_mat) ## [,1] [,2] [,3] ## x1 4 64 324 ## x2 36 0 16 # Apply function to each matrix row y &lt;- apply(x_mat, 1, sum)^2 # ?apply #checks the function details y - sum_squared(x, x) # tests if there are any differences ## [1] 192 9 160 -39 96 -119 0 -231 -128 -375 There are many possible functions you can apply # Return Y-value with minimum absolute difference from 3 abs_diff_y &lt;- abs( y - 3 ) abs_diff_y # is this the luckiest number? ## x1 x2 ## 193 22 #min(abs_diff_y) #which.min(abs_diff_y) y[ which.min(abs_diff_y) ] ## x2 ## 25 There are also some useful built in functions m &lt;- matrix(c(1:3,2*(1:3)),byrow=TRUE,ncol=3) m ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 2 4 6 # normalize rows m/rowSums(m) ## [,1] [,2] [,3] ## [1,] 0.1666667 0.3333333 0.5 ## [2,] 0.1666667 0.3333333 0.5 # normalize columns t(t(m)/colSums(m)) ## [,1] [,2] [,3] ## [1,] 0.3333333 0.3333333 0.3333333 ## [2,] 0.6666667 0.6666667 0.6666667 # de-mean rows sweep(m,1,rowMeans(m), &#39;-&#39;) ## [,1] [,2] [,3] ## [1,] -1 0 1 ## [2,] -2 0 2 # de-mean columns sweep(m,2,colMeans(m), &#39;-&#39;) ## [,1] [,2] [,3] ## [1,] -0.5 -1 -1.5 ## [2,] 0.5 1 1.5 Matrix Algebra. And you can also use matrix algebra x_mat1 &lt;- matrix(2:7,2,3) x_mat1 ## [,1] [,2] [,3] ## [1,] 2 4 6 ## [2,] 3 5 7 x_mat2 &lt;- matrix(4:-1,2,3) x_mat2 ## [,1] [,2] [,3] ## [1,] 4 2 0 ## [2,] 3 1 -1 tcrossprod(x_mat1, x_mat2) #x_mat1 %*% t(x_mat2) ## [,1] [,2] ## [1,] 16 4 ## [2,] 22 7 crossprod(x_mat1, x_mat2) ## [,1] [,2] [,3] ## [1,] 17 7 -3 ## [2,] 31 13 -5 ## [3,] 45 19 -7 # x_mat1 * x_mat2 3.6 Arrays Generalization of matrices (often used in spatial econometrics) a &lt;- array(data = 1:24, dim = c(2, 3, 4)) a ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 9 11 ## [2,] 8 10 12 ## ## , , 3 ## ## [,1] [,2] [,3] ## [1,] 13 15 17 ## [2,] 14 16 18 ## ## , , 4 ## ## [,1] [,2] [,3] ## [1,] 19 21 23 ## [2,] 20 22 24 a[1, , , drop = FALSE] # Row 1 ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 3 5 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 9 11 ## ## , , 3 ## ## [,1] [,2] [,3] ## [1,] 13 15 17 ## ## , , 4 ## ## [,1] [,2] [,3] ## [1,] 19 21 23 #a[, 1, , drop = FALSE] # Column 1 #a[, , 1, drop = FALSE] # Layer 1 a[ 1, 1, ] # Row 1, column 1 ## [1] 1 7 13 19 #a[ 1, , 1] # Row 1, &quot;layer&quot; 1 #a[ , 1, 1] # Column 1, &quot;layer&quot; 1 a[1 , 1, 1] # Row 1, column 1, &quot;layer&quot; 1 ## [1] 1 Apply extends to arrays apply(a, 1, mean) # Row means ## [1] 12 13 apply(a, 2, mean) # Column means ## [1] 10.5 12.5 14.5 apply(a, 3, mean) # &quot;Layer&quot; means ## [1] 3.5 9.5 15.5 21.5 apply(a, 1:2, mean) # Row/Column combination ## [,1] [,2] [,3] ## [1,] 10 12 14 ## [2,] 11 13 15 Outer products yield arrays x &lt;- c(1,2,3) x_mat1 &lt;- outer(x, x) # x %o% x x_mat1 ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 2 4 6 ## [3,] 3 6 9 is.array(x_mat) # Matrices are arrays ## [1] TRUE x_mat2 &lt;- matrix(6:1,2,3) outer(x_mat2, x) ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 6 4 2 ## [2,] 5 3 1 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 12 8 4 ## [2,] 10 6 2 ## ## , , 3 ## ## [,1] [,2] [,3] ## [1,] 18 12 6 ## [2,] 15 9 3 # outer(x_mat2, matrix(x)) # outer(x_mat2, t(x)) # outer(x_mat1, x_mat2) "],["data.html", " 4 Data 4.1 Types 4.2 Empirical Distributions 4.3 Joint Distributions 4.4 Conditional Distributions 4.5 Random Variables 4.6 Further Reading", " 4 Data 4.1 Types The two basic types of data are cardinal and factor data. We can further distinguish between whether cardinal data are discrete or continuous. We can also further distinguish between whether factor data are ordered or not cardinal: the difference between elements always mean the same thing. discrete: E.g., 2-1=3-2. continuous: E.g., 2.11-1.4444=3.11-2.4444 factor: the difference between elements does not always mean the same thing. ordered: E.g., First place - Second place ?? Second place - Third place. unordered (categorical): E.g., A - B ???? d1d &lt;- 1:3 # Cardinal data (Discrete) d1d ## [1] 1 2 3 class(d1d) ## [1] &quot;integer&quot; d1c &lt;- c(1.1, 2/3, 3) # Cardinal data (Continuous) d1c ## [1] 1.1000000 0.6666667 3.0000000 class(d1c) ## [1] &quot;numeric&quot; d2o &lt;- factor(c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;), ordered=T) # Factor data (Ordinal) d2o ## [1] A B C ## Levels: A &lt; B &lt; C class(d2o) ## [1] &quot;ordered&quot; &quot;factor&quot; d2c &lt;- factor(c(&#39;Leipzig&#39;,&#39;Los Angeles&#39;,&#39;Logan&#39;), ordered=F) # Factor data (Categorical) d2c ## [1] Leipzig Los Angeles Logan ## Levels: Leipzig Logan Los Angeles class(d2c) ## [1] &quot;factor&quot; R also allows for more unstructured data types. c(&#39;hello world&#39;, &#39;hi mom&#39;) # character strings ## [1] &quot;hello world&quot; &quot;hi mom&quot; list(d1c, d2c) # lists ## [[1]] ## [1] 1.1000000 0.6666667 3.0000000 ## ## [[2]] ## [1] Leipzig Los Angeles Logan ## Levels: Leipzig Logan Los Angeles list(d1c, c(&#39;hello world&#39;), list(d1d, list(&#39;...inception...&#39;))) # lists ## [[1]] ## [1] 1.1000000 0.6666667 3.0000000 ## ## [[2]] ## [1] &quot;hello world&quot; ## ## [[3]] ## [[3]][[1]] ## [1] 1 2 3 ## ## [[3]][[2]] ## [[3]][[2]][[1]] ## [1] &quot;...inception...&quot; # data.frames: your most common data type # matrix of different data-types # well-ordered lists d0 &lt;- data.frame(y=d1c, x=d2c) d0 ## y x ## 1 1.1000000 Leipzig ## 2 0.6666667 Los Angeles ## 3 3.0000000 Logan Strings. paste( &#39;hi&#39;, &#39;mom&#39;) ## [1] &quot;hi mom&quot; paste( c(&#39;hi&#39;, &#39;mom&#39;), collapse=&#39;--&#39;) ## [1] &quot;hi--mom&quot; kingText &lt;- &quot;The king infringes the law on playing curling.&quot; gsub(pattern=&quot;ing&quot;, replacement=&quot;&quot;, kingText) ## [1] &quot;The k infres the law on play curl.&quot; # advanced usage #gsub(&quot;[aeiouy]&quot;, &quot;_&quot;, kingText) #gsub(&quot;([[:alpha:]]{3,})ing\\\\b&quot;, &quot;\\\\1&quot;, kingText) See https://meek-parfait-60672c.netlify.app/docs/M1_R-intro_03_text.html https://raw.githubusercontent.com/rstudio/cheatsheets/main/regex.pdf Initial Inspection. You typically begin by inspecting your data by examining the first few observations. Consider, for example, historical data on crime in the US. head(USArrests) ## Murder Assault UrbanPop Rape ## Alabama 13.2 236 58 21.2 ## Alaska 10.0 263 48 44.5 ## Arizona 8.1 294 80 31.0 ## Arkansas 8.8 190 50 19.5 ## California 9.0 276 91 40.6 ## Colorado 7.9 204 78 38.7 # Check NA values sum(is.na(x)) ## [1] 0 To further examine a particular variable, we look at its distribution. 4.2 Empirical Distributions In what follows, we will denote the data for a single variable as \\(\\{X_{i}\\}_{i=1}^{N}\\), where there are \\(N\\) observations and \\(X_{i}\\) is the value of the \\(i\\)th one. Histogram. The histogram divides the range of \\(\\{X_{i}\\}_{i=1}^{N}\\) into \\(L\\) exclusive bins of equal-width \\(h=[\\text{max}(X_{i}) - \\text{min}(X_{i})]/L\\), and counts the number of observations within each bin. We often scale the counts to interpret the numbers as a density. Mathematically, for an exclusive bin with midpoint \\(x\\), we compute \\[\\begin{eqnarray} \\widehat{f}_{HIST}(x) &amp;=&amp; \\frac{ \\sum_{i}^{N} \\mathbf{1}\\left( X_{i} \\in \\left[x-\\frac{h}{2}, x+\\frac{h}{2} \\right) \\right) }{N h}. \\end{eqnarray}\\] We compute \\(\\widehat{f}_{HIST}(x)\\) for each \\(x \\in \\left\\{ \\frac{\\ell h}{2} + \\text{min}(X) \\right\\}_{\\ell=1}^{L}\\). hist(USArrests$Murder, freq=F, border=NA, main=&#39;&#39;, xlab=&#39;Murder Arrests&#39;) # Raw Observations rug(USArrests$Murder, col=grey(0,.5)) Note that if you your data is discrete, you can directly plot the counts. E.g., x &lt;- floor(USArrests$Murder) #Discretized plot(table(x), xlab=&#39;Murder Rate (Discrete)&#39;, ylab=&#39;Count&#39;) Empirical Cumulative Distribution Function. The ECDF counts the proportion of observations whose values \\(X_{i}\\) are less than \\(x\\); \\[\\begin{eqnarray} \\widehat{F}_{ECDF}(x) = \\frac{1}{N} \\sum_{i}^{N} \\mathbf{1}(X_{i} \\leq x) \\end{eqnarray}\\] for each unique value of \\(x\\) in the dataset. F_murder &lt;- ecdf(USArrests$Murder) # proportion of murders &lt; 10 F_murder(10) ## [1] 0.7 # proportion of murders &lt; x, for all x plot(F_murder, main=&#39;&#39;, xlab=&#39;Murder Arrests&#39;, pch=16, col=grey(0,.5)) Boxplots. Boxplots summarize the distribution of data using quantiles: the \\(q\\)th quantile is the value where \\(q\\) percent of the data are below and (\\(1-q\\)) percent are above. The “median” is the point where half of the data has lower values and the other half has higher values. The “lower quartile” is the point where 25% of the data has lower values and the other 75% has higher values. The “min” is the smallest value (or largest negative value if there are any) where 0% of the data has lower values. x &lt;- USArrests$Murder # quantiles median(x) ## [1] 7.25 range(x) ## [1] 0.8 17.4 quantile(x, probs=c(0,.25,.5)) ## 0% 25% 50% ## 0.800 4.075 7.250 # deciles are quantiles quantile(x, probs=seq(0,1, by=.1)) ## 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ## 0.80 2.56 3.38 4.75 6.00 7.25 8.62 10.12 12.12 13.32 17.40 To compute quantiles, we sort the observations from smallest to largest as \\(X_{(1)}, X_{(2)},... X_{(N)}\\), and then compute quantiles as \\(X_{ (q*N) }\\). Note that \\((q*N)\\) is rounded and there are different ways to break ties. xo &lt;- sort(x) xo ## [1] 0.8 2.1 2.1 2.2 2.2 2.6 2.6 2.7 3.2 3.3 3.4 3.8 4.0 4.3 4.4 ## [16] 4.9 5.3 5.7 5.9 6.0 6.0 6.3 6.6 6.8 7.2 7.3 7.4 7.9 8.1 8.5 ## [31] 8.8 9.0 9.0 9.7 10.0 10.4 11.1 11.3 11.4 12.1 12.2 12.7 13.0 13.2 13.2 ## [46] 14.4 15.4 15.4 16.1 17.4 # median xo[length(xo)*.5] ## [1] 7.2 quantile(x, probs=.5, type=4) ## 50% ## 7.2 # min xo[1] ## [1] 0.8 min(xo) ## [1] 0.8 quantile(xo,probs=0) ## 0% ## 0.8 The boxplot shows the median (solid black line) and interquartile range (\\(IQR=\\) upper quartile \\(-\\) lower quartile; filled box),1 as well extreme values as outliers beyond the \\(1.5\\times IQR\\) (points beyond whiskers). boxplot(USArrests$Murder, main=&#39;&#39;, ylab=&#39;Murder Arrests&#39;) # Raw Observations stripchart(USArrests$Murder, pch=&#39;-&#39;, col=grey(0,.5), cex=2, vert=T, add=T) 4.3 Joint Distributions Scatterplots are used frequently to summarize the joint relationship between two variables. They can be enhanced in several ways. As a default, use semi-transparent points so as not to hide any points (and perhaps see if your observations are concentrated anywhere). You can also add regression lines (and confidence intervals), although I will defer this until later. plot(Murder~UrbanPop, USArrests, pch=16, col=grey(0.,.5)) # Add the line of best fit for pooled data #reg &lt;- lm(Murder~UrbanPop, data=USArrests) #abline(reg, lty=2) Marginal Distributions. You can also show the distributions of each variable along each axis. # Setup Plot layout( matrix(c(2,0,1,3), ncol=2, byrow=TRUE), widths=c(9/10,1/10), heights=c(1/10,9/10)) # Scatterplot par(mar=c(4,4,1,1)) plot(Murder~UrbanPop, USArrests, pch=16, col=rgb(0,0,0,.5)) # Add Marginals par(mar=c(0,4,1,1)) xhist &lt;- hist(USArrests$UrbanPop, plot=FALSE) barplot(xhist$counts, axes=FALSE, space=0, border=NA) par(mar=c(4,0,1,1)) yhist &lt;- hist(USArrests$Murder, plot=FALSE) barplot(yhist$counts, axes=FALSE, space=0, horiz=TRUE, border=NA) 4.4 Conditional Distributions It is easy to show how distributions change according to a third variable using data splits. E.g., # Tailored Histogram ylim &lt;- c(0,8) xbks &lt;- seq(min(USArrests$Murder)-1, max(USArrests$Murder)+1, by=1) # Also show more information # Split Data by Urban Population above/below mean pop_mean &lt;- mean(USArrests$UrbanPop) murder_lowpop &lt;- USArrests[USArrests$UrbanPop&lt; pop_mean,&#39;Murder&#39;] murder_highpop &lt;- USArrests[USArrests$UrbanPop&gt;= pop_mean,&#39;Murder&#39;] cols &lt;- c(low=rgb(0,0,1,.75), high=rgb(1,0,0,.75)) par(mfrow=c(1,2)) hist(murder_lowpop, breaks=xbks, col=cols[1], main=&#39;Urban Pop &gt;= Mean&#39;, font.main=1, xlab=&#39;Murder Arrests&#39;, border=NA, ylim=ylim) hist(murder_highpop, breaks=xbks, col=cols[2], main=&#39;Urban Pop &lt; Mean&#39;, font.main=1, xlab=&#39;Murder Arrests&#39;, border=NA, ylim=ylim) It is sometimes it is preferable to show the ECDF instead. And you can glue various combinations together to convey more information all at once par(mfrow=c(1,2)) # Full Sample Density hist(USArrests$Murder, main=&#39;Density Function Estimate&#39;, font.main=1, xlab=&#39;Murder Arrests&#39;, breaks=xbks, freq=F, border=NA) # Split Sample Distribution Comparison F_lowpop &lt;- ecdf(murder_lowpop) plot(F_lowpop, col=cols[1], pch=16, xlab=&#39;Murder Arrests&#39;, main=&#39;Distribution Function Estimates&#39;, font.main=1, bty=&#39;n&#39;) F_highpop &lt;- ecdf(murder_highpop) plot(F_highpop, add=T, col=cols[2], pch=16) legend(&#39;bottomright&#39;, col=cols, pch=16, bty=&#39;n&#39;, inset=c(0,.1), title=&#39;% Urban Pop.&#39;, legend=c(&#39;Low (&lt;= Mean)&#39;,&#39;High (&gt;= Mean)&#39;)) You can also split data into grouped boxplots in the same way layout( t(c(1,2,2))) boxplot(USArrests$Murder, main=&#39;&#39;, xlab=&#39;All Data&#39;, ylab=&#39;Murder Arrests&#39;) # K Groups with even spacing K &lt;- 3 USArrests$UrbanPop_Kcut &lt;- cut(USArrests$UrbanPop,K) Kcols &lt;- hcl.colors(K,alpha=.5) boxplot(Murder~UrbanPop_Kcut, USArrests, main=&#39;&#39;, col=Kcols, xlab=&#39;Urban Population&#39;, ylab=&#39;&#39;) # 4 Groups with equal numbers of observations #Qcuts &lt;- c( # &#39;0%&#39;=min(USArrests$UrbanPop)-10*.Machine$double.eps, # quantile(USArrests$UrbanPop, probs=c(.25,.5,.75,1))) #USArrests$UrbanPop_cut &lt;- cut(USArrests$UrbanPop, Qcuts) #boxplot(Murder~UrbanPop_cut, USArrests, col=hcl.colors(4,alpha=.5)) Conditional Relationships. You can also use size, color, and shape to further distinguish different conditional relationships. # High Assault Areas assault_high &lt;- USArrests$Assault &gt; median(USArrests$Assault) cols &lt;- ifelse(assault_high, rgb(1,0,0,.5), rgb(0,0,1,.5)) # Scatterplot # Show High Assault Areas via &#39;cex=&#39; or &#39;pch=&#39; plot(Murder~UrbanPop, USArrests, pch=16, col=cols) # Could also add regression lines y for each data split #reg_high &lt;- lm(Murder~UrbanPop, data=USArrests[assault_high,]) #abline(reg_high, lty=2, col=rgb(1,0,0,1)) #reg_low &lt;- lm(Murder~UrbanPop, data=USArrests[!assault_high,]) #abline(reg_low, lty=2, col= rgb(0,0,1,1)) 4.5 Random Variables Random variables are vectors that are generated from a probabilistic process. The sample space of a random variable refers to the set of all possible outcomes. The probability of a particular set of outcomes is the proportion that those outcomes occur in the long run. There are two basic types of sample spaces: Discrete. The random variable can take one of several discrete values. E.g., any number in \\(\\{1,2,3,...\\}\\). # Bernoulli (Coin Flip: Heads=1 Tails=0) rbinom(1, 1, 0.5) # 1 draw ## [1] 0 rbinom(4, 1, 0.5) # 4 draws ## [1] 0 0 1 1 x0 &lt;- rbinom(600, 1, 0.5) # Cumulative Averages x0_t &lt;- seq_len(length(x0)) x0_mt &lt;- cumsum(x0)/x0_t plot(x0_t, x0_mt, type=&#39;l&#39;, ylab=&#39;Cumulative Average&#39;, xlab=&#39;Flip #&#39;) # Long run proportions x0 &lt;- rbinom(2000, 1, 0.5) hist(x0, breaks=50, border=NA, main=NA, freq=T) # Bernoulli (Unfair Coin Flip) x0 &lt;- rbinom(2000, 1, 0.2) hist(x0, breaks=50, border=NA, main=NA, freq=T) # Discrete Uniform (numbers 1,...4) # sample(1:4, 1, replace=T, prob=rep(1/4,4) ) # 1 draw, equal probabilities x1 &lt;- sample(1:4, 2000, replace=T, prob=rep(1/4,4)) hist(x1, breaks=50, border=NA, main=NA, freq=T) # Multinoulli (aka Categorical) x1 &lt;- sample(1:4, 2000, replace=T, prob=c(3,4,1,2)/10) # unequal probabilities hist(x1, breaks=50, border=NA, main=NA, freq=T) Continuous. The random variable can take one value out of an uncountably infinite number. E.g., any number between \\([0,1]\\) allowing for any number of decimal points. # Continuous Uniform runif(3) # 3 draws ## [1] 0.02080043 0.56223900 0.57047505 x2 &lt;- runif(2000) hist(x2, breaks=20, border=NA, main=NA, freq=F) # Normal (Gaussian) rnorm(3) # 3 draws ## [1] -0.7031871 -2.4176092 -0.4144168 x3 &lt;- rnorm(2000) hist(x3, breaks=20, border=NA, main=NA, freq=F) We might further distinguish types of random variables based on whether their maximum value is theoretically finite or infinite. Probability distributions. Random variables are drawn from probability distributions. The most common ones are easily accessible. All random variables have an associated theoretical Cumulative Distribution Function: \\(F_{X}(x) =\\) Probability\\((X_{i} \\leq x)\\). Continuous random variables have an associated density function: \\(f_{X}\\), as well as a quantile function: \\(Q_{X}(p)\\), which is the inverse of the CDF: the \\(x\\) value where \\(p\\) percent of the data fall below it. Here is an example of the Beta distribution pars &lt;- expand.grid( c(.5,1,2), c(.5,1,2) ) par(mfrow=c(3,3)) apply(pars, 1, function(p){ x &lt;- seq(0,1,by=.01) fx &lt;- dbeta( x,p[1], p[2]) plot(x, fx, type=&#39;l&#39;, xlim=c(0,1), ylim=c(0,4), lwd=2) #hist(rbeta(2000, p[1], p[2]), breaks=50, border=NA, main=NA, freq=F) }) title(&#39;Beta densities&#39;, outer=T, line=-1) Here is a more in-depth example using the Dagum distribution # Quantile Function (VGAM::qdagum) # (In addition to the main equation, there are many checks and options for consistency with other functions) qdagum &lt;- function(p, scale = 1, shape1.a, shape2.p, lower.tail = TRUE, log.p = FALSE) { LLL &lt;- max(length(p), length(shape1.a), length(scale), length(shape2.p)) if (length(p) &lt; LLL) p &lt;- rep_len(p, LLL) if (length(shape1.a) &lt; LLL) shape1.a &lt;- rep_len(shape1.a, LLL) if (length(scale) &lt; LLL) scale &lt;- rep_len(scale, LLL) if (length(shape2.p) &lt; LLL) shape2.p &lt;- rep_len(shape2.p, LLL) if (lower.tail) { if (log.p) { ln.p &lt;- p ans &lt;- scale * (expm1(-ln.p / shape2.p))^(-1 / shape1.a) ans[ln.p &gt; 0] &lt;- NaN } else { ans &lt;- scale * (expm1(-log(p) / shape2.p))^(-1 / shape1.a) ans[p &lt; 0] &lt;- NaN ans[p == 0] &lt;- 0 ans[p == 1] &lt;- Inf ans[p &gt; 1] &lt;- NaN } } else { if (log.p) { ln.p &lt;- p ans &lt;- scale * (expm1(-log(-expm1(ln.p)) / shape2.p))^(-1 / shape1.a) ans[ln.p &gt; 0] &lt;- NaN } else { # Main equation (theoretically derived from the CDF) ans &lt;- scale * (expm1(-log1p(-p) / shape2.p))^(-1 / shape1.a) ans[p &lt; 0] &lt;- NaN ans[p == 0] &lt;- Inf ans[p == 1] &lt;- 0 ans[p &gt; 1] &lt;- NaN } } ans[scale &lt;= 0 | shape1.a &lt;= 0 | shape2.p &lt;= 0] &lt;- NaN return(ans) } # Generate Random Variables (VGAM::rdagum) rdagum &lt;-function(n, scale=1, shape1.a, shape2.p){ p &lt;- runif(n) # generate a random quantile qdagum(p, scale=scale, shape1.a=shape1.a, shape2.p=shape2.p) } # Example set.seed(123) x &lt;- rdagum(3000,1,3,1) # Empirical Distribution Fx_hat &lt;- ecdf(x) plot(Fx_hat, lwd=2, xlim=c(0,5)) # Two Quantiles p &lt;- c(.25, .9) cols &lt;- c(2,4) Qx_hat &lt;- quantile(x, p) segments(Qx_hat,p,-10,p, col=cols) segments(Qx_hat,p,Qx_hat,0, col=cols) mtext( round(Qx_hat,2), 1, at=Qx_hat, col=cols) We will return to the theory behind probability distributions in a later chapter. 4.6 Further Reading For plotting histograms and marginals, see https://www.r-bloggers.com/2011/06/example-8-41-scatterplot-with-marginal-histograms/ https://r-graph-gallery.com/histogram.html https://r-graph-gallery.com/74-margin-and-oma-cheatsheet.html https://jtr13.github.io/cc21fall2/tutorial-for-scatter-plot-with-marginal-distribution.html. Technically, the upper and lower ``hinges’’ use two different versions of the first and third quartile. See https://stackoverflow.com/questions/40634693/lower-and-upper-quartiles-in-boxplot-in-r↩︎ "],["statistics.html", " 5 Statistics 5.1 Mean and Variance 5.2 Shape Statistics 5.3 Other Center/Spread Statistics 5.4 Associations 5.5 Beyond Basics 5.6 Further Reading", " 5 Statistics We often summarize distributions with statistics: functions of data. The most basic way to do this is summary( runif(1000)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000653 0.2433963 0.5054190 0.5039852 0.7685290 0.9995000 summary( rnorm(1000) ) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -2.84855 -0.65619 -0.05057 -0.02011 0.64258 3.42110 The values in “summary” can all be calculated individually. (E.g., the “mean” computes the [sum of all values] divided by [number of values].) There are many other combinations of statistics you can use. 5.1 Mean and Variance The most basic statistics summarize the center of a distribution and how far apart the values are spread. Mean. Perhaps the most common statistic is the mean; \\[\\overline{X}=\\frac{\\sum_{i=1}^{N}X_{i}}{N},\\] where \\(X_{i}\\) denotes the value of the \\(i\\)th observation. # compute the mean of a random sample x &lt;- runif(100) hist(x, border=NA, main=NA) m &lt;- mean(x) #sum(x)/length(x) abline(v=m, col=2, lwd=2) title(paste0(&#39;mean= &#39;, round(m,2)), font.main=1) # is m close to it&#39;s true value (1-0)/2=.5? Variance. Perhaps the second most common statistic is the variance: the average squared deviation from the mean \\[V_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\overline{X}]^2}{N}.\\] The standard deviation is simply \\(s_{X} = \\sqrt{V_{X}}\\).2 s &lt;- sd(x) # sqrt(var(x)) hist(x, border=NA, main=NA, freq=F) s_lh &lt;- c(m - s, m + s) abline(v=s_lh, col=4) text(s_lh, -.02, c( expression(bar(X)-s[X]), expression(bar(X)+s[X])), col=4, adj=0) title(paste0(&#39;sd= &#39;, round(s,2)), font.main=1) # Note a small sample correction: # var(x) # mean( (x - mean(x))^2 ) Together, these statistics summarize the central tendency and dispersion of a distribution. In some special cases, such as with the normal distribution, they completely describe the distribution. Other distributions are easier to describe with other statistics. 5.2 Shape Statistics Central tendency and dispersion are often insufficient to describe a distribution. To further describe shape, we can compute these to “standard moments”: \\[Skew_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\overline{X}]^3 / N}{ [s_{X}]^3 }\\] \\[Kurt_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\overline{X}]^4 / N}{ [s_{X}]^4 }.\\] Skewness. Skew captures how symmetric the distribution is. x &lt;- rweibull(1000, shape=1) hist(x, border=NA, main=NA, freq=F, breaks=20) skewness &lt;- function(x) { x_bar &lt;- mean(x) m3 &lt;- mean((x - x_bar)^3) skew &lt;- m3/(sd(x)^3) return(skew) } skewness( rweibull(1000, shape=1)) ## [1] 1.721855 skewness( rweibull(1000, shape=10) ) ## [1] -0.7698367 Kurtosis. Kurt captures how many “outliers” there are. x &lt;- rweibull(1000, shape=1) boxplot(x, main=NA) kurtosis &lt;- function(x) { x_bar &lt;- mean(x) m4 &lt;- mean((x - x_bar)^4) kurt &lt;- m4/(sd(x)^4) - 3 return(kurt) } kurtosis( rweibull(1000, shape=1)) ## [1] 6.318697 kurtosis( rweibull(1000, shape=10) ) ## [1] 0.6659003 Clusters/Gaps. You can also describe distributions in terms of how clustered the values are # Number of Modes x &lt;- rbeta(1000, .6, .6) hist(x, border=NA, main=NA, freq=F, breaks=20) But remember: a picture is worth a thousand words. # Random Number Generator r_ugly1 &lt;- function(n, theta1=c(-8,-1), theta2=c(-2,2), rho=.25){ omega &lt;- rbinom(n, size=1, rho) epsilon &lt;- omega * runif(n, theta1[1], theta2[1]) + (1-omega) * rnorm(n, theta1[2], theta2[2]) return(epsilon) } # Large Sample par(mfrow=c(1,1)) X &lt;- seq(-12,6,by=.001) rx &lt;- r_ugly1(1000000) hist(rx, breaks=1000, freq=F, border=NA, xlab=&quot;x&quot;, main=&#39;&#39;) # Show True Density #d_ugly1 &lt;- function(x, theta1=c(-8,-1), theta2=c(-2,2), rho=.25){ # rho * dunif(x, theta1[1], theta2[1]) + # (1-rho) * dnorm(x, theta1[2], theta2[2]) } #dx &lt;- d_ugly1(X) #lines(X, dx, col=1) 5.3 Other Center/Spread Statistics Median, Interquartile Range, Median Absolute Deviation. Recall that the \\(q\\)th quantile is the value where \\(q\\) percent of the data are below and (\\(1-q\\)) percent are above. The median (\\(q=.5\\)) is the point where half of the data is lower values and the other half is higher. The first and third quartiles (\\(q=.25\\) and \\(q=.75\\)) together measure is the middle 50 percent of the data. The size of that range (interquartile range: the difference between the quartiles) represents “spread” or “dispersion” of the data. The mean absolute deviation also measures spread \\[ \\tilde{X} = Med(X_{i}) \\\\ MAD_{X} = Med\\left( | X_{i} - \\tilde{X} | \\right). \\] x &lt;- rgeom(50, .4) x ## [1] 0 4 0 7 0 0 0 3 3 0 0 0 2 0 0 2 1 0 4 3 0 0 6 2 0 2 0 1 0 0 0 3 2 0 0 1 0 4 ## [39] 0 0 2 1 2 0 0 2 7 4 1 0 plot(table(x)) #mean(x) median(x) ## [1] 0 #sd(x) #IQR(x) # diff( quantile(x, probs=c(.25,.75))) mad(x, constant=1) # median( abs(x - median(x)) ) ## [1] 0 # other absolute deviations: #mean( abs(x - mean(x)) ) #mean( abs(x - median(x)) ) #median( abs(x - mean(x)) ) Mode and Share Concentration. Sometimes, none of the above work well. With categorical data, for example, distributions are easier to describe with other statistics. The mode is the most common observation: the value with the highest observed frequency. We can also measure the spread/dispersion of the frequencies, or compare the highest frequency to the average frequency to measure concentration at the mode. # Draw 3 Random Letters K &lt;- length(LETTERS) x_id &lt;- rmultinom(3, 1, prob=rep(1/K,K)) x_id ## [,1] [,2] [,3] ## [1,] 0 0 0 ## [2,] 0 0 0 ## [3,] 0 0 0 ## [4,] 0 0 0 ## [5,] 0 0 0 ## [6,] 1 0 0 ## [7,] 0 0 0 ## [8,] 0 0 0 ## [9,] 0 0 0 ## [10,] 0 0 0 ## [11,] 0 0 1 ## [12,] 0 0 0 ## [13,] 0 0 0 ## [14,] 0 0 0 ## [15,] 0 0 0 ## [16,] 0 0 0 ## [17,] 0 0 0 ## [18,] 0 0 0 ## [19,] 0 1 0 ## [20,] 0 0 0 ## [21,] 0 0 0 ## [22,] 0 0 0 ## [23,] 0 0 0 ## [24,] 0 0 0 ## [25,] 0 0 0 ## [26,] 0 0 0 # Draw Random Letters 100 Times x_id &lt;- rowSums(rmultinom(100, 1, prob=rep(1/K,K))) x &lt;- lapply(1:K, function(k){ rep(LETTERS[k], x_id[k]) }) x &lt;- factor(unlist(x), levels=LETTERS) plot(x) tx &lt;- table(x) # mode(s) names(tx)[tx==max(tx)] ## [1] &quot;H&quot; # freq. spread sx &lt;- tx/sum(tx) sd(sx) # mad(sx) ## [1] 0.01541228 # freq. concentration max(tx)/mean(tx) ## [1] 1.82 5.4 Associations There are several ways to quantitatively describe the relationship between two variables, \\(Y\\) and \\(X\\). The major differences surround whether the variables are cardinal, ordinal, or categorical. Pearson (Linear) Correlation. Suppose \\(X\\) and \\(Y\\) are both cardinal data. As such, you can compute the most famous measure of association, the covariance: \\[ C_{XY} = \\sum_{i} [X_i - \\overline{X}] [Y_i - \\overline{Y}] / N \\] # Bivariate Data from USArrests xy &lt;- USArrests[,c(&#39;Murder&#39;,&#39;UrbanPop&#39;)] #plot(xy, pch=16, col=grey(0,.25)) cov(xy) ## Murder UrbanPop ## Murder 18.970465 4.386204 ## UrbanPop 4.386204 209.518776 Note that \\(C_{XX}=V_{X}\\). For ease of interpretation, we rescale this statistic to always lay between \\(-1\\) and \\(1\\) \\[ r_{XY} = \\frac{ C_{XY} }{ \\sqrt{V_X} \\sqrt{V_Y}} \\] cor(xy)[2] ## [1] 0.06957262 Falk Codeviance. The Codeviance is a robust alternative to Covariance. Instead of relying on means (which can be sensitive to outliers), it uses medians (\\(\\tilde{X}\\)) to capture the central tendency.3 We can also scale the Codeviance by the median absolute deviation to compute the median correlation. \\[ \\text{CoDev}(X,Y) = \\text{Med}\\left\\{ |X_i - \\tilde{X}| |Y_i - \\tilde{Y}| \\right\\} \\\\ \\tilde{r}_{XY} = \\frac{ \\text{CoDev}(X,Y) }{ \\text{MAD}(X) \\text{MAD}(Y) }. \\] cor_m &lt;- function(xy) { # Compute medians for each column med &lt;- apply(xy, 2, median) # Subtract the medians from each column xm &lt;- sweep(xy, 2, med, &quot;-&quot;) # Compute CoDev CoDev &lt;- median(xm[, 1] * xm[, 2]) # Compute the medians of absolute deviation MadProd &lt;- prod( apply(abs(xm), 2, median) ) # Return the robust correlation measure return( CoDev / MadProd) } cor_m(xy) ## [1] 0.005707763 Kendall’s Tau. Suppose \\(X\\) and \\(Y\\) are both ordered variables. Kendall’s Tau measures the strength and direction of association by counting the number of concordant pairs (where the ranks agree) versus discordant pairs (where the ranks disagree). A value of \\(\\tau = 1\\) implies perfect agreement in rankings, \\(\\tau = -1\\) indicates perfect disagreement, and \\(\\tau = 0\\) suggests no association in the ordering. \\[ \\tau = \\frac{2}{n(n-1)} \\sum_{i} \\sum_{j &gt; i} \\text{sgn} \\Bigl( (X_i - X_j)(Y_i - Y_j) \\Bigr), \\] where the sign function is: \\[ \\text{sgn}(z) = \\begin{cases} +1 &amp; \\text{if } z &gt; 0\\\\ 0 &amp; \\text{if } z = 0 \\\\ -1 &amp; \\text{if} z &lt; 0 \\end{cases}. \\] xy &lt;- USArrests[,c(&#39;Murder&#39;,&#39;UrbanPop&#39;)] xy[,1] &lt;- rank(xy[,1] ) xy[,2] &lt;- rank(xy[,2] ) # plot(xy, pch=16, col=grey(0,.25)) tau &lt;- cor(xy[, 1], xy[, 2], method = &quot;kendall&quot;) round(tau, 3) ## [1] 0.074 Cramer’s V. Suppose \\(X\\) and \\(Y\\) are both categorical variables; the value of \\(X\\) is one of \\(1...r\\) categories and the value of \\(Y\\) is one of \\(1...k\\) categories. Cramer’s V quantifies the strength of association by adjusting a “chi-squared” statistic to provide a measure that ranges from 0 to 1; 0 indicates no association while a value closer to 1 signifies a strong association. First, consider a contingency table for \\(X\\) and \\(Y\\) with \\(r\\) rows and \\(k\\) columns. The chi-square statistic is then defined as: \\[ \\chi^2 = \\sum_{i=1}^{r} \\sum_{j=1}^{k} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}. \\] where \\(O_{ij}\\) denote the observed frequency in cell \\((i, j)\\), \\(E_{ij} = \\frac{R_i \\cdot C_j}{n}\\) is the expected frequency for each cell if \\(X\\) and \\(Y\\) are independent \\(R_i\\) denote the total frequency for row \\(i\\) (i.e., \\(R_i = \\sum_{j=1}^{k} O_{ij}\\)), \\(C_j\\) denote the total frequency for column \\(j\\) (i.e., \\(C_j = \\sum_{i=1}^{r} O_{ij}\\)), \\(n\\) be the grand total of observations, so that \\(n = \\sum_{i=1}^{r} \\sum_{j=1}^{k} O_{ij}\\). Second, normalize the chi-square statistic with the sample size and the degrees of freedom to compute Cramer’s V. \\[ V = \\sqrt{\\frac{\\chi^2 / n}{\\min(k - 1, \\, r - 1)}}, \\] where: \\(n\\) is the total sample size, \\(k\\) is the number of categories for one variable, \\(r\\) is the number of categories for the other variable. xy &lt;- USArrests[,c(&#39;Murder&#39;,&#39;UrbanPop&#39;)] xy[,1] &lt;- cut(xy[,1],3) xy[,2] &lt;- cut(xy[,2],4) table(xy) ## UrbanPop ## Murder (31.9,46.8] (46.8,61.5] (61.5,76.2] (76.2,91.1] ## (0.783,6.33] 4 5 8 5 ## (6.33,11.9] 0 4 7 6 ## (11.9,17.4] 2 4 2 3 cor_v &lt;- function(xy){ # Create a contingency table from the categorical variables tbl &lt;- table(xy) # Compute the chi-square statistic (without Yates&#39; continuity correction) chi2 &lt;- chisq.test(tbl, correct=FALSE)$statistic # Total sample size n &lt;- sum(tbl) # Compute the minimum degrees of freedom (min(rows-1, columns-1)) df_min &lt;- min(nrow(tbl) - 1, ncol(tbl) - 1) # Calculate Cramer&#39;s V V &lt;- sqrt((chi2 / n) / df_min) return(V) } cor_v(xy) ## X-squared ## 0.2307071 # DescTools::CramerV( table(xy) ) 5.5 Beyond Basics Use expansion “packages” for less common procedures and more functionality CRAN. Most packages can be found on CRAN and can be easily installed # commonly used packages install.packages(&quot;stargazer&quot;) install.packages(&quot;data.table&quot;) install.packages(&quot;plotly&quot;) # other statistical packages install.packages(&quot;extraDistr&quot;) install.packages(&quot;twosamples&quot;) # install.packages(&quot;purrr&quot;) # install.packages(&quot;reshape2&quot;) The most common tasks also have cheatsheets you can use. For example, to generate ‘exotic’ probability distributions library(extraDistr) par(mfrow=c(1,2)) for(p in c(-.5,0)){ x &lt;- rgev(2000, mu=0, sigma=1, xi=p) hist(x, breaks=50, border=NA, main=NA, freq=F) } title(&#39;GEV densities&#39;, outer=T, line=-1) library(extraDistr) par(mfrow=c(1,3)) for(p in c(-1, 0,2)){ x &lt;- rtlambda(2000, p) hist(x, breaks=100, border=NA, main=NA, freq=F) } title(&#39;Tukey-Lambda densities&#39;, outer=T, line=-1) 5.6 Further Reading Many random variables are related to each other https://en.wikipedia.org/wiki/Relationships_among_probability_distributions https://www.math.wm.edu/~leemis/chart/UDR/UDR.html https://qiangbo-workspace.oss-cn-shanghai.aliyuncs.com/2018-11-11-common-probability-distributions/distab.pdf Note that numbers randomly generated on your computer cannot be truly random, they are “Pseudorandom”. Note that a “corrected version” is used by R and many statisticians: \\(V_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\overline{X}]^2}{N-1}\\).↩︎ See also Theil-Sen Estimator, which may be seen as a precursor.↩︎ "],["resampling.html", " 6 (Re)Sampling 6.1 Sample Distributions 6.2 Intervals 6.3 Resampling 6.4 Value of More Data 6.5 Further Reading", " 6 (Re)Sampling 6.1 Sample Distributions The sampling distribution of a statistic shows us how much a statistic varies from sample to sample. For example, see how the mean varies from sample to sample to sample. # Three Sample Example par(mfrow=c(1,3)) sapply(1:3, function(i){ x &lt;- runif(100) m &lt;- mean(x) hist(x, breaks=seq(0,1,by=.1), #for comparability main=NA, border=NA) abline(v=m, col=2, lwd=2) title(paste0(&#39;mean= &#39;, round(m,2)), font.main=1) return(m) }) ## [1] 0.5425521 0.4771145 0.5038657 Examine the sampling distribution of the mean sample_means &lt;- sapply(1:1000, function(i){ m &lt;- mean(runif(100)) return(m) }) hist(sample_means, breaks=50, border=NA, col=2, font.main=1, main=&#39;Sampling Distribution of the mean&#39;) This is one of the most profound results known in statistics, known as the central limit theorem: the sampling distribution of the mean is approximately standard normal. central limit theorem. There are actually many different variants of the central limit theorem, as it applies more generally: the sampling distribution of many statistics are standard normal. For example, examine the sampling distribution of the standard deviation. three_sds &lt;- c( sd(runif(100)), sd(runif(100)), sd(runif(100)) ) three_sds ## [1] 0.2866693 0.2907758 0.2814790 sample_sds &lt;- sapply(1:1000, function(i){ s &lt;- sd(runif(100)) return(s) }) hist(sample_sds, breaks=50, border=NA, col=4, font.main=1, main=&#39;Sampling Distribution of the sd&#39;) It is beyond this class to prove this result, but you should know that not all sampling distributions are standard normal. For example, examine the sampling distribution of the three main “order statistics” # Create 300 samples, each with 1000 random uniform variables x &lt;- sapply(1:300, function(i) runif(1000) ) # Each row is a new sample length(x[1,]) ## [1] 300 # Median looks normal, Maximum and Minumum do not! xmin &lt;- apply(x,1,quantile, probs=0) xmed &lt;- apply(x,1,quantile, probs=.5) xmax &lt;- apply(x,1,quantile, probs=1) par(mfrow=c(1,3)) hist(xmin, breaks=100, border=NA, main=&#39;Min&#39;, font.main=1) hist(xmed, breaks=100, border=NA, main=&#39;Med&#39;, font.main=1) hist(xmax, breaks=100, border=NA, main=&#39;Max&#39;, font.main=1) title(&#39;Sampling Distributions&#39;, outer=T, line=-1) # To explore, try any function! fun_of_rv &lt;- function(f, n=100){ x &lt;- runif(n) y &lt;- f(x) return(y) } fun_of_rv( f=mean ) ## [1] 0.4634983 fun_of_rv( f=function(i){ diff(range(exp(i))) } ) ## [1] 1.602588 6.2 Intervals Using either the bootstrap or jackknife distribution, we can calculate confidence interval: range your statistic varies across different samples. standard error: variance of your statistic across different samples. sample_means &lt;- apply(x,1,mean) # standard error sd(sample_means) ## [1] 0.01675691 Note that in some cases (not discussed here), you can estimate the standard error to get a confidence interval. x00 &lt;- x[1,] # standard error s00 &lt;- sd(x00)/sqrt(length(x00)) ci &lt;- mean(x00) + c(1.96, -1.96)*s00 Confidence Interval. Compute the upper and lower quantiles of the sampling distribution. bks &lt;- seq(.4,.6,by=.001) hist(sample_means, breaks=bks, border=NA, col=rgb(0,0,0,.25), font.main=1, main=&#39;Confidence Interval for the mean&#39;) # Middle 90% mq &lt;- quantile(sample_means, probs=c(.05,.95)) abline(v=mq) paste0(&#39;we are 90% confident that the mean is between &#39;, round(mq[1],2), &#39; and &#39;, round(mq[2],2) ) ## [1] &quot;we are 90% confident that the mean is between 0.47 and 0.53&quot; sample_quants &lt;- apply(x,1,quantile, probs=.99) bks &lt;- seq(.92,1,by=.001) hist(sample_quants, breaks=bks, border=NA, col=rgb(0,0,0,.25), font.main=1, main=&#39;Confidence Interval for the 99% percentile&#39;) # Middle 95% mq &lt;- quantile(sample_quants, probs=c(.025,.975)) abline(v=mq) paste0(&#39;we are 95% confident that the upper percentile is between &#39;, round(mq[1],2), &#39; and &#39;, round(mq[2],2) ) ## [1] &quot;we are 95% confident that the upper percentile is between 0.97 and 1&quot; (See also https://online.stat.psu.edu/stat200/lesson/4/4.4/4.4.2) Prediction Interval. Compute the frequency each value was covered. # Middle 90% of values xq0 &lt;- quantile(x, probs=c(.05,.95)) bks &lt;- seq(0,1,by=.01) hist(x, breaks=bks, border=NA, main=&#39;Prediction Interval&#39;, font.main=1) abline(v=xq0) paste0(&#39;we are 90% confident that the a future data point will be between &#39;, round(xq0[1],2), &#39; and &#39;, round(xq0[2],2) ) ## [1] &quot;we are 90% confident that the a future data point will be between 0.05 and 0.95&quot; Advanced Intervals. In many cases, we want a X% interval to mean that X% of the intervals we generate will contain the mean (confidence interval) or new observations (prediction interval). E.g., a 50% CI means that half of intervals we create contain the true mean. # Confidence Interval for each sample xq &lt;- apply(x,1, function(r){ mean(r) + c(-1,1)*sd(r) }) # First 4 interval estimates xq[,1:4] ## [,1] [,2] [,3] [,4] ## [1,] 0.1935914 0.2085894 0.2303030 0.1979923 ## [2,] 0.8023608 0.7771637 0.7939178 0.7716116 # Frequency each point was in an interval bks &lt;- seq(0,1,by=.01) xcov &lt;- sapply(bks, function(b){ bl &lt;- b &gt;= xq[1,] bu &lt;- b &lt;= xq[2,] mean( bl &amp; bu ) }) plot.new() plot.window(xlim=c(0,1), ylim=c(0,1)) polygon( c(bks, rev(bks)), c(xcov, xcov*0), col=grey(.5,.5), border=NA) mtext(&#39;Frequency each value was in an interval&#39;,2, line=2.5) axis(1) axis(2) # 50\\% Coverage c_ul &lt;- range(bks[xcov&gt;=.5]) abline(h=.5, lwd=2) segments(c_ul,0,c_ul,.5, lty=2) c_ul # 50% confidence interval ## [1] 0.22 0.78 # True mean abline(v=.5, col=2, lwd=2) 6.3 Resampling Often, we only have one sample. sample_dat &lt;- USArrests$Murder mean(sample_dat) ## [1] 7.788 How then can we estimate the sampling distribution of a statistic? We can “resample” our data. Hesterberg (2015) provides a nice illustration of the idea. The two most basic versions are the jackknife and the bootstrap, which are discussed below. Jackknife Distribution. Here, we compute all “leave-one-out” estimates. Specifically, for a dataset with \\(n\\) observations, the jackknife uses \\(n-1\\) observations other than \\(i\\) for each unique subsample. Taking the mean, for example, we have sample_dat &lt;- USArrests$Murder sample_mean &lt;- mean(sample_dat) # Jackknife Estimates n &lt;- length(sample_dat) Jmeans &lt;- sapply(1:n, function(i){ dati &lt;- sample_dat[-i] mean(dati) }) hist(Jmeans, breaks=25, border=NA, main=&#39;&#39;, xlab=expression(bar(X)[-i])) abline(v=sample_mean, col=&#39;red&#39;, lty=2) Bootstrap Distribution. Here, we draw \\(n\\) observations with replacement from the original data to create a bootstrap sample and calculate a statistic. Each bootstrap sample \\(b=1...B\\) uses a random set of observations (denoted \\(N_{b}\\)) to compute a statistic. We repeat that many times, say \\(B=9999\\), to estimate the sampling distribution. Consider the sample mean as an example; # Bootstrap estimates set.seed(2) Bmeans &lt;- sapply(1:10^4, function(i) { dat_b &lt;- sample(sample_dat, replace=T) # c.f. jackknife mean(dat_b) }) hist(Bmeans, breaks=25, border=NA, main=&#39;&#39;, xlab=expression(bar(X)[b])) abline(v=sample_mean, col=&#39;red&#39;, lty=2) Caveat. Note that we do not use the mean of the bootstrap or jackknife statistics as a replacement for the original estimate. This is because the bootstrap and jackknife distributions are centered at the observed statistic, not the population parameter. (The bootstrapped mean is centered at the sample mean, not the population mean.) This means that we cannot use the bootstrap to improve on \\(\\overline{x}\\); no matter how many bootstrap samples we take. We can, however, use the jackknife and bootstrap to estimate sampling variability. Intervals. Note that both methods provide imperfect estimates, and can give different numbers. Until you know more, a conservative approach is to take the larger estimate. # Boot CI boot_ci &lt;- quantile(Bmeans, probs=c(.025, .975)) boot_ci ## 2.5% 97.5% ## 6.58200 8.97005 # Jack CI jack_ci &lt;- quantile(Jmeans, probs=c(.025, .975)) jack_ci ## 2.5% 97.5% ## 7.621582 7.904082 # more conservative estimate ci_est &lt;- boot_ci Also note that the standard deviation refers to variance within a single sample, and is hence different from the standard error. Nonetheless, they can both be used to estimate the variability of a statistic. boot_se &lt;- sd(Bmeans) sample_sd &lt;- sd(sample_dat) c(boot_se, sample_sd/sqrt(n)) ## [1] 0.6056902 0.6159621 6.4 Value of More Data Each additional data point you have provides more information, which ultimately decreases the standard error of your estimates. However, it does so at a decreasing rate (known in economics as diminishing returns). Nseq &lt;- seq(1,100, by=1) # Sample sizes B &lt;- 1000 # Number of draws per sample SE &lt;- sapply(Nseq, function(n){ sample_statistics &lt;- sapply(1:B, function(b){ x &lt;- rnorm(n) # Sample of size N quantile(x,probs=.4) # Statistic }) sd(sample_statistics) }) par(mfrow=c(1,2)) plot(Nseq, SE, pch=16, col=grey(0,.5), main=&#39;Absolute Gain&#39;, font.main=1, ylab=&#39;standard error&#39;, xlab=&#39;sample size&#39;) plot(Nseq[-1], abs(diff(SE)), pch=16, col=grey(0,.5), main=&#39;Marginal Gain&#39;, font.main=1, ylab=&#39;decrease in standard error&#39;, xlab=&#39;sample size&#39;) 6.5 Further Reading See https://www.r-bloggers.com/2025/02/bootstrap-vs-standard-error-confidence-intervals/ "],["hypothesis-tests.html", " 7 Hypothesis Tests 7.1 Basic Ideas 7.2 Default Statistics 7.3 Two-Sample Differences 7.4 Distributional Tests", " 7 Hypothesis Tests 7.1 Basic Ideas In this section, we test hypotheses using data-driven methods that assume much less about the data generating process. There are two main ways to conduct a hypothesis test to do so: inverting a confidence interval and imposing the null. Invert a CI. One main way to conduct hypothesis tests is to examine whether a confidence interval contains a hypothesized value. We then have this decision rule reject the null if value falls outside of the interval fail to reject the null if value falls inside of the interval sample_dat &lt;- USArrests$Murder sample_mean &lt;- mean(sample_dat) n &lt;- length(sample_dat) Jmeans &lt;- sapply(1:n, function(i){ dati &lt;- sample_dat[-i] mean(dati) }) hist(Jmeans, breaks=25, border=NA, xlim=c(7.5,8.1), main=&#39;&#39;, xlab=expression( bar(X)[-i])) # CI ci_95 &lt;- quantile(Jmeans, probs=c(.025, .975)) abline(v=ci_95, lwd=2) # H0: mean=8 abline(v=8, col=2, lwd=2) Impose the Null. We can also compute a null distribution: the sampling distribution of the statistic under the null hypothesis (assuming your null hypothesis was true). We focus on the simplest, the bootstrap, where loop through a large number of simulations. In each iteration of the loop, we drop impose the null hypothesis and reestimate the statistic of interest. We then calculate the standard deviation of the statistic across all ``resamples’’. Specifically, we compute the distribution of t-values on data with randomly reshuffled outcomes (imposing the null), and compare how extreme the observed value is. sample_dat &lt;- USArrests$Murder sample_mean &lt;- mean(sample_dat) # Bootstrap estimates set.seed(1) Bmeans0 &lt;- sapply(1:10^4, function(i) { dat_b &lt;- sample(sample_dat, replace=T) mean_b &lt;- mean(dat_b) + (8 - sample_mean) # impose the null by recentering return(mean_b) }) hist(Bmeans0, breaks=25, border=NA, main=&#39;&#39;, xlab=expression( bar(X)[b]) ) ci_95 &lt;- quantile(Bmeans0, probs=c(.025, .975)) abline(v=ci_95, lwd=2) abline(v=sample_mean, lwd=2, col=2) 7.2 Default Statistics p-values. A p-value is the frequency you would see something as extreme as your statistic when sampling from the null distribution. # P( boot0_means &gt; sample_mean) # NULL: mean=8 That_NullDist1 &lt;- ecdf(Bmeans0) plot(That_NullDist1, xlab=expression( beta[b] ), main=&#39;Null Bootstrap Distribution for means&#39;, font.main=1) abline(v=sample_mean, col=&#39;red&#39;) p &lt;- That_NullDist1(sample_mean) p ## [1] 0.3751 There are three associated tests: the two-sided test (observed statistic is extremely high or low) or one of the one-sided tests (observed statistic is extremely low, observed statistic is extremely high). In either case, typically “p&lt;.05: statistically significant” and “p&gt;.05: statistically insignificant”.4 # One-Sided Test, ALTERNATIVE: mean &lt; 8 if(p &gt;.05){ message(&#39;fail to reject the null that sample_mean=8 at the 5% level&#39;) } else { message(&#39;reject the null that sample_mean=8 in favor of &lt;8 at the 5% level&#39;) } # Two-Sided Test, ALTERNATIVE: mean &lt; 8 or mean &gt;8 if( p &gt;.025 | p &gt;.975){ message(&#39;fail to reject the null that sample_mean=8 at the 5% level&#39;) } else { message(&#39;reject the null that sample_mean=8 in favor of either &lt;8 or &gt;8 at the 5% level&#39;) } t-values. A t-value standardizes the statistic you are using for hypothesis testing. jack_se &lt;- sd(Jmeans) mean0 &lt;- 8 jack_t &lt;- (sample_mean - mean0)/jack_se There are several benefits to this: makes the statistic comparable across different studies makes the null distribution theoretically known (at least approximately) makes the null distribution not depend on theoretical parameters (\\(\\sigma\\)) # Two-Sided Test, based on theory # 1-pt( abs(jack_t), n-1) + pt(-abs(jack_t), n-1) In another statistics class, you will learn the math behind the null t-distribution. In this class, we skip this because we can simply bootstrap the t-statistic too. set.seed(1) boot_t0 &lt;- sapply(1:10^4, function(i) { dat_b &lt;- sample(sample_dat, replace=T) mean_b &lt;- mean(dat_b) + (8 - sample_mean) # impose the null by recentering jack_t &lt;- (mean_b - mean0)/jack_se }) # Two Sided Test for P(t &gt; jack_t or t &lt; -jack_t | Null) That_NullDist2 &lt;- ecdf(abs(boot_t0)) plot(That_NullDist2, xlim=range(boot_t0, jack_t), xlab=expression( abs(hat(t)[b]) ), main=&#39;Null Bootstrap Distribution for t&#39;, font.main=1) abline(v=abs(jack_t), col=&#39;red&#39;) p &lt;- That_NullDist2( abs(jack_t) ) p ## [1] 0.2659 if(p &gt;.05){ message(&#39;fail to reject the null that sample_mean=8 at the 5% level&#39;) } else { message(&#39;reject the null that sample_mean=8 in favor of either &lt;8 or &gt;8 at the 5% level&#39;) } 7.3 Two-Sample Differences Suppose we have 2 samples of data. Each \\(X_{is}\\) is an individual observation \\(i\\) from the sample \\(s=1,2\\). (For example, the wages for men and women in Canada. For another example, homicide rates in two different American states.) library(wooldridge) x1 &lt;- wage1[wage1$educ == 15, &#39;wage&#39;] x2 &lt;- wage1[wage1$educ == 16, &#39;wage&#39;] Although it not necessary, we will assume that each \\(X_{is}\\) is an independent observation for simplicity. # Sample 1 n1 &lt;- 100 x1 &lt;- rnorm(n1, 0, 2) # Sample 2 n2 &lt;- 80 x2 &lt;- rnorm(n1, 1, 1) par(mfrow=c(1,2)) bks &lt;- seq(-7,7, by=.5) hist(x1, border=NA, breaks=bks, main=&#39;Sample 1&#39;, font.main=1) hist(x2, border=NA, breaks=bks, main=&#39;Sample 2&#39;, font.main=1) There may be several differences between these samples. Often, the first summary statistic we investigate is the difference in means. Equal Means. The sample mean \\(\\overline{X}_{s}\\) is the average value of all the observations in the sample. We want to know if the means are different. To test this hypothesis, we examine the differences term \\[\\begin{eqnarray} D = \\overline{X}_{1} - \\overline{X}_{2}, \\end{eqnarray}\\] with a null hypothesis of \\(D=0\\). # Differences between means m1 &lt;- mean(x1) m2 &lt;- mean(x2) d &lt;- m1-m2 # Bootstrap Distribution boot_d &lt;- sapply(1:10^4, function(b){ x1_b &lt;- sample(x1, replace=T) x2_b &lt;- sample(x2, replace=T) m1_b &lt;- mean(x1_b) m2_b &lt;- mean(x2_b) d_b &lt;- m1_b - m2_b return(d_b) }) hist(boot_d, border=NA, font.main=1, main=&#39;Difference in Means&#39;) # 2-Sided Test boot_ci &lt;- quantile(boot_d, probs=c(.025, .975)) abline(v=boot_ci, lwd=2) abline(v=0, lwd=2, col=2) ecdf(boot_d)(0) ## [1] 1 Just as with one sample tests, we can standardize \\(D\\) into a \\(t\\) statistic. (In which case we also theoretically know the distribution.) Similarly, we can also compute one or two sided hypothesis tests. Equal Quantiles or Variances. The above procedure generalized from “means” to other statistics like “variances” or “quantiles”. # Bootstrap Distribution Function boot_fun &lt;- function( fun, B=10^4, ...){ boot_d &lt;- sapply(1:B, function(b){ x1_b &lt;- sample(x1, replace=T) x2_b &lt;- sample(x2, replace=T) f1_b &lt;- fun(x1_b, ...) f2_b &lt;- fun(x2_b, ...) d_b &lt;- f1_b - f2_b return(d_b) }) return(boot_d) } # 2-Sided Test for Median Differences # d &lt;- median(x2) - median(x1) boot_d &lt;- boot_fun(median) hist(boot_d, border=NA, font.main=1, main=&#39;Difference in Medians&#39;) abline(v=quantile(boot_d, probs=c(.025, .975)), lwd=2) abline(v=0, lwd=2, col=2) ecdf(boot_d)(0) ## [1] 0.9995 # 2-Sided Test for SD Differences #d &lt;- sd(x2) - sd(x1) boot_d &lt;- boot_fun(sd) hist(boot_d, border=NA, font.main=1, main=&#39;Difference in Standard Deviations&#39;) abline(v=quantile(boot_d, probs=c(.025, .975)), lwd=2) abline(v=0, lwd=2, col=2) ecdf(boot_d)(0) ## [1] 0 # Try any function! # boot_fun( function(xs) { IQR(xs)/median(xs) } ) 7.4 Distributional Tests We can also examine whether there are any differences between the entire distributions # Compute Quantiles quants &lt;- seq(0,1,length.out=101) Q1 &lt;- quantile(x1, probs=quants) Q2 &lt;- quantile(x2, probs=quants) # Compare Distributions via Quantiles rx &lt;- range(c(x1, x2)) par(mfrow=c(1,2)) plot(rx, c(0,1), type=&#39;n&#39;, font.main=1, main=&#39;Distributional Comparison&#39;, xlab=expression(Q[s]), ylab=expression(F[s])) lines(Q1, quants, col=2) lines(Q2, quants, col=4) legend(&#39;topleft&#39;, col=c(2,4), lty=1, legend=c(&#39;F1&#39;, &#39;F2&#39;)) # Compare Quantiles plot(Q1, Q2, xlim=rx, ylim=rx, main=&#39;Quantile-Quantile Plot&#39;, font.main=1, pch=16, col=grey(0,.25)) abline(a=0,b=1,lty=2) We can also test for a differences in entire distributions, using all sample data \\(x \\in \\{X_1\\} \\cup \\{X_2\\}\\). # Sorted Sample Data x1 &lt;- sort(x1) x2 &lt;- sort(x2) x &lt;- sort(c(x1, x2)) # Distributions F1 &lt;- ecdf(x1)(x) F2 &lt;- ecdf(x2)(x) library(twosamples) The starting point is the Kolmogorov-Smirnov Statistic: the maximum absolute difference between two CDF’s. \\[\\begin{eqnarray} KS &amp;=&amp; \\max_{x} |F_{1}(x)- F_{2}(x)|^{p}. \\end{eqnarray}\\] # Kolmogorov-Smirnov KSq &lt;- which.max(abs(F2 - F1)) KSqv &lt;- round(twosamples::ks_stat(x1, x2),2) plot(range(x), c(0,1), type=&quot;n&quot;, xlab=&#39;x&#39;, ylab=&#39;ECDF&#39;) title(paste0(&#39;KS = &#39;, KSqv), font.main=1) segments(x[KSq], F1[KSq], x[KSq], F2[KSq], lwd=1, col=grey(0,.5)) lines(x, F1, col=2, lwd=2) lines(x, F2, col=4, lwd=2) legend(&#39;bottomright&#39;, col=c(2,4), lty=1, legend=c(expression(F[1]), expression(F[2]))) An intuitive alternative is the Cramer-von Mises Statistic: the sum of absolute distances (raised to a power) between two CDF’s. \\[\\begin{eqnarray} CVM=\\sum_{x} |F_{1}(x)- F_{2}(x)|^{p}. \\end{eqnarray}\\] # Cramer-von Mises Statistic (p=2) CVMqv &lt;- round(twosamples::cvm_stat(x1, x2, power=2), 2) plot(range(x), c(0,1), type=&quot;n&quot;, xlab=&#39;x&#39;, ylab=&#39;ECDF&#39;) segments(x, F1, x, F2, lwd=.5, col=grey(0,.1)) lines(x, F1, col=2, lwd=2) lines(x, F2, col=4, lwd=2) title(paste0(&#39;CVM = &#39;,CVMqv), font.main=1) Just as before, you use bootstrapping for hypothesis testing. twosamples::cvm_test(x1, x2) ## Test Stat P-Value ## 11.30520 0.00025 Note that the p-value is not the ``probability that we reject the null based on the data we have and given the null is true’’. This is called the statistical power of the test.↩︎ "],["data-analysis.html", " 8 Data Analysis 8.1 Reading In 8.2 Cleaning Data 8.3 Polishing 8.4 Interactive 8.5 Custom Figures", " 8 Data Analysis 8.1 Reading In The first step in data analysis is getting data into R. There are many ways to do this, depending on your data structure. Perhaps the most common case is reading in a csv file. # Read in csv (downloaded from online) # download source &#39;http://www.stern.nyu.edu/~wgreene/Text/Edition7/TableF19-3.csv&#39; # download destination &#39;~/TableF19-3.csv&#39; read.csv(&#39;~/TableF19-3.csv&#39;) # Can read in csv (directly from online) # dat_csv &lt;- read.csv(&#39;http://www.stern.nyu.edu/~wgreene/Text/Edition7/TableF19-3.csv&#39;) Reading in other types of data can require the use of “packages”. For example, the “wooldridge” package contains datasets on crime. To use this data, we must first install the package on our computer. Then, to access the data, we must first load the package. # Install R Data Package and Load in install.packages(&#39;wooldridge&#39;) # only once library(wooldridge) # anytime you want to use the data data(&#39;crime2&#39;) data(&#39;crime4&#39;) We can use packages to access many different types of data. To read in a Stata data file, for example, we can use the “haven” package. # Read in stata data file from online #library(haven) #dat_stata &lt;- read_dta(&#39;https://www.ssc.wisc.edu/~bhansen/econometrics/DS2004.dta&#39;) #dat_stata &lt;- as.data.frame(dat_stata) # For More Introductory Econometrics Data, see # https://www.ssc.wisc.edu/~bhansen/econometrics/Econometrics%20Data.zip # https://pages.stern.nyu.edu/~wgreene/Text/Edition7/tablelist8new.htm # R packages: wooldridge, causaldata, Ecdat, AER, .... Github. Sometimes you will want to install a package from GitHub. For this, you can use devtools or its light-weight version remotes install.packages(&quot;devtools&quot;) install.packages(&quot;remotes&quot;) Note that to install devtools, you also need to have developer tools installed on your computer. Windows: Rtools Mac: Xcode To color terminal output on Linux systems, you can use the colorout package library(remotes) # Install https://github.com/jalvesaq/colorout # to .libPaths()[1] install_github(&#39;jalvesaq/colorout&#39;) library(colorout) Base. While additional packages can make your code faster, they also create dependancies that can lead to problems. So learn base R well before becoming dependant on other packages https://bitsofanalytics.org/posts/base-vs-tidy/ https://jtr13.github.io/cc21fall2/comparison-among-base-r-tidyverse-and-datatable.html 8.2 Cleaning Data Data transformation is often necessary before analysis, so remember to be careful and check your code is doing what you want. (If you have large datasets, you can always test out the code on a sample.) # Function to Create Sample Datasets make_noisy_data &lt;- function(n, b=0){ # Simple Data Generating Process x &lt;- seq(1,10, length.out=n) e &lt;- rnorm(n, mean=0, sd=10) y &lt;- b*x + e # Obervations xy_mat &lt;- data.frame(ID=seq(x), x=x, y=y) return(xy_mat) } # Two simulated datasets dat1 &lt;- make_noisy_data(6) dat2 &lt;- make_noisy_data(6) # Merging data in long format dat_merged_long &lt;- rbind( cbind(dat1,DF=1), cbind(dat2,DF=2)) Now suppose we want to transform into wide format # Merging data in wide format, First Attempt dat_merged_wide &lt;- cbind( dat1, dat2) names(dat_merged_wide) &lt;- c(paste0(names(dat1),&#39;.1&#39;), paste0(names(dat2),&#39;.2&#39;)) # Merging data in wide format, Second Attempt # higher performance dat_merged_wide2 &lt;- merge(dat1, dat2, by=&#39;ID&#39;, suffixes=c(&#39;.1&#39;,&#39;.2&#39;)) ## CHECK they are the same. identical(dat_merged_wide, dat_merged_wide2) ## [1] FALSE # Inspect any differences # Merging data in wide format, Third Attempt with dedicated package # (highest performance but with new type of object) library(data.table) dat_merged_longDT &lt;- as.data.table(dat_merged_long) dat_melted &lt;- melt(dat_merged_longDT, id.vars=c(&#39;ID&#39;, &#39;DF&#39;)) dat_merged_wide3 &lt;- dcast(dat_melted, ID~DF+variable) ## CHECK they are the same. identical(dat_merged_wide, dat_merged_wide3) ## [1] FALSE Often, however, we ultimately want data in long format # Merging data in long format, Second Attempt with dedicated package dat_melted2 &lt;- melt(dat_merged_wide3, measure=c(&quot;1_x&quot;,&quot;1_y&quot;,&quot;2_x&quot;,&quot;2_y&quot;)) melt_vars &lt;- strsplit(as.character(dat_melted2$variable),&#39;_&#39;) dat_melted2$DF &lt;- sapply(melt_vars, `[[`,1) dat_melted2$variable &lt;- sapply(melt_vars, `[[`,2) dat_merged_long2 &lt;- dcast(dat_melted2, DF+ID~variable) dat_merged_long2 &lt;- as.data.frame(dat_merged_long2) ## CHECK they are the same. identical( dat_merged_long2, dat_merged_long) ## [1] FALSE # Further Inspect dat_merged_long2 &lt;- dat_merged_long2[,c(&#39;ID&#39;,&#39;x&#39;,&#39;y&#39;,&#39;DF&#39;)] mapply( identical, dat_merged_long2, dat_merged_long) ## ID x y DF ## TRUE TRUE TRUE FALSE For more tips, see https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-import.pdf and https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reshape.html 8.3 Polishing Your first figures are typically standard. # Random Data x &lt;- seq(1, 10, by=.0002) e &lt;- rnorm(length(x), mean=0, sd=1) y &lt;- .25*x + e # First Drafts # qqplot(x, y) # plot(x, y) Edit your plot to focus on the most useful information. For others to easily comprehend your work, you must also polish the plot. # Second Draft: Focus # (In this example: comparing shapes) xs &lt;- scale(x) ys &lt;- scale(y) # qqplot(xs, ys) # Third Draft: Polish qqplot(ys, xs, xlab=expression(&#39;[&#39;~X-bar(X)~&#39;] /&#39;~s[X]), ylab=expression(&#39;[&#39;~Y-bar(Y)~&#39;] /&#39;~s[Y]), pch=16, cex=.5, col=grey(0,.2)) abline(a=0, b=1, lty=2) When polishing, you must do two things Add details that are necessary to understand the figure Remove unnecessary details (see e.g., https://www.edwardtufte.com/notebook/chartjunk/ and https://www.biostat.wisc.edu/~kbroman/topten_worstgraphs/) # Another Example xy_dat &lt;- data.frame(x=x, y=y) par(fig=c(0,1,0,0.9), new=F) plot(y~x, xy_dat, pch=16, col=rgb(0,0,0,.05), cex=.5, xlab=&#39;&#39;, ylab=&#39;&#39;) # Format Axis Labels Seperately mtext( &#39;y=0.25 x + e\\n e ~ standard-normal&#39;, 2, line=2.2) mtext( expression(x%in%~&#39;[0,10]&#39;), 1, line=2.2) abline( lm(y~x, data=xy_dat), lty=2) title(&#39;Plot with good features, but too excessive in several ways&#39;, adj=0, font.main=1) # Outer Legend (https://stackoverflow.com/questions/3932038/) outer_legend &lt;- function(...) { opar &lt;- par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), mar=c(0, 0, 0, 0), new=TRUE) on.exit(par(opar)) plot(0, 0, type=&#39;n&#39;, bty=&#39;n&#39;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;) legend(...) } outer_legend(&#39;topright&#39;, legend=&#39;single data point&#39;, title=&#39;do you see the normal distribution?&#39;, pch=16, col=rgb(0,0,0,.1), cex=1, bty=&#39;n&#39;) For useful tips, see C. Wilke (2019) “Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures” https://clauswilke.com/dataviz/ Saving. You can export figures with specific dimensions pdf( &#39;Figures/plot_example.pdf&#39;, height=5, width=5) # plot goes here dev.off() For plotting math, see https://astrostatistics.psu.edu/su07/R/html/grDevices/html/plotmath.html and https://library.virginia.edu/data/articles/mathematical-annotation-in-r For exporting options, see ?pdf. For saving other types of files, see png(\"*.png\"), tiff(\"*.tiff\"), and jpeg(\"*.jpg\") For some things to avoid, see https://www.data-to-viz.com/caveats.html Tables. library(stargazer) # summary statistics stargazer(USArrests, type=&#39;html&#39;, summary=T, title=&#39;Summary Statistics for USArrests&#39;) Summary Statistics for USArrests Statistic N Mean St. Dev. Min Max Murder 50 7.788 4.356 0.800 17.400 Assault 50 170.760 83.338 45 337 UrbanPop 50 65.540 14.475 32 91 Rape 50 21.232 9.366 7.300 46.000 8.4 Interactive Tables. You can create a basic interactive table to explore raw data. data(&quot;USArrests&quot;) library(reactable) reactable(USArrests, filterable=T, highlight=T) For further data exploration, your plots can also be made interactive via https://plotly.com/r/. For more details, see examples and then applications. #install.packages(&quot;plotly&quot;) library(plotly) Histograms. See https://plotly.com/r/histograms/ pop_mean &lt;- mean(USArrests$UrbanPop) murder_lowpop &lt;- USArrests[USArrests$UrbanPop&lt; pop_mean,&#39;Murder&#39;] murder_highpop &lt;- USArrests[USArrests$UrbanPop&gt;= pop_mean,&#39;Murder&#39;] fig &lt;- plot_ly(alpha=0.6, hovertemplate=&quot;%{y}&quot;) fig &lt;- fig %&gt;% add_histogram(murder_lowpop, name=&#39;Low Pop. (&lt; Mean)&#39;) fig &lt;- fig %&gt;% add_histogram(murder_highpop, name=&#39;High Pop (&gt;= Mean)&#39;) fig &lt;- fig %&gt;% layout(barmode=&quot;stack&quot;) # barmode=&quot;overlay&quot; fig &lt;- fig %&gt;% layout( title=&quot;Crime and Urbanization in America 1975&quot;, xaxis = list(title=&#39;Murders Arrests per 100,000 People&#39;), yaxis = list(title=&#39;Number of States&#39;), legend=list(title=list(text=&#39;&lt;b&gt; % Urban Pop. &lt;/b&gt;&#39;)) ) fig Boxplots. See https://plotly.com/r/box-plots/ USArrests$ID &lt;- rownames(USArrests) fig &lt;- plot_ly(USArrests, y=~Murder, color=~cut(UrbanPop,4), alpha=0.6, type=&quot;box&quot;, pointpos=0, boxpoints = &#39;all&#39;, hoverinfo=&#39;text&#39;, text = ~paste(&#39;&lt;b&gt;&#39;, ID, &#39;&lt;/b&gt;&#39;, &quot;&lt;br&gt;Urban :&quot;, UrbanPop, &quot;&lt;br&gt;Assault:&quot;, Assault, &quot;&lt;br&gt;Murder :&quot;, Murder)) fig &lt;- layout(fig, showlegend=FALSE, title=&#39;Crime and Urbanization in America 1975&#39;, xaxis = list(title = &#39;Percent of People in an Urban Area&#39;), yaxis = list(title = &#39;Murders Arrests per 100,000 People&#39;)) fig Scatterplots. See https://plotly.com/r/bubble-charts/ # Simple Scatter Plot #plot(Assault~UrbanPop, USArrests, col=grey(0,.5), pch=16, # cex=USArrests$Murder/diff(range(USArrests$Murder))*2, # main=&#39;US Murder arrests (per 100,000)&#39;) # Scatter Plot USArrests$ID &lt;- rownames(USArrests) fig &lt;- plot_ly( USArrests, x = ~UrbanPop, y = ~Assault, mode=&#39;markers&#39;, type=&#39;scatter&#39;, hoverinfo=&#39;text&#39;, text = ~paste(&#39;&lt;b&gt;&#39;, ID, &#39;&lt;/b&gt;&#39;, &quot;&lt;br&gt;Urban :&quot;, UrbanPop, &quot;&lt;br&gt;Assault:&quot;, Assault, &quot;&lt;br&gt;Murder :&quot;, Murder), color=~Murder, marker=list( size=~Murder, opacity=0.5, showscale=T, colorbar = list(title=&#39;Murder Arrests (per 100,000)&#39;))) fig &lt;- layout(fig, showlegend=F, title=&#39;Crime and Urbanization in America 1975&#39;, xaxis = list(title = &#39;Percent of People in an Urban Area&#39;), yaxis = list(title = &#39;Assault Arrests per 100,000 People&#39;)) fig If you have many point, you can also use a 2D histogram instead. https://plotly.com/r/2D-Histogram/. fig &lt;- plot_ly( USArrests, x = ~UrbanPop, y = ~Assault) fig &lt;- add_histogram2d(fig, nbinsx=25, nbinsy=25) fig 8.5 Custom Figures Many of the best plots are custom made (see https://www.r-graph-gallery.com/). Here are some ones that I have made over the years. "],["reporting.html", " 9 Reporting 9.1 R and R-Markdown 9.2 Simple Reports 9.3 Posters and Slides 9.4 More Literature", " 9 Reporting 9.1 R and R-Markdown We will use R Markdown for communicating results to each other. Note that R and R Markdown are both languages. R studio interprets R code make statistical computations and interprets R Markdown code to produce pretty documents that contain both writing and statistics. Altogether, your project will use R: does statistical computations R Markdown: formats statistical computations for sharing Rstudio: graphical user interface that allows you to easily use both R and R Markdown. Homework reports are probably the smallest document you can create. These little reports are almost entirely self-contained (showing both code and output). To make them, you will need to First install Pandoc on your computer. Then install any required packages # Packages for Rmarkdown install.packages(&quot;knitr&quot;) install.packages(&quot;rmarkdown&quot;) # Other packages frequently used #install.packages(&quot;plotly&quot;) #for interactive plots #install.packages(&quot;sf&quot;) #for spatial data 9.2 Simple Reports We will create reproducible reports via R Markdown. Example 1: Data Scientism. See DataScientism.html and then create it by Clicking the “Code” button in the top right and then “Download Rmd” Open with Rstudio Change the name and title to your own, make other edits Then point-and-click “knit” Alternatively, Download the source file from DataScientism.Rmd Change the name and title to your own, make other edits Use the console to run rmarkdown::render(&#39;DataScientism.Rmd&#39;) Example 2: Homework Assignment. Below is a template of what homework questions (and answers) look like. Create a new .Rmd file from scratch and produce a .html file that looks similar to this: Problem: Simulate 100 random observations of the form \\(y=x\\beta+\\epsilon\\) and plot the relationship. Plot and explore the data interactively via plotly, https://plotly.com/r/line-and-scatter/. Then play around with different styles, https://www.r-graph-gallery.com/13-scatter-plot.html, to best express your point. Solution: I simulate \\(400\\) observations for \\(\\epsilon \\sim 2\\times N(0,1)\\) and \\(\\beta=4\\), as seen in this single chunk. Notice an upward trend. # Simulation n &lt;- 100 E &lt;- rnorm(n) X &lt;- seq(n) Y &lt;- 4*X + 2*E # Plot library(plotly) dat &lt;- data.frame(X=X,Y=Y) plot_ly( data=dat, x=~X, y=~Y) # To Do: # 1. Fit a regression line # 2. Color points by their residual value 9.3 Posters and Slides Posters and presentations are another important type of scientific document. R markdown is good at creating both of these, and actually very good with some additional packages. So we will also use flexdashboard for posters and beamer for presentations. Poster. See DataScientism_Poster.html and recreate from the source file DataScientism_Poster.Rmd. Simply change the name to your own, and knit the document. Slides. See DataScientism_Slides.pdf Since beamer is a pdf output, you will need to install Latex. Alternatively, you can install a lightweight version TinyTex from within R install.packages(&#39;tinytex&#39;) tinytex::install_tinytex() # install TinyTeX Then download source file DataScientism_Slides.Rmd, change the name to your own, and knit the document. If you cannot install Latex, then you must specify a different output. For example, change output: beamer_presentation to output: ioslides_presentation on line 6 of the source file. 9.4 More Literature For more guidance on how to create Rmarkdown documents, see https://github.com/rstudio/cheatsheets/blob/main/rmarkdown.pdf https://cran.r-project.org/web/packages/rmarkdown/vignettes/rmarkdown.html http://rmarkdown.rstudio.com https://bookdown.org/yihui/rmarkdown/ https://bookdown.org/yihui/rmarkdown-cookbook/ https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/rmarkdown.html An Introduction to the Advanced Theory and Practice of Nonparametric Econometrics. Raccine 2019. Appendices B &amp; D. https://rmd4sci.njtierney.com/using-rmarkdown.html https://alexd106.github.io/intro2R/Rmarkdown_intro.html If you are still lost, try one of the many online tutorials (such as these) https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet https://www.neonscience.org/resources/learning-hub/tutorials/rmd-code-intro https://m-clark.github.io/Introduction-to-Rmarkdown/ https://www.stat.cmu.edu/~cshalizi/rmarkdown/ http://math.wsu.edu/faculty/xchen/stat412/HwWriteUp.Rmd http://math.wsu.edu/faculty/xchen/stat412/HwWriteUp.html https://holtzy.github.io/Pimp-my-rmd/ https://ntaback.github.io/UofT_STA130/Rmarkdownforclassreports.html https://crd150.github.io/hw_guidelines.html https://r4ds.had.co.nz/r-markdown.html http://www.stat.cmu.edu/~cshalizi/rmarkdown http://www.ssc.wisc.edu/sscc/pubs/RFR/RFR_RMarkdown.html http://kbroman.org/knitr_knutshell/pages/Rmarkdown.html Some other good packages for posters/presenting you should be aware of https://github.com/mathematicalcoffee/beamerposter-rmarkdown-example https://github.com/rstudio/pagedown https://github.com/brentthorne/posterdown https://odeleongt.github.io/postr/ https://wytham.rbind.io/post/making-a-poster-in-r/ https://www.animateyour.science/post/How-to-design-an-award-winning-conference-poster "],["probability-theory.html", " 10 Probability Theory 10.1 Mean and Variance 10.2 Bivariate Distributions 10.3 Further Reading", " 10 Probability Theory 10.1 Mean and Variance Discrete. If the sample space is discrete, we can compute the theoretical mean (or expected value) as \\[ \\mu = \\sum_{i} x_{i} P(X=x_{i}), \\] where \\(P(X=x_{i})\\) is the probability the random variable takes the particular value \\(x_{i}\\). Similarly, we can compute the theoretical variance as \\[ \\sigma^2 = \\sum_{i} [x_{i} - \\mu]^2 P(X=x_{i}), \\] For example, consider an unfair coin with a \\(.75\\) probability of heads (\\(x_{i}=1\\)) and a \\(.25\\) probability of tails (\\(x_{i}=0\\)) has a theoretical mean of \\[ \\mu = 1\\times.75 + 0 \\times .25 = .75 \\] and a theoretical variance of \\[ \\sigma^2 = [1 - .75]^2 \\times.75 + [0 - .75]^2 \\times.25 = 0.1875 \\] x &lt;- rbinom(10000, size=1, prob=.75) round( mean(x), 4) ## [1] 0.747 round( var(x), 4) ## [1] 0.189 Continuous. If the sample space is continuous, we can compute the theoretical mean (or expected value) as \\[ \\mu = \\int x f(x) d x, \\] where \\(f(x)\\) is the probability the random variable takes the particular value \\(x\\). Similarly, we can compute the theoretical variance as \\[ \\sigma^2 = \\int [x - \\mu]^2 f(x) d x, \\] For example, consider a random variable with a continuous uniform distribution over [-1, 1]. In this case, \\(f(x)=1/[1 - (-1)]=1/2\\) for each \\(x\\) in [-1, 1] and \\[ \\mu = \\int_{-1}^{1} \\frac{x}{2} d x = \\int_{-1}^{0} \\frac{x}{2} d x + \\int_{0}^{1} \\frac{x}{2} d x = 0 \\] and \\[ \\sigma^2 = \\int_{-1}^{1} x^2 \\frac{1}{2} d x = \\frac{1}{2} \\frac{x^3}{3}|_{-1}^{1} = \\frac{1}{6}[1 - (-1)] = 2/6 =1/3 \\] x &lt;- runif(10000, -1,1) round( mean(x), 4) ## [1] -0.0026 round( var(x), 4) ## [1] 0.3287 10.2 Bivariate Distributions Suppose we have two discrete variables \\(X_{1}\\) and \\(X_{2}\\). Their joint distribution is denoted as \\[\\begin{eqnarray} P(X_{1} = x_{1}, X_{2} = x_{2}) \\end{eqnarray}\\] The conditional distributions are defined as \\[\\begin{eqnarray} P(X_{1} = x_{1} | X_{2} = x_{2}) = \\frac{ P(X_{1} = x_{1}, X_{2} = x_{2})}{ P( X_{2} = x_{2} )}\\\\ P(X_{2} = x_{2} | X_{1} = x_{1}) = \\frac{ P(X_{1} = x_{1}, X_{2} = x_{2})}{ P( X_{1} = x_{1} )} \\end{eqnarray}\\] The marginal distributions are then defined as \\[\\begin{eqnarray} P(X_{1} = x_{1}) = \\sum_{x_{2}} P(X_{1} = x_{1} | X_{2} = x_{2}) P( X_{2} = x_{2} ) \\\\ P(X_{2} = x_{2}) = \\sum_{x_{1}} P(X_{2} = x_{2} | X_{1} = x_{1}) P( X_{1} = x_{1} ), \\end{eqnarray}\\] which is also known as the law of total probability. For one example, Consider flipping two coins. Denoted each coin as \\(i \\in \\{1, 2\\}\\), and mark whether “heads” is face up; \\(X_{i}=1\\) if Heads and \\(=0\\) if Tails. Suppose both coins are “fair”: \\(P(X_{1}=1)= 1/2\\) and \\(P(X_{2}=1|X_{1})=1/2\\), then the four potential outcomes have equal probabilities. The joint distribution is \\[\\begin{eqnarray} P(X_{1} = x_{1}, X_{2} = x_{2}) &amp;=&amp; P(X_{1} = x_{1}) P(X_{2} = x_{2})\\\\ P(X_{1} = 0, X_{2} = 0) &amp;=&amp; 1/2 \\times 1/2 = 1/4 \\\\ P(X_{1} = 0, X_{2} = 1) &amp;=&amp; 1/4 \\\\ P(X_{1} = 1, X_{2} = 0) &amp;=&amp; 1/4 \\\\ P(X_{1} = 1, X_{2} = 1) &amp;=&amp; 1/4 . \\end{eqnarray}\\] The marginal distribution of the second coin is \\[\\begin{eqnarray} P(X_{2} = 0) &amp;=&amp; P(X_{2} = 0 | X_{1} = 0) P(X_{1}=0) + P(X_{2} = 0 | X_{1} = 1) P(X_{1}=1)\\\\ &amp;=&amp; 1/2 \\times 1/2 + 1/2 \\times 1/2 = 1/2\\\\ P(X_{2} = 1) &amp;=&amp; P(X_{2} = 1 | X_{1} = 0) P(X_{1}=0) + P(X_{2} = 1 | X_{1} = 1) P(X_{1}=1)\\\\ &amp;=&amp; 1/2 \\times 1/2 + 1/2 \\times 1/2 = 1/2 \\end{eqnarray}\\] # Create a 2x2 matrix for the joint distribution. # Rows correspond to X1 (coin 1), and columns correspond to X2 (coin 2). P_fair &lt;- matrix(1/4, nrow = 2, ncol = 2) rownames(P_fair) &lt;- c(&quot;X1=0&quot;, &quot;X1=1&quot;) colnames(P_fair) &lt;- c(&quot;X2=0&quot;, &quot;X2=1&quot;) P_fair ## X2=0 X2=1 ## X1=0 0.25 0.25 ## X1=1 0.25 0.25 # Compute the marginal distributions. # Marginal for X1: sum across columns. P_X1 &lt;- rowSums(P_fair) P_X1 ## X1=0 X1=1 ## 0.5 0.5 # Marginal for X2: sum across rows. P_X2 &lt;- colSums(P_fair) P_X2 ## X2=0 X2=1 ## 0.5 0.5 # Compute the conditional probabilities P(X2 | X1). cond_X2_given_X1 &lt;- matrix(0, nrow = 2, ncol = 2) for (j in 1:2) { cond_X2_given_X1[, j] &lt;- P_fair[, j] / P_X1[j] } rownames(cond_X2_given_X1) &lt;- c(&quot;X2=0&quot;, &quot;X2=1&quot;) colnames(cond_X2_given_X1) &lt;- c(&quot;given X1=0&quot;, &quot;given X1=1&quot;) cond_X2_given_X1 ## given X1=0 given X1=1 ## X2=0 0.5 0.5 ## X2=1 0.5 0.5 Consider a second example, where the second coin is “Completely Unfair”, so that it is always the same as the first. The outcomes generated with a Completely Unfair coin are the same as if we only flipped one coin. \\[\\begin{eqnarray} P(X_{1} = x_{1}, X_{2} = x_{2}) &amp;=&amp; P(X_{1} = x_{1}) \\mathbf{1}( x_{1}=x_{2} )\\\\ P(X_{1} = 0, X_{2} = 0) &amp;=&amp; 1/2 \\\\ P(X_{1} = 0, X_{2} = 1) &amp;=&amp; 0 \\\\ P(X_{1} = 1, X_{2} = 0) &amp;=&amp; 0 \\\\ P(X_{1} = 1, X_{2} = 1) &amp;=&amp; 1/2 . \\end{eqnarray}\\] Note that $(X_{1}=1) $ means \\(X_{1}= 1\\) and \\(0\\) if \\(X_{1}\\neq0\\). The marginal distribution of the second coin is \\[\\begin{eqnarray} P(X_{2} = 0) &amp;=&amp; P(X_{2} = 0 | X_{1} = 0) P(X_{1}=0) + P(X_{2} = 0 | X_{1} = 1) P(X_{1}=1) \\\\ &amp;=&amp; 1/2 \\times 1 + 0 \\times 1/2 = 1/2\\\\ P(X_{2} = 1) &amp;=&amp; P(X_{2} = 1 | X_{1} =0) P( X_{1} = 0) + P(X_{2} = 1 | X_{1} = 1) P( X_{1} =1)\\\\ &amp;=&amp; 0\\times 1/2 + 1 \\times 1/2 = 1/2 \\end{eqnarray}\\] which is the same as in the first example! Different joint distributions can have the same marginal distributions. # Create the joint distribution matrix for the unfair coin case. P_unfair &lt;- matrix(c(0.5, 0, 0, 0.5), nrow = 2, ncol = 2, byrow = TRUE) rownames(P_unfair) &lt;- c(&quot;X1=0&quot;, &quot;X1=1&quot;) colnames(P_unfair) &lt;- c(&quot;X2=0&quot;, &quot;X2=1&quot;) P_unfair ## X2=0 X2=1 ## X1=0 0.5 0.0 ## X1=1 0.0 0.5 # Compute the marginal distribution for X2 in the unfair case. P_X2_unfair &lt;- colSums(P_unfair) P_X1_unfair &lt;- rowSums(P_unfair) # Compute the conditional probabilities P(X1 | X2) for the unfair coin. cond_X2_given_X1_unfair &lt;- matrix(NA, nrow = 2, ncol = 2) for (j in 1:2) { if (P_X1_unfair[j] &gt; 0) { cond_X2_given_X1_unfair[, j] &lt;- P_unfair[, j] / P_X1_unfair[j] } } rownames(cond_X2_given_X1_unfair) &lt;- c(&quot;X2=0&quot;, &quot;X2=1&quot;) colnames(cond_X2_given_X1_unfair) &lt;- c(&quot;given X1=0&quot;, &quot;given X1=1&quot;) cond_X2_given_X1_unfair ## given X1=0 given X1=1 ## X2=0 1 0 ## X2=1 0 1 Finally, note Bayes’ Theorem: \\[\\begin{eqnarray} P(X_{1} = x_{1} | X_{2} = x_{2}) P( X_{2} = x_{2}) &amp;=&amp; P(X_{1} = x_{1}, X_{2} = x_{2}) = P(X_{2} = x_{2} | X_{1} = x_{1}) P(X_{1}=x_{1})\\\\ P(X_{1} = x_{1} | X_{2} = x_{2}) &amp;=&amp; \\frac{ P(X_{2} = x_{2} | X_{1} = x_{1}) P(X_{1}=x_{1}) }{ P( X_{2} = x_{2}) } \\end{eqnarray}\\] # Verify Bayes&#39; theorem for the unfair coin case: # Compute P(X1=1 | X2=1) using the formula: # P(X1=1 | X2=1) = [P(X2=1 | X1=1) * P(X1=1)] / P(X2=1) P_X1_1 &lt;- 0.5 P_X2_1_given_X1_1 &lt;- 1 # Since coin 2 copies coin 1. P_X2_1 &lt;- P_X2_unfair[&quot;X2=1&quot;] bayes_result &lt;- (P_X2_1_given_X1_1 * P_X1_1) / P_X2_1 bayes_result ## X2=1 ## 1 10.3 Further Reading Many introductory econometrics textbooks have a good appendix on probability and statistics. There are many useful texts online too [Refresher] https://www.khanacademy.org/math/statistics-probability/probability-library/basic-theoretical-probability/a/probability-the-basics https://www.r-bloggers.com/2024/03/calculating-conditional-probability-in-r/ https://www.atmos.albany.edu/facstaff/timm/ATM315spring14/R/IPSUR.pdf https://math.dartmouth.edu/~prob/prob/prob.pdf https://bookdown.org/speegled/foundations-of-statistics/ https://bookdown.org/probability/beta/discrete-random-variables.html https://www.econometrics-with-r.org/2.1-random-variables-and-probability-distributions.html https://probability4datascience.com/ch02.html https://rc2e.com/probability https://book.stat420.org/probability-and-statistics-in-r.html https://statsthinking21.github.io/statsthinking21-R-site/probability-in-r-with-lucy-king.html https://bookdown.org/probability/statistics/ https://bookdown.org/probability/beta/ https://bookdown.org/a_shaker/STM1001_Topic_3/ https://bookdown.org/fsancier/bookdown-demo/ https://bookdown.org/kevin_davisross/probsim-book/ https://bookdown.org/machar1991/ITER/2-pt.html https://www.atmos.albany.edu/facstaff/timm/ATM315spring14/R/IPSUR.pdf https://math.dartmouth.edu/~prob/prob/prob.pdf "],["bivariate-data.html", " 11 Bivariate Data 11.1 Simple Linear Regression 11.2 Variability Estimates 11.3 Hypothesis Tests 11.4 Prediction Intervals 11.5 Locally Linear", " 11 Bivariate Data Given some data # Bivariate Data from USArrests xy &lt;- USArrests[,c(&#39;Murder&#39;,&#39;UrbanPop&#39;)] colnames(xy) &lt;- c(&#39;y&#39;,&#39;x&#39;) first inspect it, as in Part I. # Inspect Dataset # head(xy) # summary(xy) plot(y~x, xy, col=grey(0,.5), pch=16) 11.1 Simple Linear Regression Simple Linear Regression refers to fitting a linear model to bivariate data. Specifically, the model is \\[ y_i=\\beta_{0}+\\beta_{1} x_i+\\epsilon_{i} \\] and our objective function is \\[ min_{\\beta_{0}, \\beta_{1}} \\sum_{i=1}^{N} \\left( \\epsilon_{i} \\right)^2 = min_{\\beta_{0}, \\beta_{1}} \\sum_{i=1} \\left( y_i - [\\beta_{0}+\\beta_{1} x_i] \\right). \\] Minimizing the sum of squared errors yields parameter estimates \\[ \\hat{\\beta_{0}}=\\bar{y}-\\hat{\\beta_{1}}\\bar{x} = \\widehat{\\mathbb{E}}[Y] - \\hat{\\beta_{1}} \\widehat{\\mathbb{E}}[X] \\\\ \\hat{\\beta_{1}}=\\frac{\\sum_{i}^{}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i}^{}(x_i-\\bar{x})^2} = \\frac{\\widehat{Cov}[X,Y]}{\\widehat{Var}[X]} \\] and predictions \\[ \\hat{y}_i=\\hat{\\beta_{0}}+\\hat{\\beta}x_i\\\\ \\hat{\\epsilon}_i=y_i-\\hat{y}_i \\] # Estimate Regression Coefficients reg &lt;- lm(y~x, dat=xy) reg ## ## Call: ## lm(formula = y ~ x, data = xy) ## ## Coefficients: ## (Intercept) x ## 6.41594 0.02093 # Coefficient Estimates coef(reg) ## (Intercept) x ## 6.41594246 0.02093466 To qualitatively analyze the ‘’Goodness of fit’’ of our model, we plot our predictions. # Plot Data and Predictions library(plotly) xy$ID &lt;- rownames(USArrests) xy$pred &lt;- predict(reg) xy$resid &lt;- resid(reg) fig &lt;- plotly::plot_ly( xy, x=~x, y=~y, mode=&#39;markers&#39;, type=&#39;scatter&#39;, hoverinfo=&#39;text&#39;, marker=list(color=grey(0,.25), size=10), text=~paste(&#39;&lt;b&gt;&#39;, ID, &#39;&lt;/b&gt;&#39;, &#39;&lt;br&gt;Urban :&#39;, x, &#39;&lt;br&gt;Murder :&#39;, y, &#39;&lt;br&gt;Predicted Murder :&#39;, round(pred,2), &#39;&lt;br&gt;Residual :&#39;, round(resid,2))) # Add Legend fig &lt;- plotly::layout(fig, showlegend=F, title=&#39;Crime and Urbanization in America 1975&#39;, xaxis = list(title=&#39;Percent of People in an Urban Area&#39;), yaxis = list(title=&#39;Homicide Arrests per 100,000 People&#39;)) # Plot Model Predictions add_trace(fig, x=~x, y=~pred, inherit=F, hoverinfo=&#39;none&#39;, mode=&#39;lines+markers&#39;, type=&#39;scatter&#39;, color=I(&#39;black&#39;), line=list(width=1/2), marker=list(symbol=134, size=5)) To quantitatively analyze Goodness of Fit, we can intuitively compute the linear correlation between the predictions and the data \\[ R = Cor( \\hat{y}_i, y) \\] With linear models, we typically compute \\(R^2\\), known as the “coefficient of determination”, using the sums of squared errors (Total, Explained, and Residual) \\[ \\underbrace{\\sum_{i}(y_i-\\bar{y})^2}_\\text{TSS}=\\underbrace{\\sum_{i}(\\hat{y}_i-\\bar{y})^2}_\\text{ESS}+\\underbrace{\\sum_{i}\\hat{\\epsilon_{i}}^2}_\\text{RSS}\\\\ R^2 = \\frac{ESS}{TSS}=1-\\frac{RSS}{TSS} \\] # Manually Compute R2 Ehat &lt;- resid(reg) RSS &lt;- sum(Ehat^2) Y &lt;- xy$y TSS &lt;- sum((Y-mean(Y))^2) R2 &lt;- 1 - RSS/TSS R2 ## [1] 0.00484035 # Check R2 summary(reg)$r.squared ## [1] 0.00484035 # Double Check R2 R &lt;- cor(xy$y, predict(reg)) R^2 ## [1] 0.00484035 11.2 Variability Estimates A regression coefficient is a statistic. And, just like all statistics, we can calculate standard deviation: variability within a single sample. standard error: variability across different samples. confidence interval: range your statistic varies across different samples. Note that values reported by your computer do not necessarily satisfy this definition. To calculate these statistics, we will estimate variability using data-driven methods. (For some theoretical background, see, e.g., https://www.sagepub.com/sites/default/files/upm-binaries/21122_Chapter_21.pdf.) We first consider the simplest, the jackknife. In this procedure, we loop through each row of the dataset. And, in each iteration of the loop, we drop that observation from the dataset and reestimate the statistic of interest. We then calculate the standard deviation of the statistic across all ``subsamples’’. # Jackknife Standard Errors for OLS Coefficient jack_regs &lt;- lapply(1:nrow(xy), function(i){ xy_i &lt;- xy[-i,] reg_i &lt;- lm(y~x, dat=xy_i) }) jack_coefs &lt;- sapply(jack_regs, coef)[&#39;x&#39;,] jack_se &lt;- sd(jack_coefs) # classic_se &lt;- sqrt(diag(vcov(reg)))[[&#39;x&#39;]] # Jackknife Sampling Distribution hist(jack_coefs, breaks=25, main=paste0(&#39;SE est. = &#39;, round(jack_se,4)), font.main=1, border=NA, xlab=expression(beta[-i])) # Original Estimate abline(v=coef(reg)[&#39;x&#39;], lwd=2) # Jackknife Confidence Intervals jack_ci_percentile &lt;- quantile(jack_coefs, probs=c(.025,.975)) abline(v=jack_ci_percentile, lty=2) # Plot Normal Approximation # jack_ci_normal &lt;- jack_mean+c(-1.96, +1.96)*jack_se # abline(v=jack_ci_normal, col=&quot;red&quot;, lty=3) There are several resampling techniques. The other main one is the bootstrap, which resamples with replacement for an arbitrary number of iterations. When bootstrapping a dataset with \\(n\\) observations, you randomly resample all \\(n\\) rows in your data set \\(B\\) times. Random subsampling is one of many hybrid approaches that tries to combine the best of both worlds. Sample Size per Iteration Number of Iterations Resample Bootstrap \\(n\\) \\(B\\) With Replacement Jackknife \\(n-1\\) \\(n\\) Without Replacement Random Subsample \\(m &lt; n\\) \\(B\\) Without Replacement # Bootstrap boot_regs &lt;- lapply(1:399, function(b){ b_id &lt;- sample( nrow(xy), replace=T) xy_b &lt;- xy[b_id,] reg_b &lt;- lm(y~x, dat=xy_b) }) boot_coefs &lt;- sapply(boot_regs, coef)[&#39;x&#39;,] boot_se &lt;- sd(boot_coefs) hist(boot_coefs, breaks=25, main=paste0(&#39;SE est. = &#39;, round(boot_se,4)), font.main=1, border=NA, xlab=expression(beta[b])) boot_ci_percentile &lt;- quantile(boot_coefs, probs=c(.025,.975)) abline(v=boot_ci_percentile, lty=2) abline(v=coef(reg)[&#39;x&#39;], lwd=2) # Random Subsamples rs_regs &lt;- lapply(1:399, function(b){ b_id &lt;- sample( nrow(xy), nrow(xy)-10, replace=F) xy_b &lt;- xy[b_id,] reg_b &lt;- lm(y~x, dat=xy_b) }) rs_coefs &lt;- sapply(rs_regs, coef)[&#39;x&#39;,] rs_se &lt;- sd(rs_coefs) hist(rs_coefs, breaks=25, main=paste0(&#39;SE est. = &#39;, round(rs_se,4)), font.main=1, border=NA, xlab=expression(beta[b])) abline(v=coef(reg)[&#39;x&#39;], lwd=2) rs_ci_percentile &lt;- quantile(rs_coefs, probs=c(.025,.975)) abline(v=rs_ci_percentile, lty=2) We can also bootstrap other statistics, such as a t-statistic or \\(R^2\\). We do such things to test a null hypothesis, which is often ``no relationship’’. We are rarely interested in computing standard errors and conducting hypothesis tests for two variables. However, we work through the ideas in the two-variable case to better understand the multi-variable case. 11.3 Hypothesis Tests Invert a CI. One main way to conduct hypothesis tests is to examine whether a confidence interval contains a hypothesized value. Does the slope coefficient equal \\(0\\)? For reasons we won’t go into in this class, we typically normalize the coefficient by its standard error: \\[ \\hat{t} = \\frac{\\hat{\\beta}}{\\hat{\\sigma}_{\\hat{\\beta}}} \\] tvalue &lt;- coef(reg)[&#39;x&#39;]/jack_se jack_t &lt;- sapply(jack_regs, function(reg_b){ # Data xy_b &lt;- reg_b$model # Coefficient beta_b &lt;- coef(reg_b)[[&#39;x&#39;]] t_hat_b &lt;- beta_b/jack_se return(t_hat_b) }) hist(jack_t, breaks=25, main=&#39;Jackknife t Density&#39;, font.main=1, border=NA, xlab=expression(hat(t)[b]), xlim=range(c(0, jack_t)) ) abline(v=quantile(jack_t, probs=c(.025,.975)), lty=2) abline(v=0, col=&quot;red&quot;, lwd=2) Impose the Null. We can also compute a null distribution. We focus on the simplest: bootstrap simulations that each impose the null hypothesis and re-estimate the statistic of interest. Specifically, we compute the distribution of t-values on data with randomly reshuffled outcomes (imposing the null), and compare how extreme the observed value is. # Null Distribution for Beta boot_t0 &lt;- sapply( 1:399, function(b){ xy_b &lt;- xy xy_b$y &lt;- sample( xy_b$y, replace=T) reg_b &lt;- lm(y~x, dat=xy_b) beta_b &lt;- coef(reg_b)[[&#39;x&#39;]] t_hat_b &lt;- beta_b/jack_se return(t_hat_b) }) # Null Bootstrap Distribution boot_ci_percentile0 &lt;- quantile(boot_t0, probs=c(.025,.975)) hist(boot_t0, breaks=25, main=&#39;Null Bootstrap Density&#39;, font.main=1, border=NA, xlab=expression(hat(t)[b]), xlim=range(boot_t0)) abline(v=boot_ci_percentile0, lty=2) abline(v=tvalue, col=&quot;red&quot;, lwd=2) Alternatively, you can impose the null by recentering the sampling distribution around the theoretical value; \\[\\hat{t} = \\frac{\\hat{\\beta} - \\beta_{0} }{\\hat{\\sigma}_{\\hat{\\beta}}}.\\] Under some assumptions, the null distribution follows a t-distribution. (For more on parametric t-testing based on statistical theory, see https://www.econometrics-with-r.org/4-lrwor.html.) In any case, we can calculate a p-value: the probability you would see something as extreme as your statistic under the null (assuming your null hypothesis was true). We can always calculate a p-value from an explicit null distribution. # One Sided Test for P(t &gt; boot_t | Null) = 1 - P(t &lt; boot_t | Null) That_NullDist1 &lt;- ecdf(boot_t0) Phat1 &lt;- 1-That_NullDist1(jack_t) # Two Sided Test for P(t &gt; jack_t or t &lt; -jack_t | Null) That_NullDist2 &lt;- ecdf(abs(boot_t0)) plot(That_NullDist2, xlim=range(boot_t0, jack_t), xlab=expression( abs(hat(t)[b]) ), main=&#39;Null Bootstrap Distribution&#39;, font.main=1) abline(v=tvalue, col=&#39;red&#39;) Phat2 &lt;- 1-That_NullDist2( abs(tvalue)) Phat2 ## [1] 0.6265664 11.4 Prediction Intervals In addition to confidence intervals, we can also compute a prediction interval which estimates the range of variability across different samples for the outcomes. These intervals also take into account the residuals— the variability of individuals around the mean. # Bootstrap Prediction Interval boot_resids &lt;- lapply(boot_regs, function(reg_b){ e_b &lt;- resid(reg_b) x_b &lt;- reg_b$model$x res_b &lt;- cbind(e_b, x_b) }) boot_resids &lt;- as.data.frame(do.call(rbind, boot_resids)) # Homoskedastic ehat &lt;- quantile(boot_resids$e_b, probs=c(.025, .975)) x &lt;- quantile(xy$x,probs=seq(0,1,by=.1)) boot_pi &lt;- coef(reg)[1] + x*coef(reg)[&#39;x&#39;] boot_pi &lt;- cbind(boot_pi + ehat[1], boot_pi + ehat[2]) # Plot Bootstrap PI plot(y~x, dat=xy, pch=16, main=&#39;Prediction Intervals&#39;, ylim=c(-5,20), font.main=1) polygon( c(x, rev(x)), c(boot_pi[,1], rev(boot_pi[,2])), col=grey(0,.2), border=NA) # Parametric PI (For Comparison) pi &lt;- predict(reg, interval=&#39;prediction&#39;, newdata=data.frame(x)) lines( x, pi[,&#39;lwr&#39;], lty=2) lines( x, pi[,&#39;upr&#39;], lty=2) For a nice overview of different types of intervals, see https://www.jstor.org/stable/2685212. For an in-depth view, see “Statistical Intervals: A Guide for Practitioners and Researchers” or “Statistical Tolerance Regions: Theory, Applications, and Computation”. See https://robjhyndman.com/hyndsight/intervals/ for constructing intervals for future observations in a time-series context. See Davison and Hinkley, chapters 5 and 6 (also Efron and Tibshirani, or Wehrens et al.) 11.5 Locally Linear It is generally safe to assume that you could be analyzing data with nonlinear relationships. Here, our model can be represented as \\[\\begin{eqnarray} y_{i} = m(x_{i}) + e_{i}, \\end{eqnarray}\\] with \\(m\\) being some unknown but smooth function. In such cases, linear regressions can still be useful. The simplest case is segmented/piecewise regression # Globally Linear reg &lt;- lm(y~x, data=xy) # Diagnose Fit #plot( fitted(reg), resid(reg), pch=16, col=grey(0,.5)) #plot( xy$x, resid(reg), pch=16, col=grey(0,.5)) # Linear in 2 Pieces (subsets) xcut2 &lt;- cut(xy$x,2) xy_list2 &lt;- split(xy, xcut2) regs2 &lt;- lapply(xy_list2, function(xy_s){ lm(y~x, data=xy_s) }) sapply(regs2, coef) ## (31.9,61.5] (61.5,91.1] ## (Intercept) -0.2836303 4.15337509 ## x 0.1628157 0.04760783 # Linear in 3 Pieces (subsets or bins) xcut3 &lt;- cut(xy$x, seq(32,92,by=20)) # Finer Bins xy_list3 &lt;- split(xy, xcut3) regs3 &lt;- lapply(xy_list3, function(xy_s){ lm(y~x, data=xy_s) }) sapply(regs3, coef) ## (32,52] (52,72] (72,92] ## (Intercept) 4.60313390 2.36291848 8.653829140 ## x 0.08233618 0.08132841 -0.007174454 Compare Predictions pred1 &lt;- data.frame(yhat=predict(reg), x=reg$model$x) pred1 &lt;- pred1[order(pred1$x),] pred2 &lt;- lapply(regs2, function(reg){ data.frame(yhat=predict(reg), x=reg$model$x) }) pred2 &lt;- do.call(rbind,pred2) pred2 &lt;- pred2[order(pred2$x),] pred3 &lt;- lapply(regs3, function(reg){ data.frame(yhat=predict(reg), x=reg$model$x) }) pred3 &lt;- do.call(rbind,pred3) pred3 &lt;- pred3[order(pred3$x),] # Compare Predictions plot(y ~ x, pch=16, col=grey(0,.5), dat=xy) lines(yhat~x, pred1, lwd=2, col=2) lines(yhat~x, pred2, lwd=2, col=4) lines(yhat~x, pred3, lwd=2, col=3) legend(&#39;topleft&#39;, legend=c(&#39;Globally Linear&#39;, &#39;Peicewise Linear (2)&#39;,&#39;Peicewise Linear (3)&#39;), lty=1, col=c(2,4,3), cex=.8) A less simple case is a local linear regression which conducts a linear regression for each data point using a subsample of data around it. # ``Naive&quot; Smoother pred_fun &lt;- function(x0, h, xy){ # Assign equal weight to observations within h distance to x0 # 0 weight for all other observations ki &lt;- dunif(xy$x, x0-h, x0+h) llls &lt;- lm(y~x, data=xy, weights=ki) yhat_i &lt;- predict(llls, newdata=data.frame(x=x0)) } X0 &lt;- sort(unique(xy$x)) pred_lo1 &lt;- sapply(X0, pred_fun, h=2, xy=xy) pred_lo2 &lt;- sapply(X0, pred_fun, h=20, xy=xy) plot(y~x, pch=16, data=xy, col=grey(0,.5), ylab=&#39;Murder Rate&#39;, xlab=&#39;Population Density&#39;) cols &lt;- c(rgb(.8,0,0,.5), rgb(0,0,.8,.5)) lines(X0, pred_lo1, col=cols[1], lwd=1, type=&#39;o&#39;) lines(X0, pred_lo2, col=cols[2], lwd=1, type=&#39;o&#39;) legend(&#39;topleft&#39;, title=&#39;Locally Linear&#39;, legend=c(&#39;h=2 &#39;, &#39;h=20&#39;), lty=1, col=cols, cex=.8) Note that there are more complex versions of local linear regressions (see https://shinyserv.es/shiny/kreg/ for a nice illustration.) An even more complex (and more powerful) version is loess, which uses adaptive bandwidths in order to have a similar number of data points in each subsample (especially useful when \\(X\\) is not uniform.) # Adaptive-width subsamples with non-uniform weights xy0 &lt;- xy[order(xy$x),] plot(y~x, pch=16, col=grey(0,.5), dat=xy0) reg_lo4 &lt;- loess(y~x, data=xy0, span=.4) reg_lo8 &lt;- loess(y~x, data=xy0, span=.8) cols &lt;- hcl.colors(3,alpha=.75)[-3] lines(xy0$x, predict(reg_lo4), col=cols[1], type=&#39;o&#39;, pch=2) lines(xy0$x, predict(reg_lo8), col=cols[2], type=&#39;o&#39;, pch=2) legend(&#39;topleft&#39;, title=&#39;Loess&#39;, legend=c(&#39;span=.4 &#39;, &#39;span=.8&#39;), lty=1, col=cols, cex=.8) The smoothed predicted values estimate the local means. So we can also construct confidence bands # Loess xy0 &lt;- xy[order(xy$x),] X0 &lt;- unique(xy0$x) reg_lo &lt;- loess(y~x, data=xy0, span=.8) # Jackknife CI jack_lo &lt;- sapply(1:nrow(xy), function(i){ xy_i &lt;- xy[-i,] reg_i &lt;- loess(y~x, dat=xy_i, span=.8) predict(reg_i, newdata=data.frame(x=X0)) }) jack_cb &lt;- apply(jack_lo,1, quantile, probs=c(.025,.975), na.rm=T) # Plot plot(y~x, pch=16, col=grey(0,.5), dat=xy0) preds_lo &lt;- predict(reg_lo, newdata=data.frame(x=X0)) lines(X0, preds_lo, col=hcl.colors(3,alpha=.75)[2], type=&#39;o&#39;, pch=2) # Plot CI polygon( c(X0, rev(X0)), c(jack_cb[1,], rev(jack_cb[2,])), col=hcl.colors(3,alpha=.25)[2], border=NA) You can also construct prediction bands, which estimate the variability of new data rather than a statistic (a range for \\(y_{i}(x)\\) rather than for \\(m(x)\\)). plot(y~x, pch=16, col=grey(0,.5), dat=xy0, ylim=c(0, 20)) lines(X0, preds_lo, col=hcl.colors(3,alpha=.75)[2], type=&#39;o&#39;, pch=2) # Estimate Residuals CI at design points res_lo &lt;- sapply(1:nrow(xy), function(i){ y_i &lt;- xy[i,&#39;y&#39;] preds_i &lt;- jack_lo[,i] resids_i &lt;- y_i - preds_i }) res_cb &lt;- apply(res_lo, 1, quantile, probs=c(.025,.975), na.rm=T) # Plot lines( X0, preds_lo +res_cb[1,], col=hcl.colors(3,alpha=.75)[2], lt=2) lines( X0, preds_lo +res_cb[2,], col=hcl.colors(3,alpha=.75)[2], lty=2) # Smooth estimates res_lo &lt;- lapply(1:nrow(xy), function(i){ y_i &lt;- xy[i,&#39;y&#39;] x_i &lt;- xy[i,&#39;x&#39;] preds_i &lt;- jack_lo[,i] resids_i &lt;- y_i - preds_i cbind(e=resids_i, x=x_i) }) res_lo &lt;- as.data.frame(do.call(rbind, res_lo)) res_fun &lt;- function(x0, h, res_lo){ # Assign equal weight to observations within h distance to x0 # 0 weight for all other observations ki &lt;- dunif(res_lo$x, x0-h, x0+h) ei &lt;- res_lo[ki!=0,&#39;e&#39;] res_i &lt;- quantile(ei, probs=c(.025,.975), na.rm=T) } X0 &lt;- sort(unique(xy$x)) res_lo2 &lt;- sapply(X0, res_fun, h=15, res_lo=res_lo) lines( X0, preds_lo +res_lo2[1,], col=hcl.colors(3,alpha=.75)[2], lty=1, lwd=2) lines( X0, preds_lo +res_lo2[2,], col=hcl.colors(3,alpha=.75)[2], lty=1, lwd=2) "],["multivariate-data.html", " 12 Multivariate Data 12.1 Multiple Linear Regression 12.2 Variability and Hypothesis Tests 12.3 Factor Variables 12.4 Coefficient Interpretation 12.5 Transformations 12.6 Diagnostics 12.7 More Literature", " 12 Multivariate Data Given a dataset, you can summarize it using the previous tools. # Inspect Dataset on police arrests for the USA in 1973 head(USArrests) ## Murder Assault UrbanPop Rape ## Alabama 13.2 236 58 21.2 ## Alaska 10.0 263 48 44.5 ## Arizona 8.1 294 80 31.0 ## Arkansas 8.8 190 50 19.5 ## California 9.0 276 91 40.6 ## Colorado 7.9 204 78 38.7 library(psych) pairs.panels( USArrests[,c(&#39;Murder&#39;,&#39;Assault&#39;,&#39;UrbanPop&#39;)], hist.col=grey(0,.25), breaks=30, density=F, hist.border=NA, # Diagonal ellipses=F, rug=F, smoother=F, pch=16, col=&#39;red&#39; # Lower Triangle ) 12.1 Multiple Linear Regression With \\(K\\) variables, the linear model is \\[ y_i=\\beta_0+\\beta_1 x_{i1}+\\beta_2 x_{i2}+\\ldots+\\beta_K x_{iK}+\\epsilon_i = [1~~ x_{i1} ~~...~~ x_{iK}] \\beta + \\epsilon_i \\] and our objective is \\[ min_{\\beta} \\sum_{i=1}^{N} (\\epsilon_i)^2. \\] Denoting \\[ y= \\begin{pmatrix} y_{1} \\\\ \\vdots \\\\ y_{N} \\end{pmatrix} \\quad \\textbf{X} = \\begin{pmatrix} 1 &amp; x_{11} &amp; ... &amp; x_{1K} \\\\ &amp; \\vdots &amp; &amp; \\\\ 1 &amp; x_{N1} &amp; ... &amp; x_{NK} \\end{pmatrix}, \\] we can also write the model and objective in matrix form \\[ y=\\textbf{X}\\beta+\\epsilon\\\\ min_{\\beta} (\\epsilon&#39; \\epsilon) \\] Minimizing the squared errors yields coefficient estimates \\[ \\hat{\\beta}=(\\textbf{X}&#39;\\textbf{X})^{-1}\\textbf{X}&#39;y \\] and predictions \\[ \\hat{y}=\\textbf{X} \\hat{\\beta} \\\\ \\hat{\\epsilon}=y - \\hat{y} \\\\ \\] # Manually Compute Y &lt;- USArrests[,&#39;Murder&#39;] X &lt;- USArrests[,c(&#39;Assault&#39;,&#39;UrbanPop&#39;)] X &lt;- as.matrix(cbind(1,X)) XtXi &lt;- solve(t(X)%*%X) Bhat &lt;- XtXi %*% (t(X)%*%Y) c(Bhat) ## [1] 3.20715340 0.04390995 -0.04451047 # Check reg &lt;- lm(Murder~Assault+UrbanPop, data=USArrests) coef(reg) ## (Intercept) Assault UrbanPop ## 3.20715340 0.04390995 -0.04451047 To measure the ``Goodness of fit’’ of the model, we can again plot our predictions plot(USArrests$Murder, predict(reg), pch=16, col=grey(0,.5)) abline(a=0,b=1, lty=2) and compute sums of squared errors. Adding random data may sometimes improve the fit, however, so we adjust the \\(R^2\\) by the number of covariates \\(K\\). \\[ R^2 = \\frac{ESS}{TSS}=1-\\frac{RSS}{TSS}\\\\ R^2_{\\text{adj.}} = 1-\\frac{N-1}{N-K}(1-R^2) \\] ksims &lt;- 1:30 for(k in ksims){ USArrests[,paste0(&#39;R&#39;,k)] &lt;- runif(nrow(USArrests),0,20) } reg_sim &lt;- lapply(ksims, function(k){ rvars &lt;- c(&#39;Assault&#39;,&#39;UrbanPop&#39;, paste0(&#39;R&#39;,1:k)) rvars2 &lt;- paste0(rvars, collapse=&#39;+&#39;) reg_k &lt;- lm( paste0(&#39;Murder~&#39;,rvars2), data=USArrests) }) R2_sim &lt;- sapply(reg_sim, function(reg_k){ summary(reg_k)$r.squared }) R2adj_sim &lt;- sapply(reg_sim, function(reg_k){ summary(reg_k)$adj.r.squared }) plot.new() plot.window(xlim=c(0,30), ylim=c(0,1)) points(ksims, R2_sim) points(ksims, R2adj_sim, pch=16) axis(1) axis(2) mtext(expression(R^2),2, line=3) mtext(&#39;Additional Random Covariates&#39;, 1, line=3) legend(&#39;topleft&#39;, horiz=T, legend=c(&#39;Undjusted&#39;, &#39;Adjusted&#39;), pch=c(1,16)) 12.2 Variability and Hypothesis Tests To estimate the variability of our estimates, we can use the same data-driven methods introduced in the last section. As before, we can conduct independent hypothesis tests using t-values. We can also conduct joint tests that account for interdependancies in our estimates. For example, to test whether two coefficients both equal \\(0\\), we bootstrap the joint distribution of coefficients. # Bootstrap SE&#39;s boots &lt;- 1:399 boot_regs &lt;- lapply(boots, function(b){ b_id &lt;- sample( nrow(USArrests), replace=T) xy_b &lt;- USArrests[b_id,] reg_b &lt;- lm(Murder~Assault+UrbanPop, dat=xy_b) }) boot_coefs &lt;- sapply(boot_regs, coef) # Recenter at 0 to impose the null #boot_means &lt;- rowMeans(boot_coefs) #boot_coefs0 &lt;- sweep(boot_coefs, MARGIN=1, STATS=boot_means) boot_coef_df &lt;- as.data.frame(cbind(ID=boots, t(boot_coefs))) fig &lt;- plotly::plot_ly(boot_coef_df, type = &#39;scatter&#39;, mode = &#39;markers&#39;, x = ~UrbanPop, y = ~Assault, text = ~paste(&#39;&lt;b&gt; bootstrap dataset: &#39;, ID, &#39;&lt;/b&gt;&#39;, &#39;&lt;br&gt;Coef. Urban :&#39;, round(UrbanPop,3), &#39;&lt;br&gt;Coef. Murder :&#39;, round(Assault,3), &#39;&lt;br&gt;Coef. Intercept :&#39;, round(`(Intercept)`,3)), hoverinfo=&#39;text&#39;, showlegend=F, marker=list( color=&#39;rgba(0, 0, 0, 0.5)&#39;)) fig &lt;- plotly::layout(fig, showlegend=F, title=&#39;Joint Distribution of Coefficients (under the null)&#39;, xaxis = list(title=&#39;UrbanPop Coefficient&#39;), yaxis = list(title=&#39;Assualt Coefficient&#39;)) fig F-statistic. We can also use an \\(F\\) test for any \\(q\\) hypotheses. Specifically, when \\(q\\) hypotheses restrict a model, the degrees of freedom drop from \\(k_{u}\\) to \\(k_{r}\\) and the residual sum of squares \\(RSS=\\sum_{i}(y_{i}-\\widehat{y}_{i})^2\\) typically increases. We compute the statistic \\[ F_{q} = \\frac{(RSS_{r}-RSS_{u})/(k_{u}-k_{r})}{RSS_{u}/(N-k_{u})} \\] If you test whether all \\(K\\) variables are significant, the restricted model is a simple intercept and \\(RSS_{r}=TSS\\), and \\(F_{q}\\) can be written in terms of \\(R^2\\): \\(F_{K} = \\frac{R^2}{1-R^2} \\frac{N-K}{K-1}\\). The first fraction is the relative goodness of fit, and the second fraction is an adjustment for degrees of freedom (similar to how we adjusted the \\(R^2\\) term before). To conduct a hypothesis test, first compute a null distribution by randomly reshuffling the outcomes and recompute the \\(F\\) statistic, and then compare how often random data give something as extreme as your initial statistic. For some intuition on this F test, examine how the adjusted \\(R^2\\) statistic varies with bootstrap samples. # Bootstrap under the null boots &lt;- 1:399 boot_regs0 &lt;- lapply(boots, function(b){ # Generate bootstrap sample xy_b &lt;- USArrests b_id &lt;- sample( nrow(USArrests), replace=T) # Impose the null xy_b$Murder &lt;- xy_b$Murder[b_id] # Run regression reg_b &lt;- lm(Murder~Assault+UrbanPop, dat=xy_b) }) # Get null distribution for adjusted R2 R2adj_sim0 &lt;- sapply(boot_regs0, function(reg_k){ summary(reg_k)$adj.r.squared }) hist(R2adj_sim0, xlim=c(-.1,1), breaks=25, border=NA, main=&#39;&#39;, xlab=expression(&#39;adj.&#39;~R[b]^2)) # Compare to initial statistic abline(v=summary(reg)$adj.r.squared, lwd=2, col=2) Note that hypothesis testing is not to be done routinely, as additional complications arise when testing multiple hypothesis sequentially. Under some additional assumptions \\(F_{q}\\) follows an F-distribution. For more about F-testing, see https://online.stat.psu.edu/stat501/lesson/6/6.2 and https://www.econometrics.blog/post/understanding-the-f-statistic/ 12.3 Factor Variables So far, we have discussed cardinal data where the difference between units always means the same thing: e.g., \\(4-3=2-1\\). There are also factor variables Ordered: refers to Ordinal data. The difference between units means something, but not always the same thing. For example, \\(4th - 3rd \\neq 2nd - 1st\\). Unordered: refers to Categorical data. The difference between units is meaningless. For example, \\(B-A=?\\) To analyze either factor, we often convert them into indicator variables or dummies; \\(D_{c}=\\mathbf{1}( Factor = c)\\). One common case is if you have observations of individuals over time periods, then you may have two factor variables. An unordered factor that indicates who an individual is; for example \\(D_{i}=\\mathbf{1}( Individual = i)\\), and an order factor that indicates the time period; for example \\(D_{t}=\\mathbf{1}( Time \\in [month~ t, month~ t+1) )\\). There are many other cases you see factor variables, including spatial ID’s in purely cross sectional data. Be careful not to handle categorical data as if they were cardinal. E.g., generate city data with Leipzig=1, Lausanne=2, LosAngeles=3, … and then include city as if it were a cardinal number (that’s a big no-no). The same applied to ordinal data; PopulationLeipzig=2, PopulationLausanne=3, PopulationLosAngeles=1. N &lt;- 1000 x &lt;- runif(N,3,8) e &lt;- rnorm(N,0,0.4) fo &lt;- factor(rbinom(N,4,.5), ordered=T) fu &lt;- factor(rep(c(&#39;A&#39;,&#39;B&#39;),N/2), ordered=F) dA &lt;- 1*(fu==&#39;A&#39;) y &lt;- (2^as.integer(fo)*dA )*sqrt(x)+ 2*as.integer(fo)*e dat_f &lt;- data.frame(y,x,fo,fu) With factors, you can still include them in the design matrix of an OLS regression \\[ y_{it} = x_{it} \\beta_{x} + d_{t}\\beta_{t} \\] When, as commonly done, the factors are modeled as being additively seperable, they are modeled “fixed effects”.5 Simply including the factors into the OLS regression yields a “dummy variable” fixed effects estimator. Hansen Econometrics, Theorem 17.1: The fixed effects estimator of \\(\\beta\\) algebraically equals the dummy variable estimator of \\(\\beta\\). The two estimators have the same residuals. library(fixest) fe_reg1 &lt;- feols(y~x|fo+fu, dat_f) coef(fe_reg1) ## x ## 1.202273 fixef(fe_reg1)[1:2] ## $fo ## 0 1 2 3 4 ## 6.770865 9.020095 14.148339 22.655792 44.346932 ## ## $fu ## A B ## 0.00000 -23.14793 # Compare Coefficients fe_reg0 &lt;- lm(y~-1+x+fo+fu, dat_f) coef( fe_reg0 ) ## x fo0 fo1 fo2 fo3 fo4 fuB ## 1.202273 6.770865 9.020095 14.148339 22.655792 44.346932 -23.147928 With fixed effects, we can also compute averages for each group and construct a between estimator: \\(\\bar{y}_i = \\alpha + \\bar{x}_i \\beta\\). Or we can subtract the average from each group to construct a within estimator: \\((y_{it} - \\bar{y}_i) = (x_{it}-\\bar{x}_i)\\beta\\). But note that many factors are not additively separable. This is easy to check with an F-test; reg0 &lt;- lm(y~-1+x+fo+fu, dat_f) reg1 &lt;- lm(y~-1+x+fo*fu, dat_f) reg2 &lt;- lm(y~-1+x*fo*fu, dat_f) anova(reg0, reg2) ## Analysis of Variance Table ## ## Model 1: y ~ -1 + x + fo + fu ## Model 2: y ~ -1 + x * fo * fu ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 993 80781 ## 2 980 5754 13 75028 983 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(reg0, reg1, reg2) ## Analysis of Variance Table ## ## Model 1: y ~ -1 + x + fo + fu ## Model 2: y ~ -1 + x + fo * fu ## Model 3: y ~ -1 + x * fo * fu ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 993 80781 ## 2 989 11361 4 69421 2956.01 &lt; 2.2e-16 *** ## 3 980 5754 9 5607 106.11 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Break Points. Kinks and Discontinuities in \\(X\\) can also be modeled using factor variables. As such, \\(F\\)-tests can be used to examine whether a breaks is statistically significant. library(AER); data(CASchools) CASchools$score &lt;- (CASchools$read + CASchools$math) / 2 reg &lt;- lm(score~income, data=CASchools) # F Test for Break reg2 &lt;- lm(score ~ income*I(income&gt;15), data=CASchools) anova(reg, reg2) # Chow Test for Break data_splits &lt;- split(CASchools, CASchools$income &lt;= 15) resids &lt;- sapply(data_splits, function(dat){ reg &lt;- lm(score ~ income, data=dat) sum( resid(reg)^2) }) Ns &lt;- sapply(data_splits, function(dat){ nrow(dat)}) Rt &lt;- (sum(resid(reg)^2) - sum(resids))/sum(resids) Rb &lt;- (sum(Ns)-2*reg$rank)/reg$rank Ft &lt;- Rt*Rb pf(Ft,reg$rank, sum(Ns)-2*reg$rank,lower.tail=F) # See also # strucchange::sctest(y~x, data=xy, type=&quot;Chow&quot;, point=.5) # strucchange::Fstats(y~x, data=xy) # To Find Changes # segmented::segmented(reg) 12.4 Coefficient Interpretation Notice that we have gotten pretty far without actually trying to meaningfully interpret regression coefficients. That is because the above procedure will always give us number, regardless as to whether the true data generating process is linear or not. So, to be cautious, we have been interpreting the regression outputs while being agnostic as to how the data are generated. We now consider a special situation where we know the data are generated according to a linear process and are only uncertain about the parameter values. If the data generating process is \\[ y=X\\beta + \\epsilon\\\\ \\mathbb{E}[\\epsilon | X]=0, \\] then we have a famous result that lets us attach a simple interpretation of OLS coefficients as unbiased estimates of the effect of X: \\[ \\hat{\\beta} = (X&#39;X)^{-1}X&#39;y = (X&#39;X)^{-1}X&#39;(X\\beta + \\epsilon) = \\beta + (X&#39;X)^{-1}X&#39;\\epsilon\\\\ \\mathbb{E}\\left[ \\hat{\\beta} \\right] = \\mathbb{E}\\left[ (X&#39;X)^{-1}X&#39;y \\right] = \\beta + (X&#39;X)^{-1}\\mathbb{E}\\left[ X&#39;\\epsilon \\right] = \\beta \\] Generate a simulated dataset with 30 observations and two exogenous variables. Assume the following relationship: \\(y_{i} = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i\\) where the variables and the error term are realizations of the following data generating processes (DGP): N &lt;- 30 B &lt;- c(10, 2, -1) x1 &lt;- runif(N, 0, 5) x2 &lt;- rbinom(N,1,.7) X &lt;- cbind(1,x1,x2) e &lt;- rnorm(N,0,3) Y &lt;- X%*%B + e dat &lt;- data.frame(Y,X) coef(lm(Y~x1+x2, data=dat)) ## (Intercept) x1 x2 ## 10.375993 1.850176 -1.301420 Simulate the distribution of coefficients under a correctly specified model. Interpret the average. N &lt;- 30 B &lt;- c(10, 2, -1) Coefs &lt;- sapply(1:400, function(sim){ x1 &lt;- runif(N, 0, 5) x2 &lt;- rbinom(N,1,.7) X &lt;- cbind(1,x1,x2) e &lt;- rnorm(N,0,3) Y &lt;- X%*%B + e dat &lt;- data.frame(Y,x1,x2) coef(lm(Y~x1+x2, data=dat)) }) par(mfrow=c(1,2)) for(i in 2:3){ hist(Coefs[i,], xlab=bquote(beta[.(i)]), main=&#39;&#39;, border=NA) abline(v=mean(Coefs[i,]), lwd=2) abline(v=B[i], col=rgb(1,0,0)) } Many economic phenomena are nonlinear, even when including potential transforms of \\(Y\\) and \\(X\\). Sometimes the linear model may still be a good or even great approximation. But sometimes not, and it is hard to know ex-ante. Examine the distribution of coefficients under this mispecified model and try to interpret the average. N &lt;- 30 Coefs &lt;- sapply(1:600, function(sim){ x2 &lt;- runif(N, 0, 5) x3 &lt;- rbinom(N,1,.7) e &lt;- rnorm(N,0,3) Y &lt;- 10*x3 + 2*log(x2)^x3 + e dat &lt;- data.frame(Y,x2,x3) coef(lm(Y~x2+x3, data=dat)) }) par(mfrow=c(1,2)) for(i in 2:3){ hist(Coefs[i,], xlab=bquote(beta[.(i)]), main=&#39;&#39;, border=NA) abline(v=mean(Coefs[i,]), col=1, lwd=2) } In general, you can interpret your regression coefficients as “adjusted correlations”. There are (many) tests for whether the relationships in your dataset are actually additively separable and linear. 12.5 Transformations Transforming variables can often improve your model fit while still allowing it estimated via OLS. This is because OLS only requires the model to be linear in the parameters. Under the assumptions of the model is correctly specified, the following table is how we can interpret the coefficients of the transformed data. (Note for small changes, \\(\\Delta ln(x) \\approx \\Delta x / x = \\Delta x \\% \\cdot 100\\).) Specification Regressand Regressor Derivative Interpretation (If True) linear–linear \\(y\\) \\(x\\) \\(\\Delta y = \\beta_1\\cdot\\Delta x\\) Change \\(x\\) by one unit \\(\\rightarrow\\) change \\(y\\) by \\(\\beta_1\\) units. log–linear \\(ln(y)\\) \\(x\\) \\(\\Delta y \\% \\cdot 100 \\approx \\beta_1 \\cdot \\Delta x\\) Change \\(x\\) by one unit \\(\\rightarrow\\) change \\(y\\) by \\(100 \\cdot \\beta_1\\) percent. linear–log \\(y\\) \\(ln(x)\\) \\(\\Delta y \\approx \\frac{\\beta_1}{100}\\cdot \\Delta x \\%\\) Change \\(x\\) by one percent \\(\\rightarrow\\) change \\(y\\) by \\(\\frac{\\beta_1}{100}\\) units log–log \\(ln(y)\\) \\(ln(x)\\) \\(\\Delta y \\% \\approx \\beta_1\\cdot \\Delta x \\%\\) Change \\(x\\) by one percent \\(\\rightarrow\\) change \\(y\\) by \\(\\beta_1\\) percent Now recall from micro theory that an additively seperable and linear production function is referred to as ``perfect substitutes’‘. With a linear model and untranformed data, you have implicitly modelled the different regressors \\(X\\) as perfect substitutes. Further recall that the’‘perfect substitutes’’ model is a special case of the constant elasticity of substitution production function. Here, we will build on http://dx.doi.org/10.2139/ssrn.3917397, and consider box-cox transforming both \\(X\\) and \\(y\\). Specifically, apply the box-cox transform of \\(y\\) using parameter \\(\\lambda\\) and apply another box-cox transform to each \\(x\\) using the same parameter \\(\\rho\\) so that \\[ y^{(\\lambda)}_{i} = \\sum_{k=1}^{K}\\beta_{k} x^{(\\rho)}_{ik} + \\epsilon_{i}\\\\ y^{(\\lambda)}_{i} = \\begin{cases} \\lambda^{-1}[ (y_i+1)^{\\lambda}- 1] &amp; \\lambda \\neq 0 \\\\ log(y_i+1) &amp;  \\lambda=0 \\end{cases}.\\\\ x^{(\\rho)}_{i} = \\begin{cases} \\rho^{-1}[ (x_i)^{\\rho}- 1] &amp; \\rho \\neq 0 \\\\ log(x_{i}+1) &amp;  \\rho=0 \\end{cases}. \\] Notice that this nests: linear-linear \\((\\rho=\\lambda=1)\\). linear-log \\((\\rho=1, \\lambda=0)\\). log-linear \\((\\rho=0, \\lambda=1)\\). log-log \\((\\rho=\\lambda=0)\\). If \\(\\rho=\\lambda\\), we get the CES production function. This nests the ‘’perfect substitutes’’ linear-linear model (\\(\\rho=\\lambda=1\\)) , the ‘’cobb-douglas’’ log-log model (\\(\\rho=\\lambda=0\\)), and many others. We can define \\(\\lambda=\\rho/\\lambda&#39;\\) to be clear that this is indeed a CES-type transformation where \\(\\rho \\in (-\\infty,1]\\) controls the “substitutability” of explanatory variables. E.g., \\(\\rho &lt;0\\) is ‘’complementary’’. \\(\\lambda\\) determines ‘’returns to scale’‘. E.g., \\(\\lambda&lt;1\\) is’‘decreasing returns’’. We compute the mean squared error in the original scale by inverting the predictions; \\[ \\widehat{y}_{i} = \\begin{cases} [ \\widehat{y}_{i}^{(\\lambda)} \\cdot \\lambda ]^{1/\\lambda} -1 &amp; \\lambda \\neq 0 \\\\ exp( \\widehat{y}_{i}^{(\\lambda)}) -1 &amp;  \\lambda=0 \\end{cases}. \\] It is easiest to optimize parameters in a 2-step procedure called `concentrated optimization’. We first solve for \\(\\widehat{\\beta}(\\rho,\\lambda)\\) and compute the mean squared error \\(MSE(\\rho,\\lambda)\\). We then find the \\((\\rho,\\lambda)\\) which minimizes \\(MSE\\). # Box-Cox Transformation Function bxcx &lt;- function( xy, rho){ if (rho == 0L) { log(xy+1) } else if(rho == 1L){ xy } else { ((xy+1)^rho - 1)/rho } } bxcx_inv &lt;- function( xy, rho){ if (rho == 0L) { exp(xy) - 1 } else if(rho == 1L){ xy } else { (xy * rho + 1)^(1/rho) - 1 } } # Which Variables reg &lt;- lm(Murder~Assault+UrbanPop, data=USArrests) X &lt;- USArrests[,c(&#39;Assault&#39;,&#39;UrbanPop&#39;)] Y &lt;- USArrests[,&#39;Murder&#39;] # Simple Grid Search over potential (Rho,Lambda) rl_df &lt;- expand.grid(rho=seq(-2,2,by=.5),lambda=seq(-2,2,by=.5)) # Compute Mean Squared Error # from OLS on Transformed Data errors &lt;- apply(rl_df,1,function(rl){ Xr &lt;- bxcx(X,rl[[1]]) Yr &lt;- bxcx(Y,rl[[2]]) Datr &lt;- cbind(Murder=Yr,Xr) Regr &lt;- lm(Murder~Assault+UrbanPop, data=Datr) Predr &lt;- bxcx_inv(predict(Regr),rl[[2]]) Resr &lt;- (Y - Predr) return(Resr) }) rl_df$mse &lt;- colMeans(errors^2) # Want Small MSE and Interpretable layout(matrix(1:2,ncol=2), width=c(3,1), height=c(1,1)) par(mar=c(4,4,2,0)) plot(lambda~rho,rl_df, cex=8, pch=15, xlab=expression(rho), ylab=expression(lambda), col=hcl.colors(25)[cut(1/rl_df$mse,25)]) # Which min rl0 &lt;- rl_df[which.min(rl_df$mse),c(&#39;rho&#39;,&#39;lambda&#39;)] points(rl0$rho, rl0$lambda, pch=0, col=1, cex=8, lwd=2) # Legend plot(c(0,2),c(0,1), type=&#39;n&#39;, axes=F, xlab=&#39;&#39;,ylab=&#39;&#39;, cex.main=.8, main=expression(frac(1,&#39;Mean Square Error&#39;))) rasterImage(as.raster(matrix(hcl.colors(25), ncol=1)), 0, 0, 1,1) text(x=1.5, y=seq(1,0,l=10), cex=.5, labels=levels(cut(1/rl_df$mse,10))) The parameters \\(-1,0,1,2\\) are easy to interpret and might be selected instead if there is only a small loss in fit. (In the above example, we might choose \\(\\lambda=0\\) instead of the \\(\\lambda\\) which minimized the mean square error). You can also plot the specific predictions to better understand the effect of data transformation beyond mean squared error. # Plot for Specific Comparisons Xr &lt;- bxcx(X,rl0[[1]]) Yr &lt;- bxcx(Y,rl0[[2]]) Datr &lt;- cbind(Murder=Yr,Xr) Regr &lt;- lm(Murder~Assault+UrbanPop, data=Datr) Predr &lt;- bxcx_inv(predict(Regr),rl0[[2]]) cols &lt;- c(rgb(1,0,0,.5), col=rgb(0,0,1,.5)) plot(Y, Predr, pch=16, col=cols[1], ylab=&#39;Prediction&#39;, ylim=range(Y,Predr)) points(Y, predict(reg), pch=16, col=cols[2]) legend(&#39;topleft&#39;, pch=c(16), col=cols, title=expression(rho~&#39;, &#39;~lambda), legend=c( paste0(rl0, collapse=&#39;, &#39;),&#39;1, 1&#39;) ) abline(a=0,b=1, lty=2) When explicitly transforming data according to \\(\\lambda\\) and \\(\\rho\\), these parameters increase the degrees of freedom by two. The default hypothesis testing procedures do not account for you trying out different transformations, and should be adjusted by the increased degrees of freedom. Specification searches deflate standard errors and are a major source for false discoveries. 12.6 Diagnostics There’s little sense in getting great standard errors for a terrible model. Plotting your regression object a simple and easy step to help diagnose whether your model is in some way bad. We next go through what each of these figures show. reg &lt;- lm(Murder~Assault+UrbanPop, data=USArrests) par(mfrow=c(2,2)) plot(reg, pch=16, col=grey(0,.5)) Outliers. The first diagnostic plot examines outliers in terms the outcome \\(y_i\\) being far from its prediction \\(\\hat{y}_i\\). You may be interested in such outliers because they can (but do not have to) unduly influence your estimates. The third diagnostic plot examines another type of outlier, where an observation with the explanatory variable \\(x_i\\) is far from the center of mass of the other \\(x\\)’s. A point has high leverage if the estimates change dramatically when you estimate the model without that data point. N &lt;- 40 x &lt;- c(25, runif(N-1,3,8)) e &lt;- rnorm(N,0,0.4) y &lt;- 3 + 0.6*sqrt(x) + e plot(y~x, pch=16, col=grey(0,.5)) points(x[1],y[1], pch=16, col=rgb(1,0,0,.5)) abline(lm(y~x), col=2, lty=2) abline(lm(y[-1]~x[-1])) See AEJ-leverage and NBER-leverage for examples of leverage in economics. Standardized residuals are \\[ r_i=\\frac{\\hat{\\epsilon}_i}{s_{[i]}\\sqrt{1-h_i}}, \\] where \\(s_{[i]}\\) is the root mean squared error of a regression with the \\(i\\)th observation removed and \\(h_i\\) is the leverage of residual \\(\\hat{\\epsilon_i}\\). which.max(hatvalues(reg)) which.max(rstandard(reg)) (See https://www.r-bloggers.com/2016/06/leverage-and-influence-in-a-nutshell/ for a good interactive explanation, and https://online.stat.psu.edu/stat462/node/87/ for detail.) The fourth plot further assesses outlier \\(X\\) using Cook’s Distance, which sums of all prediction changes when observation i is removed and scales proportionally to the mean square error \\(s^2 = \\frac{\\sum_{i} (e_{i})^2 }{n-K}.\\)$ D_{i} = = $$ which.max(cooks.distance(reg)) car::influencePlot(reg) Normality. The second plot examines whether the residuals are normally distributed. Your OLS coefficient estimates do not depend on the normality of the residuals. (Good thing, because there’s no reason the residuals of economic phenomena should be so well behaved.) Many hypothesis tests are, however, affected by the distribution of the residuals. For these reasons, you may be interested in assessing normality par(mfrow=c(1,2)) hist(resid(reg), main=&#39;Histogram of Residuals&#39;, font.main=1, border=NA) qqnorm(resid(reg), main=&quot;Normal Q-Q Plot of Residuals&quot;, font.main=1, col=grey(0,.5), pch=16) qqline(resid(reg), col=1, lty=2) #shapiro.test(resid(reg)) Heterskedasticity may also matters for variability estimates. This is not shown in the plot, but you can conduct a simple test library(lmtest) lmtest::bptest(reg) Collinearity. This is when one explanatory variable in a multiple linear regression model can be linearly predicted from the others with a substantial degree of accuracy. Coefficient estimates may change erratically in response to small changes in the model or the data. (In the extreme case where there are more variables than observations \\(K&gt;N\\), the inverse of \\(X&#39;X\\) has an infinite number of solutions.) To diagnose collinearity, we can use the Variance Inflation Factor \\[ VIF_{k}=\\frac{1}{1-R^2_k}, \\] where \\(R^2_k\\) is the \\(R^2\\) for the regression of \\(X_k\\) on the other covariates \\(X_{-k}\\) (a regression that does not involve the response variable Y) car::vif(reg) sqrt(car::vif(reg)) &gt; 2 # problem? 12.7 More Literature For OLS, see https://bookdown.org/josiesmith/qrmbook/linear-estimation-and-minimizing-error.html https://www.econometrics-with-r.org/4-lrwor.html https://www.econometrics-with-r.org/6-rmwmr.html https://www.econometrics-with-r.org/7-htaciimr.html https://bookdown.org/ripberjt/labbook/bivariate-linear-regression.html https://bookdown.org/ripberjt/labbook/multivariable-linear-regression.html https://online.stat.psu.edu/stat462/node/137/ https://book.stat420.org/ Hill, Griffiths &amp; Lim (2007), Principles of Econometrics, 3rd ed., Wiley, S. 86f. Verbeek (2004), A Guide to Modern Econometrics, 2nd ed., Wiley, S. 51ff. Asteriou &amp; Hall (2011), Applied Econometrics, 2nd ed., Palgrave MacMillan, S. 177ff. https://online.stat.psu.edu/stat485/lesson/11/ To derive OLS coefficients in Matrix form, see https://jrnold.github.io/intro-methods-notes/ols-in-matrix-form.html https://www.fsb.miamioh.edu/lij14/411_note_matrix.pdf https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf For fixed effects, see https://www.econometrics-with-r.org/10-rwpd.html https://bookdown.org/josiesmith/qrmbook/topics-in-multiple-regression.html https://bookdown.org/ripberjt/labbook/multivariable-linear-regression.html https://www.princeton.edu/~otorres/Panel101.pdf https://www.stata.com/manuals13/xtxtreg.pdf Diagnostics https://book.stat420.org/model-diagnostics.html#leverage https://socialsciences.mcmaster.ca/jfox/Books/RegressionDiagnostics/index.html https://bookdown.org/ripberjt/labbook/diagnosing-and-addressing-problems-in-linear-regression.html Belsley, D. A., Kuh, E., and Welsch, R. E. (1980). Regression Diagnostics: Identifying influential data and sources of collinearity. Wiley. https://doi.org/10.1002/0471725153 Fox, J. D. (2020). Regression diagnostics: An introduction (2nd ed.). SAGE. https://dx.doi.org/10.4135/9781071878651 There are also random effects: the factor variable comes from a distribution that is uncorrelated with the regressors. This is rarely used in economics today, however, and are mostly included for historical reasons and special cases where fixed effects cannot be estimated due to data limitations.↩︎ "],["endogeneity-issues.html", " 13 Endogeneity Issues 13.1 Two Stage Least Squares (2SLS) 13.2 Regression Discontinuities/Kink (RD/RK) 13.3 Difference in Differences (DID) 13.4 More Literature", " 13 Endogeneity Issues Just like many economic relationships are nonlinear, many economic variables are endogenous. By this we typically mean that \\(X\\) is an outcome determined (or caused: \\(\\to\\)) by some other variable. If \\(Y \\to X\\), then we have reverse causality If \\(Y \\to X\\) and \\(X \\to Y\\), then we have simultaneity If \\(Z\\to Y\\) and either \\(Z\\to X\\) or \\(X \\to Z\\), then we have omitted a potentially important variable These endogeneity issues imply \\(X\\) and \\(\\epsilon\\) are correlated, which is a barrier to interpreting OLS estimates causally. (\\(X\\) and \\(\\epsilon\\) may be correlated for other reasons too, such as when \\(X\\) is measured with error.) # Simulate data with an endogeneity issue n &lt;- 300 z &lt;- rbinom(n,1,.5) xy &lt;- sapply(z, function(zi){ y &lt;- rnorm(1,zi,1) x &lt;- rnorm(1,zi*2,1) c(x,y) }) xy &lt;- data.frame(x=xy[1,],y=xy[2,]) plot(y~x, data=xy, pch=16, col=grey(0,.5)) abline(lm(y~x,data=xy)) With multiple linear regression, note that endogeneity biases are not just a problem your main variable. Suppose your interested in how \\(x_{1}\\) affects \\(y\\), conditional on \\(x_{2}\\). Letting \\(X=[x_{1}, x_{2}]\\), you estimate \\[\\begin{eqnarray} \\hat{\\beta}_{OLS} = [X&#39;X]^{-1}X&#39;y \\end{eqnarray}\\] You paid special attention in your research design to find a case where \\(x_{1}\\) is truly exogenous. Unfortunately, \\(x_{2}\\) is correlated with the error term. (How unfair, I know, especially after all that work). Nonetheless, \\[\\begin{eqnarray} \\mathbb{E}[X&#39;\\epsilon] = \\begin{bmatrix} 0 \\\\ \\rho \\end{bmatrix}\\\\ \\mathbb{E}[ \\hat{\\beta}_{OLS} - \\beta] = [X&#39;X]^{-1} \\begin{bmatrix} 0 \\\\ \\rho \\end{bmatrix} = \\begin{bmatrix} \\rho_{1} \\\\ \\rho_{2} \\end{bmatrix} \\end{eqnarray}\\] The magnitude of the bias for \\(x_{1}\\) thus depends on the correlations between \\(x_{1}\\) and \\(x_{2}\\) as well as \\(x_{2}\\) and \\(\\epsilon\\). Three statistical tools: 2SLS, RDD, and DID, are designed to address endogeneity issues. The elementary versions of these tools are linear regression. Because there are many textbooks and online notebooks that explain these methods at both high and low levels of technical detail, they are not covered extensively in this notebook. Instead, I will focus on the seminal economic example to provide some intuition. Competitive Market Equilibrium. This model has three structural relationships: (1) market supply is the sum of quantities supplied by individual firms at a given price, (2) market demand is the sum of quantities demanded by individual people at a given price, and (3) market supply equals market demand in equilibrium. Assuming market supply and demand are linear, we can write these three relationships as \\[\\begin{eqnarray} Q_{S}(P) &amp;=&amp; A_{S} + B_{S} P + E_{S},\\\\ Q_{D}(P) &amp;=&amp; A_{D} - B_{D} P + E_{D},\\\\ Q_{D} &amp;=&amp; Q_{S} = Q. %% $Q_{D}(P) = \\sum_{i} q_{D}_{i}(P)$, \\end{eqnarray}\\] This last equation implies a simultaneous “reduced form” relationship where both the price and the quantity are outcomes. With a linear parametric structure to these equations, we can use algebra to solve for the equilibrium price and quantity analytically as \\[\\begin{eqnarray} P^{*} &amp;=&amp; \\frac{A_{D}-A_{S}}{B_{D}+B_{S}} + \\frac{E_{D} - E_{S}}{B_{D}+B_{S}}, \\\\ Q^{*} &amp;=&amp; \\frac{A_{S}B_{D}+ A_{D}B_{S}}{B_{D}+B_{S}} + \\frac{E_{S}B_{D}+ E_{D}B_{S}}{B_{D}+B_{S}}. \\end{eqnarray}\\] # Demand Curve Simulator qd_fun &lt;- function(p, Ad=8, Bd=-.8, Ed_sigma=.25){ Qd &lt;- Ad + Bd*p + rnorm(1,0,Ed_sigma) return(Qd) } # Supply Curve Simulator qs_fun &lt;- function(p, As=-8, Bs=1, Es_sigma=.25){ Qs &lt;- As + Bs*p + rnorm(1,0,Es_sigma) return(Qs) } # Quantity Supplied and Demanded at 3 Prices cbind(P=8:10, D=qd_fun(8:10), S=qs_fun(8:10)) ## P D S ## [1,] 8 1.58717562 0.2227425 ## [2,] 9 0.78717562 1.2227425 ## [3,] 10 -0.01282438 2.2227425 # Market Equilibrium Finder eq_fun &lt;- function(demand, supply, P){ # Compute EQ (what we observe) eq_id &lt;- which.min( abs(demand-supply) ) eq &lt;- c(P=P[eq_id], Q=demand[eq_id]) return(eq) } # Simulations Parameters N &lt;- 300 # Number of Market Interactions P &lt;- seq(5,10,by=.01) # Price Range to Consider # Generate Data from Competitive Market # Plot Underlying Process plot.new() plot.window(xlim=c(0,2), ylim=range(P)) EQ1 &lt;- sapply(1:N, function(n){ # Market Data Generating Process demand &lt;- qd_fun(P) supply &lt;- qs_fun(P) eq &lt;- eq_fun(demand, supply, P) # Plot Theoretical Supply and Demand lines(demand, P, col=grey(0,.01)) lines(supply, P, col=grey(0,.01)) points(eq[2], eq[1], col=grey(0,.05), pch=16) # Save Data return(eq) }) axis(1) axis(2) mtext(&#39;Quantity&#39;,1, line=2) mtext(&#39;Price&#39;,2, line=2) Suppose we ask “what is the effect of price on quantity?” You can simply run a regression of quantity (“Y”) on price (“X”): \\(\\widehat{\\beta}_{OLS} = Cov(Q^{*}, P^{*}) / Var(P^{*})\\). You get a number back, but it is hard to interpret meaningfully. # Analyze Market Data dat1 &lt;- data.frame(t(EQ1), cost=&#39;1&#39;) reg1 &lt;- lm(Q~P, data=dat1) summary(reg1) ## ## Call: ## lm(formula = Q ~ P, data = dat1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.52930 -0.11257 -0.01442 0.11182 0.61647 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.27494 0.45159 0.609 0.543 ## P 0.07046 0.05087 1.385 0.167 ## ## Residual standard error: 0.1707 on 298 degrees of freedom ## Multiple R-squared: 0.006396, Adjusted R-squared: 0.003062 ## F-statistic: 1.918 on 1 and 298 DF, p-value: 0.1671 This simple derivation has a profound insight: price-quantity data does not generally tell you how price affects quantity (or vice-versa). The reason is simultaneity: price and quantity mutually cause one another in markets.6 Moreover, this example also clarifies that our initial question “what is the effect of price on quantity?” is misguided. We could more sensibly ask “what is the effect of price on quantity supplied?” or “what is the effect of price on quantity demanded?” 13.1 Two Stage Least Squares (2SLS) If you have exogenous variation on one side of the market, “shocks”, you can get information on the other. For example, lower costs shift out supply (more is produced at given price), allowing you to trace out part of a demand curve. Experimental manipulation of \\(A_{S}\\) leads to \\[\\begin{eqnarray} \\label{eqn:comp_market_statics} \\frac{d P^{*}}{d A_{S}} = \\frac{-1}{B_{D}+B_{S}}, \\\\ \\frac{d Q^{*}}{d A_{S}} = \\frac{B_{D}}{B_{D}+B_{S}}. \\end{eqnarray}\\] So, absent any other changes, we could recover \\(-B_{D}=d Q^{*}/d P^{*}\\). In this case, the supply shock has identified the demand slope.7 # New Observations After Cost Change EQ2 &lt;- sapply(1:N, function(n){ demand &lt;- qd_fun(P) supply2 &lt;- qs_fun(P, As=-6.5) # More Supplied at Given Price eq &lt;- eq_fun(demand, supply2, P) return(eq) # lines(supply2, P, col=rgb(0,0,1,.01)) #points(eq[2], eq[1], col=rgb(0,0,1,.05), pch=16) }) dat2 &lt;- data.frame(t(EQ2), cost=&#39;2&#39;) # Plot Market Data dat2 &lt;- rbind(dat1, dat2) cols &lt;- ifelse(as.numeric(dat2$cost)==2, rgb(0,0,1,.2), rgb(0,0,0,.2)) plot.new() plot.window(xlim=c(0,2), ylim=range(P)) points(dat2$Q, dat2$P, col=cols, pch=16) axis(1) axis(2) mtext(&#39;Quantity&#39;,1, line=2) mtext(&#39;Price&#39;,2, line=2) We are not quite done yet, however. We have pooled two datasets that are seperately problematic, and the noisiness of the process within each group affects our OLS estimate: \\(\\widehat{\\beta}_{OLS}=Cov(Q^{*}, P^{*}) / Var(P^{*})\\). # Not exactly right, but at least right sign reg2 &lt;- lm(Q~P, data=dat2) summary(reg2) ## ## Call: ## lm(formula = Q ~ P, data = dat2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.71367 -0.15020 0.00091 0.15738 0.74054 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.53965 0.17251 37.91 &lt;2e-16 *** ## P -0.62811 0.02036 -30.85 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2293 on 598 degrees of freedom ## Multiple R-squared: 0.6141, Adjusted R-squared: 0.6134 ## F-statistic: 951.5 on 1 and 598 DF, p-value: &lt; 2.2e-16 Although the individual observations are noisy, we can compute the change in the expected values \\(d \\mathbb{E}[Q^{*}] / d \\mathbb{E}[P^{*}] =-B_{D}\\). Empirically, this is estimated via the change in average value. # Wald (1940) Estimate dat_mean &lt;- rbind( colMeans(dat2[dat2$cost==1,1:2]), colMeans(dat2[dat2$cost==2,1:2])) dat_mean ## P Q ## [1,] 8.874700 0.9002745 ## [2,] 8.044233 1.5520859 B_est &lt;- diff(dat_mean[,2])/diff(dat_mean[,1]) round(B_est, 2) ## [1] -0.78 We can also seperately recover \\(d \\mathbb{E}[Q^{*}] / d \\mathbb{E}[A_{S}]\\) and \\(d \\mathbb{E}[P^{*}] / d \\mathbb{E}[A_{S}]\\) from seperate regressions # Heckman (2000, p.58) Estimate ols_1 &lt;- lm(P~cost, data=dat2) ols_2 &lt;- lm(Q~cost, data=dat2) B_est2 &lt;- coef(ols_2)/coef(ols_1) round(B_est2[[2]],2) ## [1] -0.78 Mathematically, we can also do this in a single step by exploiting linear algebra: \\[\\begin{eqnarray} \\frac{\\frac{ Cov(Q^{*},A_{S})}{ V(A_{S}) } }{\\frac{ Cov(P^{*},A_{S})}{ V(A_{S}) }} &amp;=&amp; \\frac{Cov(Q^{*},A_{S} )}{ Cov(P^{*},A_{S})}. \\end{eqnarray}\\] Alternatively, we can recover the same estimate using an instrumental variables regression that has two equations: \\[\\begin{eqnarray} P &amp;=&amp; \\alpha_{1} + A_{S} \\beta_{1} + \\epsilon_{1} \\\\ Q &amp;=&amp; \\alpha_{2} + \\hat{P} \\beta_{2} + \\epsilon_{2}. \\end{eqnarray}\\] In the first regression, we estimate the average effect of the cost shock on prices. In the second equation, we estimate how the average effect of prices which are exogenous to demand affect quantity demanded. To see this, first substitute the equilibrium condition into the supply equation: \\(Q_{D}=Q_{S}=A_{S}+B_{S} P + E_{S}\\), lets us rewrite \\(P\\) as a function of \\(Q_{D}\\). This yields two theoretical equations \\[\\begin{eqnarray} \\label{eqn:linear_supply_iv} P &amp;=&amp; -\\frac{A_{S}}{{B_{S}}} + \\frac{Q_{D}}{B_{S}} - \\frac{E_{S}}{B_{S}} \\\\ \\label{eqn:linear_demand_iv} Q_{D} &amp;=&amp; A_{D} + B_{D} P + E_{D}. \\end{eqnarray}\\] # Two Stage Least Squares Estimate ols_1 &lt;- lm(P~cost, data=dat2) dat2_new &lt;- cbind(dat2, Phat=predict(ols_1)) reg_2sls &lt;- lm(Q~Phat, data=dat2_new) summary(reg_2sls) ## ## Call: ## lm(formula = Q ~ Phat, data = dat2_new) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.54245 -0.11816 -0.00671 0.11828 0.62530 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.86579 0.14344 54.84 &lt;2e-16 *** ## Phat -0.78487 0.01694 -46.34 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1723 on 598 degrees of freedom ## Multiple R-squared: 0.7822, Adjusted R-squared: 0.7818 ## F-statistic: 2148 on 1 and 598 DF, p-value: &lt; 2.2e-16 # One Stage Instrumental Variables Estimate library(fixest) reg2_iv &lt;- feols(Q~1|P~cost, data=dat2) summary(reg2_iv) ## TSLS estimation - Dep. Var.: Q ## Endo. : P ## Instr. : cost ## Second stage: Dep. Var.: Q ## Observations: 600 ## Standard-errors: IID ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.865793 0.200187 39.2923 &lt; 2.2e-16 *** ## fit_P -0.784874 0.023636 -33.2071 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## RMSE: 0.24 Adj. R2: 0.575103 ## F-test (1st stage), P: stat = 2,648.2, p &lt; 2.2e-16, on 1 and 598 DoF. ## Wu-Hausman: stat = 467.0, p &lt; 2.2e-16, on 1 and 597 DoF. Within Group Variance. You can experiment with the effect of different variances on both OLS and IV in the code below. And note that if we had multiple supply shifts and recorded their magnitudes, then we could recover more information about demand, perhaps tracing it out entirely. # Examine Egrid &lt;- expand.grid(Ed_sigma=c(.001, .25, 1), Es_sigma=c(.001, .25, 1)) Egrid_regs &lt;- lapply(1:nrow(Egrid), function(i){ Ed_sigma &lt;- Egrid[i,1] Es_sigma &lt;- Egrid[i,2] EQ1 &lt;- sapply(1:N, function(n){ demand &lt;- qd_fun(P, Ed_sigma=Ed_sigma) supply &lt;- qs_fun(P, Es_sigma=Es_sigma) return(eq_fun(demand, supply, P)) }) EQ2 &lt;- sapply(1:N, function(n){ demand &lt;- qd_fun(P,Ed_sigma=Ed_sigma) supply2 &lt;- qs_fun(P, As=-6.5,Es_sigma=Es_sigma) return(eq_fun(demand, supply2, P)) }) dat &lt;- rbind( data.frame(t(EQ1), cost=&#39;1&#39;), data.frame(t(EQ2), cost=&#39;2&#39;)) return(dat) }) Egrid_OLS &lt;- sapply(Egrid_regs, function(dat) coef( lm(Q~P, data=dat))) Egrid_IV &lt;- sapply(Egrid_regs, function(dat) coef( feols(Q~1|P~cost, data=dat))) #cbind(Egrid, coef_OLS=t(Egrid_OLS)[,2], coef_IV=t(Egrid_IV)[,2]) lapply( list(Egrid_OLS, Egrid_IV), function(ei){ Emat &lt;- matrix(ei[2,],3,3) rownames(Emat) &lt;- paste0(&#39;Ed_sigma.&#39;,c(.001, .25, 1)) colnames(Emat) &lt;- paste0(&#39;Es_sigma.&#39;,c(.001, .25, 1)) return( round(Emat,2)) }) ## [[1]] ## Es_sigma.0.001 Es_sigma.0.25 Es_sigma.1 ## Ed_sigma.0.001 -0.80 -0.80 -0.80 ## Ed_sigma.0.25 -0.65 -0.60 -0.71 ## Ed_sigma.1 0.38 0.25 -0.06 ## ## [[2]] ## Es_sigma.0.001 Es_sigma.0.25 Es_sigma.1 ## Ed_sigma.0.001 -0.80 -0.80 -0.80 ## Ed_sigma.0.25 -0.83 -0.75 -0.80 ## Ed_sigma.1 -0.69 -0.75 -0.94 Caveats. Regression analysis with instrumental variables can be very insightful and is applied to many different areas. But I also want to stress some caveats about using IV in practice. We always get coefficients back when running feols, and sometimes the computed p-values are very small. The interpretation of those numbers rests on many assumptions: only supply was affected (Instrument exogeneity) the shock is large enough to be picked up statistically (Instrument relevance) supply and demand are linear and additively separable (Functional form) we were not cycling through different IV’s (Multiple hypotheses) We are rarely sure that all of these assumptions hold, and this is one reason why researchers often also report their OLS results. 13.2 Regression Discontinuities/Kink (RD/RK) The basic idea here is to examine how a variable changes in response to an exogenous shock. The Regression Discontinuities estimate of the cost shock is the difference in the outcome variable just before and just after the shock. We now turn to our canonical competitive market example. The RD estimate is the difference between the lines at \\(T=300\\). dat2$T &lt;- 1:nrow(dat2) cols &lt;- ifelse(as.numeric(dat2$cost)==2, rgb(0,0,1,.5), rgb(0,0,0,.5)) plot(P~T, dat2, main=&#39;Effect of Cost Shock on Price&#39;, font.main=1, pch=16, col=cols) regP1 &lt;- lm(P~T, dat2[dat2$cost==1,]) lines(regP1$model$T, predict(regP1), col=rgb(0,0,0), lwd=2) regP2 &lt;- lm(P~T, dat2[dat2$cost==2,]) lines(regP2$model$T, predict(regP2), col=rgb(0,0,1), lwd=2) regP &lt;- lm(P~T*cost, dat2) summary(regP) ## ## Call: ## lm(formula = P ~ T * cost, data = dat2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.53982 -0.13305 0.00303 0.13125 0.59387 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.893e+00 2.288e-02 388.716 &lt;2e-16 *** ## T -1.239e-04 1.318e-04 -0.940 0.347 ## cost2 -7.852e-01 6.463e-02 -12.148 &lt;2e-16 *** ## T:cost2 -1.808e-05 1.863e-04 -0.097 0.923 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1976 on 596 degrees of freedom ## Multiple R-squared: 0.8164, Adjusted R-squared: 0.8155 ## F-statistic: 883.5 on 3 and 596 DF, p-value: &lt; 2.2e-16 plot(Q~T, dat2, main=&#39;Effect of Cost Shock on Quantity&#39;, font.main=1, pch=16, col=cols) regQ1 &lt;- lm(Q~T, dat2[dat2$cost==1,]) lines(regQ1$model$T, predict(regQ1), col=rgb(0,0,0), lwd=2) regQ2 &lt;- lm(Q~T, dat2[dat2$cost==2,]) lines(regQ2$model$T, predict(regQ2), col=rgb(0,0,1), lwd=2) regQ &lt;- lm(Q~T*cost, dat2) summary(regQ) ## ## Call: ## lm(formula = Q ~ T * cost, data = dat2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5404 -0.1158 -0.0073 0.1181 0.6029 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.9264820 0.0199338 46.478 &lt;2e-16 *** ## T -0.0001741 0.0001148 -1.517 0.130 ## cost2 0.6073393 0.0563109 10.785 &lt;2e-16 *** ## T:cost2 0.0002147 0.0001624 1.322 0.187 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1722 on 596 degrees of freedom ## Multiple R-squared: 0.7831, Adjusted R-squared: 0.782 ## F-statistic: 717.2 on 3 and 596 DF, p-value: &lt; 2.2e-16 Remember that this is effect is local: different magnitudes of the cost shock or different demand curves generally yield different estimates. 13.3 Difference in Differences (DID) The basic idea here is to examine how a variable changes in response to an exogenous shock, compared to a control group. EQ3 &lt;- sapply(1:(2*N), function(n){ # Market Mechanisms demand &lt;- qd_fun(P) supply &lt;- qs_fun(P) # Compute EQ (what we observe) eq_id &lt;- which.min( abs(demand-supply) ) eq &lt;- c(P=P[eq_id], Q=demand[eq_id]) # Return Equilibrium Observations return(eq) }) dat3 &lt;- data.frame(t(EQ3), cost=&#39;1&#39;, T=1:ncol(EQ3)) dat3$Pre &lt;- dat3$T &lt; N # Size of first treatment group # Plot Price Data par(mfrow=c(1,2)) plot(P~T, dat2, main=&#39;Effect of Cost Shock on Price&#39;, font.main=1, pch=16, col=cols, cex=.5) lines(regP1$model$T, predict(regP1), col=rgb(0,0,0), lwd=2) lines(regP2$model$T, predict(regP2), col=rgb(0,0,1), lwd=2) # W/ Control group points(P~T, dat3, pch=16, col=rgb(1,0,0,.5), cex=.5) regP3a &lt;- lm(P~T, dat3[dat3$Pre,]) lines(regP3a$model$T, predict(regP3a), col=rgb(1,0,0), lwd=2) regP3b &lt;- lm(P~T, dat3[!dat3$Pre,]) lines(regP3b$model$T, predict(regP3b), col=rgb(1,0,0), lwd=2) # Plot Quantity Data plot(Q~T, dat2, main=&#39;Effect of Cost Shock on Quantity&#39;, font.main=1, pch=17, col=cols, cex=.5) lines(regQ1$model$T, predict(regQ1), col=rgb(0,0,0), lwd=2) lines(regQ2$model$T, predict(regQ2), col=rgb(0,0,1), lwd=2) # W/ Control group points(Q~T, dat3, pch=16, col=rgb(1,0,0,.5), cex=.5) regQ3a &lt;- lm(Q~T, dat3[dat3$Pre,]) lines(regQ3a$model$T, predict(regQ3a), col=rgb(1,0,0), lwd=2) regQ3b &lt;- lm(Q~T, dat3[!dat3$Pre,]) lines(regQ3b$model$T, predict(regQ3b), col=rgb(1,0,0), lwd=2) # Single Regression Estimates # Pool Data dat2$Pre &lt;- dat2$T &lt; N # Size of first treatment group dat2$EverTreat &lt;- 1 dat3$EverTreat &lt;- 0 dat &lt;- rbind(dat2, dat3) # Estimate Intercept Shifts regP &lt;- lm(P~Pre*EverTreat, dat) summary(regP) ## ## Call: ## lm(formula = P ~ Pre * EverTreat, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.55767 -0.13401 -0.00465 0.12599 1.03233 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.884651 0.011479 773.977 &lt;2e-16 *** ## PreTRUE 0.003376 0.016261 0.208 0.836 ## EverTreat -0.836977 0.016234 -51.557 &lt;2e-16 *** ## PreTRUE:EverTreat 0.822963 0.022997 35.786 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1992 on 1196 degrees of freedom ## Multiple R-squared: 0.7681, Adjusted R-squared: 0.7675 ## F-statistic: 1320 on 3 and 1196 DF, p-value: &lt; 2.2e-16 regQ &lt;- lm(Q~Pre*EverTreat, dat) summary(regQ) ## ## Call: ## lm(formula = Q ~ Pre * EverTreat, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.56157 -0.11207 -0.00495 0.11498 0.62590 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.8832893 0.0100168 88.181 &lt;2e-16 *** ## PreTRUE -0.0001174 0.0141895 -0.008 0.993 ## EverTreat 0.6672305 0.0141658 47.101 &lt;2e-16 *** ## PreTRUE:EverTreat -0.6507313 0.0200670 -32.428 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1738 on 1196 degrees of freedom ## Multiple R-squared: 0.7323, Adjusted R-squared: 0.7317 ## F-statistic: 1091 on 3 and 1196 DF, p-value: &lt; 2.2e-16 13.4 More Literature You are directed to the following resources which discusses endogeneity in more detail and how it applies generally. Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction https://www.mostlyharmlesseconometrics.com/ https://www.econometrics-with-r.org https://bookdown.org/paul/applied-causal-analysis/ https://mixtape.scunning.com/ https://theeffectbook.net/ https://www.r-causal.org/ https://matheusfacure.github.io/python-causality-handbook/landing-page.html For IV, https://cameron.econ.ucdavis.edu/e240a/ch04iv.pdf https://mru.org/courses/mastering-econometrics/introduction-instrumental-variables-part-one https://www.econometrics-with-r.org/12-ivr.html https://bookdown.org/paul/applied-causal-analysis/estimation-2.html https://mixtape.scunning.com/07-instrumental_variables https://theeffectbook.net/ch-InstrumentalVariables.html http://www.urfie.net/read/index.html#page/247 For RDD and DID, https://bookdown.org/paul/applied-causal-analysis/rdd-regression-discontinuity-design.html https://mixtape.scunning.com/06-regression_discontinuity https://theeffectbook.net/ch-RegressionDiscontinuity.html https://mixtape.scunning.com/09-difference_in_differences https://theeffectbook.net/ch-DifferenceinDifference.html http://www.urfie.net/read/index.html#page/226 Although there are many ways this simultaneity can happen, economic theorists have made great strides in analyzing the simultaneity problem as it arises from equilibrium market relationships. In fact, 2SLS arose to understand agricultural markets.↩︎ Notice that even in this linear model, however, all effects are conditional: The effect of a cost change on quantity or price depends on the demand curve. A change in costs affects quantity supplied but not quantity demanded (which then affects equilibrium price) but the demand side of the market still matters! The change in price from a change in costs depends on the elasticity of demand.↩︎ "],["data-scientism.html", " 14 Data Scientism 14.1 Data Errors 14.2 P-Hacking 14.3 Spurious Regression 14.4 Spurious Causal Impacts", " 14 Data Scientism In practice, it is hard to find a good natural experiment. For example, suppose we asked “what is the effect of wages on police demanded?” and examined a policy which lowered the educational requirements from 4 years to 2 to become an officer. This increases the labour supply, but it also affects the demand curve through “general equilibrium”: as some of the new officers were potentially criminals and, with fewer criminals, the demand for police shifts down. In practice, it is also easy to find a bad instrument. Paradoxically, natural experiments are something you are supposed to find but never search for. As you search for good instruments, for example, sometimes random noise will appear like a good instrument (spurious instruments). In this age of big data, we are getting increasingly more data and, perhaps surprisingly, this makes it easier to make false discoveries. We will consider three classical ways for false discoveries to arise. After that, there are examples with the latest and greatest empirical recipes—we don’t have so many theoretical results yet but I think you can understand the issue with the numerical example. Although it is difficult to express numerically, you must also know that if you search for a good natural experiment for too long, you can also be led astray from important questions. There are good reasons to be excited about empirical social science, but we would be wise to recall some earlier wisdom from economists on the matter. The most reckless and treacherous of all theorists is he who professes to let facts and figures speak for themselves, who keeps in the background the part he has played, perhaps unconsciously, in selecting and grouping them — Alfred Marshall, 1885 The blind transfer of the striving for quantitative measurements to a field where the specific conditions are not present which give it its basic importance in the natural sciences is the result of an entirely unfounded prejudice. It is probably responsible for the worst aberrations and absurdities produced by scientism in the social sciences. It not only leads frequently to the selection for study of the most irrelevant aspects of the phenomena because they happen to be measurable, but also to “measurements” and assignments of numerical values which are absolutely meaningless. What a distinguished philosopher recently wrote about psychology is at least equally true of the social sciences, namely that it is only too easy “to rush off to measure something without considering what it is we are measuring, or what measurement means. In this respect some recent measurements are of the same logical type as Plato’s determination that a just ruler is 729 times as happy as an unjust one.” — F.A. Hayek, 1943 if you torture the data long enough, it will confess — R. Coase (Source Unknown) the definition of a causal parameter is not always clearly stated, and formal statements of identifying conditions in terms of well-specified economic models are rarely presented. Moreover, the absence of explicit structural frameworks makes it difficult to cumulate knowledge across studies conducted within this framework. Many studies produced by this research program have a `stand alone’ feature and neither inform nor are influenced by the general body of empirical knowledge in economics. — J.J. Heckman, 2000 without explicit prior consideration of the effect of the instrument choice on the parameter being estimated, such a procedure is effectively the opposite of standard statistical practice in which a parameter of interest is defined first, followed by an estimator that delivers that parameter. Instead, we have a procedure in which the choice of the instrument, which is guided by criteria designed for a situation in which there is no heterogeneity, is implicitly allowed to determine the parameter of interest. This goes beyond the old story of looking for an object where the light is strong enough to see; rather, we have at least some control over the light but choose to let it fall where it may and then proclaim that whatever it illuminates is what we were looking for all along. — A. Deaton, 2010 14.1 Data Errors A huge amount of data normally means a huge amount of data cleaning/merging/aggregating. This avoids many copy-paste errors, which are a recipe for disaster, but may also introduce other types of errors. Some spurious results are driven by honest errors in data cleaning. According to one estimate, this is responsible for around one fifth of all medical science retractions (there is even a whole book about this!). Although there are not similar meta-analysis in economics, there are some high-profile examples. This includes papers that are highly influential, like Lott, Levitt and Reinhart and Rogoff as well as others the top economics journals, like the RESTUD and AER. There are some reasons to think such errors are more widespread across the social sciences; e.g., in Census data and Aid data. So be careful! Note: one reason to plot your data is to help spot such errors. 14.2 P-Hacking Another class of errors pertains to P-hacking (and it’s various synonyms: data drudging, star mining,….). While there are cases of fraudulent data manipulation (which can be considered as a dishonest data error), P-hacking is a much more pernicious and widespread set.seed(123) n &lt;- 50 X1 &lt;- runif(n) # Regression Machine: # repeatedly finds covariate, runs regression # stops when statistically significant at .1% p &lt;- 1 i &lt;- 0 while(p &gt;= .001){ # Get Random Covariate X2 &lt;- runif(n) # Merge and `Analyze&#39; dat_i &lt;- data.frame(X1,X2) reg_i &lt;- lm(X1~X2, data=dat_i) # update results in global environment p &lt;- summary(reg_i)$coefficients[2,4] i &lt;- i+1 } plot(X1~X2, data=dat_i, pch=16, col=grey(0,.5), font.main=1, main=paste0(&#39;Random Dataset &#39;, i,&quot;: p=&quot;, formatC(p,digits=2, format=&#39;fg&#39;))) abline(reg_i) #summary(reg_i) # P-hacking 2SLS library(fixest) p &lt;- 1 ii &lt;- 0 set.seed(123) while(p &gt;= .05){ # Get Random Covariates X2 &lt;- runif(n) X3 &lt;- runif(n) # Create Treatment Variable based on Cutoff cutoffs &lt;- seq(0,1,length.out=11)[-c(1,11)] for(tau in cutoffs){ T3 &lt;- 1*(X3 &gt; tau) # Merge and `Analyze&#39; dat_i &lt;- data.frame(X1,X2,T3) ivreg_i &lt;- feols(X1~1|X2~T3, data=dat_i) # Update results in global environment ptab &lt;- summary(ivreg_i)$coeftable if( nrow(ptab)==2){ p &lt;- ptab[2,4] ii &lt;- ii+1 } } } summary(ivreg_i) ## TSLS estimation - Dep. Var.: X1 ## Endo. : X2 ## Instr. : T3 ## Second stage: Dep. Var.: X1 ## Observations: 50 ## Standard-errors: IID ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.95e-14 1.28e-13 -7.750700e-01 0.4421 ## fit_X2 1.00e+00 2.46e-13 4.060978e+12 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## RMSE: 5.81e-14 Adj. R2: 1 ## F-test (1st stage), X2: stat = 0.664884, p = 0.418869, on 1 and 48 DoF. ## Wu-Hausman: stat = 0.232185, p = 0.632145, on 1 and 47 DoF. 14.3 Spurious Regression Even without any coding errors or p-hacking, you can sometimes make a false discovery. We begin with a motivating empirical example of “US Gov’t Spending on Science”. First, get and inspect some data from https://tylervigen.com/spurious-correlations # Your data is not made up in the computer (hopefully!) vigen_csv &lt;- read.csv( paste0( &#39;https://raw.githubusercontent.com/the-mad-statter/&#39;, &#39;whysospurious/master/data-raw/tylervigen.csv&#39;) ) class(vigen_csv) ## [1] &quot;data.frame&quot; names(vigen_csv) ## [1] &quot;year&quot; &quot;science_spending&quot; ## [3] &quot;hanging_suicides&quot; &quot;pool_fall_drownings&quot; ## [5] &quot;cage_films&quot; &quot;cheese_percap&quot; ## [7] &quot;bed_deaths&quot; &quot;maine_divorce_rate&quot; ## [9] &quot;margarine_percap&quot; &quot;miss_usa_age&quot; ## [11] &quot;steam_murders&quot; &quot;arcade_revenue&quot; ## [13] &quot;computer_science_doctorates&quot; &quot;noncom_space_launches&quot; ## [15] &quot;sociology_doctorates&quot; &quot;mozzarella_percap&quot; ## [17] &quot;civil_engineering_doctorates&quot; &quot;fishing_drownings&quot; ## [19] &quot;kentucky_marriage_rate&quot; &quot;oil_imports_norway&quot; ## [21] &quot;chicken_percap&quot; &quot;train_collision_deaths&quot; ## [23] &quot;oil_imports_total&quot; &quot;pool_drownings&quot; ## [25] &quot;nuclear_power&quot; &quot;japanese_cars_sold&quot; ## [27] &quot;motor_vehicle_suicides&quot; &quot;spelling_bee_word_length&quot; ## [29] &quot;spider_deaths&quot; &quot;math_doctorates&quot; ## [31] &quot;uranium&quot; vigen_csv[1:5,1:5] ## year science_spending hanging_suicides pool_fall_drownings cage_films ## 1 1996 NA NA NA NA ## 2 1997 NA NA NA NA ## 3 1998 NA NA NA NA ## 4 1999 18079 5427 109 2 ## 5 2000 18594 5688 102 2 # similar `apply&#39; functions lapply(vigen_csv[,1:5], class) # like apply, but for lists ## $year ## [1] &quot;integer&quot; ## ## $science_spending ## [1] &quot;integer&quot; ## ## $hanging_suicides ## [1] &quot;integer&quot; ## ## $pool_fall_drownings ## [1] &quot;integer&quot; ## ## $cage_films ## [1] &quot;integer&quot; sapply(vigen_csv[,1:5], class) # lapply, formatted to a vector ## year science_spending hanging_suicides pool_fall_drownings ## &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; ## cage_films ## &quot;integer&quot; The US government spending on science is ruining cinema (p&lt;.001)!? # Drop Data before 1999 vigen_csv &lt;- vigen_csv[vigen_csv$year &gt;= 1999,] # Run OLS Regression reg1 &lt;- lm(cage_films ~ -1 + science_spending, data=vigen_csv) summary(reg1) ## ## Call: ## lm(formula = cage_films ~ -1 + science_spending, data = vigen_csv) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.7670 -0.7165 0.1447 0.7890 1.4531 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## science_spending 9.978e-05 1.350e-05 7.39 2.34e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.033 on 10 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.8452, Adjusted R-squared: 0.8297 ## F-statistic: 54.61 on 1 and 10 DF, p-value: 2.343e-05 It’s not all bad, people in maine stay married longer? plot.new() plot.window(xlim=c(1999, 2009), ylim=c(7,9)) lines(log(maine_divorce_rate*1000)~year, data=vigen_csv) lines(log(science_spending/10)~year, data=vigen_csv, lty=2) axis(1) axis(2) legend(&#39;topright&#39;, lty=c(1,2), legend=c( &#39;log(maine_divorce_rate*1000)&#39;, &#39;log(science_spending/10)&#39;)) Some other great examples par(mfrow=c(1,2), mar=c(2,2,2,1)) plot.new() plot.window(xlim=c(1999, 2009), ylim=c(5,9)*1000) lines(science_spending/3~year, data=vigen_csv, lty=1, col=2, pch=16) text(2003, 8200, &#39;US spending on science, space, technology (USD/3)&#39;, col=2, cex=.6, srt=30) lines(hanging_suicides~year, data=vigen_csv, lty=1, col=4, pch=16) text(2004, 6500, &#39;US Suicides by hanging, strangulation, suffocation (Deaths)&#39;, col=4, cex=.6, srt=30) axis(1) axis(2) plot.new() plot.window(xlim=c(2002, 2009), ylim=c(0,5)) lines(cage_films~year, data=vigen_csv[vigen_csv$year&gt;=2002,], lty=1, col=2, pch=16) text(2006, 0.5, &#39;Number of films with Nicolas Cage (Films)&#39;, col=2, cex=.6, srt=0) lines(pool_fall_drownings/25~year, data=vigen_csv[vigen_csv$year&gt;=2002,], lty=1, col=4, pch=16) text(2006, 4.5, &#39;Number of drownings by falling into pool (US Deaths/25)&#39;, col=4, cex=.6, srt=0) axis(1) axis(2) # Include an intercept to regression 1 #reg2 &lt;- lm(cage_films ~ science_spending, data=vigen_csv) #suppressMessages(library(stargazer)) #stargazer(reg1, reg2, type=&#39;html&#39;) The same principles apply to regression-based approaches to endogeneity issues. For example, we now run IV regressions for different variable combinations in the dataset of spurious relationships knames &lt;- names(vigen_csv)[2:11] # First 10 Variables #knames &lt;- names(vigen_csv)[-1] # Try All Variables p &lt;- 1 ii &lt;- 1 ivreg_list &lt;- vector(&quot;list&quot;, factorial(length(knames))/factorial(length(knames)-3)) # Choose 3 variable for( k1 in knames){ for( k2 in setdiff(knames,k1)){ for( k3 in setdiff(knames,c(k1,k2)) ){ X1 &lt;- vigen_csv[,k1] X2 &lt;- vigen_csv[,k2] X3 &lt;- vigen_csv[,k3] # Merge and `Analyze&#39; dat_i &lt;- na.omit(data.frame(X1,X2,X3)) ivreg_i &lt;- feols(X1~1|X2~X3, data=dat_i) ivreg_list[[ii]] &lt;- list(ivreg_i, c(k1,k2,k3)) ii &lt;- ii+1 }}} pvals &lt;- sapply(ivreg_list, function(ivreg_i){ivreg_i[[1]]$coeftable[2,4]}) plot(ecdf(pvals), xlab=&#39;p-value&#39;, ylab=&#39;CDF&#39;, font.main=1, main=&#39;Frequency IV is Statistically Significant&#39;) abline(v=c(.01,.05), col=c(2,4)) # Most Significant Spurious Combinations pvars &lt;- sapply(ivreg_list, function(ivreg_i){ivreg_i[[2]]}) pdat &lt;- data.frame(t(pvars), pvals) pdat &lt;- pdat[order(pdat$pvals),] head(pdat) ## X1 X2 X3 pvals ## 4 science_spending hanging_suicides bed_deaths 3.049883e-08 ## 76 hanging_suicides science_spending bed_deaths 3.049883e-08 ## 3 science_spending hanging_suicides cheese_percap 3.344890e-08 ## 75 hanging_suicides science_spending cheese_percap 3.344890e-08 ## 485 maine_divorce_rate margarine_percap cheese_percap 3.997738e-08 ## 557 margarine_percap maine_divorce_rate cheese_percap 3.997738e-08 For more intuition on spurious correlations, try http://shiny.calpoly.sh/Corr_Reg_Game/ 14.4 Spurious Causal Impacts In practice, it is hard to find a good natural experiment. For example, suppose we asked “what is the effect of wages on police demanded?” and examined a policy which lowered the educational requirements from 4 years to 2 to become an officer. This increases the labour supply, but it also affects the demand curve through “general equilibrium”: as some of the new officers were potentially criminals. With fewer criminals, the demand for likely police shifts down. In practice, it is surprisingly easy to find a bad instrument. Paradoxically, natural experiments are something you are supposed to find but never search for. As you search for good instruments, for example, sometimes random noise will appear like a good instrument (Spurious instruments). Worse, if you search for a good instrument for too long, you can also be led astray from important questions. We apply the three major credible methods (IV, RDD, DID) to random walks. Each time, we find a result that fits mold and add various extensions that make it appear robust. One could tell a story about how \\(X_{2}\\) affects \\(X_{1}\\) but \\(X_{1}\\) might also affect \\(X_{2}\\), and how they discovered an instrument \\(X_{3}\\) to provide the first causal estimate of \\(X_{2}\\) on \\(X_{1}\\). The analysis looks scientific and the story sounds plausible, so you could probably be convinced if it were not just random noise. n &lt;- 1000 n_index &lt;- seq(n) set.seed(1) random_walk1 &lt;- cumsum(runif(n,-1,1)) set.seed(2) random_walk2 &lt;- cumsum(runif(n,-1,1)) par(mfrow=c(1,2)) plot(random_walk1, pch=16, col=rgb(1,0,0,.25), xlab=&#39;Time&#39;, ylab=&#39;Random Value&#39;) plot(random_walk2, pch=16, col=rgb(0,0,1,.25), xlab=&#39;Time&#39;, ylab=&#39;Random Value&#39;) IV. First, find an instrument that satisfy various statistical criterion to provide a causal estimate of \\(X_{2}\\) on \\(X_{1}\\). # &quot;Find&quot; &quot;valid&quot; ingredients library(fixest) random_walk3 &lt;- cumsum(runif(n,-1,1)) dat_i &lt;- data.frame( X1=random_walk1, X2=random_walk2, X3=random_walk3) ivreg_i &lt;- feols(X1~1|X2~X3, data=dat_i) summary(ivreg_i) ## TSLS estimation - Dep. Var.: X1 ## Endo. : X2 ## Instr. : X3 ## Second stage: Dep. Var.: X1 ## Observations: 1,000 ## Standard-errors: IID ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.53309 1.644285 5.18954 2.5533e-07 *** ## fit_X2 1.79901 0.472285 3.80916 1.4796e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## RMSE: 6.25733 Adj. R2: -1.29152 ## F-test (1st stage), X2: stat = 10.8, p = 0.001048, on 1 and 998 DoF. ## Wu-Hausman: stat = 23.4, p = 1.518e-6, on 1 and 997 DoF. # After experimenting with different instruments # you can find even stronger results! RDD. Second, find a large discrete change in the data that you can associate with a policy. You can use this as an instrument too, also providing a causal estimate of \\(X_{2}\\) on \\(X_{1}\\). # Let the data take shape # (around the large differences before and after) n1 &lt;- 290 wind1 &lt;- c(n1-300,n1+300) dat1 &lt;- data.frame(t=n_index, y=random_walk1, d=1*(n_index &gt; n1)) dat1_sub &lt;- dat1[ n_index&gt;wind1[1] &amp; n_index &lt; wind1[2],] # Then find your big break reg0 &lt;- lm(y~t, data=dat1_sub[dat1_sub$d==0,]) reg1 &lt;- lm(y~t, data=dat1_sub[dat1_sub$d==1,]) # The evidence should show openly (it&#39;s just science) plot(random_walk1, pch=16, col=rgb(0,0,1,.25), xlim=wind1, xlab=&#39;Time&#39;, ylab=&#39;Random Value&#39;) abline(v=n1, lty=2) lines(reg0$model$t, reg0$fitted.values, col=1) lines(reg1$model$t, reg1$fitted.values, col=1) # Dress with some statistics for added credibility rdd_sub &lt;- lm(y~d+t+d*t, data=dat1_sub) rdd_full &lt;- lm(y~d+t+d*t, data=dat1) stargazer::stargazer(rdd_sub, rdd_full, type=&#39;html&#39;, title=&#39;Recipe RDD&#39;, header=F, omit=c(&#39;Constant&#39;), notes=c(&#39;First column uses a dataset around the discontinuity.&#39;, &#39;Smaller windows are more causal, and where the effect is bigger.&#39;)) Recipe RDD Dependent variable: y (1) (2) d -13.169*** -9.639*** (0.569) (0.527) t 0.011*** 0.011*** (0.001) (0.002) d:t 0.009*** 0.004* (0.002) (0.002) Observations 589 1,000 R2 0.771 0.447 Adjusted R2 0.770 0.446 Residual Std. Error 1.764 (df = 585) 3.081 (df = 996) F Statistic 658.281*** (df = 3; 585) 268.763*** (df = 3; 996) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 First column uses a dataset around the discontinuity. Smaller windows are more causal, and where the effect is bigger. DID. Third, find a change in the data that you can associate with a policy where the control group has parallel trends. This also provides a causal estimate of \\(X_{2}\\) on \\(X_{1}\\). # Find a reversal of fortune # (A good story always goes well with a nice pre-trend) n2 &lt;- 318 wind2 &lt;- c(n2-20,n2+20) plot(random_walk2, pch=16, col=rgb(0,0,1,.5), xlim=wind2, ylim=c(-15,15), xlab=&#39;Time&#39;, ylab=&#39;Random Value&#39;) points(random_walk1, pch=16, col=rgb(1,0,0,.5)) abline(v=n2, lty=2) # Knead out any effects that are non-causal (aka correlation) dat2A &lt;- data.frame(t=n_index, y=random_walk1, d=1*(n_index &gt; n2), RWid=1) dat2B &lt;- data.frame(t=n_index, y=random_walk2, d=0, RWid=2) dat2 &lt;- rbind(dat2A, dat2B) dat2$RWid &lt;- as.factor(dat2$RWid) dat2$tid &lt;- as.factor(dat2$t) dat2_sub &lt;- dat2[ dat2$t&gt;wind2[1] &amp; dat2$t &lt; wind2[2],] # Report the stars for all to enjoy # (what about the intercept?) # (stable coefficients are the good ones?) did_fe1 &lt;- lm(y~d+tid, data=dat2_sub) did_fe2 &lt;- lm(y~d+RWid, data=dat2_sub) did_fe3 &lt;- lm(y~d*RWid+tid, data=dat2_sub) stargazer::stargazer(did_fe1, did_fe2, did_fe3, type=&#39;html&#39;, title=&#39;Recipe DID&#39;, header=F, omit=c(&#39;tid&#39;,&#39;RWid&#39;, &#39;Constant&#39;), notes=c( &#39;Fixed effects for time in column 1, for id in column 2, and both in column 3.&#39;, &#39;Fixed effects control for most of your concerns.&#39;, &#39;Anything else creates a bias in the opposite direction.&#39;)) Recipe DID Dependent variable: y (1) (2) (3) d 1.804* 1.847*** 5.851*** (0.892) (0.652) (0.828) Observations 78 78 78 R2 0.227 0.164 0.668 Adjusted R2 -0.566 0.142 0.309 Residual Std. Error 2.750 (df = 38) 2.035 (df = 75) 1.827 (df = 37) F Statistic 0.287 (df = 39; 38) 7.379*** (df = 2; 75) 1.860** (df = 40; 37) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Fixed effects for time in column 1, for id in column 2, and both in column 3. Fixed effects control for most of your concerns. Anything else creates a bias in the opposite direction. "],["why.html", " 15 Why? 15.1 An example workflow 15.2 An alternative workflow 15.3 Task Views", " 15 Why? You should make your work reproducible, and we will cover some of the basics of how to do this in R. You also want your work to be replicable Replicable: someone collecting new data comes to the same results. Reproducibile: someone reusing your data comes to the same results. You can read more about the distinction in many places, including https://www.annualreviews.org/doi/10.1146/annurev-psych-020821-114157 https://nceas.github.io/sasap-training/materials/reproducible_research_in_r_fairbanks/ My main sell to you is that being reproducible is in your own self-interest. 15.1 An example workflow Taking First Steps … Step 1: Some Ideas and Data \\(X_{1} \\to Y_{1}\\) You copy some data into a spreadsheet, manually aggregate do some calculations and tables the same spreadsheet some other analysis from here and there, using this software and that. Step 2: Pursuing the lead for a week or two you extend your dataset with more observations copy in a spreadsheet data, manually aggregate do some more calculations and tables, same as before Then, a Little Way Down the Road … 1 month later, someone asks about another factor: \\(X_{2}\\) you download some other type of data You repeat Step 2 with some data on \\(X_{2}\\). The details from your “point and click” method are a bit fuzzy. It takes a little time, but you successfully redo the analysis. 4 months later, someone asks about another factor: \\(X_{3}\\to Y_{1}\\) You again repeat Step 2 with some data on \\(X_{3}\\). You’re pretty sure none of tables your tried messed up the order of the rows or columns. It takes more time and effort. The data processing was not transparent, but you eventually redo the analysis. 6 months later, you want to explore: \\(X_{2} \\to Y_{2}\\). You found out Excel had some bugs in it’s statistical calculations (see e.g., https://biostat.app.vumc.org/wiki/pub/Main/TheresaScott/StatsInExcel.TAScot.handout.pdf). You now use a new version of the spreadsheet You’re not sure you merged everything correctly. After much time and effort, most (but not all) of the numbers match exactly. 2 years later, you want to replicate: \\(\\{ X_{1}, X_{2}, X_{3} \\} \\to Y_{1}\\) A rival has proposed something new. Their idea doesn’t actually make any sense, but their figures and statistics look better. You don’t even use that computer anymore and a collaborator who handled the data on \\(X_{2}\\) has moved on. 15.2 An alternative workflow Suppose you decided to code what you did beginning with Step 2. It does not take much time to update or replicate your results. Your computer runs for 2 hours and reproduces the figures and tables. You also rewrote your big calculations to use multiple cores, this took two hours to do but saved 6 hours each time you rerun your code. You add some more data. It adds almost no time to see whether much has changed. Your results are transparent and easier to build on. You see the exact steps you took and found an error glad you found it before publication! See https://retractionwatch.com/ and https://econjwatch.org/ Google “worst excell errors” and note the frequency they arise from copy/paste via the “point-and-click” approach. Future economists should also read https://core.ac.uk/download/pdf/300464894.pdf. You try out a new plot you found in The Visual Display of Quantitative Information, by Edward Tufte. It’s not a standard plot, but google answers most of your questions. Tutorials help avoid bad practices, such as plotting 2D data as a 3D object (see e.g., https://clauswilke.com/dataviz/no-3d.html). You try out an obscure statistical approach that’s hot in your field. it doesn’t make the paper, but you have some confidence that candidate issue isn’t a big problem Note that R (and Rmarkdown) is a good choice to address this issue http://www.r-bloggers.com/the-reproducibility-crisis-in-science-and-prospects-for-r/ http://fmwww.bc.edu/GStat/docs/pointclick.html https://github.com/qinwf/awesome-R\\#reproducible-research A Guide to Reproducible Code in Ecology and Evolution https://biostat.app.vumc.org/wiki/pub/Main/TheresaScott/ReproducibleResearch.TAScott.handout.pdf 15.3 Task Views Task views list relevant packages. For all students and early researchers, https://cran.r-project.org/web/views/ReproducibleResearch.html For microeconometrics, https://cran.r-project.org/web/views/Econometrics.html For spatial econometrics https://cran.r-project.org/web/views/Spatial.html https://cran.r-project.org/web/views/SpatioTemporal.html Multiple packages may have the same function name for different commands. In this case use the syntax package::function to specify the package. For example devtools::install_github remotes::install_github Don’t fret Sometimes there is not a specific package for your data. Odds are, you can do most of what you want with base code. Packages just wrap base code in convient formats see https://cran.r-project.org/web/views/ for topical overviews Statisticians might have different naming conventions if the usual software just spits out a nice plot you might have to dig a little to know precisely what you want your data are fundamentally numbers, strings, etc… You only have to figure out how to read it in. "],["large-projects.html", " 16 Large Projects 16.1 Scripting 16.2 Organization 16.3 Logging/Sinking 16.4 Class Projects", " 16 Large Projects As you scale up a project, then you will have to be more organized. 16.1 Scripting Save the following code as MyFirstScript.R # Define New Function sum_squared &lt;- function(x1, x2) { y &lt;- (x1 + x2)^2 return(y) } # Test New Function x &lt;- c(0,1,3,10,6) sum_squared(x[1], x[3]) sum_squared(x, x[2]) sum_squared(x, x[7]) sum_squared(x, x) message(&#39;Script Completed&#39;) Restart Rstudio.8 Replicate in another tab via source(&#39;MyFirstScript.R&#39;) Note that you may first need to setwd() so your computer knows where you saved your code.9 After you get this working: add a the line print(sum_squared(y, y)) to the bottom of MyFirstCode.R. apply the function to a vectors specified outside of that script record the session information # Pass Objects/Functions *to* Script y &lt;- c(3,1,NA) source(&#39;MyFirstScript.R&#39;) # Pass Objects/Functions *from* Script z &lt;- sqrt(y)/2 sum_squared(z,z) # Report all information relevant for replication sessionInfo() Note that you can open a new terminal in RStudio in the top bar by clicking ‘tools &gt; terminal &gt; new terminal’ 16.2 Organization Large sized projects should have their own Project folder on your computer with files, subdirectories with files, and subsubdirectories with files. It should look like this Project └── README.txt └── /Code └── MAKEFILE.R └── RBLOCK_001_DataClean.R └── RBLOCK_002_Figures.R └── RBLOCK_003_ModelsTests.R └── RBLOCK_004_Robust.R └── /Logs └── MAKEFILE.Rout └── /Data └── /Raw └── Source1.csv └── Source2.shp └── Source3.txt └── /Clean └── AllDatasets.Rdata └── MainDataset1.Rds └── MainDataset2.csv └── /Output └── MainFigure.pdf └── AppendixFigure.pdf └── MainTable.tex └── AppendixTable.tex └── /Writing └── /TermPaper └── TermPaper.tex └── TermPaper.bib └── TermPaper.pdf └── /Slides └── Slides.Rmd └── Slides.html └── Slides.pdf └── /Poster └── Poster.Rmd └── Poster.html └── Poster.pdf └── /Proposal └── Proposal.Rmd └── Proposal.html └── Proposal.pdf There are two main meta-files README.txt overviews the project structure and what the codes are doing MAKEFILE explicitly describes and executes all codes (and typically logs the output). MAKEFILE. If all code is written with the same program (such as R) the makefile can be written in a single language. For us, this looks like # Project Structure home_dir &lt;- path.expand(&quot;~/Desktop/Project/&quot;) data_dir_r &lt;- paste0(data_dir, &quot;Data/Raw/&quot;) data_dir_c &lt;- paste0(data_dir, &quot;Data/Clean/&quot;) out_dir &lt;- paste0(hdir, &quot;Output/&quot;) code_dir &lt;- paste0(hdir, &quot;Code/&quot;) # Execute Codes # libraries are loaded within each RBLOCK setwd( code_dir ) source( &quot;RBLOCK_001_DataClean.R&quot; ) source( &quot;RBLOCK_002_Figures.R&quot; ) source( &quot;RBLOCK_003_ModelsTests.R&quot; ) source( &quot;RBLOCK_004_Robust.R&quot; ) # Report all information relevant for replication sessionInfo() Notice there is a lot of documentation # like this, which is crucial for large projects. Also notice that anyone should be able to replicate the entire project by downloading a zip file and simply changing home_dir. If some folders or files need to be created, you can do this within R # list the files and directories list.files(recursive=TRUE, include.dirs=TRUE) # create directory called &#39;Data&#39; dir.create(&#39;Data&#39;) 16.3 Logging/Sinking When executing the makefile, you can also log the output in three different ways: Inserting some code into the makefile that “sinks” the output # Project Structure home_dir &lt;- path.expand(&quot;~/Desktop/Project/&quot;) data_dir_r &lt;- paste0(data_dir, &quot;Data/Raw/&quot;) data_dir_c &lt;- paste0(data_dir, &quot;Data/Clean/&quot;) out_dir &lt;- paste0(hdir, &quot;Output/&quot;) code_dir &lt;- paste0(hdir, &quot;Code/&quot;) # Log Output set.wd( code_dir ) sink(&quot;MAKEFILE.Rout&quot;, append=TRUE, split=TRUE) # Execute Codes source( &quot;RBLOCK_001_DataClean.R&quot; ) source( &quot;RBLOCK_002_Figures.R&quot; ) source( &quot;RBLOCK_003_ModelsTests.R&quot; ) source( &quot;RBLOCK_004_Robust.R&quot; ) sessionInfo() # Stop Logging Output sink() Starting a session that “sinks” the makefile sink(&quot;MAKEFILE.Rout&quot;, append=TRUE, split=TRUE) source(&quot;MAKEFILE.R&quot;) sink() Execute the makefile via the commandline R CMD BATCH MAKEFILE.R MAKEFILE.Rout 16.4 Class Projects Zip your project into a single file that is easy for others to identify: Class_Project_LASTNAME_FIRSTNAME.zip Your code should be readable and error free. For code writing guides, see https://google.github.io/styleguide/Rguide.html https://style.tidyverse.org/ https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/codestyle.html http://adv-r.had.co.nz/Style.html https://www.burns-stat.com/pages/Tutor/R_inferno.pdf For organization guidelines, see https://guides.lib.berkeley.edu/c.php?g=652220&amp;p=4575532 https://kbroman.org/steps2rr/pages/organize.html https://drivendata.github.io/cookiecutter-data-science/ https://ecorepsci.github.io/reproducible-science/project-organization.html For additional logging capabilities, see https://cran.r-project.org/web/packages/logr/ For very large projects, there are many more tools available at https://cran.r-project.org/web/views/ReproducibleResearch.html For larger scale projects, use scripts https://kbroman.org/steps2rr/pages/scripts.html https://kbroman.org/steps2rr/pages/automate.html Alternatively, clean the workspace by 1: clearing the environment and history (use the broom in top right panel). 2: clearing unsaved plots (use the broom in bottom right panel).↩︎ You can also use GUI: point-click click ‘Source &gt; Source as a local job’ on top right↩︎ "],["performance.html", " 17 Performance 17.1 Debugging 17.2 Optimizing 17.3 Advanced Optimizing 17.4 More Literature", " 17 Performance 17.1 Debugging In R, you use multiple functions on different types of data objects. Moreover, you “typically solve complex problems by decomposing them into simple functions, not simple objects.” (H. Wickham) Problems print to the console warning(&quot;This is what a warning looks like&quot;) stop(&quot;This is what an error looks like&quot;) ## Error: This is what an error looks like Nonproblems also print to the console message(&quot;This is what a message looks like&quot;) cat(&#39;cat\\n&#39;) ## cat print(&#39;print&#39;) ## [1] &quot;print&quot; Tracing. Consider this example of an error process (originally taken from https://adv-r.hadley.nz/ ). # Let i() check if its argument is numeric i &lt;- function(i0) { if ( !is.numeric(i0) ) { stop(&quot;`d` must be numeric&quot;, call.=FALSE) } i0 + 10 } # Let f() call g() call h() call i() h &lt;- function(i0) i(i0) g &lt;- function(h0) h(h0) f &lt;- function(g0) g(g0) # Observe Error f(&quot;a&quot;) ## Error: `d` must be numeric First try simple print debugging f2 &lt;- function(g0) { cat(&quot;f2 calls g2()\\n&quot;) g2(g0) } g2 &lt;- function(h0) { cat(&quot;g2 calls h2() \\n&quot;) cat(&quot;b =&quot;, h0, &quot;\\n&quot;) h2(h0) } h2 &lt;- function(i0) { cat(&quot;h2 call i() \\n&quot;) i(i0) } f2(&quot;a&quot;) ## f2 calls g2() ## g2 calls h2() ## b = a ## h2 call i() ## Error: `d` must be numeric If that fails, try traceback debugging traceback() ## No traceback available And if that fails, try an Interactive approach g3 &lt;- function(h0) { browser() h(h0) } f3 &lt;- function(g0){ g3(g0) } f3(&quot;a&quot;) ## Called from: g3(g0) ## debug: h(h0) ## Error: `d` must be numeric Isolating. To inspect objects is.object(f) is.object(c(1,1)) class(f) class(c(1,1)) # Storage Mode Type typeof(f) typeof(c(1,1)) storage.mode(f) storage.mode(c(1,1)) To check for valid inputs/outputs x &lt;- c(NA, NULL, NaN, Inf, 0) cat(&quot;Vector to inspect: &quot;) x cat(&quot;NA: &quot;) is.na(x) cat(&quot;NULL: &quot;) is.null(x) cat(&quot;NaN: &quot;) is.nan(x) cat(&quot;Finite: &quot;) is.finite(x) cat(&quot;Infinite: &quot;) is.infinite(x) # Many others To check for values all( x &gt; -2 ) any( x &gt; -2 ) # Check Matrix Rows rowAny &lt;- function(x) rowSums(x) &gt; 0 rowAll &lt;- function(x) rowSums(x) == ncol(x) Handling. Simplest example x &lt;- &#39;A&#39; tryCatch( expr = log(x), error = function(e) { message(&#39;Caught an error but did not break&#39;) print(e) return(NA) }) Another example x &lt;- -2 tryCatch( expr = log(x), error = function(e) { message(&#39;Caught an error but did not break&#39;) print(e) return(NA) }, warning = function(w){ message(&#39;Caught a warning!&#39;) print(w) return(NA) }, finally = { message(&quot;Returned log(x) if successfull or NA if Error or Warning&quot;) } ) ## &lt;simpleWarning in log(x): NaNs produced&gt; ## [1] NA Safe Functions # Define log_safe &lt;- function(x){ lnx &lt;- tryCatch( expr = log(x), error = function(e){ cat(&#39;Error Caught: \\n\\t&#39;) print(e) return(NA) }, warning = function(w){ cat(&#39;Warning Caught: \\n\\t&#39;) print(w) return(NA) }) return(lnx) } # Test log_safe( 1) ## [1] 0 log_safe(-1) ## Warning Caught: ## &lt;simpleWarning in log(x): NaNs produced&gt; ## [1] NA log_safe(&#39; &#39;) ## Error Caught: ## &lt;simpleError in log(x): non-numeric argument to mathematical function&gt; ## [1] NA # Further Tests s &lt;- sapply(list(&quot;A&quot;,Inf, -Inf, NA, NaN, 0, -1, 1), log_safe) ## Error Caught: ## &lt;simpleError in log(x): non-numeric argument to mathematical function&gt; ## Warning Caught: ## &lt;simpleWarning in log(x): NaNs produced&gt; ## Warning Caught: ## &lt;simpleWarning in log(x): NaNs produced&gt; s ## [1] NA Inf NA NA NaN -Inf NA 0 17.2 Optimizing In General, clean code is faster and less error prone. By optimizing repetitive tasks, you end up with code that is cleaner, faster, and more general can be easily parallelized So, after identifying a bottleneck, try vectorize your code use a dedicated package use parallel computations compile your code in C++ But remember Don’t waste time optimizing code that is not holding you back. Look at what has already done. Benchmarking. For identifying bottlenecks, the simplest approach is to time how long a code-chunk runs system.time({ x0 &lt;- runif(1e5) x1 &lt;- sqrt(x0) x2 &lt;- paste0(&#39;J-&#39;, x1) }) ## user system elapsed ## 0.130 0.008 0.139 You can visually identify bottlenecks in larger blocks # Generate Large Random Dataset n &lt;- 2e6 x &lt;- runif(n) y &lt;- runif(n) z &lt;- runif(n) XYZ &lt;- cbind(x,y,z) # Inspect 4 equivalent `row mean` calculations profvis::profvis({ m &lt;- rowSums(XYZ)/ncol(XYZ) m &lt;- rowMeans(XYZ) m &lt;- apply(XYZ, 1, mean) m &lt;- rep(NA, n); for(i in 1:n){ m[i] &lt;- (x[i] + y[i] + z[i]) / 3 } }) # rowSums(), colSums(), rowMeans(), and colMeans() are vectorised and fast. # for loop is not the slowest, but the ugliest. For systematic speed comparisons, try a benchmarking package # 3 Equivalent calculations of the mean of a vector mean1 &lt;- function(x,p=1) mean(x^p) mean2 &lt;- function(x,p=1) sum(x^p) / length(x) mean3 &lt;- function(x,p=1) mean.default(x^p) # Time them x &lt;- runif(1e6) microbenchmark::microbenchmark( mean1(x,.5), mean2(x,.5), mean3(x,.5), times=20 ) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## mean1(x, 0.5) 21.22159 24.05092 27.19083 26.93219 29.28603 38.83637 20 a ## mean2(x, 0.5) 22.64585 23.40534 25.91066 24.89418 27.29905 35.28588 20 a ## mean3(x, 0.5) 21.49653 24.04798 26.32646 25.70899 29.43275 31.42388 20 a # Time them (w/ memory) bench::mark( mean1(x,.5), mean2(x,.5), mean3(x,.5), iterations=20 ) ## # A tibble: 3 × 6 ## expression min median `itr/sec` mem_alloc `gc/sec` ## &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt; &lt;dbl&gt; &lt;bch:byt&gt; &lt;dbl&gt; ## 1 mean1(x, 0.5) 20.8ms 22.7ms 44.7 7.63MB 0 ## 2 mean2(x, 0.5) 19.4ms 22ms 47.2 7.63MB 2.49 ## 3 mean3(x, 0.5) 20.8ms 23.1ms 44.2 7.63MB 2.33 Vectorize. Computers are really good at math, so exploit this. First try vectors Then try apply functions See https://uscbiostats.github.io/software-dev-site/handbook-slow-patterns.html Vector operations are generally faster and easier to read than loops # Compare 2 moving averages x &lt;- runif(2e6) # First Try ma1 &lt;- function(y){ z &lt;- y*NA for(i in 2:length(y)){ z[i] &lt;- (y[i]-y[i-1])/2 } return(z) } # Optimized using diff diff( c(2,2,10,9) ) ## [1] 0 8 -1 ma2 &lt;- function(y){ z2 &lt;- diff(y)/2 z2 &lt;- c(NA, z2) return(z2) } all.equal(ma1(x),ma2(x)) ## [1] TRUE system.time( ma1(x) ) ## user system elapsed ## 0.153 0.000 0.154 system.time( ma2(x) ) ## user system elapsed ## 0.018 0.011 0.029 ma3 &lt;- compiler::cmpfun(ma2) system.time( ma3(x) ) ## user system elapsed ## 0.02 0.00 0.02 Likewise, matrix operations are often faster than vector operations. Memory Usage. For finding problematic blocks utils::Rprof(memory.profiling = TRUE) logs total memory usage of R at regular time intervals. E.g. Rprof( interval = 0.005) # Create Data x &lt;- runif(2e6) y &lt;- sqrt(x) # Loop Format Data z &lt;- y*NA for(i in 2:length(y)){ z[i] &lt;- (y[i]-y[i-1])/2 } # Regression X &lt;- cbind(1,x)[-1,] Z &lt;- z[-1] reg_fast &lt;- .lm.fit(X, Z) Rprof(NULL) summaryRprof() For finding problematic functions: utils::Rprofmem() logs memory usage at each call For finding problematic scripts, see “Advanced Optimization”. (With Rprof, you can identifying bottlenecks on a cluster without a GUI.) Packages. Before creating your own program, check if there is a faster or more memory efficient version. E.g., data.table or Rfast2 for basic data manipulation. Some functions are simply wrappers for the function you want, and calling it directly can speed things up. X &lt;- cbind(1, runif(1e6)) Y &lt;- X %*% c(1,2) + rnorm(1e6) DAT &lt;- as.data.frame(cbind(Y,X)) system.time({ lm(Y~X, data=DAT) }) ## user system elapsed ## 0.550 0.010 0.179 system.time({ .lm.fit(X, Y) }) ## user system elapsed ## 0.104 0.000 0.030 system.time({ solve(t(X)%*%X) %*% (t(X)%*%Y) }) ## user system elapsed ## 0.017 0.000 0.015 Note that such functions to have fewer checks and return less information, so you must know exactly what you are putting in and getting out. Parallel. Sometimes there will still be a problematic bottleneck. Your next step should be parallelism: Write the function as a general vectorized function. Apply the same function to every element in a list at the same time # lapply in parallel on {m}ultiple {c}ores x &lt;- c(10,20,30,40,50) f &lt;- function(element) { element^element } parallel::mclapply( x, mc.cores=2, FUN=f) ## [[1]] ## [1] 1e+10 ## ## [[2]] ## [1] 1.048576e+26 ## ## [[3]] ## [1] 2.058911e+44 ## ## [[4]] ## [1] 1.208926e+64 ## ## [[5]] ## [1] 8.881784e+84 More power is often not the solution # vectorize and compile e_power_e_fun &lt;- compiler::cmpfun( function(vector){ vector^vector} ) # base R x &lt;- 0:1E6 s_vc &lt;- system.time( e_power_e_vec &lt;- e_power_e_fun(x) ) s_vc ## user system elapsed ## 0.020 0.001 0.021 # brute power x &lt;- 0:1E6 s_bp &lt;- system.time({ e_power_e_mc &lt;- unlist( parallel::mclapply(x, mc.cores=2, FUN=e_power_e_fun)) }) s_bp ## user system elapsed ## 1.186 0.204 0.878 # Same results all(e_power_e_vec==e_power_e_mc) ## [1] TRUE Note that parallelism does not go well with a GUI. 17.3 Advanced Optimizing If you still are stuck, you can try Amazon Web Server for more brute-power rewrite bottlenecks with a working C++ compiler or Fortran compiler. Before doing that, however, look into https://cran.r-project.org/web/views/HighPerformanceComputing.html In what follows, note that there are alternative ways to run R via the command line. For example, # Method 1 R -e &quot;source(&#39;MyFirstScript.R&#39;)&quot; # Method 2 R CMD BATCH MyFirstScript.R Cluster Computing For parallel computations on a computer cluster, you will need to use both R and the linux command line. R --slave -e &#39;1:10&#39; R --slave -e &#39; 1:10 seq(0,1,by=.2) paste(c(&quot;A&quot;,&quot;D&quot;), 1:2) &#39; ## [1] 1 2 3 4 5 6 7 8 9 10 ## [1] 1 2 3 4 5 6 7 8 9 10 ## [1] 0.0 0.2 0.4 0.6 0.8 1.0 ## [1] &quot;A 1&quot; &quot;D 2&quot; R --slave -e &#39; .libPaths(&quot;~/R-Libs&quot;) options(repos=&quot;https://cloud.r-project.org/&quot;) update.packages(ask=F) suppressPackageStartupMessages(library(stargazer)) &#39; You will often want to rerun entire scripts with a different parameter. To do so, you need to edit your R scripts to accept parameters from the command line args &lt;- commandArgs(TRUE) For example R --slave -e &#39; args &lt;- commandArgs(TRUE) paste0(1, args) &#39; --args a b c R --slave -e &#39; args &lt;- commandArgs(TRUE) my_numbers &lt;- as.numeric(args) my_numbers + 1 &#39; --args $(seq 1 10) ## [1] &quot;1a&quot; &quot;1b&quot; &quot;1c&quot; ## [1] 2 3 4 5 6 7 8 9 10 11 You can also store the output and computer resources of a script. For example, save the last script as RBLOCK.R in the folder $HOME/R_Code and run the following # Which Code RDIR=$HOME/R_Code #main directory infile=$RDIR/RBLOCK.R #specific code outfile=$RDIR/R_Logs/RBLOCK$design_N.Rout #log R output memfile=$RDIR/R_Logs/RBLOCK$design_N.Rtime #log computer resources # Execute the Script and store resource useage via `time` command time -o $memfile -v \\ R CMD BATCH --no-save --quiet --no-restore &quot;--args 1 2 3&quot; $infile $outfile Note that you need to have https://ftp.gnu.org/gnu/time installed On an academic computing cluster, you may have to use a scheduler like slurm. In which case you can submit a bash script sbatch --mem 10GB --time=0-01:00:00 -a 50 SlurmJob.sh where SlurmJob.sh looks like #!/bin/bash #SBATCH --ntasks=1 #SBATCH --error=$HOME/R_Code/Slurm_Logs/%x.error-%j #SBATCH --output=$HOME/R_Code/Slurm_Logs/%x.out-%j # Which Code RDIR=$HOME/R_Code infile=$RDIR/RBLOCK.R outfile=$RDIR/R_Logs/RBLOCK$design_N.Rout memfile=$RDIR/R_Logs/RBLOCK$design_N.Rtime # Which Parameter a=&quot;${SLURM_ARRAY_TASK_ID}&quot; # Execute the Script with a specific parameter, and store memory/time useage command time -o $memfile -v \\ R CMD BATCH --no-save --quiet --no-restore &quot;--args $a&quot; $infile $outfile # Summarize memory/time useage echo &quot;design_N: $design_N Gridpoints&quot; cat $memfile | awk &#39;/User time/ {printf &quot;Time: %.2f Hours\\n&quot;, $4/3600}&#39; cat $memfile | awk &#39;/Maximum resident set size/ {printf &quot;Memory: %.2f GB\\n&quot;, $6/1048576}&#39; echo &quot;Partition: $SLURM_JOB_PARTITION&quot; Compiled Code. You can use C++ code within R to speed up a specific chunk. To get C++ on your computer On Windows, install Rtools. On Mac, install Xcode from the app store. On Linux, sudo apt-get install r-base-dev or similar. To call C++ from R use package Rcpp Rcpp::cppFunction(&#39; int add(int x, int y, int z) { int sum = x + y + z; return sum; }&#39; ) add(1, 2, 3) ## [1] 6 For help getting started with Rcpp, see https://cran.r-project.org/web/packages/Rcpp/vignettes/Rcpp-quickref.pdf First try to use C++ (or Fortran) code that others have written .C .Fortran For a tutorial, see https://masuday.github.io/fortran_tutorial/r.html 17.4 More Literature Advanced Programming https://rmd4sci.njtierney.com/ https://smac-group.github.io/ds/high-performance-computing.html https://www.stat.umn.edu/geyer/3701/notes/arithmetic.Rmd For debugging tips https://cran.r-project.org/doc/manuals/R-lang.html#Debugging https://cran.r-project.org/doc/manuals/R-exts.html#Debugging https://adv-r.hadley.nz/debugging.html https://adv-r.hadley.nz/conditions.html https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/debugging.html https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/functions.html For optimization tips https://cran.r-project.org/doc/manuals/R-exts.html#Tidying-and-profiling-R-code https://cran.r-project.org/doc/manuals/R-lang.html#Exception-handling https://adv-r.hadley.nz/perf-measure.html.libPaths() https://adv-r.hadley.nz/perf-improve.html https://cran.r-project.org/doc/manuals/R-exts.html#System-and-foreign-language-interfaces https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/profiling.html https://adv-r.hadley.nz/rcpp.html https://bookdown.dongzhuoer.com/hadley/adv-r/ For parallel programming https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/parallel.html https://bookdown.org/rdpeng/rprogdatascience/parallel-computation.html https://grantmcdermott.com/ds4e/parallel.html https://psu-psychology.github.io/r-bootcamp-2018/talks/parallel_r.html For general tips https://github.com/christophergandrud/Rep-Res-Book Efficient R programming. C. Gillespie R. Lovelace. 2021. https://csgillespie.github.io/efficientR/ Data Science at the Command Line, 1e. Janssens J. 2020. https://www.datascienceatthecommandline.com/1e/ R Programming for Data Science. Peng R. 2020. https://bookdown.org/rdpeng/rprogdatascience/ Advanced R. H. Wickham 2019. https://adv-r.hadley.nz/ Econometrics in R. Grant Farnsworth. 2008. http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf The R Inferno. https://www.burns-stat.com/documents/books/the-r-inferno/ "],["applications.html", " 18 Applications 18.1 More Literature", " 18 Applications Shiny is an R package to build web applications. Shiny Flexdashboards are nicely formatted Shiny Apps. While it is possible to use Shiny without the Flexdashboard formatting, I think it is easier to remember .R files are codes for statistical analysis .Rmd files are for communicating: reports, slides, posters, and apps Example: Histogram. Download the source file TrialApp1_Histogram_Dashboard.Rmd and open it with rstudio. Then run it with rmarkdown::run(&#39;TrialApp1_Histogram_Dashboard.Rmd&#39;) Within the app, experiment with how larger sample sizes change the distribution. Edit the app to let the user specify the number of breaks in the histogram. If you are having difficulty, you can try working first with the barebones shiny code. To do this, download TrialApp0_Histogram.Rmd and edit it in Rstudio. You can run the code with rmarkdown::run('TrialApp0_Histogram.Rmd'). 18.1 More Literature Overview https://bookdown.org/yihui/rmarkdown/shiny-documents.html https://shiny.rstudio.com/tutorial/ https://shiny.rstudio.com/articles/ https://shiny.rstudio.com/gallery/ https://rstudio.github.io/leaflet/shiny.html https://mastering-shiny.org/ More Help with Shiny Apps https://shiny.rstudio.com/tutorial/written-tutorial/lesson1/ https://mastering-shiny.org/basic-app.html https://towardsdatascience.com/beginners-guide-to-creating-an-r-shiny-app-1664387d95b3 https://shiny.rstudio.com/articles/interactive-docs.html https://bookdown.org/yihui/rmarkdown/shiny-documents.html https://shiny.rstudio.com/gallery/plot-interaction-basic.html https://www.brodrigues.co/blog/2021-03-02-no_shiny_dashboard/ https://bookdown.org/yihui/rmarkdown/shiny.html https://shinyserv.es/shiny/ https://bookdown.org/egarpor/NP-UC3M/kre-i-kre.html#fig:kreg https://engineering-shiny.org/ "],["software.html", " 19 Software 19.1 Latest versions 19.2 General Workflow 19.3 Sweave 19.4 Stata", " 19 Software The current version of R (and any packages) used to make this document are sessionInfo() ## R version 4.4.3 (2025-02-28) ## Platform: x86_64-redhat-linux-gnu ## Running under: Fedora Linux 42 (Workstation Edition) ## ## Matrix products: default ## BLAS/LAPACK: FlexiBLAS OPENBLAS-OPENMP; LAPACK version 3.12.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## time zone: Europe/Berlin ## tzcode source: system (glibc) ## ## attached base packages: ## [1] stats graphics grDevices utils datasets compiler methods ## [8] base ## ## other attached packages: ## [1] colorout_1.3-2 ## ## loaded via a namespace (and not attached): ## [1] Matrix_1.7-3 jsonlite_2.0.0 Rcpp_1.0.14 ## [4] parallel_4.4.3 jquerylib_0.1.4 splines_4.4.3 ## [7] yaml_2.3.10 fastmap_1.2.0 lattice_0.22-7 ## [10] TH.data_1.1-3 R6_2.6.1 microbenchmark_1.5.0 ## [13] knitr_1.50 htmlwidgets_1.6.4 MASS_7.3-65 ## [16] tibble_3.2.1 bookdown_0.43 profvis_0.4.0 ## [19] bslib_0.9.0 pillar_1.10.2 rlang_1.1.6 ## [22] utf8_1.2.4 multcomp_1.4-28 cachem_1.1.0 ## [25] xfun_0.52 sass_0.4.10 cli_3.6.4 ## [28] magrittr_2.0.3 digest_0.6.37 grid_4.4.3 ## [31] mvtnorm_1.3-3 sandwich_3.1-1 lifecycle_1.0.4 ## [34] vctrs_0.6.5 bench_1.1.4 evaluate_1.0.3 ## [37] glue_1.8.0 codetools_0.2-20 zoo_1.8-14 ## [40] survival_3.8-3 profmem_0.6.0 rmarkdown_2.29 ## [43] tools_4.4.3 pkgconfig_2.0.3 htmltools_0.5.8.1 With Rstudio, you can update both R and Rstudio. 19.1 Latest versions Make sure R is up to date. Make sure your R packages are up to date. update.packages() After updating R, you can update all packages stored in all .libPaths() with the following command update.packages(checkBuilt=T, ask=F) # install.packages(old.packages(checkBuilt=T)[,&quot;Package&quot;]) Used Rarely: To find specific broken packages after an update library(purrr) set_names(.libPaths()) %&gt;% map(function(lib) { .packages(all.available = TRUE, lib.loc = lib) %&gt;% keep(function(pkg) { f &lt;- system.file(&#39;Meta&#39;, &#39;package.rds&#39;, package = pkg, lib.loc = lib) tryCatch({readRDS(f); FALSE}, error = function(e) TRUE) }) }) # https://stackoverflow.com/questions/31935516/installing-r-packages-error-in-readrdsfile-error-reading-from-connection/55997765 To remove packages duplicated in multiple libraries # Libraries i &lt;- installed.packages() libs &lt;- .libPaths() # Find Duplicated Packages i1 &lt;- i[ i[,&#39;LibPath&#39;]==libs[1], ] i2 &lt;- i[ i[,&#39;LibPath&#39;]==libs[2], ] dups &lt;- i2[,&#39;Package&#39;] %in% i1[,&#39;Package&#39;] all( dups ) # Remove remove.packages( i2[,&#39;Package&#39;], libs[2] ) 19.2 General Workflow If you want to go further down the reproducibility route (recommended, but not required for our class), consider making your entire workflow use Free Open Source Software Linux: An alternative to windows and mac operating systems. Used in computing clusters, big labs, and phones. E.g., Ubuntu and Fedora are popular brands https://www.r-bloggers.com/linux-data-science-virtual-machine-new-and-upgraded-tools/, http://www.howtogeek.com/249966/how-to-install-and-use-the-linux-bash-shell-on-windows-10/ On Fedora, you can open RStudio on the commandline with rstudio Alternatively, you are encouraged to try using R without a GUI. E.g., on Fedora, this document can be created directly via Rscript -e &quot;rmarkdown::render(&#39;RMarkown.Rmd&#39;)&quot; Latex: An alternative to Microsoft Word. Great for writing many equations and typesetting. Easy to integrate Figures, Tables, and References. Steep learning curve. easiest to get started online with Overleaf can also download yourself via Tex Live and GUI TexStudio To begin programming, see https://biostat.app.vumc.org/wiki/pub/Main/TheresaScott/Intro.to.LaTeX.TAScott.pdf https://www.tug.org/begin.html 19.3 Sweave Knitr: You can produce a pdf from an .Rnw file via knitr Rscript -e &quot;knitr::Sweave2knitr(&#39;Sweave_file.Rnw&#39;)&quot; Rscript -e &quot;knitr::knit2pdf(&#39;Sweave_file-knitr.Rnw&#39;)&quot; For background on knitr https://yihui.org/knitr/ https://kbroman.org/knitr_knutshell/pages/latex.html https://sachsmc.github.io/knit-git-markr-guide/knitr/knit.html Sweave: is an alternative to Rmarkdown for integrating latex and R. While Rmarkdown “writes R and latex within markdown”, Sweave “write R in latex”. Sweave files end in “.Rnw” and can be called within R Sweave(&#39;Sweavefile.Rnw&#39;) or directly from the command line R CMD Sweave Sweavefile.Rnw In both cases, a latex file Sweavefile.tex is produced, which can then be converted to Sweavefile.pdf. For more on Sweave, https://rpubs.com/YaRrr/SweaveIntro https://support.rstudio.com/hc/en-us/articles/200552056-Using-Sweave-and-knitr https://www.statistik.lmu.de/~leisch/Sweave/Sweave-manual.pdf 19.4 Stata For those transitioning from Stata or replicating others’ Stata work, you can work with Stata data and code within R. Translations of common procedures is provided by https://stata2r.github.io/. See also the textbook “R for Stata Users” by Robert A. Muenchen and Joseph M. Hilbe. Many packages allows you to read data created by different programs. As of right now, haven is a particularly useful for reading in Stata files library(haven) read_dta() # See also foreign::read.dta You can also execute stata commands directly in R via package Rstata. (Last time I checked, Rstata requires you to have purchased a non-student version of Stata.) Moreover, you can include stata in the markdown reports via package Statamarkdown: library(Rstata) library(Statamarkdown) There are many R packages to replicate or otherwise directly copy what Stata does. For example, see the margins package https://cran.r-project.org/web/packages/margins/vignettes/Introduction.html For more information on R and Stata, see https://github.com/lbraglia/RStata https://ignacioriveros1.github.io/r/2020/03/22/r_and_stata.html https://bookdown.org/yihui/rmarkdown-cookbook/eng-stata.html https://rpubs.com/quarcs-lab/stata-from-Rstudio https://clanfear.github.io/Stata_R_Equivalency/docs/r_stata_commands.html https://libguides.bates.edu/c.php?g=209169&amp;p=7233333 You can also use other software (such as Python) within R. You can also use R within Stata, or both within Python. With R, you can easily import many different data types https://cran.r-project.org/doc/manuals/R-data.html https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-import.pdf "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
