[["index.html", " 1 Preface", " 1 Preface This Rbook contains three primers that should get you started with econometrics in R Part I: Programming in R Part II: Linear Regression in R Part III: Reproducible Research in R Compared to other introductory econometrics books, this Rbook focuses more on simulations. This makes it easy to absorb basic statistical ideas without parametric assumptions and formulas. This Rbook also includes much programming materials, including how to analyze data interactively and communicate results. So, in many ways, it is a modern version of “Introductory Econometrics: Using Monte Carlo Simulation with Microsoft Excel” by Barreto and Howland, updated to give students the best tools for their labor market and adhere to modern statistics teaching guidelines. This Rbook also deemphasizes inference with linear models, treating it more as a technical prerequisite for more advanced courses. (We operate under the maxim “All models are wrong” and do not prove unbiasedness.) There is also a novel chapter on “Data scientism” that more clearly illustrates the ways that simplistic approaches can mislead rather than illuminate. Overall, their is a humble view towards what we can infer from statistics alone and hence more room for economic theory in model development and interpretation. Much material was originally copied from elsewhere and then modified over the years on an ad-hoc basis. Parts I and III synthesize a lot of programming guidance and examples available on the internet, and I include online references when I could (typically at the end of a chapter). Much of Part II originally came from other econometrics textbooks with a parametric or structural focus. In particular, I learned from and still recommend “Introduction to Econometrics” by Stock and Watson (Parts 1,2, and 3) and “Applied Econometrics” by Asterlou (Parts I,II, and III). Although any interested reader may find it useful, it is being primarily developed for my students. Please report any errors or issues at https://github.com/Jadamso/Rbooks/issues. Last updated: 02.04.2024 "],["first-steps.html", " 2 First Steps 2.1 Why R 2.2 Install R 2.3 Interfacing with R", " 2 First Steps 2.1 Why R We focus on R because it is good for complex stats, concise figures, and coherent organization. It is built and developed by applied statisticians for statistics, and used by many in academia and industry. For students, think about labor demand and what may be good for getting a job. Do some of your own research to best understand how much to invest. 2.2 Install R First Install R. Then Install Rstudio. For help setting up https://learnr-examples.shinyapps.io/ex-setup-r/ https://rstudio-education.github.io/hopr/starting.html https://a-little-book-of-r-for-bioinformatics.readthedocs.io/en/latest/src/installr.html https://cran.r-project.org/doc/manuals/R-admin.html https://courses.edx.org/courses/UTAustinX/UT.7.01x/3T2014/56c5437b88fa43cf828bff5371c6a924/ https://owi.usgs.gov/R/training-curriculum/installr/ https://www.earthdatascience.org/courses/earth-analytics/document-your-science/setup-r-rstudio/ Make sure you have the latest version of R and Rstudio for class. If not, then reinstall. 2.3 Interfacing with R Rstudio is easiest to get going with. (There are other GUI’s.) There are 4 panes. The top left is where you write and save code Create and save a new R Script file My_First_Script.R could also use a plain .txt file. The pane below is where your code is executed. For all following examples, make sure to both execute and store your code. Note that the coded examples generally have inputs, outputs, and comments. For example, ## This is a comment CodeInput &lt;- c(&#39;output looks like this&#39;) CodeInput ## [1] &quot;output looks like this&quot; "],["mathematics.html", " 3 Mathematics 3.1 Scalars 3.2 Vectors 3.3 Functions 3.4 Matrices 3.5 Arrays", " 3 Mathematics 3.1 Scalars xs &lt;- 2 ## Your first scalar xs ## Print the scalar ## [1] 2 (xs+1)^2 ## Perform and print a simple calculation ## [1] 9 xs + NA ## often used for missing values ## [1] NA xs*2 ## [1] 4 3.2 Vectors x &lt;- c(0,1,3,10,6) ## Your First Vector x ## Print the vector ## [1] 0 1 3 10 6 x[2] ## Print the 2nd Element; 1 ## [1] 1 x+2 ## Print simple calculation; 2,3,5,8,12 ## [1] 2 3 5 12 8 x*2 ## [1] 0 2 6 20 12 x^2 ## [1] 0 1 9 100 36 x+x ## [1] 0 2 6 20 12 x*x ## [1] 0 1 9 100 36 x^x ## [1] 1.0000e+00 1.0000e+00 2.7000e+01 1.0000e+10 4.6656e+04 c(1) ## scalars are vectors ## [1] 1 1:7 ## [1] 1 2 3 4 5 6 7 seq(0,1,by=.1) ## [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 3.3 Functions Function of a vector ## Add two to any vector add2 &lt;- function(x1) { x1+2 } add2(x) ## [1] 2 3 5 12 8 ## Generalization addn &lt;- function(x1,n=2) { x1+n } addn(x) ## [1] 2 3 5 12 8 addn(x,3) ## [1] 3 4 6 13 9 Function for two vectors sum_squared &lt;- function(x1, x2) { y &lt;- (x1 + x2)^2 return(y) } sum_squared(1, 3) ## [1] 16 sum_squared(x, 2) ## [1] 4 9 25 144 64 sum_squared(x, NA) ## [1] NA NA NA NA NA sum_squared(x, x) ## [1] 0 4 36 400 144 sum_squared(x, 2*x) ## [1] 0 9 81 900 324 Applying the same function over and over again sapply(1:3, exp) ## [1] 2.718282 7.389056 20.085537 exp(1:3) ## [1] 2.718282 7.389056 20.085537 ## mapply takes multiple vectors mapply(sum, 1:3, exp(1:3) ) ## [1] 3.718282 9.389056 23.085537 recursive functions ## For Loop x &lt;- rep(1, 3) for(i in 2:length(x) ){ x[i] &lt;- (x[i-1]+1)^2 } x ## [1] 1 4 25 r_fun &lt;- function(n){ x &lt;- rep(1,n) for(i in 2:length(x) ){ x[i] &lt;- (x[i-1]+1)^2 } return(x) } r_fun(5) ## [1] 1 4 25 676 458329 Functions can take functions as arguments fun_of_seq &lt;- function(f){ x &lt;- seq(1,3, length.out=12) y &lt;- f(x) return(y) } fun_of_seq(mean) ## [1] 2 fun_of_seq(mean) ## [1] 2 3.4 Matrices x1 &lt;- c(1,4,9) x2 &lt;- c(3,0,2) x_mat &lt;- rbind(x1, x2) x_mat ## Print full matrix ## [,1] [,2] [,3] ## x1 1 4 9 ## x2 3 0 2 x_mat[2,] ## Print Second Row ## [1] 3 0 2 x_mat[,2] ## Print Second Column ## x1 x2 ## 4 0 x_mat[2,2] ## Print Element in Second Column and Second Row ## x2 ## 0 ## x_mat+2 ## [,1] [,2] [,3] ## x1 3 6 11 ## x2 5 2 4 x_mat*2 ## [,1] [,2] [,3] ## x1 2 8 18 ## x2 6 0 4 x_mat^2 ## [,1] [,2] [,3] ## x1 1 16 81 ## x2 9 0 4 x_mat + x_mat ## [,1] [,2] [,3] ## x1 2 8 18 ## x2 6 0 4 x_mat*x_mat ## [,1] [,2] [,3] ## x1 1 16 81 ## x2 9 0 4 x_mat^x_mat ## [,1] [,2] [,3] ## x1 1 256 387420489 ## x2 27 1 4 y &lt;- apply(x_mat, 1, sum)^2 ## Apply function to each row ## ?apply #checks the function details y - sum_squared(x, x) ## tests if there are any differences ## Warning in y - sum_squared(x, x): longer object length is not a multiple of ## shorter object length ## [1] 192 -39 -2304 Many Other Functions x_mat1 &lt;- matrix(2:7,2,3) x_mat1 ## [,1] [,2] [,3] ## [1,] 2 4 6 ## [2,] 3 5 7 x_mat2 &lt;- matrix(4:-1,2,3) x_mat2 ## [,1] [,2] [,3] ## [1,] 4 2 0 ## [2,] 3 1 -1 ## x_mat1 * x_mat2 ## [,1] [,2] [,3] ## [1,] 8 8 0 ## [2,] 9 5 -7 tcrossprod(x_mat1, x_mat2) ##x_mat1 %*% t(x_mat2) ## [,1] [,2] ## [1,] 16 4 ## [2,] 22 7 crossprod(x_mat1, x_mat2) ## [,1] [,2] [,3] ## [1,] 17 7 -3 ## [2,] 31 13 -5 ## [3,] 45 19 -7 Example Calculations ## Return Y-value with minimum absolute difference from 3 abs_diff_y &lt;- abs( y - 3 ) abs_diff_y ## is this the luckiest number? ## x1 x2 ## 193 22 min(abs_diff_y) ## [1] 22 which.min(abs_diff_y) ## x2 ## 2 y[ which.min(abs_diff_y) ] ## x2 ## 25 3.5 Arrays Generalization of matrices used in spatial econometrics a &lt;- array(data = 1:24, dim = c(2, 3, 4)) a ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 9 11 ## [2,] 8 10 12 ## ## , , 3 ## ## [,1] [,2] [,3] ## [1,] 13 15 17 ## [2,] 14 16 18 ## ## , , 4 ## ## [,1] [,2] [,3] ## [1,] 19 21 23 ## [2,] 20 22 24 a[1, , , drop = FALSE] # Row 1 ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 3 5 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 9 11 ## ## , , 3 ## ## [,1] [,2] [,3] ## [1,] 13 15 17 ## ## , , 4 ## ## [,1] [,2] [,3] ## [1,] 19 21 23 a[, 1, , drop = FALSE] # Column 1 ## , , 1 ## ## [,1] ## [1,] 1 ## [2,] 2 ## ## , , 2 ## ## [,1] ## [1,] 7 ## [2,] 8 ## ## , , 3 ## ## [,1] ## [1,] 13 ## [2,] 14 ## ## , , 4 ## ## [,1] ## [1,] 19 ## [2,] 20 a[, , 1, drop = FALSE] # Layer 1 ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 a[ 1, 1, ] # Row 1, column 1 ## [1] 1 7 13 19 a[ 1, , 1] # Row 1, &quot;layer&quot; 1 ## [1] 1 3 5 a[ , 1, 1] # Column 1, &quot;layer&quot; 1 ## [1] 1 2 a[1 , 1, 1] # Row 1, column 1, &quot;layer&quot; 1 ## [1] 1 Apply extends to arrays apply(a, 1, mean) # Row means ## [1] 12 13 apply(a, 2, mean) # Column means ## [1] 10.5 12.5 14.5 apply(a, 3, mean) # &quot;Layer&quot; means ## [1] 3.5 9.5 15.5 21.5 apply(a, 1:2, mean) # Row/Column combination ## [,1] [,2] [,3] ## [1,] 10 12 14 ## [2,] 11 13 15 Outer Products yield arrays x &lt;- c(1,2,3) x_mat1 &lt;- outer(x, x) ## x %o% x x_mat1 ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 2 4 6 ## [3,] 3 6 9 is.array(x_mat) ## Matrices are arrays ## [1] TRUE x_mat2 &lt;- matrix(6:1,2,3) outer(x_mat2, x) ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 6 4 2 ## [2,] 5 3 1 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 12 8 4 ## [2,] 10 6 2 ## ## , , 3 ## ## [,1] [,2] [,3] ## [1,] 18 12 6 ## [2,] 15 9 3 ## outer(x_mat2, matrix(x)) ## outer(x_mat2, t(x)) ## outer(x_mat1, x_mat2) "],["statistics.html", " 4 Statistics 4.1 Data Types 4.2 Random Variables 4.3 Functions of Data 4.4 Value of More Data 4.5 Further Reading", " 4 Statistics 4.1 Data Types The most commom types are l1 &lt;- 1:3 ## cardinal numbers l1 ## [1] 1 2 3 l2 &lt;- factor(c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;), ordered=T) ## ordinal numbers l2 ## [1] A B C ## Levels: A &lt; B &lt; C l3 &lt;- factor(c(&#39;Leipzig&#39;,&#39;Los Angeles&#39;,&#39;Logan&#39;), ordered=F) ## categorical numbers l3 ## [1] Leipzig Los Angeles Logan ## Levels: Leipzig Logan Los Angeles l4 &lt;- c(&#39;hello world&#39;, &#39;hi mom&#39;) ## character strings l4 ## [1] &quot;hello world&quot; &quot;hi mom&quot; l5 &lt;- list(l1, l2, list(l3, list(&#39;...inception...&#39;))) ## lists l5 ## [[1]] ## [1] 1 2 3 ## ## [[2]] ## [1] A B C ## Levels: A &lt; B &lt; C ## ## [[3]] ## [[3]][[1]] ## [1] Leipzig Los Angeles Logan ## Levels: Leipzig Logan Los Angeles ## ## [[3]][[2]] ## [[3]][[2]][[1]] ## [1] &quot;...inception...&quot; ## data.frames: your most common data type ## matrix of different data-types ## well-ordered lists l5 &lt;- data.frame(x=l1, y=l2, z=l3) l5 ## x y z ## 1 1 A Leipzig ## 2 2 B Los Angeles ## 3 3 C Logan 4.2 Random Variables The different types of data can be randomly generated on your computer. Random variables are vectors that appear to be generated from a probabilistic process. ## Random bernoulli (Coin Flip: Heads=1) rbinom(1, 1, 0.5) ## 1 Flip ## [1] 1 rbinom(4, 1, 0.5) ## 4 Flips in row ## [1] 1 0 1 0 x0 &lt;- rbinom(1000, 1, 0.5) hist(x0) ## random standard-normal rnorm(4) ## [1] 1.1501297 1.3943096 0.4829421 0.6595033 x1 &lt;- rnorm(1000) hist(x1) ## random uniform runif(4) ## [1] 0.2003222 0.8964903 0.6221971 0.7381879 x2 &lt;- runif(1000) hist(x2) 4.3 Functions of Data Two definitions to remember statistic a function of data sampling distribution how a statistic varies from sample to sample The mean is a statistic ## compute the mean of a random sample x &lt;- runif(100) hist(x) m &lt;- mean(x) abline(v=m, col=2) ## is m close to it&#39;s true value (1-0)/2=.5? ## what about mean(runif(1000)) ? ## what about mean( rbinom(100, 1, 0.5) )? see how the mean varies from sample to sample to sample par(mfrow=c(1,3)) sapply(1:3, function(i){ x &lt;- runif(100) m &lt;- mean(x) hist(x, main=paste0(&#39;mean= &#39;, round(m,4)), breaks=seq(0,1,by=.1)) abline(v=m, col=2) return(m) }) ## [1] 0.5616801 0.5312499 0.5223141 examine the sampling distribution of the mean sample_means &lt;- sapply(1:1000, function(i) mean(runif(100)) ) hist(sample_means, breaks=50, col=2, main=&#39;Sampling Distribution of the mean&#39;) examine the sampling distribution of the standard deviation three_sds &lt;- c( sd(runif(100)), sd(runif(100)), sd(runif(100)) ) three_sds ## [1] 0.2888336 0.2678063 0.2719016 sample_sds &lt;- sapply(1:1000, function(i) sd(runif(100)) ) hist(sample_sds, breaks=50, col=4, main=&#39;Sampling Distribution of the sd&#39;) examine the sampling distribution of “order statistics” ## Create 300 samples, each with 1000 random uniform variables x &lt;- sapply(1:300, function(i) runif(1000) ) ## Median also looks normal xmed &lt;- apply(x,1,quantile, probs=.5) hist(xmed,breaks=100) ## Maximum and Minumum do not! xmin &lt;- apply(x,1,quantile, probs=0) xmax &lt;- apply(x,1,quantile, probs=1) par(mfrow=c(1,2)) hist(xmin,breaks=100) hist(xmax,breaks=100) ## Upper and Lower Quantiles xq &lt;- apply(x,1,quantile, probs=c(.05,.95)) bks &lt;- seq(0,1,by=.01) hist(xq[1,], main=&#39;quantile estimates&#39;, col=rgb(0,0,1,.5), xlim=c(0,1), breaks=bks) hist(xq[2,], col=rgb(1,0,0,.5), add=T, breaks=seq(0,1,by=.01)) ## Coverage xcov &lt;- sapply(bks, function(b){ bl &lt;- b &gt;= xq[1,] bu &lt;- b &lt;= xq[2,] mean( bl&amp;bu ) }) plot.new() plot.window(xlim=c(0,1), ylim=c(0,1)) polygon( c(bks, rev(bks)), c(xcov, xcov*0), col=grey(.5,.5), border=NA) title(&#39;coverage frequency&#39;) axis(1) axis(2) ## Try any function! fun_of_rv &lt;- function(f, n=100){ x &lt;- runif(n) y &lt;- f(x) return(y) } fun_of_rv( function(i){range(exp(i))}) ## [1] 1.002528 2.696439 4.4 Value of More Data Each additional data point you have provides more information, which ultimately decreases the standard error of your estimates. However, it does so at a decreasing rate (known in economics as diminishing marginal returns). Nseq &lt;- seq(1,100, by=1) ## Sample sizes B &lt;- 1000 ## Number of draws per sample SE &lt;- sapply(Nseq, function(n){ sample_statistics &lt;- sapply(1:B, function(b){ x &lt;- rnorm(n) ## Sample of size N quantile(x,probs=.4) ## Statistic }) sd(sample_statistics) }) par(mfrow=c(1,2)) plot(Nseq, SE, pch=16, col=grey(0,.5), main=&#39;Absolute Gain&#39;, ylab=&#39;standard error&#39;, xlab=&#39;sample size&#39;) plot(Nseq[-1], abs(diff(SE)), pch=16, col=grey(0,.5), main=&#39;Marginal Gain&#39;, ylab=&#39;decrease in standard error&#39;, xlab=&#39;sample size&#39;) 4.5 Further Reading Many introductory econometrics textbooks have a good appendix on probability and statistics. There are many useful texts online too https://bookdown.org/probability/statistics/ https://bookdown.org/probability/beta/ https://bookdown.org/a_shaker/STM1001_Topic_3/ https://bookdown.org/fsancier/bookdown-demo/ https://bookdown.org/kevin_davisross/probsim-book/ https://bookdown.org/machar1991/ITER/2-pt.html "],["data-analysis.html", " 5 Data Analysis 5.1 Cleaning Data 5.2 Static Plots 5.3 Interactive Plots", " 5 Data Analysis Reading in ## Install R Data Package and Load in install.packages(&#39;wooldridge&#39;) library(wooldridge) data(&#39;crime2&#39;) data(&#39;crime4&#39;) ## Read in csv from online dat_csv &lt;- read.csv(&#39;http://www.stern.nyu.edu/~wgreene/Text/Edition7/TableF19-3.csv&#39;) dat_csv &lt;- as.data.frame(dat_csv) ## Read in csv from online dat_stata &lt;- haven::read_dta(&#39;https://www.ssc.wisc.edu/~bhansen/econometrics/DS2004.dta&#39;) dat_stata &lt;- as.data.frame(dat_stata) ## For More Introductory Econometrics Data, see # https://www.ssc.wisc.edu/~bhansen/econometrics/Econometrics%20Data.zip # https://pages.stern.nyu.edu/~wgreene/Text/Edition7/tablelist8new.htm # R packages: wooldridge, causaldata, Ecdat, AER, .... Read in some historical data on crime in the US head(USArrests) ## Murder Assault UrbanPop Rape ## Alabama 13.2 236 58 21.2 ## Alaska 10.0 263 48 44.5 ## Arizona 8.1 294 80 31.0 ## Arkansas 8.8 190 50 19.5 ## California 9.0 276 91 40.6 ## Colorado 7.9 204 78 38.7 summary(USArrests) ## Murder Assault UrbanPop Rape ## Min. : 0.800 Min. : 45.0 Min. :32.00 Min. : 7.30 ## 1st Qu.: 4.075 1st Qu.:109.0 1st Qu.:54.50 1st Qu.:15.07 ## Median : 7.250 Median :159.0 Median :66.00 Median :20.10 ## Mean : 7.788 Mean :170.8 Mean :65.54 Mean :21.23 ## 3rd Qu.:11.250 3rd Qu.:249.0 3rd Qu.:77.75 3rd Qu.:26.18 ## Max. :17.400 Max. :337.0 Max. :91.00 Max. :46.00 5.1 Cleaning Data Data transformation is often necessary before analysis, so remember to be careful and check your code is doing what you want. (If you have large datasets, you can always test out the code on a sample.) ## Function to Create Sample Datasets make_noisy_data &lt;- function(n, b=0){ ## Simple Data Generating Process x &lt;- seq(1,10, length.out=n) e &lt;- rnorm(length(x), mean=0, sd=10) y &lt;- b*x + e ## Obervations xy_mat &lt;- data.frame(ID=seq(x), x=x, y=y) return(xy_mat) } ## Two simulated datasets dat1 &lt;- make_noisy_data(6) dat2 &lt;- make_noisy_data(6) ## Merging data in long format dat_merged_long &lt;- rbind( cbind(dat1,DF=1), cbind(dat2,DF=2)) Now suppose we want to transform into long format ## Merging data in wide format, First Attempt dat_merged_wide &lt;- cbind( dat1, dat2) names(dat_merged_wide) &lt;- c(paste0(names(dat1),&#39;.1&#39;), paste0(names(dat2),&#39;.2&#39;)) ## Merging data in wide format, Second Attempt ## higher performance dat_merged_wide2 &lt;- merge(dat1, dat2, by=&#39;ID&#39;, suffixes=c(&#39;.1&#39;,&#39;.2&#39;)) ## CHECK they are the same. identical(dat_merged_wide, dat_merged_wide2) ## [1] FALSE ## Merging data in wide format, Third Attempt ## more flexibility dat_melted &lt;- reshape2::melt(dat_merged_long, id.vars=c(&#39;ID&#39;, &#39;DF&#39;)) dat_merged_wide3 &lt;- reshape2::dcast(dat_melted, ID~DF+variable) ## Merging data in wide format, Fourth Attempt ## highest performance but with new type of object library(data.table) dat_merged_longDT &lt;- as.data.table(dat_merged_long) dat_melted &lt;- data.table::melt(dat_merged_longDT, id.vars=c(&#39;ID&#39;, &#39;DF&#39;)) dat_merged_wide4 &lt;- data.table::dcast(dat_melted, ID~DF+variable) ## dat_merged_wide4 &lt;- as.data.frame(dat_merged_wide4) ## CHECK they are the same. identical(dat_merged_wide3, dat_merged_wide4) ## [1] FALSE Often, however, we ultimately want data in long format ## Merging data in long format, Second Attempt dat_melted2 &lt;- data.table::melt(dat_merged_wide4, measure=c(&quot;1_x&quot;,&quot;1_y&quot;,&quot;2_x&quot;,&quot;2_y&quot;)) melt_vars &lt;- strsplit(as.character(dat_melted2$variable),&#39;_&#39;) dat_melted2$DF &lt;- sapply(melt_vars, `[[`,1) dat_melted2$variable &lt;- sapply(melt_vars, `[[`,2) dat_merged_long2 &lt;- data.table::dcast(dat_melted2, DF+ID~variable) dat_merged_long2 &lt;- as.data.frame(dat_merged_long2) ## CHECK they are the same. identical( dat_merged_long2, dat_merged_long) ## [1] FALSE ## Further Inspect dat_merged_long2 &lt;- dat_merged_long2[,c(&#39;ID&#39;,&#39;x&#39;,&#39;y&#39;,&#39;DF&#39;)] mapply( identical, dat_merged_long2, dat_merged_long) ## ID x y DF ## TRUE TRUE TRUE FALSE For more tips, see https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-import.pdf and https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reshape.html 5.2 Static Plots 5.2.1 Histograms Histograms Summarize Distributions. Easy to show data splits. Can glue together to convey more information all at once par(mfrow=c(1,2)) ## All Data xbks &lt;- seq(min(USArrests$Murder), max(USArrests$Murder), length.out=10) hist(USArrests$Murder, main=&#39;All Data&#39;, xlab=&#39;Murder Arrests&#39;, breaks=xbks) ## Split Data by Urban Population above/below mean u &lt;- mean(USArrests$UrbanPop) m1 &lt;- USArrests[USArrests$UrbanPop&lt;u,&#39;Murder&#39;] m2 &lt;- USArrests[USArrests$UrbanPop&gt;=u,&#39;Murder&#39;] cols &lt;- c(rgb(0,0,1,.5), rgb(1,0,0,.5)) hist(m1, col=cols[1], breaks=xbks, xlab=&#39;Murder Arrests&#39;, main=&#39;Split Data&#39;) hist(m2, add=T, col=cols[2], breaks=xbks) legend(&#39;topright&#39;, col=cols, pch=15, bty=&#39;n&#39;, title=&#39;% Urban Pop.&#39;, legend=c(&#39;Above Mean&#39;, &#39;Below Mean&#39;)) Sometimes it is better to make separate figures for each data split par(fig=c(0,1,0,0.5), new=F) hist(USArrests$Murder, breaks=xbks, main=&#39;All Data&#39;, xlab=&#39;Murder Arrests&#39;) par(fig=c(0,.5,0.5,1), new=TRUE) hist(m1, breaks=xbks, col=rgb(0,0,1,.5), main=&#39;Urban Pop &gt;= Mean&#39;, xlab=&#39;Murder Arrests&#39;) par(fig=c(0.5,1,0.5,1), new=TRUE) hist(m2,breaks=xbks, col=rgb(1,0,0,.5), main=&#39;Urban Pop &lt; Mean&#39;, xlab=&#39;Murder Arrests&#39;) For more histogram visuals, see https://r-graph-gallery.com/histogram.html 5.2.2 Boxplots Boxplots show median, interquartile range, and outliers. As with histograms, you can also split data into groups and glue together layout( t(c(1,2,2))) boxplot(USArrests$Murder, main=&#39;&#39;, xlab=&#39;All Data&#39;, ylab=&#39;Murder Arrests&#39;) ## 3 Groups with even spacing USArrests$UrbanPop_cut &lt;- cut(USArrests$UrbanPop,3) boxplot(Murder~UrbanPop_cut, USArrests, main=&#39;&#39;, col=hcl.colors(3,alpha=.5), xlab=&#39;Urban Population&#39;, ylab=&#39;&#39;) ## 4 Groups with equal observations #qcuts &lt;- c( # &#39;0%&#39;=min(USArrests$UrbanPop)-10*.Machine$double.eps, # quantile(USArrests$UrbanPop, probs=c(.25,.5,.75,1))) #USArrests$UrbanPop_cut &lt;- cut(USArrests$UrbanPop, qcuts) 5.2.3 Scatterplots Scatterplots are used frequently to summarize the relationship between two variables. They can be enhanced in several ways. Fit Lines and Color You can add regression lines (and confidence intervals). As a default, use semi-transparent points to see where your observations are concentrated. You can also use color to distinguish subsets. ## High Assault Areas cols &lt;- ifelse(USArrests$Assault&gt;median(USArrests$Assault), rgb(1,0,0,.5), rgb(0,0,1,.5)) ## Scatterplot plot(Murder~UrbanPop, USArrests, pch=16, col=cols) ## Add the line of best fit for pooled data ## Could also do separately for each data split reg &lt;- lm(Murder~UrbanPop, data=USArrests) abline(reg, lty=2) ## Can Also Add Confidence Intervals ## https://rpubs.com/aaronsc32/regression-confidence-prediction-intervals Your first plot is typically standard. For others to easily comprehend your work, you must polish the plot. ## Data Generating Process x &lt;- seq(1, 10, by=.0002) e &lt;- rnorm(length(x), mean=0, sd=1) y &lt;- .25*x + e xy_dat &lt;- data.frame(x=x, y=y) ## Plot par(fig=c(0,1,0,0.9), new=F) plot(y~x, xy_dat, pch=16, col=rgb(0,0,0,.05), cex=.5, xlab=&#39;&#39;, ylab=&#39;&#39;) ## Format Axis Labels Seperately mtext( &#39;y=0.25 x + e\\n e ~ standard-normal&#39;, 2, line=2.2) mtext( expression(x%in%~&#39;[0,10]&#39;), 1, line=2.2) abline( lm(y~x, data=xy_dat), lty=2) title(&#39;Plot with good features and excessive notation&#39;, adj=0) ## Outer Legend (https://stackoverflow.com/questions/3932038/) outer_legend &lt;- function(...) { opar &lt;- par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), mar=c(0, 0, 0, 0), new=TRUE) on.exit(par(opar)) plot(0, 0, type=&#39;n&#39;, bty=&#39;n&#39;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;) legend(...) } outer_legend(&#39;topright&#39;, legend=&#39;single data point&#39;, title=&#39;do you see the normal distribution?&#39;, pch=16, col=rgb(0,0,0,.1), cex=1, bty=&#39;n&#39;) Can export figure with specific dimensions pdf( &#39;Figures/plot_example.pdf&#39;, height=5, width=5) ## plot goes here dev.off() For plotting math, see https://astrostatistics.psu.edu/su07/R/html/grDevices/html/plotmath.html https://library.virginia.edu/data/articles/mathematical-annotation-in-r For exporting options, see ?pdf. For saving other types of files, see png(\"*.png\"), tiff(\"*.tiff\"), and jpeg(\"*.jpg\") Marginal distributions par(fig=c(0,0.8,0,0.8), new=F) plot(Murder~UrbanPop, USArrests, pch=16, col=rgb(0,0,0,.5)) par(fig=c(0,0.8,0.55,1), new=TRUE) boxplot(USArrests$Murder, horizontal=TRUE, axes=FALSE) par(fig=c(0.65,1,0,0.8),new=TRUE) boxplot(USArrests$UrbanPop, axes=FALSE) ## https://www.r-bloggers.com/2011/06/example-8-41-scatterplot-with-marginal-histograms/ ## Setup Plot layout( matrix(c(2,0,1,3), ncol=2, byrow=TRUE), widths=c(4/5,1/5), heights=c(1/5,4/5)) ## Scatterplot par(mar=c(4,4,1,1)) plot(Murder~UrbanPop, USArrests, pch=16, col=rgb(0,0,0,.5)) ## Add Marginals par(mar=c(0,4,1,1)) xhist &lt;- hist(USArrests$UrbanPop, plot=FALSE) barplot(xhist$counts, axes=FALSE, space=0) par(mar=c(4,0,1,1)) yhist &lt;- hist(USArrests$Murder, plot=FALSE) barplot(yhist$counts, axes=FALSE, space=0, horiz=TRUE) For plotting marginals, see https://r-graph-gallery.com/74-margin-and-oma-cheatsheet.html and https://jtr13.github.io/cc21fall2/tutorial-for-scatter-plot-with-marginal-distribution.html 5.3 Interactive Plots Especially for data exploration, your plots can also be interactive via https://plotly.com/r/. For more details, see https://plotly-r.com/ #install.packages(&quot;plotly&quot;) library(plotly) Histograms https://plotly.com/r/histograms/ u &lt;- mean(USArrests$UrbanPop) m1 &lt;- USArrests[USArrests$UrbanPop&lt;u,&#39;Murder&#39;] m2 &lt;- USArrests[USArrests$UrbanPop&gt;=u,&#39;Murder&#39;] fig &lt;- plot_ly(alpha=0.6, hovertemplate=&quot;%{y}&quot;) fig &lt;- fig %&gt;% add_histogram(m1, name=&#39;&lt; Mean&#39;) fig &lt;- fig %&gt;% add_histogram(m2, name=&#39;&gt;= Mean&#39;) fig &lt;- fig %&gt;% layout(barmode=&quot;stack&quot;) ## barmode=&quot;overlay&quot; fig &lt;- fig %&gt;% layout( title=&quot;Crime and Urbanization in America 1975&quot;, xaxis = list(title=&#39;Murders Arrests per 100,000 People&#39;), yaxis = list(title=&#39;Number of States&#39;), legend=list(title=list(text=&#39;&lt;b&gt; Urban Pop. &lt;/b&gt;&#39;)) ) fig Boxplots https://plotly.com/r/box-plots/ USArrests$ID &lt;- rownames(USArrests) fig &lt;- plot_ly(USArrests, y=~Murder, color=~cut(UrbanPop,4), alpha=0.6, type=&quot;box&quot;, pointpos=0, boxpoints = &#39;all&#39;, hoverinfo=&#39;text&#39;, text = ~paste(&#39;&lt;b&gt;&#39;, ID, &#39;&lt;/b&gt;&#39;, &quot;&lt;br&gt;Urban :&quot;, UrbanPop, &quot;&lt;br&gt;Assault:&quot;, Assault, &quot;&lt;br&gt;Murder :&quot;, Murder)) fig &lt;- plotly::layout(fig, showlegend=FALSE, title=&#39;Crime and Urbanization in America 1975&#39;, xaxis = list(title = &#39;Percent of People in an Urban Area&#39;), yaxis = list(title = &#39;Murders Arrests per 100,000 People&#39;)) fig Scatterplots https://plotly.com/r/bubble-charts/ ## Simple Scatter Plot #plot(Assault~UrbanPop, USArrests, col=grey(0,.5), pch=16, # cex=USArrests$Murder/diff(range(USArrests$Murder))*2, # main=&#39;US Murder arrests (per 100,000)&#39;) # Scatter Plot USArrests$ID &lt;- rownames(USArrests) fig &lt;- plotly::plot_ly( USArrests, x = ~UrbanPop, y = ~Assault, mode=&#39;markers&#39;, type=&#39;scatter&#39;, hoverinfo=&#39;text&#39;, text = ~paste(&#39;&lt;b&gt;&#39;, ID, &#39;&lt;/b&gt;&#39;, &quot;&lt;br&gt;Urban :&quot;, UrbanPop, &quot;&lt;br&gt;Assault:&quot;, Assault, &quot;&lt;br&gt;Murder :&quot;, Murder), color=~Murder, marker=list( size=~Murder, opacity=0.5, showscale=T, colorbar = list(title=&#39;Murder Arrests (per 100,000)&#39;))) fig &lt;- plotly::layout(fig, showlegend=F, title=&#39;Crime and Urbanization in America 1975&#39;, xaxis = list(title = &#39;Percent of People in an Urban Area&#39;), yaxis = list(title = &#39;Assault Arrests per 100,000 People&#39;)) fig "],["beyond-basics.html", " 6 Beyond Basics 6.1 The R Ecosystem 6.2 Introductions to R 6.3 Custom Figures", " 6 Beyond Basics 6.1 The R Ecosystem Use expansion “packages” for common procedures and more functionality 6.1.1 Packages CRAN Most packages can be found on CRAN and can be easily installed ## commonly used packages install.packages(&quot;stargazer&quot;) install.packages(&quot;data.table&quot;) ## install.packages(&quot;purrr&quot;) ## install.packages(&quot;reshape2&quot;) The most common tasks also have cheatsheets you can use. E.g., https://github.com/rstudio/cheatsheets/blob/main/rstudio-ide.pdf Github Sometimes you will want to install a package from GitHub. For this, you can use devtools or its light-weight version remotes install.packages(&quot;devtools&quot;) install.packages(&quot;remotes&quot;) Note that to install devtools, you also need to have developer tools installed on your computer. Windows: Rtools Mac: Xcode To color terminal output on Linux systems, you can use the colorout package library(remotes) # Install https://github.com/jalvesaq/colorout # to .libPaths()[1] install_github(&#39;jalvesaq/colorout&#39;) library(colorout) Base While additional packages can make your code faster, they also create dependancies that can lead to problems. So learn base R well before becoming dependant on other packages https://bitsofanalytics.org/posts/base-vs-tidy/ https://jtr13.github.io/cc21fall2/comparison-among-base-r-tidyverse-and-datatable.html 6.1.2 Task Views Task views list relevant packages. For all students and early researchers, https://cran.r-project.org/web/views/ReproducibleResearch.html For microeconometrics, https://cran.r-project.org/web/views/Econometrics.html For spatial econometrics https://cran.r-project.org/web/views/Spatial.html https://cran.r-project.org/web/views/SpatioTemporal.html Multiple packages may have the same function name for different commands. In this case use the syntax package::function to specify the package. For example devtools::install_github remotes::install_github Don’t fret Sometimes there is not a specific package for your data. Odds are, you can do most of what you want with base code. Packages just wrap base code in convient formats see https://cran.r-project.org/web/views/ for topical overviews Statisticians might have different naming conventions if the usual software just spits out a nice plot you might have to dig a little to know precisely what you want your data are fundamentally numbers, strings, etc… You only have to figure out how to read it in. 6.2 Introductions to R There are many good yet free programming books online. Some of my examples originally come from https://r4ds.had.co.nz/ and I recommend https://intro2r.com. But I have used online material from many places over the years, including https://cran.r-project.org/doc/manuals/R-intro.html R Graphics Cookbook, 2nd edition. Winston Chang. 2021. https://r-graphics.org/ R for Data Science. H. Wickham and G. Grolemund. 2017. https://r4ds.had.co.nz/index.html An Introduction to R. W. N. Venables, D. M. Smith, R Core Team. 2017. https://colinfay.me/intro-to-r/ Introduction to R for Econometrics. Kieran Marray. https://bookdown.org/kieranmarray/intro_to_r_for_econometrics/ Wollschläger, D. (2020). Grundlagen der Datenanalyse mit R: eine anwendungsorientierte Einführung. http://www.dwoll.de/rexrepos/ Spatial Data Science with R: Introduction to R. Robert J. Hijmans. 2021. https://rspatial.org/intr/index.html What we cover in this primer should be enough to get you going. But there are also many good yet free-online tutorials and courses. https://www.econometrics-with-r.org/1.2-a-very-short-introduction-to-r-and-rstudio.html https://rafalab.github.io/dsbook/ https://moderndive.com/foreword.html https://rstudio.cloud/learn/primers/1.2 https://cran.r-project.org/manuals.html https://stats.idre.ucla.edu/stat/data/intro_r/intro_r_interactive_flat.html https://cswr.nrhstat.org/app-r 6.3 Custom Figures Many of the best plots are custom made (see https://www.r-graph-gallery.com/). Here are some ones that I have made over the years. "],["simple-ols.html", " 7 Simple OLS 7.1 Variability Estimates 7.2 Hypothesis Tests 7.3 Prediction Intervals 7.4 Value of More Data 7.5 Locally Linear", " 7 Simple OLS Model and objective \\[ y_i=\\alpha+\\beta x_i+\\epsilon_{i} \\\\ \\epsilon_{i} = y_i - [\\alpha+\\beta x_i]\\\\ min_{\\beta} \\sum_{i=1}^{n} (\\epsilon_{i})^2 \\] Point Estimates \\[ \\hat{\\alpha}=\\bar{y}-\\hat{\\beta}\\bar{x} = \\widehat{\\mathbb{E}}[Y] - \\hat{\\beta} \\widehat{\\mathbb{E}}[X] \\\\ \\hat{\\beta}=\\frac{\\sum_{i}^{}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i}^{}(x_i-\\bar{x})^2} = \\frac{\\widehat{Cov}[X,Y]}{\\widehat{\\mathbb{V}}[X]}\\\\ \\hat{y}_i=\\hat{\\alpha}+\\hat{\\beta}x_i\\\\ \\hat{\\epsilon}_i=y_i-\\hat{y}_i\\\\ \\] Before fitting the model to your data, explore your data (as in Part I) ## Inspect Dataset xy &lt;- USArrests[,c(&#39;Murder&#39;,&#39;UrbanPop&#39;)] colnames(xy) &lt;- c(&#39;y&#39;,&#39;x&#39;) ## head(xy) ## Plot Data plot(y~x, xy, col=grey(.5,.5), pch=16) ## Estimate Regression Coefficients reg &lt;- lm(y~x, dat=xy) reg ## ## Call: ## lm(formula = y ~ x, data = xy) ## ## Coefficients: ## (Intercept) x ## 6.41594 0.02093 ## Point Estimates coef(reg) ## (Intercept) x ## 6.41594246 0.02093466 To measure the ‘’Goodness of fit’’, we analyze sums of squared errors (Total, Explained, and Residual) as \\[ \\underbrace{\\sum_{i}(y_i-\\bar{y})^2}_\\text{TSS}=\\underbrace{\\sum_{i}(\\hat{y}_i-\\bar{y})^2}_\\text{ESS}+\\underbrace{\\sum_{i}\\hat{\\epsilon_{i}}^2}_\\text{RSS}\\\\ R^2 = \\frac{ESS}{TSS}=1-\\frac{RSS}{TSS} \\] Note that \\(R^2\\) is also called the coefficient of determination. ## Manually Compute Goodness of Fit Ehat &lt;- resid(reg) RSS &lt;- sum(Ehat^2) Y &lt;- xy$y TSS &lt;- sum((Y-mean(Y))^2) R2 &lt;- 1 - RSS/TSS R2 ## [1] 0.00484035 ## Check R2 summary(reg)$r.squared ## [1] 0.00484035 7.1 Variability Estimates A regression coefficient is a statistic. And, just like all statistics, we can calculate standard deviation: variability within a single sample. standard error: variability across different samples. confidence interval: range your statistic varies across different samples. null distribution: the sampling distribution of the statistic under the null hypothesis (assuming your null hypothesis was true). p-value the probability you would see something as extreme as your statistic when sampling from the null distribution. To calculate these variability statistics, we will estimate variabilty using data-driven methods.1 We first consider the simplest, the jackknife. In this procedure, we loop through each row of the dataset. And, in each iteration of the loop, we drop that observation from the dataset and reestimate the statistic of interest. We then calculate the standard deviation of the statistic across all ``resamples’’. ## Example 1 Continued ## Jackknife Standard Errors for Beta jack_regs &lt;- lapply(1:nrow(xy), function(i){ xy_i &lt;- xy[-i,] reg_i &lt;- lm(y~x, dat=xy_i) }) jack_coefs &lt;- sapply(jack_regs, coef)[&#39;x&#39;,] jack_mean &lt;- mean(jack_coefs) jack_se &lt;- sd(jack_coefs) ## Jackknife Confidence Intervals jack_ci_percentile &lt;- quantile(jack_coefs, probs=c(.025,.975)) hist(jack_coefs, breaks=25, main=paste0(&#39;SE est. = &#39;, round(jack_se,4)), xlab=expression(beta[-i])) abline(v=jack_mean, col=&quot;red&quot;, lwd=2) abline(v=jack_ci_percentile, col=&quot;red&quot;, lty=2) ## Plot Full-Sample Estimate ## abline(v=coef(reg)[&#39;x&#39;], lty=1, col=&#39;blue&#39;, lwd=2) ## Plot Normal Approximation ## jack_ci_normal &lt;- jack_mean+c(-1.96, +1.96)*jack_se ## abline(v=jack_ci_normal, col=&quot;red&quot;, lty=3) There are several other resampling techniques. We consider the other main one, the bootstrap, which resamples with replacement for an arbitrary number of iterations. When bootstrapping a dataset with \\(n\\) observations, you randomly resample all \\(n\\) rows in your data set \\(B\\) times. Sample Size per Iteration Number of Iterations Resample Bootstrap \\(n\\) \\(B\\) With Replacement Jackknife \\(n-1\\) \\(n\\) Without Replacement ## Bootstrap Standard Errors for Beta boots &lt;- 1:399 boot_regs &lt;- lapply(boots, function(b){ b_id &lt;- sample( nrow(xy), replace=T) xy_b &lt;- xy[b_id,] reg_b &lt;- lm(y~x, dat=xy_b) }) boot_coefs &lt;- sapply(boot_regs, coef)[&#39;x&#39;,] boot_mean &lt;- mean(boot_coefs) boot_se &lt;- sd(boot_coefs) ## Bootstrap Confidence Intervals boot_ci_percentile &lt;- quantile(boot_coefs, probs=c(.025,.975)) hist(boot_coefs, breaks=25, main=paste0(&#39;SE est. = &#39;, round(boot_se,4)), xlab=expression(beta[b])) abline(v=boot_mean, col=&quot;red&quot;, lwd=2) abline(v=boot_ci_percentile, col=&quot;red&quot;, lty=2) ## Normal Approximation ## boot_ci_normal &lt;- boot_mean+c(-1.96, +1.96)*boot ## Parametric CI ## x &lt;- data.frame(x=quantile(xy$x,probs=seq(0,1,by=.1))) ## ci &lt;- predict(reg, interval=&#39;confidence&#39;, newdata=data.frame(x)) ## polygon( c(x, rev(x)), c(ci[,&#39;lwr&#39;], rev(ci[,&#39;upr&#39;])), col=grey(0,.2), border=0) We can also bootstrap other statistics, such as a t-statistic or \\(R^2\\). We do such things to test a null hypothesis, which is often ``no relationship’’. We are rarely interested in computing standard errrors and conducting hypothesis tests for two variables. However, we work through the ideas in the two-variable case to better understand the multi-variable case. 7.2 Hypothesis Tests There are two main ways to conduct a hypothesis test. Invert a CI One main way to conduct hypothesis tests is to examine whether a confidence interval contains a hypothesized value. Often, this is \\(0\\). ## Example 1 Continued Yet Again ## Bootstrap Distribution boot_ci_percentile &lt;- quantile(boot_coefs, probs=c(.025,.975)) hist(boot_coefs, breaks=25, main=paste0(&#39;SE est. = &#39;, round(boot_se,4)), xlab=expression(beta[b]), xlim=range(c(0, boot_coefs)) ) abline(v=boot_ci_percentile, lty=2, col=&quot;red&quot;) abline(v=0, col=&quot;red&quot;, lwd=2) Impose the Null We can also compute a null distribution using data-driven methods that assume much less about the data generating process. We focus on the simplest, the bootstrap, where loop through a large number of simulations. In each iteration of the loop, we drop impose the null hypothesis and reestimate the statistic of interest. We then calculate the standard deviation of the statistic across all ``resamples’’. ## Example 1 Continued Again ## Null Distribution for Beta boots &lt;- 1:399 boot_regs0 &lt;- lapply(boots, function(b){ xy_b &lt;- xy xy_b$y &lt;- sample( xy_b$y, replace=T) reg_b &lt;- lm(y~x, dat=xy_b) }) boot_coefs0 &lt;- sapply(boot_regs0, coef)[&#39;x&#39;,] ## Null Bootstrap Distribution boot_ci_percentile0 &lt;- quantile(boot_coefs0, probs=c(.025,.975)) hist(boot_coefs0, breaks=25, main=&#39;&#39;, xlab=expression(beta[b]), xlim=range(c(boot_coefs0, coef(reg)[&#39;x&#39;]))) abline(v=boot_ci_percentile0, col=&quot;red&quot;, lty=2) abline(v=coef(reg)[&#39;x&#39;], col=&quot;red&quot;, lwd=2) Regardless of how we calculate standard errors, we can use them to conduct a t-test. We also compute the distribution of t-values under the null hypothesis, and compare how extreme the oberved value is. \\[ \\hat{t} = \\frac{\\hat{\\beta} - \\beta_{0} }{\\hat{\\sigma}_{\\hat{\\beta}}} \\] ## T Test B0 &lt;- 0 boot_t &lt;- (coef(reg)[&#39;x&#39;]-B0)/boot_se ## Compute Bootstrap T-Values (without refinement) boot_t_boot0 &lt;- sapply(boot_regs0, function(reg_b){ beta_b &lt;- coef(reg_b)[[&#39;x&#39;]] t_hat_b &lt;- (beta_b)/boot_se return(t_hat_b) }) hist(boot_t_boot0, breaks=100, main=&#39;Bootstrapped t values&#39;, xlab=&#39;t&#39;, xlim=range(c(boot_t_boot0, boot_t)) ) abline(v=boot_t, lwd=2, col=&#39;red&#39;) From this, we can calculate a p-value: the probability you would see something as extreme as your statistic under the null (assuming your null hypothesis was true). Note that the \\(p\\) reported by your computer does not necessarily satisfy this definition. We can always calcuate a p-value from an explicit null distribution. ## One Sided Test for P(t &gt; boot_t | Null)=1- P(t &lt; boot_t | Null) That_NullDist1 &lt;- ecdf(boot_t_boot0) Phat1 &lt;- 1-That_NullDist1(boot_t) ## Two Sided Test for P(t &gt; jack_t or t &lt; -jack_t | Null) That_NullDist2 &lt;- ecdf(abs(boot_t_boot0)) plot(That_NullDist2, xlim=range(boot_t_boot0, boot_t)) abline(v=quantile(That_NullDist2,probs=.95), lty=3) abline(v=boot_t, col=&#39;red&#39;) Phat2 &lt;- 1-That_NullDist2(boot_t) Phat2 ## [1] 0.6240602 Under some assumptions, the null distribution is distributed \\(t_{n-2}\\). (For more on parametric t-testing based on statistical theory, see https://www.econometrics-with-r.org/4-lrwor.html.) 7.3 Prediction Intervals In addition to confidence intervales, we can also compute a prediction interval which estimates the range of variability across different samples for the outcomes. These intervals also take into account the residuals— the variability of individuals around the mean. ## Bootstrap Prediction Interval boot_resids &lt;- lapply(boot_regs, function(reg_b){ e_b &lt;- resid(reg_b) x_b &lt;- reg_b$model$x res_b &lt;- cbind(e_b, x_b) }) boot_resids &lt;- as.data.frame(do.call(rbind, boot_resids)) ## Homoskedastic ehat &lt;- quantile(boot_resids$e_b, probs=c(.025, .975)) x &lt;- quantile(xy$x,probs=seq(0,1,by=.1)) boot_pi &lt;- coef(reg)[1] + x*coef(reg)[&#39;x&#39;] boot_pi &lt;- cbind(boot_pi + ehat[1], boot_pi + ehat[2]) ## Plot Bootstrap PI plot(y~x, dat=xy, pch=16, main=&#39;Prediction Intervals&#39;, ylim=c(-5,20)) polygon( c(x, rev(x)), c(boot_pi[,1], rev(boot_pi[,2])), col=grey(0,.2), border=NA) ## Parametric PI (For Comparison) pi &lt;- predict(reg, interval=&#39;prediction&#39;, newdata=data.frame(x)) lines( x, pi[,&#39;lwr&#39;], lty=2) lines( x, pi[,&#39;upr&#39;], lty=2) There are many ways to improve upon the prediction intervals you just created. Probably the most basic way is to allow the residuals to be heteroskedastic. ## Estimate Residual Quantiles seperately around X points boot_resid_list &lt;- split(boot_resids, cut(boot_resids$x_b, x) ) boot_resid_est &lt;- lapply(boot_resid_list, function(res_b) { if( nrow(res_b)==0){ ## If Empty, Return Nothing ehat &lt;- c(NA,NA) } else{ ## Estimate Quantiles of Residuals ehat &lt;- quantile(res_b$e_b, probs=c(.025, .975)) } return(ehat) }) boot_resid_est &lt;- do.call(rbind, boot_resid_est) ## Construct PI at x points boot_x &lt;- x[-1] - diff(x)/2 boot_pi &lt;- coef(reg)[1] + boot_x*coef(reg)[&#39;x&#39;] boot_pi &lt;- cbind(boot_pi + boot_resid_est[,1], boot_pi + boot_resid_est[,2]) plot(y~x, dat=xy, pch=16, main=&#39;Heteroskedastic P.I.&#39;) polygon( c(boot_x, rev(boot_x)), c(boot_pi[,1], rev(boot_pi[,2])), col=grey(0,.2), border=NA) rug(boot_x) For a nice overview of different types of intervals, see https://www.jstor.org/stable/2685212. For an indepth view, see “Statistical Intervals: A Guide for Practitioners and Researchers” or “Statistical Tolerance Regions: Theory, Applications, and Computation”. See https://robjhyndman.com/hyndsight/intervals/ for constructing intervals for future observations in a time-series context. See Davison and Hinkley, chapters 5 and 6 (also Efron and Tibshirani, or Wehrens et al.) 7.4 Value of More Data Just as before, there are diminishing returns to larger sample sizes with simple OLS. B &lt;- 300 Nseq &lt;- seq(3,100, by=1) SE &lt;- sapply(Nseq, function(n){ sample_statistics &lt;- sapply(1:B, function(b){ x &lt;- rnorm(n) e &lt;- rnorm(n) y &lt;- x*2 + e reg &lt;- lm(y~x) coef(reg) #se &lt;- sqrt(diag(vcov(vcov))) }) sd(sample_statistics) }) par(mfrow=c(1,2)) plot(Nseq, SE, pch=16, col=grey(0,.5), main=&#39;Absolute Gain&#39;, ylab=&#39;standard error&#39;, xlab=&#39;sample size&#39;) plot(Nseq[-1], abs(diff(SE)), pch=16, col=grey(0,.5), main=&#39;Marginal Gain&#39;, ylab=&#39;decrease in standard error&#39;, xlab=&#39;sample size&#39;) 7.5 Locally Linear Segmented/piecewise regression library(AER) ## Loading required package: car ## Loading required package: carData ## Loading required package: lmtest ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric ## Loading required package: sandwich ## Loading required package: survival data(CASchools) CASchools$score &lt;- (CASchools$read + CASchools$math) / 2 reg &lt;- lm(score ~ income, data = CASchools) plot( CASchools$income, resid(reg), pch=16, col=grey(0,.5)) ## Add Piecewise Term CASchools$IncomeCut &lt;- cut(CASchools$income,2) reg2 &lt;- lm(score ~ income*IncomeCut, data=CASchools) summary(reg2) ## ## Call: ## lm(formula = score ~ income * IncomeCut, data = CASchools) ## ## Residuals: ## Min 1Q Median 3Q Max ## -44.710 -8.897 0.730 8.210 32.445 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 617.4010 2.1013 293.821 &lt; 2e-16 *** ## income 2.4732 0.1426 17.338 &lt; 2e-16 *** ## IncomeCut(30.3,55.4] 59.6336 16.9170 3.525 0.00047 *** ## income:IncomeCut(30.3,55.4] -2.0904 0.4499 -4.646 4.55e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.88 on 416 degrees of freedom ## Multiple R-squared: 0.5462, Adjusted R-squared: 0.5429 ## F-statistic: 166.9 on 3 and 416 DF, p-value: &lt; 2.2e-16 ## F Test for Break anova(reg, reg2) ## Analysis of Variance Table ## ## Model 1: score ~ income ## Model 2: score ~ income * IncomeCut ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 418 74905 ## 2 416 69033 2 5871.7 17.692 4.226e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Chow Test for Break data_splits &lt;- split(CASchools, CASchools$IncomeCut) resids &lt;- sapply(data_splits, function(dat){ reg &lt;- lm(score ~ income, data=dat) sum( resid(reg)^2) }) Ns &lt;- sapply(data_splits, function(dat){ nrow(dat)}) Rt &lt;- (sum(resid(reg)^2) - sum(resids))/sum(resids) Rb &lt;- (sum(Ns)-2*reg$rank)/reg$rank Ft &lt;- Rt*Rb pf(Ft,reg$rank, sum(Ns)-2*reg$rank,lower.tail=F) ## [1] 4.225896e-08 Multiple Breaks CASchools$IncomeCut2 &lt;- cut(CASchools$income, seq(0,60,by=20)) ## Finer Bins reg3 &lt;- lm(score ~ income*IncomeCut2, data=CASchools) summary(reg3) ## ## Call: ## lm(formula = score ~ income * IncomeCut2, data = CASchools) ## ## Residuals: ## Min 1Q Median 3Q Max ## -42.323 -9.048 0.258 8.279 31.702 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 612.0820 2.7523 222.386 &lt; 2e-16 *** ## income 2.9200 0.2073 14.089 &lt; 2e-16 *** ## IncomeCut2(20,40] 23.0794 8.7824 2.628 0.008910 ** ## IncomeCut2(40,60] 89.1941 37.6356 2.370 0.018249 * ## income:IncomeCut2(20,40] -1.3601 0.3805 -3.574 0.000393 *** ## income:IncomeCut2(40,60] -3.0429 0.8525 -3.570 0.000400 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.78 on 414 degrees of freedom ## Multiple R-squared: 0.5553, Adjusted R-squared: 0.5499 ## F-statistic: 103.4 on 5 and 414 DF, p-value: &lt; 2.2e-16 ## Make Predictions income &lt;- seq(min(CASchools$income), max(CASchools$income), length.out=1001) newdata &lt;- data.frame(income, IncomeCut=cut(income,2), IncomeCut2=cut(income,seq(0,60,by=20))) pred1 &lt;- predict(reg, newdata=newdata) pred2 &lt;- predict(reg2, newdata=newdata) pred3 &lt;- predict(reg3,newdata=newdata) ## Compare Predictions plot(score ~ income, pch=16, col=grey(0,.5), dat=CASchools) lines(income, pred1, lwd=2, col=2) lines(income, pred2, lwd=2, col=4) lines(income, pred3, lwd=2, col=3) legend(&#39;topleft&#39;, legend=c(&#39;OLS&#39;,&#39;Peicewise Linear (2)&#39;,&#39;Peicewise Linear (3)&#39;), lty=1, col=c(2,4,3), cex=.8) ## To Test for Any Break ## strucchange::sctest(score ~ income, data=CASchools, type=&quot;Chow&quot;, point=.5) ## strucchange::Fstats(score ~ income, data=CASchools) ## To Find Changes ## segmented::segmented(reg) Smoothing (Linear Regression using subsample around each data point) plot(score ~ income, pch=16, col=grey(0,.5), dat=CASchools) ## design points CASchoolsO &lt;- CASchools[order(CASchools$income),] ## Loess (adaptive subsamples) reg_lo &lt;- loess(score ~ income, data=CASchoolsO, span=.8) lines(CASchoolsO$income, predict(reg_lo), col=&#39;orange&#39;, type=&#39;o&#39;, pch=8) ## llls (fixed width subsamples) library(np) ## Nonparametric Kernel Methods for Mixed Datatypes (version 0.60-17) ## [vignette(&quot;np_faq&quot;,package=&quot;np&quot;) provides answers to frequently asked questions] ## [vignette(&quot;np&quot;,package=&quot;np&quot;) an overview] ## [vignette(&quot;entropy_np&quot;,package=&quot;np&quot;) an overview of entropy-based methods] reg_np &lt;- npreg(score ~ income, data=CASchoolsO, bws=2, bandwidth.compute=F) lines(CASchoolsO$income, predict(reg_np), col=&#39;purple&#39;, type=&#39;o&#39;, pch=12) For some technical background, see, e.g., https://www.sagepub.com/sites/default/files/upm-binaries/21122_Chapter_21.pdf. Also note that we can compute classic estimates for variability: denoting the Standard Error of the Regression as \\(\\hat{\\sigma}\\), and the Standard Error of the Coefficient Estimates as \\(\\hat{\\sigma}_{\\hat{\\alpha}}\\) and \\(\\hat{\\sigma}_{\\hat{\\beta}}~~\\) (or simply Standard Errors). \\[ \\hat{\\sigma}^2 = \\frac{1}{n-2}\\sum_{i}\\hat{\\epsilon_{i}}^2\\\\ \\hat{\\sigma}^2_{\\hat{\\alpha}}=\\hat{\\sigma}^2\\left[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum_{i}(x_i-\\bar{x})^2}\\right]\\\\ \\hat{\\sigma}^2_{\\hat{\\beta}}=\\frac{\\hat{\\sigma}^2}{\\sum_{i}(x_i-\\bar{x})^2}. \\] These equations are motivated by particular data generating proceses, which you can read more about this at https://www.econometrics-with-r.org/4-lrwor.html.↩︎ "],["ols-multiple-linear-regression.html", " 8 OLS (multiple linear regression) 8.1 Variability Estimates and Hypothesis Tests 8.2 Factor Variables 8.3 Coefficient Interpretation 8.4 Diagnostics 8.5 Linear in Parameters 8.6 More Literature", " 8 OLS (multiple linear regression) Model and objective \\[ y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\ldots+\\beta_kx_{ik}+\\epsilon_i = X_{i}\\beta +\\epsilon_i \\\\ min_{\\beta} \\sum_{i=1}^{n} (\\epsilon_i)^2 \\] Can also be written in matrix form \\[ y=\\textbf{X}\\beta+\\epsilon\\\\ min_{\\beta} (\\epsilon&#39; \\epsilon) \\] Point Estimates \\[ \\hat{\\beta}=(\\textbf{X}&#39;\\textbf{X})^{-1}\\textbf{X}&#39;y \\] 2 Before fitting the model, summarize your data (as in Part I) ## Inspect Dataset on police arrests for the USA in 1973 head(USArrests) ## Murder Assault UrbanPop Rape ## Alabama 13.2 236 58 21.2 ## Alaska 10.0 263 48 44.5 ## Arizona 8.1 294 80 31.0 ## Arkansas 8.8 190 50 19.5 ## California 9.0 276 91 40.6 ## Colorado 7.9 204 78 38.7 library(psych) ## ## Attaching package: &#39;psych&#39; ## The following object is masked from &#39;package:car&#39;: ## ## logit pairs.panels( USArrests[,c(&#39;Murder&#39;,&#39;Assault&#39;,&#39;UrbanPop&#39;)], hist.col=grey(0,.25), breaks=30, density=F, ## Diagonal ellipses=F, rug=F, smoother=F, pch=16, col=grey(0,.5) ## Lower Triangle ) Now we fit the model to the data ## Manually Compute Y &lt;- USArrests[,&#39;Murder&#39;] X &lt;- USArrests[,c(&#39;Assault&#39;,&#39;UrbanPop&#39;)] X &lt;- as.matrix(cbind(1,X)) XtXi &lt;- solve(t(X)%*%X) Bhat &lt;- XtXi %*% (t(X)%*%Y) c(Bhat) ## [1] 3.20715340 0.04390995 -0.04451047 ## Check reg &lt;- lm(Murder~Assault+UrbanPop, data=USArrests) coef(reg) ## (Intercept) Assault UrbanPop ## 3.20715340 0.04390995 -0.04451047 To measure the ``Goodness of fit’’ of the model, we can again compute sums of squared srrors. Adding random data may sometimes improve the fit, however, so we adjust the \\(R^2\\) by the number of covariates \\(K\\). \\[ R^2 = \\frac{ESS}{TSS}=1-\\frac{RSS}{TSS}\\\\ R^2_{\\text{adj.}} = 1-\\frac{n-1}{n-K}(1-R^2) \\] ksims &lt;- 1:30 for(k in ksims){ USArrests[,paste0(&#39;R&#39;,k)] &lt;- runif(nrow(USArrests),0,20) } reg_sim &lt;- lapply(ksims, function(k){ rvars &lt;- c(&#39;Assault&#39;,&#39;UrbanPop&#39;, paste0(&#39;R&#39;,1:k)) rvars2 &lt;- paste0(rvars, collapse=&#39;+&#39;) reg_k &lt;- lm( paste0(&#39;Murder~&#39;,rvars2), data=USArrests) }) R2_sim &lt;- sapply(reg_sim, function(reg_k){ summary(reg_k)$r.squared }) R2adj_sim &lt;- sapply(reg_sim, function(reg_k){ summary(reg_k)$adj.r.squared }) plot.new() plot.window(xlim=c(0,30), ylim=c(0,1)) points(ksims, R2_sim) points(ksims, R2adj_sim, pch=16) axis(1) axis(2) mtext(expression(R^2),2, line=3) mtext(&#39;Additional Random Covariates&#39;, 1, line=3) legend(&#39;topleft&#39;, horiz=T, legend=c(&#39;Undjusted&#39;, &#39;Adjusted&#39;), pch=c(1,16)) 8.1 Variability Estimates and Hypothesis Tests To estimate the variability of our estimates, we can use the same data-driven methods introduced with simple OLS. ## Bootstrap SE&#39;s boots &lt;- 1:399 boot_regs &lt;- lapply(boots, function(b){ b_id &lt;- sample( nrow(USArrests), replace=T) xy_b &lt;- USArrests[b_id,] reg_b &lt;- lm(Murder~Assault+UrbanPop, dat=xy_b) }) boot_coefs &lt;- sapply(boot_regs, coef) boot_mean &lt;- apply(boot_coefs,1, mean) boot_se &lt;- apply(boot_coefs,1, sd) Also as before, we can conduct independant hypothesis tests.3 We can conduct joint tests, such as whether two coefficients are equal, by looking at the their joint distribution. boot_coef_df &lt;- as.data.frame(cbind(ID=boots, t(boot_coefs))) fig &lt;- plotly::plot_ly(boot_coef_df, type = &#39;scatter&#39;, mode = &#39;markers&#39;, x = ~UrbanPop, y = ~Assault, text = ~paste(&#39;&lt;b&gt; boot: &#39;, ID, &#39;&lt;/b&gt;&#39;), hoverinfo=&#39;text&#39;, showlegend=F, marker=list( color=&#39;rgba(0, 0, 0, 0.5)&#39;)) fig ## Show Histogram of Coefficients ## plotly::add_histogram2d(fig, nbinsx=20, nbinsy=20) ## Show 95% Contour ## plotly::add_histogram2dcontour(fig) ## fig &lt;- layout(fig, ## yaxis = list(title=expression(beta[3])), ## xaxis = list(title=expression(beta[2]))) We can also use an \\(F\\) test for \\(q\\) hypotheses; \\[ \\hat{F}_{q} = \\frac{(ESS_{restricted}-ESS_{unrestricted})/q}{ESS_{unrestricted}/(n-K)}, \\] and \\(\\hat{F}\\) can be written in terms of unrestricted and restricted \\(R^2\\). Under some additional assumptions \\(\\hat{F}_{q} \\sim F_{q,n-K}\\). For some inuition, we will examine how the \\(R^2\\) statistic varies with bootstrap samples. Specifically, compute a null \\(R^2\\) distribution by randomly reshuffling the outcomes and compare it to the observed \\(R^2\\). ## Bootstrap NULL boots &lt;- 1:399 boot_regs0 &lt;- lapply(boots, function(b){ xy_b &lt;- USArrests b_id &lt;- sample( nrow(USArrests), replace=T) xy_b$Murder &lt;- xy_b$Murder[b_id] reg_b &lt;- lm(Murder~Assault+UrbanPop, dat=xy_b) }) boot_coefs0 &lt;- sapply(boot_regs0, function(reg_k){ coef(reg_k) }) R2_sim0 &lt;- sapply(boot_regs0, function(reg_k){ summary(reg_k)$r.squared }) R2adj_sim0 &lt;- sapply(boot_regs0, function(reg_k){ summary(reg_k)$adj.r.squared }) hist(R2adj_sim0, xlim=c(0,1), breaks=25, main=&#39;&#39;, xlab=expression(&#39;adj.&#39;~R[b]^2)) abline(v=summary(reg)$adj.r.squared, col=&quot;red&quot;, lwd=2) Hypothesis Testing is not to be done routinely and additional complications arise when testing multiple hypothesis. 8.2 Factor Variables So far, we have discussed cardinal data where the difference between units always means the same thing: e.g., \\(4-3=2-1\\). There are also factor variables Ordered: refers to Ordinal data. The difference between units means something, but not always the same thing. For example, \\(4th - 3rd \\neq 2nd - 1st\\). Unordered: refers to Categorical data. The difference between units is meaningless. For example, \\(B-A=?\\) To analyze either factor, we often convert them into indicator variables or dummies; \\(D_{c}=\\mathbf{1}( Factor = c)\\). One common case is if you have observations of individuals over time periods, then you may have two factor variables. An unordered factor that indicates who an individual is; for example \\(D_{i}=\\mathbf{1}( Individual = i)\\), and an order factor that indicates the time period; for example \\(D_{t}=\\mathbf{1}( Time \\in [month~ t, month~ t+1) )\\). There are many other cases you see factor variables, including spatial ID’s in purely cross sectional data. Be careful not to handle categorical data as if they were cardinal. E.g., generate city data with Leipzig=1, Lausanne=2, LosAngeles=3, … and then include city as if it were a cardinal number (that’s a big no-no). The same applied to ordinal data; PopulationLeipzig=2, PopulationLausanne=3, PopulationLosAngeles=1. N &lt;- 1000 x &lt;- runif(N,3,8) e &lt;- rnorm(N,0,0.4) fo &lt;- factor(rbinom(N,4,.5), ordered=T) fu &lt;- factor(rep(c(&#39;A&#39;,&#39;B&#39;),N/2), ordered=F) dA &lt;- 1*(fu==&#39;A&#39;) y &lt;- (2^as.integer(fo)*dA )*sqrt(x)+ 2*as.integer(fo)*e dat_f &lt;- data.frame(y,x,fo,fu) With factors, you can still include them in the design matrix of an OLS regression \\[ y_{it} = x_{it} \\beta_{x} + d_{c}\\beta_{c} \\] When, as commonly done, the factors are modeled as being additively seperable, they are modelled as either “fixed” or “random” effects. Simply including the factors into the OLS regression yields a “dummy variable” fixed effects estimator. fe_reg0 &lt;- lm(y~x+fo+fu, dat_f) summary(fe_reg0) ## ## Call: ## lm(formula = y ~ x + fo + fu, data = dat_f) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39.266 -5.762 0.161 5.903 34.834 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.6976 1.2053 18.832 &lt; 2e-16 *** ## x 0.9137 0.1991 4.590 5.01e-06 *** ## fo.L 30.4562 1.0966 27.773 &lt; 2e-16 *** ## fo.Q 14.4263 0.9618 15.000 &lt; 2e-16 *** ## fo.C 4.8210 0.7479 6.446 1.78e-10 *** ## fo^4 1.4428 0.5619 2.568 0.0104 * ## fuB -23.6404 0.5891 -40.126 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.271 on 993 degrees of freedom ## Multiple R-squared: 0.7477, Adjusted R-squared: 0.7461 ## F-statistic: 490.3 on 6 and 993 DF, p-value: &lt; 2.2e-16 We can also compute averages for each group and construct a “between estimator” \\[ \\overline{y}_i = \\alpha + \\overline{x}_i \\beta \\] Or we can subtract the average from each group to construct a “within estimator”, \\[ (y_{it} - \\overline{y}_i) = (x_{it}-\\overline{x}_i)\\beta\\\\ \\] that tends to be more computationally efficient, has corrections for standard errors, and has additional summary statistics. library(fixest) ## ## Attaching package: &#39;fixest&#39; ## The following object is masked from &#39;package:np&#39;: ## ## se fe_reg1 &lt;- feols(y~x|fo+fu, dat_f) summary(fe_reg1) ## OLS estimation, Dep. Var.: y ## Observations: 1,000 ## Fixed-effects: fo: 5, fu: 2 ## Standard-errors: Clustered (fo) ## Estimate Std. Error t value Pr(&gt;|t|) ## x 0.913706 0.27842 3.28175 0.03045 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## RMSE: 9.23823 Adj. R2: 0.746126 ## Within R2: 0.020772 ## Compare Coefficients coef( lm(y~-1+x+fo+fu, dat_f) ) ## x fo0 fo1 fo2 fo3 fo4 ## 0.9137058 9.7945102 11.5702042 16.0210372 24.7341968 51.3679374 ## fuB ## -23.6403531 fixef(fe_reg1) ## $fo ## 0 1 2 3 4 ## 9.79451 11.57020 16.02104 24.73420 51.36794 ## ## $fu ## A B ## 0.00000 -23.64035 ## ## attr(,&quot;class&quot;) ## [1] &quot;fixest.fixef&quot; &quot;list&quot; ## attr(,&quot;references&quot;) ## fo fu ## 0 1 ## attr(,&quot;exponential&quot;) ## [1] FALSE Hansen Econometrics, Theorem 17.1: The fixed effects estimator of \\(\\beta\\) algebraically equals the dummy variable estimator of \\(\\beta\\). The two estimators have the same residuals. Consistency is a great property, but only if the data generating process does in fact match the model. Many factor variables have effects that are not additively seperable. reg1 &lt;- feols(y~x|fo^fu, dat_f) summary(reg1) ## OLS estimation, Dep. Var.: y ## Observations: 1,000 ## Fixed-effects: fo^fu: 10 ## Standard-errors: Clustered (fo^fu) ## Estimate Std. Error t value Pr(&gt;|t|) ## x 1.03138 0.510805 2.01913 0.074226 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## RMSE: 3.42866 Adj. R2: 0.964889 ## Within R2: 0.163794 reg2 &lt;- lm(y~x*fo*fu, dat_f) summary(reg2) ## ## Call: ## lm(formula = y ~ x * fo * fu, data = dat_f) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.6185 -1.5506 -0.0306 1.3451 8.3707 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.35419 0.59451 25.827 &lt; 2e-16 *** ## x 2.48293 0.10271 24.175 &lt; 2e-16 *** ## fo.L 27.50539 1.70617 16.121 &lt; 2e-16 *** ## fo.Q 9.30430 1.48435 6.268 5.46e-10 *** ## fo.C 1.98641 1.13576 1.749 0.0806 . ## fo^4 -1.03778 0.81517 -1.273 0.2033 ## fuB -15.23195 0.87766 -17.355 &lt; 2e-16 *** ## x:fo.L 4.66923 0.29335 15.917 &lt; 2e-16 *** ## x:fo.Q 2.02712 0.25565 7.929 5.98e-15 *** ## x:fo.C 0.48973 0.19748 2.480 0.0133 * ## x:fo^4 0.27254 0.14341 1.900 0.0577 . ## x:fuB -2.47465 0.14923 -16.583 &lt; 2e-16 *** ## fo.L:fuB -27.09116 2.53818 -10.673 &lt; 2e-16 *** ## fo.Q:fuB -11.04996 2.20272 -5.017 6.25e-07 *** ## fo.C:fuB -4.23465 1.65776 -2.554 0.0108 * ## fo^4:fuB 0.49632 1.16753 0.425 0.6709 ## x:fo.L:fuB -4.67916 0.42929 -10.900 &lt; 2e-16 *** ## x:fo.Q:fuB -1.73116 0.37352 -4.635 4.06e-06 *** ## x:fo.C:fuB -0.06135 0.28335 -0.217 0.8286 ## x:fo^4:fuB -0.19264 0.20321 -0.948 0.3434 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.438 on 980 degrees of freedom ## Multiple R-squared: 0.9828, Adjusted R-squared: 0.9824 ## F-statistic: 2944 on 19 and 980 DF, p-value: &lt; 2.2e-16 #reg2 &lt;- feols(y~x*fo*fu|fo^fu, dat_f) With Random Effects, the factor variable is modelled as coming from a distribution that is uncorrelated with the regressors. This is rarely used in economics today, and mostly included for historical reasons and a few cases where fixed effects cannot be estimates. 8.3 Coefficient Interpretation Notice that we have gotten pretty far without actually trying to meaningfully interpret regression coefficients. That is because the above procedure will always give us number, regardless as to whether the true data generating process is linear or not. So, to be cautious, we have been interpretting the regression outputs while being agnostic as to how the data are generated. We now consider a special situation where we know the data are generated according to a linear process and are only uncertain about the parameter values. If the data generating process is \\[ y=X\\beta + \\epsilon\\\\ \\mathbb{E}[\\epsilon | X]=0, \\] then we have a famous result that lets us attach a simple interpretation of OLS coefficients as unbiased estimates of the effect of X: \\[ \\hat{\\beta} = (X&#39;X)^{-1}X&#39;y = (X&#39;X)^{-1}X&#39;(X\\beta + \\epsilon) = \\beta + (X&#39;X)^{-1}X&#39;\\epsilon\\\\ \\mathbb{E}\\left[ \\hat{\\beta} \\right] = \\mathbb{E}\\left[ (X&#39;X)^{-1}X&#39;y \\right] = \\beta + (X&#39;X)^{-1}\\mathbb{E}\\left[ X&#39;\\epsilon \\right] = \\beta \\] Generate a simulated dataset with 30 observations and two exogenous variables. Assume the following relationship: \\(y_{i} = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\epsilon_i\\) where the variables and the error term are realizations of the following data generating processes (DGP): N &lt;- 30 B &lt;- c(10, 2, -1) x1 &lt;- runif(N, 0, 5) x2 &lt;- rbinom(N,1,.7) X &lt;- cbind(1,x1,x2) e &lt;- rnorm(N,0,3) Y &lt;- X%*%B + e dat &lt;- data.frame(Y,X) coef(lm(Y~x1+x2, data=dat)) ## (Intercept) x1 x2 ## 10.9934900 1.6503856 -0.3715245 Simulate the distribution of coefficients under a correctly specified model. Interpret the average. N &lt;- 30 B &lt;- c(10, 2, -1) Coefs &lt;- sapply(1:400, function(sim){ x1 &lt;- runif(N, 0, 5) x2 &lt;- rbinom(N,1,.7) X &lt;- cbind(1,x1,x2) e &lt;- rnorm(N,0,3) Y &lt;- X%*%B + e dat &lt;- data.frame(Y,x1,x2) coef(lm(Y~x1+x2, data=dat)) }) par(mfrow=c(1,2)) for(i in 2:3){ hist(Coefs[i,], xlab=bquote(beta[.(i)]), main=&#39;&#39;) abline(v=mean(Coefs[i,]), col=1, lty=2) abline(v=B[i], col=2) } Many economic phenomena are nonlinear, even when including potential transforms of \\(Y\\) and \\(X\\). Sometimes the linear model may still be a good or even great approximation (how good depends on the research question). In any case, you are safe to interpret your OLS coefficients as “conditional correlations”. For example, examine the distribution of coefficients under this mispecified model. Interpret the average. N &lt;- 30 Coefs &lt;- sapply(1:600, function(sim){ x1 &lt;- runif(N, 0, 5) x2 &lt;- rbinom(N,1,.7) e &lt;- rnorm(N,0,3) Y &lt;- 10*x2 + 2*log(x1)^x2 + e dat &lt;- data.frame(Y,x1,x2) coef(lm(Y~x1+x2, data=dat)) }) par(mfrow=c(1,2)) for(i in 2:3){ hist(Coefs[i,], xlab=bquote(beta[.(i)]), main=&#39;&#39;) abline(v=mean(Coefs[i,]), col=1, lty=2) } 8.4 Diagnostics There’s little sense in getting great standard errors for a terrible model. Plotting your regression object a simple and easy step to help diagnose whether your model is in some way bad. reg &lt;- lm(Murder~Assault+UrbanPop, data=USArrests) par(mfrow=c(2,2)) plot(reg) We now go through what these figures show, and then some additional Outliers The first plot examines outlier \\(Y\\) and \\(\\hat{Y}\\). ``In our \\(y_i = a + b x_i + e_i\\) regression, the residuals are, of course, \\(e_i\\) – they reveal how much our fitted value \\(\\hat{y}_i = a + b x_i\\) differs from the observed \\(y_i\\). A point \\((x_i ,y_i)\\) with a corresponding large residual is called an outlier. Say that you are interested in outliers because you somehow think that such points will exert undue influence on your estimates. Your feelings are generally right, but there are exceptions. A point might have a huge residual and yet not affect the estimated \\(b\\) at all’’ Stata Press (2015) Base Reference Manual, Release 14, p. 2138. plot(fitted(reg), resid(reg),col = &quot;grey&quot;, pch = 20, xlab = &quot;Fitted&quot;, ylab = &quot;Residual&quot;, main = &quot;Fitted versus Residuals&quot;) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) # car::outlierTest(reg) The third plot examines outlier \\(X\\) via ``leverage’’ “\\((x_i ,y_i)\\) can be an outlier in another way – just as \\(y_i\\) can be far from \\(\\hat{y}_i\\), \\(x_i\\) can be far from the center of mass of the other \\(x\\)’s. Such an `outlier’ should interest you just as much as the more traditional outliers. Picture a scatterplot of \\(y\\) against \\(x\\) with thousands of points in some sort of mass at the lower left of the graph and one point at the upper right of the graph. Now run a regression line through the points—the regression line will come close to the point at the upper right of the graph and may in fact, go through it. That is, this isolated point will not appear as an outlier as measured by residuals because its residual will be small. Yet this point might have a dramatic effect on our resulting estimates in the sense that, were you to delete the point, the estimates would change markedly. Such a point is said to have high leverage’’ Stata Press (2015) Base Reference Manual, Release 14, pp. 2138-39. N &lt;- 40 x &lt;- c(25, runif(N-1,3,8)) e &lt;- rnorm(N,0,0.4) y &lt;- 3 + 0.6*sqrt(x) + e plot(y~x, pch=16, col=grey(.5,.5)) points(x[1],y[1], pch=16, col=rgb(1,0,0,.5)) abline(lm(y~x), col=2, lty=2) abline(lm(y[-1]~x[-1])) See https://www.r-bloggers.com/2016/06/leverage-and-influence-in-a-nutshell/ for a good interactive explaination. Leverage Vector: Distance within explanatory variables \\[ H = [h_{1}, h_{2}, ...., h_{N}] \\] \\(h_i\\) is the leverage of residual \\(\\hat{\\epsilon_i}\\). Studentized residuals \\[ r_i=\\frac{\\hat{\\epsilon}_i}{s_{[i]}\\sqrt{1-h_i}} \\] and \\(s_{(i)}\\) the root mean squared error of a regression with the \\(i\\)th observation removed. reg &lt;- lm(y~x) which.max(hatvalues(reg)) ## 1 ## 1 which.max(rstandard(reg)) ## 37 ## 37 The fourth plot further assesses outlier \\(X\\) using “Cook’s Distance”. Cook’s Distance is defined as the sum of all the changes in the regression model when observation i is removed from. \\[ D_{i} = \\frac{\\sum_{j} \\left( \\hat{y_j} - \\hat{y_j}_{[i]} \\right)^2 }{ p s^2 } = \\frac{[e_{i}]^2}{p s^2 } \\frac{h_i}{(1-h_i)^2}\\\\ s^2 = \\frac{\\sum_{i} (e_{i})^2 }{n-K} \\] which.max(cooks.distance(reg)) ## 1 ## 1 car::influencePlot(reg) ## StudRes Hat CookD ## 1 -3.528709 0.82508224 22.56662371 ## 37 2.937407 0.02711964 0.10015471 ## 40 -1.474646 0.04279714 0.04715585 Note that we can also calculate \\(H\\) directly from our OLS projection matrix \\(\\hat{P}\\), since \\(H=diag(\\hat{P})\\) and \\[ \\hat{P}=X(X&#39;X)^{-1}X&#39;\\\\ \\hat{\\epsilon}=y-X\\hat{\\beta}=y-X(X&#39;X)^{-1}X&#39;y=y-\\hat{P}y\\\\ \\hat{P}y=X(X&#39;X)^{-1}X&#39;y=y-(y-X(X&#39;X)^{-1}X&#39;y)=y-\\hat{\\epsilon}=\\hat{y}\\\\ \\] Ehat &lt;- Y - X%*% Bhat ## Ehat ## resid(reg) Pmat &lt;- X%*%XtXi%*%t(X) Yhat &lt;- Pmat%*%Y ## Yhat ## predict(reg) There are many other diagnostics (which can often be written in terms of Cooks Distance or Vice Versa). # Sall, J. (1990) Leverage plots for general linear hypotheses. American Statistician *44*, 308-315. # car::leveragePlots(reg) (Welsch and Kuh. 1977; Belsley, Kuh, and Welsch. 1980) attempt to summarize the information in the leverage versus residual-squared plot into one DFITS statistic where \\(DFITS &gt; 2\\sqrt{{k}/{n}}\\) should be examined. \\[ \\text{DFITS}_i=r_i\\sqrt{\\frac{h_i}{1-h_i}}\\\\ \\] See also “dfbetas” and “covratio” #dfbetas(reg) #dffits(reg) #covratio(reg) #hatvalues(reg) head(influence.measures(reg)$infmat) ## dfb.1_ dfb.x dffit cov.r cook.d hat ## 1 5.895593376 -7.546857967 -7.663858558 3.375743 2.256662e+01 0.82508224 ## 2 -0.054160385 -0.044130894 -0.191911003 1.008004 1.824283e-02 0.02639579 ## 3 0.023873568 0.009686202 0.065910396 1.072814 2.220851e-03 0.02555185 ## 4 0.005799585 -0.001646485 0.008974371 1.082620 4.135465e-05 0.02587080 ## 5 0.007365469 0.018880718 0.051571358 1.080907 1.362449e-03 0.02886954 ## 6 0.017180030 -0.011682579 0.018385232 1.100488 1.735400e-04 0.04193046 Normality The second plot examines whether the residuals are normally distributed. OLS point estimates do not depend on the normality of the residuals. (Good thing, because there’s no reason the residuals of economic phenomena should be so well behaved.) Many hypothesis tests of the regression estimates are, however, affected by the distribution of the residuals. For these reasons, you may be interested in assessing normality par(mfrow=c(1,2)) hist(resid(reg), main=&#39;Histogram of Residuals&#39;, border=NA) qqnorm(resid(reg), main=&quot;Normal Q-Q Plot of Residuals&quot;, col=&quot;darkgrey&quot;) qqline(resid(reg), col=&quot;dodgerblue&quot;, lwd=2) shapiro.test(resid(reg)) ## ## Shapiro-Wilk normality test ## ## data: resid(reg) ## W = 0.95995, p-value = 0.1668 # car::qqPlot(reg) Heterskedasticity may also matters for variance estimates. This is not shown in the plot, but you can run a simple test library(lmtest) lmtest::bptest(reg) ## ## studentized Breusch-Pagan test ## ## data: reg ## BP = 0.29693, df = 1, p-value = 0.5858 Collinearity This is when one explanatory variable in a multiple linear regression model can be linearly predicted from the others with a substantial degree of accuracy. Coefficient estimates may change erratically in response to small changes in the model or the data. (In the extreme case where there are more variables than observations \\(K&gt;\\geq N\\), \\(X&#39;X\\) has an infinite number of solutions and is not invertible.) To diagnose this, we can use the Variance Inflation Factor \\[ VIF_{k}=\\frac{1}{1-R^2_k}, \\] where \\(R^2_k\\) is the \\(R^2\\) for the regression of \\(X_k\\) on the other covariates \\(X_{-k}\\) (a regression that does not involve the response variable Y) car::vif(reg) sqrt(car::vif(reg)) &gt; 2 # problem? 8.5 Linear in Parameters Data transformations can often improve model fit and still be estimated via OLS. This is because OLS only requires the model to be linear in the parameters. Under the assumptions of the model is correctly specified, the following table is how we can interpret the coefficients of the transformed data. (Note for small changes, \\(\\Delta ln(x) \\approx \\Delta x / x = \\Delta x \\% \\cdot 100\\).) Specification Regressand Regressor Derivative Interpretation (If True) linear–linear \\(y\\) \\(x\\) \\(\\Delta y = \\beta_1\\cdot\\Delta x\\) Change \\(x\\) by one unit \\(\\rightarrow\\) change \\(y\\) by \\(\\beta_1\\) units. log–linear \\(ln(y)\\) \\(x\\) \\(\\Delta y \\% \\cdot 100 \\approx \\beta_1 \\cdot \\Delta x\\) Change \\(x\\) by one unit \\(\\rightarrow\\) change \\(y\\) by \\(100 \\cdot \\beta_1\\) percent. linear–log \\(y\\) \\(ln(x)\\) \\(\\Delta y \\approx \\frac{\\beta_1}{100}\\cdot \\Delta x \\%\\) Change \\(x\\) by one percent \\(\\rightarrow\\) change \\(y\\) by \\(\\frac{\\beta_1}{100}\\) units log–log \\(ln(y)\\) \\(ln(x)\\) \\(\\Delta y \\% \\approx \\beta_1\\cdot \\Delta x \\%\\) Change \\(x\\) by one percent \\(\\rightarrow\\) change \\(y\\) by \\(\\beta_1\\) percent Now recall from micro theory that an additively seperable and linear production function is referred to as ``perfect substitutes’‘. With a linear model and untranformed data, you have implicitly modelled the different regressors \\(X\\) as perfect substitutes. Further recall that the’‘perfect substitutes’’ model is a special case of the constant elasticity of substitution production function. Here, we will build on http://dx.doi.org/10.2139/ssrn.3917397, and consider box-cox transforming both \\(X\\) and \\(y\\). Specifically, apply the box-cox transform of \\(y\\) using parameter \\(\\lambda\\) and apply another box-cox transform to each \\(x\\) using the same parameter \\(\\rho\\) so that \\[ y^{(\\lambda)}_{i} = \\sum_{k}\\beta_{k} x^{(\\rho)}_{k,i} + \\epsilon_{i}\\\\ y^{(\\lambda)}_{i} = \\begin{cases} \\lambda^{-1}[ (y_i+1)^{\\lambda}- 1] &amp; \\lambda \\neq 0 \\\\ log(y_i+1) &amp;  \\lambda=0 \\end{cases}.\\\\ x^{(\\rho)}_{i} = \\begin{cases} \\rho^{-1}[ (x_i)^{\\rho}- 1] &amp; \\rho \\neq 0 \\\\ log(x_{i}+1) &amp;  \\rho=0 \\end{cases}. \\] Notice that this nests: linear-linear \\((\\rho=\\lambda=1)\\). linear-log \\((\\rho=1, \\lambda=0)\\). log-linear \\((\\rho=0, \\lambda=1)\\). log-log \\((\\rho=\\lambda=0)\\). If \\(\\rho=\\lambda\\), we get the CES production function. This nests the ‘’perfect substitutes’’ linear-linear model (\\(\\rho=\\lambda=1\\)) , the ‘’cobb-douglas’’ log-log model (\\(\\rho=\\lambda=0\\)), and many others. We can define \\(\\lambda=\\rho/\\lambda&#39;\\) to be clear that this is indeed a CES-type transformation where \\(\\rho \\in (-\\infty,1]\\) controls the “substitutability” of explanatory variables. E.g., \\(\\rho &lt;0\\) is ‘’complementary’’. \\(\\lambda\\) determines ‘’returns to scale’‘. E.g., \\(\\lambda&lt;1\\) is’‘decreasing returns’’. We compute the mean squared error in the original scale by inverting the predictions; \\[ \\widehat{y}_{i} = \\begin{cases} [ \\widehat{y^{(\\lambda)}}_{i} \\cdot \\lambda ]^{1/\\lambda} -1 &amp; \\lambda \\neq 0 \\\\ exp( \\widehat{y^{(\\lambda)}}_{i}) -1 &amp;  \\lambda=0 \\end{cases}. \\] It is easiest to optimize parameters in a 2-step procedure called `concentrated optimization’. We first solve for \\(\\widehat{\\beta}(\\rho,\\lambda)\\) and compute the mean squared error \\(MSE(\\rho,\\lambda)\\). We then find the \\((\\rho,\\lambda)\\) which minimizes \\(MSE\\). ## Box-Cox Transformation Function bxcx &lt;- function( xy, rho){ if (rho == 0L) { log(xy+1) } else if(rho == 1L){ xy } else { ((xy+1)^rho - 1)/rho } } bxcx_inv &lt;- function( xy, rho){ if (rho == 0L) { exp(xy) - 1 } else if(rho == 1L){ xy } else { (xy * rho + 1)^(1/rho) - 1 } } ## Which Variables reg &lt;- lm(Murder~Assault+UrbanPop, data=USArrests) X &lt;- USArrests[,c(&#39;Assault&#39;,&#39;UrbanPop&#39;)] Y &lt;- USArrests[,&#39;Murder&#39;] ## Simple Grid Search ## Which potential (Rho,Lambda) rl_df &lt;- expand.grid(rho=seq(-2,2,by=.5),lambda=seq(-2,2,by=.5)) ## Compute Mean Squared Error ## from OLS on Transformed Data errors &lt;- apply(rl_df,1,function(rl){ Xr &lt;- bxcx(X,rl[[1]]) Yr &lt;- bxcx(Y,rl[[2]]) Datr &lt;- cbind(Murder=Yr,Xr) Regr &lt;- lm(Murder~Assault+UrbanPop, data=Datr) Predr &lt;- bxcx_inv(predict(Regr),rl[[2]]) Resr &lt;- (Y - Predr) return(Resr) }) rl_df$mse &lt;- colMeans(errors^2) ## Want Small MSE and Interpretable ## (-1,0,1,2 are Easy to interpretable) library(ggplot2) ## ## Attaching package: &#39;ggplot2&#39; ## The following objects are masked from &#39;package:psych&#39;: ## ## %+%, alpha ggplot(rl_df, aes(rho, lambda, fill=log(mse) )) + geom_tile() + ggtitle(&#39;Mean Squared Error&#39;) ## Which min rl0 &lt;- rl_df[which.min(rl_df$mse),c(&#39;rho&#39;,&#39;lambda&#39;)] ## Which give NA? ## which(is.na(errors), arr.ind=T) ## Plot Xr &lt;- bxcx(X,rl0[[1]]) Yr &lt;- bxcx(Y,rl0[[2]]) Datr &lt;- cbind(Murder=Yr,Xr) Regr &lt;- lm(Murder~Assault+UrbanPop, data=Datr) Predr &lt;- bxcx_inv(predict(Regr),rl0[[2]]) cols &lt;- c(rgb(1,0,0,.5), col=rgb(0,0,1,.5)) plot(Y, Predr, pch=16, col=cols[1], ylab=&#39;Prediction&#39;) points(Y, predict(reg), pch=16, col=cols[2]) legend(&#39;topleft&#39;, pch=c(16), col=cols, title=&#39;Rho,Lambda&#39;, legend=c( paste0(rl0, collapse=&#39;,&#39;),&#39;1,1&#39;) ) Note that the default hypothesis testing procedures do not account for you trying out different transformations. Specification searches deflate standard errors and are a major source for false discoveries. 8.6 More Literature For OLS, see https://bookdown.org/josiesmith/qrmbook/linear-estimation-and-minimizing-error.html https://www.econometrics-with-r.org/4-lrwor.html https://www.econometrics-with-r.org/6-rmwmr.html https://www.econometrics-with-r.org/7-htaciimr.html https://bookdown.org/ripberjt/labbook/bivariate-linear-regression.html https://bookdown.org/ripberjt/labbook/multivariable-linear-regression.html https://online.stat.psu.edu/stat462/node/137/ https://book.stat420.org/ Hill, Griffiths &amp; Lim (2007), Principles of Econometrics, 3rd ed., Wiley, S. 86f. Verbeek (2004), A Guide to Modern Econometrics, 2nd ed., Wiley, S. 51ff. Asteriou &amp; Hall (2011), Applied Econometrics, 2nd ed., Palgrave MacMillan, S. 177ff. https://online.stat.psu.edu/stat485/lesson/11/ For fixed effects, see https://www.econometrics-with-r.org/10-rwpd.html https://bookdown.org/josiesmith/qrmbook/topics-in-multiple-regression.html https://bookdown.org/ripberjt/labbook/multivariable-linear-regression.html https://www.princeton.edu/~otorres/Panel101.pdf https://www.stata.com/manuals13/xtxtreg.pdf Diagnostics https://book.stat420.org/model-diagnostics.html#leverage https://socialsciences.mcmaster.ca/jfox/Books/RegressionDiagnostics/index.html https://bookdown.org/ripberjt/labbook/diagnosing-and-addressing-problems-in-linear-regression.html Belsley, D. A., Kuh, E., and Welsch, R. E. (1980). Regression Diagnostics: Identifying influential data and sources of collinearity. Wiley. https://doi.org/10.1002/0471725153 Fox, J. D. (2020). Regression diagnostics: An introduction (2nd ed.). SAGE. https://dx.doi.org/10.4135/9781071878651 To derive OLS coefficients in Matrix form, see * https://jrnold.github.io/intro-methods-notes/ols-in-matrix-form.html * https://www.fsb.miamioh.edu/lij14/411_note_matrix.pdf * https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf↩︎ This is done using t-values \\[\\hat{t}_{j} = \\frac{\\hat{\\beta}_j - \\beta_{0} }{\\hat{\\sigma}_{\\hat{\\beta}_j}}\\]. Under some additional assumptions \\(\\hat{t}_{j} \\sim t_{n-K}\\).↩︎ "],["endogeneity-issues.html", " 9 Endogeneity Issues 9.1 Two Stage Least Squares (2SLS) 9.2 Regression Discontinuities/Kink (RD/RK) 9.3 Difference in Differences (DID) 9.4 More Literature", " 9 Endogeneity Issues Just like many economic relationships are nonlinear, many economic variables are endogenous. By this we typically mean that \\(X\\) is an outcome determined (or caused: \\(\\to\\)) by some other variable. If \\(Y \\to X\\), then we have reverse causality If \\(Y \\to X\\) and \\(X \\to Y\\), then we have simultaneity If \\(Z\\to Y\\) and either \\(Z\\to X\\) or \\(X \\to Z\\), then we have omitted a potentially important variable These endogeneity issues imply \\(X\\) and \\(\\epsilon\\) are correlated, which is a barrier to interpreting OLS estimates causally. (\\(X\\) and \\(\\epsilon\\) may be correlated for other reasons too, such as when \\(X\\) is measured with error.) ## Simulate data with an endogeneity issue n &lt;- 300 z &lt;- rbinom(n,1,.5) xy &lt;- sapply(z, function(zi){ y &lt;- rnorm(1,zi,1) x &lt;- rnorm(1,zi*2,1) c(x,y) }) xy &lt;- data.frame(x=xy[1,],y=xy[2,]) plot(y~x, data=xy, pch=16, col=grey(.5,.5)) abline(lm(y~x,data=xy)) With multiple linear regression, note that endogeneity biases are not just a problem your main variable. Suppose your interested in how \\(x_{1}\\) affects \\(y\\), conditional on \\(x_{2}\\). Letting \\(X=[x_{1}, x_{2}]\\), you estimate \\[\\begin{eqnarray} \\hat{\\beta}_{OLS} = [X&#39;X]^{-1}X&#39;y \\end{eqnarray}\\] You paid special attention in your research design to find a case where \\(x_{1}\\) is truly exogenous. Unfortunately, \\(x_{2}\\) is correlated with the error term. (How unfair, I know, especially after all that work). Nonetheless, \\[\\begin{eqnarray} \\mathbb{E}[X&#39;\\epsilon] = \\begin{bmatrix} 0 \\\\ \\rho \\end{bmatrix}\\\\ \\mathbb{E}[ \\hat{\\beta}_{OLS} - \\beta] = [X&#39;X]^{-1} \\begin{bmatrix} 0 \\\\ \\rho \\end{bmatrix} = \\begin{bmatrix} \\rho_{1} \\\\ \\rho_{2} \\end{bmatrix} \\end{eqnarray}\\] The magnitude of the bias for \\(x_{1}\\) thus depends on the correlations between \\(x_{1}\\) and \\(x_{2}\\) as well as \\(x_{2}\\) and \\(\\epsilon\\). Three statistical tools: 2SLS, RDD, and DID, are designed to address endogeneity issues. The elementary versions of these tools are linear regression. Because there are many textbooks and online notebooks that explain these methods at both high and low levels of technical detail, they are not covered extensively in this notebook. 9.1 Two Stage Least Squares (2SLS) There are many approaches to 2SLS, but I will focus on the seminal example in economics. Suppose we ask “what is the effect of price on quantity?” You can simply run a regression of one variable on another, but you will need much luck to correctly intepret the resulting coefficient. The reason is simultaneity: price and quantity mutually cause one another in markets.4 Competitive Market Equilibrium has three structural relationships: (1) market supply is the sum of quantities supplied by individual firms at a given price, (2) market demand is the sum of quantities demanded by individual people at a given price, and (3) market supply equals market demand in equilibrium. Assuming market supply and demand are linear, we can write these three “structural equations” as \\[\\begin{eqnarray} Q_{S}(P) &amp;=&amp; A_{S} + B_{S} P + E_{S},\\\\ Q_{D}(P) &amp;=&amp; A_{D} - B_{D} P + E_{D},\\\\ Q_{D} &amp;=&amp; Q_{S} = Q. %% $Q_{D}(P) = \\sum_{i} q_{D}_{i}(P)$, \\end{eqnarray}\\] This last equation implies a simultaneous “reduced form” relationship where both the price and the quantity are outcomes. With a linear parametric structure to these equations, we can use algebra to solve for the equilibrium price and quantity analytically as \\[\\begin{eqnarray} P^{*} &amp;=&amp; \\frac{A_{D}-A_{S}}{B_{D}+B_{S}} + \\frac{E_{D} - E_{S}}{B_{D}+B_{S}}, \\\\ Q^{*} &amp;=&amp; \\frac{A_{S}B_{D}+ A_{D}B_{S}}{B_{D}+B_{S}} + \\frac{E_{S}B_{D}+ E_{D}B_{S}}{B_{D}+B_{S}}. \\end{eqnarray}\\] ## Demand Curve Simulator qd_fun &lt;- function(p, Ad=8, Bd=-.8, Ed_sigma=.25){ Qd &lt;- Ad + Bd*p + rnorm(1,0,Ed_sigma) return(Qd) } ## Supply Curve Simulator qs_fun &lt;- function(p, As=-8, Bs=1, Es_sigma=.25){ Qs &lt;- As + Bs*p + rnorm(1,0,Es_sigma) return(Qs) } ## Quantity Supplied and Demanded at 3 Prices cbind(P=8:10, D=qd_fun(8:10), S=qs_fun(8:10)) ## P D S ## [1,] 8 1.4258219 -0.07388062 ## [2,] 9 0.6258219 0.92611938 ## [3,] 10 -0.1741781 1.92611938 ## Market Equilibrium Finder eq_fun &lt;- function(demand, supply, P){ ## Compute EQ (what we observe) eq_id &lt;- which.min( abs(demand-supply) ) eq &lt;- c(P=P[eq_id], Q=demand[eq_id]) return(eq) } ## Simulations Parameters N &lt;- 300 ## Number of Market Interactions P &lt;- seq(5,10,by=.01) ## Price Range to Consider ## Generate Data from Competitive Market ## Plot Underlying Process plot.new() plot.window(xlim=c(0,2), ylim=range(P)) EQ1 &lt;- sapply(1:N, function(n){ ## Market Data Generating Process demand &lt;- qd_fun(P) supply &lt;- qs_fun(P) eq &lt;- eq_fun(demand, supply, P) ## Plot Theoretical Supply and Demand lines(demand, P, col=grey(0,.01)) lines(supply, P, col=grey(0,.01)) points(eq[2], eq[1], col=grey(0,.05), pch=16) ## Save Data return(eq) }) axis(1) axis(2) mtext(&#39;Quantity&#39;,1, line=2) mtext(&#39;Price&#39;,2, line=2) Now regress quantity (“Y”) on price (“X”): \\(\\widehat{\\beta}_{OLS} = Cov(Q^{*}, P^{*}) / Var(P^{*})\\). You get a number back, but it is hard to interpret meaningfully. ## Analyze Market Data dat1 &lt;- data.frame(t(EQ1), cost=&#39;1&#39;) reg1 &lt;- lm(Q~P, data=dat1) summary(reg1) ## ## Call: ## lm(formula = Q ~ P, data = dat1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5782 -0.1367 -0.0132 0.1216 0.5108 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.10465 0.48444 0.216 0.829 ## P 0.08830 0.05447 1.621 0.106 ## ## Residual standard error: 0.188 on 298 degrees of freedom ## Multiple R-squared: 0.00874, Adjusted R-squared: 0.005414 ## F-statistic: 2.628 on 1 and 298 DF, p-value: 0.1061 This simple derivation has a profound insight: price-quantity data does not generally tell you how price affects quantity (or vice-versa). Moreover, it also clarifies that our initial question “what is the effect of price on quantity?” is misguided. We could more sensibly ask “what is the effect of price on quantity supplied?” or “what is the effect of price on quantity demanded?” If you have exogeneous variation on one side of the market, “shocks”, you can get information on the other. For example, lower costs shift out supply (more is produced at given price), allowing you to trace out part of a demand curve. Experimental manipulation of \\(A_{S}\\) leads to \\[\\begin{eqnarray} \\label{eqn:comp_market_statics} \\frac{d P^{*}}{d A_{S}} = \\frac{-1}{B_{D}+B_{S}}, \\\\ \\frac{d Q^{*}}{d A_{S}} = \\frac{B_{D}}{B_{D}+B_{S}}. \\end{eqnarray}\\] So, absent any other changes, we could recover \\(-B_{D}=d Q^{*}/d P^{*}\\). In this case, the supply shock has identified the demand slope.5 ## New Observations After Cost Change EQ2 &lt;- sapply(1:N, function(n){ demand &lt;- qd_fun(P) supply2 &lt;- qs_fun(P, As=-6.5) ## More Supplied at Given Price eq &lt;- eq_fun(demand, supply2, P) return(eq) ## lines(supply2, P, col=rgb(0,0,1,.01)) #points(eq[2], eq[1], col=rgb(0,0,1,.05), pch=16) }) dat2 &lt;- data.frame(t(EQ2), cost=&#39;2&#39;) ## Plot Market Data dat2 &lt;- rbind(dat1, dat2) cols &lt;- ifelse(as.numeric(dat2$cost)==2, rgb(0,0,1,.2), rgb(0,0,0,.2)) plot.new() plot.window(xlim=c(0,2), ylim=range(P)) points(dat2$Q, dat2$P, col=cols, pch=16) axis(1) axis(2) mtext(&#39;Quantity&#39;,1, line=2) mtext(&#39;Price&#39;,2, line=2) We are not quite done yet, however. We have pooled two datasets that are seperately problematic, and the noisiness of the process within each group affects our OLS estimate: \\(\\widehat{\\beta}_{OLS}=Cov(Q^{*}, P^{*}) / Var(P^{*})\\). ## Not exactly right, but at least right sign reg2 &lt;- lm(Q~P, data=dat2) summary(reg2) ## ## Call: ## lm(formula = Q ~ P, data = dat2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.73060 -0.17541 -0.00829 0.17068 0.74994 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.67257 0.19204 34.75 &lt;2e-16 *** ## P -0.64218 0.02261 -28.41 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2507 on 598 degrees of freedom ## Multiple R-squared: 0.5744, Adjusted R-squared: 0.5737 ## F-statistic: 807 on 1 and 598 DF, p-value: &lt; 2.2e-16 Although the individual observations are noisy, we can compute the change in the expected values \\(d \\mathbb{E}[Q^{*}] / d \\mathbb{E}[P^{*}] =-B_{D}\\). Empirically, this is estimated via the change in average value. ## Wald (1940) Estimate dat_mean &lt;- rbind( colMeans(dat2[dat2$cost==1,1:2]), colMeans(dat2[dat2$cost==2,1:2])) dat_mean ## P Q ## [1,] 8.891167 0.8897032 ## [2,] 8.074800 1.5602511 B_est &lt;- diff(dat_mean[,2])/diff(dat_mean[,1]) round(B_est, 2) ## [1] -0.82 We can also seperately recover \\(d \\mathbb{E}[Q^{*}] / d \\mathbb{E}[A_{S}]\\) and \\(d \\mathbb{E}[P^{*}] / d \\mathbb{E}[A_{S}]\\) from seperate regressions ## Heckman (2000, p.58) Estimate ols_1 &lt;- lm(P~cost, data=dat2) ols_2 &lt;- lm(Q~cost, data=dat2) B_est2 &lt;- coef(ols_2)/coef(ols_1) round(B_est2[[2]],2) ## [1] -0.82 Mathematically, we can also do this in a single step by exploiting linear algebra: \\[\\begin{eqnarray} \\frac{\\frac{ Cov(Q^{*},A_{S})}{ V(A_{S}) } }{\\frac{ Cov(P^{*},A_{S})}{ V(A_{S}) }} &amp;=&amp; \\frac{Cov(Q^{*},A_{S} )}{ Cov(P^{*},A_{S})}. \\end{eqnarray}\\] Alternatively, we can recover the same estimate using an instrumental variables regression that has two equations: \\[\\begin{eqnarray} P &amp;=&amp; \\alpha_{1} + A_{S} \\beta_{1} + \\epsilon_{1} \\\\ Q &amp;=&amp; \\alpha_{2} + \\hat{P} \\beta_{2} + \\epsilon_{2}. \\end{eqnarray}\\] In the first regression, we estimate the average effect of the cost shock on prices. In the second equation, we estimate how the average effect of prices which are exogenous to demand affect quantity demanded. To see this, first substitute the equilibrium condition into the supply equation: \\(Q_{D}=Q_{S}=A_{S}+B_{S} P + E_{S}\\), lets us rewrite \\(P\\) as a function of \\(Q_{D}\\). This yields two theoretical equations \\[\\begin{eqnarray} \\label{eqn:linear_supply_iv} P &amp;=&amp; -\\frac{A_{S}}{{B_{S}}} + \\frac{Q_{D}}{B_{S}} - \\frac{E_{S}}{B_{S}} \\\\ \\label{eqn:linear_demand_iv} Q_{D} &amp;=&amp; A_{D} + B_{D} P + E_{D}. \\end{eqnarray}\\] ## Two Stage Least Squares Estimate ols_1 &lt;- lm(P~cost, data=dat2) dat2_new &lt;- cbind(dat2, Phat=predict(ols_1)) reg_2sls &lt;- lm(Q~Phat, data=dat2_new) summary(reg_2sls) ## ## Call: ## lm(formula = Q ~ Phat, data = dat2_new) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.57568 -0.12985 -0.00523 0.12429 0.55144 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.19274 0.15868 51.63 &lt;2e-16 *** ## Phat -0.82138 0.01868 -43.96 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1868 on 598 degrees of freedom ## Multiple R-squared: 0.7637, Adjusted R-squared: 0.7633 ## F-statistic: 1933 on 1 and 598 DF, p-value: &lt; 2.2e-16 ## One Stage Instrumental Variables Estimate library(fixest) reg2_iv &lt;- feols(Q~1|P~cost, data=dat2) summary(reg2_iv) ## TSLS estimation, Dep. Var.: Q, Endo.: P, Instr.: cost ## Second stage: Dep. Var.: Q ## Observations: 600 ## Standard-errors: IID ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.192736 0.22387 36.5960 &lt; 2.2e-16 *** ## fit_P -0.821381 0.02636 -31.1602 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## RMSE: 0.263118 Adj. R2: 0.52886 ## F-test (1st stage), P: stat = 2,595.4, p &lt; 2.2e-16, on 1 and 598 DoF. ## Wu-Hausman: stat = 500.6, p &lt; 2.2e-16, on 1 and 597 DoF. Within Group Variance You can experiment with the effect of different variances on both OLS and IV in the code below. And note that if we had multiple supply shifts and recorded their magnitudes, then we could recover more information about demand, perhaps tracing it out entirely. ## Examine Egrid &lt;- expand.grid(Ed_sigma=c(.001, .25, 1), Es_sigma=c(.001, .25, 1)) Egrid_regs &lt;- lapply(1:nrow(Egrid), function(i){ Ed_sigma &lt;- Egrid[i,1] Es_sigma &lt;- Egrid[i,2] EQ1 &lt;- sapply(1:N, function(n){ demand &lt;- qd_fun(P, Ed_sigma=Ed_sigma) supply &lt;- qs_fun(P, Es_sigma=Es_sigma) return(eq_fun(demand, supply, P)) }) EQ2 &lt;- sapply(1:N, function(n){ demand &lt;- qd_fun(P,Ed_sigma=Ed_sigma) supply2 &lt;- qs_fun(P, As=-6.5,Es_sigma=Es_sigma) return(eq_fun(demand, supply2, P)) }) dat &lt;- rbind( data.frame(t(EQ1), cost=&#39;1&#39;), data.frame(t(EQ2), cost=&#39;2&#39;)) return(dat) }) Egrid_OLS &lt;- sapply(Egrid_regs, function(dat) coef( lm(Q~P, data=dat))) Egrid_IV &lt;- sapply(Egrid_regs, function(dat) coef( feols(Q~1|P~cost, data=dat))) #cbind(Egrid, coef_OLS=t(Egrid_OLS)[,2], coef_IV=t(Egrid_IV)[,2]) lapply( list(Egrid_OLS, Egrid_IV), function(ei){ Emat &lt;- matrix(ei[2,],3,3) rownames(Emat) &lt;- paste0(&#39;Ed_sigma.&#39;,c(.001, .25, 1)) colnames(Emat) &lt;- paste0(&#39;Es_sigma.&#39;,c(.001, .25, 1)) return( round(Emat,2)) }) ## [[1]] ## Es_sigma.0.001 Es_sigma.0.25 Es_sigma.1 ## Ed_sigma.0.001 -0.80 -0.80 -0.80 ## Ed_sigma.0.25 -0.61 -0.64 -0.71 ## Ed_sigma.1 0.30 0.32 -0.09 ## ## [[2]] ## Es_sigma.0.001 Es_sigma.0.25 Es_sigma.1 ## Ed_sigma.0.001 -0.80 -0.80 -0.80 ## Ed_sigma.0.25 -0.79 -0.80 -0.77 ## Ed_sigma.1 -0.61 -0.82 -0.72 Caveats Regression analysis with instrumental variables can be very insightful and is applied to many different areas. But I also want to stress some caveats about using IV in practice. We always get coefficients back when running feols, and sometimes the computed p-values are very small. The interpretation of those numbers rests on many assumptions: only supply was affected. (Instrument exogeneity) the shock is large enough to be picked up statistically. (Instrument relevance) supply and demand are linear and additively seperable. (Functional form) we were not cycling through different IV’s. (Multiple hypotheses) We are rarely sure that all of these assumptions hold, and this is one reason why researchers often also report their OLS results. In practice, it is hard to find a good instrument. For example, suppose we asked “what is the effect of wages on police demanded?” and examined a policy which lowered the educational requirements from 4 years to 2 to become an officer. This increases the labour supply, but it also affects the demand curve through ``general equilibrium’’: as some of the new officers were potentially criminals. With fewer criminals, the demand for likely police shifts down. In practice, it is also easy to find a bad instrument. As you search for good instruments, sometimes random noise will appear like a good instrument (Spurious Instruments). Worse, if you search for a good instrument for too long, you can also be led astray from important questions (Spurious Research). 9.2 Regression Discontinuities/Kink (RD/RK) The basic idea here is to examine how a variable changes in response to an exogenous shock. The Regression Discontinuities estimate of the cost shock is the difference in the outcome variable just before and just after the shock. We now turn to our canonical competitive market example. The RD estimate is the difference between the lines at \\(T=300\\). dat2$T &lt;- 1:nrow(dat2) plot(P~T, dat2, main=&#39;Effect of Cost Shock on Price&#39;, pch=16, col=grey(0,.5)) regP1 &lt;- lm(P~T, dat2[dat2$cost==1,]) lines(regP1$model$T, predict(regP1), col=2) regP2 &lt;- lm(P~T, dat2[dat2$cost==2,]) lines(regP2$model$T, predict(regP2), col=4) regP &lt;- lm(P~T*cost, dat2) summary(regP) ## ## Call: ## lm(formula = P ~ T * cost, data = dat2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5312 -0.1370 0.0052 0.1345 0.5963 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.883e+00 2.275e-02 390.491 &lt;2e-16 *** ## T 5.379e-05 1.310e-04 0.411 0.682 ## cost2 -7.776e-01 6.426e-02 -12.100 &lt;2e-16 *** ## T:cost2 -1.220e-04 1.853e-04 -0.658 0.511 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1965 on 596 degrees of freedom ## Multiple R-squared: 0.8129, Adjusted R-squared: 0.8119 ## F-statistic: 863 on 3 and 596 DF, p-value: &lt; 2.2e-16 plot(Q~T, dat2, main=&#39;Effect of Cost Shock on Quantity&#39;, pch=16, col=grey(0,.5)) regQ1 &lt;- lm(Q~T, dat2[dat2$cost==1,]) lines(regQ1$model$T, predict(regQ1), col=2) regQ2 &lt;- lm(Q~T, dat2[dat2$cost==2,]) lines(regQ2$model$T, predict(regQ2), col=4) regQ &lt;- lm(Q~T*cost, dat2) summary(regQ) ## ## Call: ## lm(formula = Q ~ T * cost, data = dat2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.57599 -0.12996 -0.00584 0.12462 0.55278 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.905e-01 2.166e-02 41.109 &lt;2e-16 *** ## T -5.007e-06 1.247e-04 -0.040 0.968 ## cost2 6.640e-01 6.119e-02 10.852 &lt;2e-16 *** ## T:cost2 1.780e-05 1.764e-04 0.101 0.920 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1871 on 596 degrees of freedom ## Multiple R-squared: 0.7637, Adjusted R-squared: 0.7625 ## F-statistic: 642.1 on 3 and 596 DF, p-value: &lt; 2.2e-16 Remember that this is effect is local: different magnitudes of the cost shock or different demand curves generally yeild different estimates. 9.3 Difference in Differences (DID) The basic idea here is to examine how a variable changes in response to an exogenous shock, compared to a control group. EQ3 &lt;- sapply(1:(2*N), function(n){ ## Market Mechanisms demand &lt;- qd_fun(P) supply &lt;- qs_fun(P) ## Compute EQ (what we observe) eq_id &lt;- which.min( abs(demand-supply) ) eq &lt;- c(P=P[eq_id], Q=demand[eq_id]) ## Return Equilibrium Observations return(eq) }) dat3 &lt;- data.frame(t(EQ3), cost=&#39;1&#39;, T=1:ncol(EQ3)) par(mfrow=c(1,2)) plot(P~T, dat2, main=&#39;Effect of Cost Shock on Price&#39;, pch=17,col=rgb(0,0,1,.25)) points(P~T, dat3, pch=16, col=rgb(1,0,0,.25)) plot(Q~T, dat2, main=&#39;Effect of Cost Shock on Quantity&#39;, pch=17,col=rgb(0,0,1,.25)) points(Q~T, dat3, pch=16, col=rgb(1,0,0,.25)) dat &lt;- rbind(dat2, dat3) regP &lt;- lm(P~T*cost, dat) summary(regP) ## ## Call: ## lm(formula = P ~ T * cost, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.54065 -0.13001 0.01023 0.13147 0.60107 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.888e+00 1.185e-02 749.736 &lt;2e-16 *** ## T 4.605e-06 3.946e-05 0.117 0.907 ## cost2 -7.825e-01 6.120e-02 -12.786 &lt;2e-16 *** ## T:cost2 -7.279e-05 1.367e-04 -0.532 0.594 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1963 on 1196 degrees of freedom ## Multiple R-squared: 0.764, Adjusted R-squared: 0.7634 ## F-statistic: 1291 on 3 and 1196 DF, p-value: &lt; 2.2e-16 regQ &lt;- lm(Q~T*cost, dat) summary(regQ) ## ## Call: ## lm(formula = Q ~ T * cost, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.57328 -0.12445 -0.00129 0.12419 0.55278 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.883e-01 1.110e-02 80.025 &lt;2e-16 *** ## T -1.140e-05 3.695e-05 -0.308 0.758 ## cost2 6.662e-01 5.731e-02 11.625 &lt;2e-16 *** ## T:cost2 2.419e-05 1.280e-04 0.189 0.850 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1838 on 1196 degrees of freedom ## Multiple R-squared: 0.7171, Adjusted R-squared: 0.7164 ## F-statistic: 1011 on 3 and 1196 DF, p-value: &lt; 2.2e-16 9.4 More Literature You are directed to the following resources which discusses endogeneity in more detail and how it applies generally. Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction https://www.mostlyharmlesseconometrics.com/ https://www.econometrics-with-r.org https://bookdown.org/paul/applied-causal-analysis/ https://mixtape.scunning.com/ https://theeffectbook.net/ https://www.r-causal.org/ https://matheusfacure.github.io/python-causality-handbook/landing-page.html For IV, https://cameron.econ.ucdavis.edu/e240a/ch04iv.pdf https://mru.org/courses/mastering-econometrics/introduction-instrumental-variables-part-one https://www.econometrics-with-r.org/12-ivr.html https://bookdown.org/paul/applied-causal-analysis/estimation-2.html https://mixtape.scunning.com/07-instrumental_variables https://theeffectbook.net/ch-InstrumentalVariables.html http://www.urfie.net/read/index.html#page/247 For RDD and DID, https://bookdown.org/paul/applied-causal-analysis/rdd-regression-discontinuity-design.html https://mixtape.scunning.com/06-regression_discontinuity https://theeffectbook.net/ch-RegressionDiscontinuity.html https://mixtape.scunning.com/09-difference_in_differences https://theeffectbook.net/ch-DifferenceinDifference.html http://www.urfie.net/read/index.html#page/226 Although there are many ways this simultaneity can happen, economic theorists have made great strides in analyzing the simultaneity problem as it arises from market relationships. In fact, the 2SLS statistical approach arose to understand the equilibrium of competitive agricultural markets.↩︎ Notice that even in this linear model, however, all effects are conditional: The effect of a cost change on quantity or price depends on the demand curve. A change in costs affects quantity supplied but not quantity demanded (which then affects equilibrium price) but the demand side of the market still matters! The change in price from a change in costs depends on the elasticity of demand.↩︎ "],["data-scientism.html", " 10 Data Scientism 10.1 Data Errors 10.2 P-Hacking 10.3 Spurious Regression 10.4 Spurious Causal Impacts", " 10 Data Scientism There is currently a boom in empirical research centered around linear regression analysis. This is not for the first boom in empirical research, and we’d be wise to recall some earlier wisdom from economists on the matter. The most reckless and treacherous of all theorists is he who professes to let facts and figures speak for themselves, who keeps in the background the part he has played, perhaps unconsciously, in selecting and grouping them — Alfred Marshall, 1885 The blind transfer of the striving for quantitative measurements to a field where the specific conditions are not present which give it its basic importance in the natural sciences is the result of an entirely unfounded prejudice. It is probably responsible for the worst aberrations and absurdities produced by scientism in the social sciences. It not only leads frequently to the selection for study of the most irrelevant aspects of the phenomena because they happen to be measurable, but also to “measurements” and assignments of numerical values which are absolutely meaningless. What a distinguished philosopher recently wrote about psychology is at least equally true of the social sciences, namely that it is only too easy “to rush off to measure something without considering what it is we are measuring, or what measurement means. In this respect some recent measurements are of the same logical type as Plato’s determination that a just ruler is 729 times as happy as an unjust one.” — F.A. Hayek, 1943 if you torture the data long enough, it will confess — R. Coase (Source Unknown) the definition of a causal parameter is not always clearly stated, and formal statements of identifying conditions in terms of well-specified economic models are rarely presented. Moreover, the absence of explicit structural frameworks makes it difficult to cumulate knowledge across studies conducted within this framework. Many studies produced by this research program have a `stand alone’ feature and neither inform nor are influenced by the general body of empirical knowledge in economics. — J.J. Heckman, 2000 without explicit prior consideration of the effect of the instrument choice on the parameter being estimated, such a procedure is effectively the opposite of standard statistical practice in which a parameter of interest is defined first, followed by an estimator that delivers that parameter. Instead, we have a procedure in which the choice of the instrument, which is guided by criteria designed for a situation in which there is no heterogeneity, is implicitly allowed to determine the parameter of interest. This goes beyond the old story of looking for an object where the light is strong enough to see; rather, we have at least some control over the light but choose to let it fall where it may and then proclaim that whatever it illuminates is what we were looking for all along. — A. Deaton, 2010 In this age of big data, we are getting more and more data. Perhaps surprisingly, this makes it easier to make false discoveries. We consider three main ways for these to arise. After that, there are examples of scientism with the ‘’latest and greatest’’ empirical recipes—we don’t have so many theoretical results yet but I think you can understand the issue with the numerical example. 10.1 Data Errors A huge amount of data normally means a huge amount of data cleaning/merging/aggregating. This avoids many copy-paste errors, which are a recipe for disaster, but may also introduce other types of errors. Some spurious results are driven by honest errors in data cleaning. According to one estimate, this is responsible for around one fifth of all medical science retractions (there is even a whole book about this!). Although there are not similar meta-analysis in economics, there are some high-profile examples. This includes papers that are highly influential, like Lott, Levitt and Reinhart and Rogoff as well as others the top economics journals, like the RESTUD and AER. There are some reasons to think such errors are more widespread across the social sciences; e.g., in Census data and Aid data. So be careful! Note: one reason to plot your data is to help spot such errors. 10.2 P-Hacking Another class of errors pertains to P-hacking (and it’s various synonyms: data drudging, star mining,….). While there are cases of fraudulent data manipulation (which can be considered as a dishonest data error), P-hacking is a much more pernicious and widespread set.seed(123) n &lt;- 50 X1 &lt;- runif(n) ## Regression Machine: ## repeatedly finds covariate, runs regression ## stops when statistically significant at .1\\% p &lt;- 1 i &lt;- 0 while(p &gt;= .001){ ## Get Random Covariate X2 &lt;- runif(n) ## Merge and `Analyze&#39; dat_i &lt;- data.frame(X1,X2) reg_i &lt;- lm(X1~X2, data=dat_i) ## update results in global environment p &lt;- summary(reg_i)$coefficients[2,4] i &lt;- i+1 } plot(X1~X2, data=dat_i, pch=16, col=grey(.5,.5), main=paste0(&#39;Random Dataset &#39;, i,&quot;: p=&quot;, formatC(p,digits=2, format=&#39;fg&#39;))) abline(reg_i) #summary(reg_i) ## P-hacking 2SLS library(fixest) p &lt;- 1 ii &lt;- 0 set.seed(123) while(p &gt;= .05){ ## Get Random Covariates X2 &lt;- runif(n) X3 &lt;- runif(n) ## Create Treatment Variable based on Cutoff cutoffs &lt;- seq(0,1,length.out=11)[-c(1,11)] for(tau in cutoffs){ T3 &lt;- 1*(X3 &gt; tau) ## Merge and `Analyze&#39; dat_i &lt;- data.frame(X1,X2,T3) ivreg_i &lt;- feols(X1~1|X2~T3, data=dat_i) ## Update results in global environment ptab &lt;- summary(ivreg_i)$coeftable if( nrow(ptab)==2){ p &lt;- ptab[2,4] ii &lt;- ii+1 } } } summary(ivreg_i) ## TSLS estimation, Dep. Var.: X1, Endo.: X2, Instr.: T3 ## Second stage: Dep. Var.: X1 ## Observations: 50 ## Standard-errors: IID ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.95e-14 1.28e-13 -7.750700e-01 0.4421 ## fit_X2 1.00e+00 2.46e-13 4.060978e+12 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## RMSE: 5.81e-14 Adj. R2: 1 ## F-test (1st stage), X2: stat = 0.664884, p = 0.418869, on 1 and 48 DoF. ## Wu-Hausman: stat = 0.232185, p = 0.632145, on 1 and 47 DoF. 10.3 Spurious Regression Even without any coding errors or p-hacking, you can sometimes make a false discovery. We begin with a motivating empirical example of “US Gov’t Spending on Science”. First, get and inspect some data from https://tylervigen.com/spurious-correlations ## Your data is not made up in the computer (hopefully!) vigen_csv &lt;- read.csv( paste0( &#39;https://raw.githubusercontent.com/the-mad-statter/&#39;, &#39;whysospurious/master/data-raw/tylervigen.csv&#39;) ) class(vigen_csv) ## [1] &quot;data.frame&quot; names(vigen_csv) ## [1] &quot;year&quot; &quot;science_spending&quot; ## [3] &quot;hanging_suicides&quot; &quot;pool_fall_drownings&quot; ## [5] &quot;cage_films&quot; &quot;cheese_percap&quot; ## [7] &quot;bed_deaths&quot; &quot;maine_divorce_rate&quot; ## [9] &quot;margarine_percap&quot; &quot;miss_usa_age&quot; ## [11] &quot;steam_murders&quot; &quot;arcade_revenue&quot; ## [13] &quot;computer_science_doctorates&quot; &quot;noncom_space_launches&quot; ## [15] &quot;sociology_doctorates&quot; &quot;mozzarella_percap&quot; ## [17] &quot;civil_engineering_doctorates&quot; &quot;fishing_drownings&quot; ## [19] &quot;kentucky_marriage_rate&quot; &quot;oil_imports_norway&quot; ## [21] &quot;chicken_percap&quot; &quot;train_collision_deaths&quot; ## [23] &quot;oil_imports_total&quot; &quot;pool_drownings&quot; ## [25] &quot;nuclear_power&quot; &quot;japanese_cars_sold&quot; ## [27] &quot;motor_vehicle_suicides&quot; &quot;spelling_bee_word_length&quot; ## [29] &quot;spider_deaths&quot; &quot;math_doctorates&quot; ## [31] &quot;uranium&quot; vigen_csv[1:5,1:5] ## year science_spending hanging_suicides pool_fall_drownings cage_films ## 1 1996 NA NA NA NA ## 2 1997 NA NA NA NA ## 3 1998 NA NA NA NA ## 4 1999 18079 5427 109 2 ## 5 2000 18594 5688 102 2 ## similar `apply&#39; functions lapply(vigen_csv[,1:5], class) ## like apply, but for lists ## $year ## [1] &quot;integer&quot; ## ## $science_spending ## [1] &quot;integer&quot; ## ## $hanging_suicides ## [1] &quot;integer&quot; ## ## $pool_fall_drownings ## [1] &quot;integer&quot; ## ## $cage_films ## [1] &quot;integer&quot; sapply(vigen_csv[,1:5], class) ## lapply, formatted to a vector ## year science_spending hanging_suicides pool_fall_drownings ## &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; ## cage_films ## &quot;integer&quot; The US government spending on science is ruining cinema (p&lt;.001)!? ## Drop Data before 1999 vigen_csv &lt;- vigen_csv[vigen_csv$year &gt;= 1999,] ## Run OLS Regression $ reg1 &lt;- lm(cage_films ~ -1 + science_spending, data=vigen_csv) summary(reg1) ## ## Call: ## lm(formula = cage_films ~ -1 + science_spending, data = vigen_csv) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.7670 -0.7165 0.1447 0.7890 1.4531 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## science_spending 9.978e-05 1.350e-05 7.39 2.34e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.033 on 10 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.8452, Adjusted R-squared: 0.8297 ## F-statistic: 54.61 on 1 and 10 DF, p-value: 2.343e-05 It’s not all bad, people in maine stay married longer? plot.new() plot.window(xlim=c(1999, 2009), ylim=c(7,9)) lines(log(maine_divorce_rate*1000)~year, data=vigen_csv) lines(log(science_spending/10)~year, data=vigen_csv, lty=2) axis(1) axis(2) legend(&#39;topright&#39;, lty=c(1,2), legend=c( &#39;log(maine_divorce_rate*1000)&#39;, &#39;log(science_spending/10)&#39;)) par(mfrow=c(1,2), mar=c(2,2,2,1)) plot.new() plot.window(xlim=c(1999, 2009), ylim=c(5,9)*1000) lines(science_spending/3~year, data=vigen_csv, lty=1, col=2, pch=16) text(2003, 8200, &#39;US spending on science, space, technology (USD/3)&#39;, col=2, cex=.6, srt=30) lines(hanging_suicides~year, data=vigen_csv, lty=1, col=4, pch=16) text(2004, 6500, &#39;US Suicides by hanging, strangulation, suffocation (Deaths)&#39;, col=4, cex=.6, srt=30) axis(1) axis(2) plot.new() plot.window(xlim=c(2002, 2009), ylim=c(0,5)) lines(cage_films~year, data=vigen_csv[vigen_csv$year&gt;=2002,], lty=1, col=2, pch=16) text(2006, 0.5, &#39;Number of films with Nicolas Cage (Films)&#39;, col=2, cex=.6, srt=0) lines(pool_fall_drownings/25~year, data=vigen_csv[vigen_csv$year&gt;=2002,], lty=1, col=4, pch=16) text(2006, 4.5, &#39;Number of drownings by falling into pool (US Deaths/25)&#39;, col=4, cex=.6, srt=0) axis(1) axis(2) ## Include an intercept to regression 1 #reg2 &lt;- lm(cage_films ~ science_spending, data=vigen_csv) #suppressMessages(library(stargazer)) #stargazer(reg1, reg2, type=&#39;html&#39;) We now run IV regressions for different variable combinations in the dataset knames &lt;- names(vigen_csv)[2:11] ## First 10 Variables #knames &lt;- names(vigen_csv)[-1] ## All Variables p &lt;- 1 ii &lt;- 1 ivreg_list &lt;- vector(&quot;list&quot;, factorial(length(knames))/factorial(length(knames)-3)) ## Choose 3 variable for( k1 in knames){ for( k2 in setdiff(knames,k1)){ for( k3 in setdiff(knames,c(k1,k2)) ){ X1 &lt;- vigen_csv[,k1] X2 &lt;- vigen_csv[,k2] X3 &lt;- vigen_csv[,k3] ## Merge and `Analyze&#39; dat_i &lt;- na.omit(data.frame(X1,X2,X3)) ivreg_i &lt;- feols(X1~1|X2~X3, data=dat_i) ivreg_list[[ii]] &lt;- list(ivreg_i, c(k1,k2,k3)) ii &lt;- ii+1 }}} pvals &lt;- sapply(ivreg_list, function(ivreg_i){ivreg_i[[1]]$coeftable[2,4]}) plot(ecdf(pvals), xlab=&#39;p-value&#39;, ylab=&#39;CDF&#39;, main=&#39;Frequency IV is Statistically Significant&#39;) abline(v=c(.01,.05), col=c(2,4)) ## Most Significant Spurious Combinations pvars &lt;- sapply(ivreg_list, function(ivreg_i){ivreg_i[[2]]}) pdat &lt;- data.frame(t(pvars), pvals) pdat &lt;- pdat[order(pdat$pvals),] head(pdat) ## X1 X2 X3 pvals ## 4 science_spending hanging_suicides bed_deaths 3.049883e-08 ## 76 hanging_suicides science_spending bed_deaths 3.049883e-08 ## 3 science_spending hanging_suicides cheese_percap 3.344890e-08 ## 75 hanging_suicides science_spending cheese_percap 3.344890e-08 ## 485 maine_divorce_rate margarine_percap cheese_percap 3.997738e-08 ## 557 margarine_percap maine_divorce_rate cheese_percap 3.997738e-08 For more intuition on spurious correlations, try http://shiny.calpoly.sh/Corr_Reg_Game/ 10.4 Spurious Causal Impacts We apply the three major credible methods (IV, RDD, DID) to random walks. Each time, we find a result that fits mold and add various extensions that make it appear robust. One could tell a story about how \\(X_{2}\\) affects \\(X_{1}\\) but \\(X_{1}\\) might also affect \\(X_{2}\\), and how they discovered an instrument \\(X_{3}\\) to provide the first causal estimate of \\(X_{2}\\) on \\(X_{1}\\). The analysis looks scientific and the story sounds plausible, so you could probably be convinced if it were not just random noise. n &lt;- 1000 n_index &lt;- seq(n) set.seed(1) random_walk1 &lt;- cumsum(runif(n,-1,1)) set.seed(2) random_walk2 &lt;- cumsum(runif(n,-1,1)) par(mfrow=c(1,2)) plot(random_walk1, pch=16, col=rgb(1,0,0,.25), xlab=&#39;Time&#39;, ylab=&#39;Random Value&#39;) plot(random_walk2, pch=16, col=rgb(0,0,1,.25), xlab=&#39;Time&#39;, ylab=&#39;Random Value&#39;) IV First, find an instrument that satisfy various statistical criterion to provide a causal estimate of \\(X_{2}\\) on \\(X_{1}\\). ## &quot;Find&quot; &quot;valid&quot; ingredients library(fixest) random_walk3 &lt;- cumsum(runif(n,-1,1)) dat_i &lt;- data.frame( X1=random_walk1, X2=random_walk2, X3=random_walk3) ivreg_i &lt;- feols(X1~1|X2~X3, data=dat_i) summary(ivreg_i) ## TSLS estimation, Dep. Var.: X1, Endo.: X2, Instr.: X3 ## Second stage: Dep. Var.: X1 ## Observations: 1,000 ## Standard-errors: IID ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.53309 1.644285 5.18954 2.5533e-07 *** ## fit_X2 1.79901 0.472285 3.80916 1.4796e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## RMSE: 6.25733 Adj. R2: -1.29152 ## F-test (1st stage), X2: stat = 10.8, p = 0.001048, on 1 and 998 DoF. ## Wu-Hausman: stat = 23.4, p = 1.518e-6, on 1 and 997 DoF. ## After experimenting with different instruments ## you can find even stronger results! RDD Second, find a large discrete change in the data that you can associate with a policy. You can use this as an instrument too, also providing a causal estimate of \\(X_{2}\\) on \\(X_{1}\\). ## Let the data take shape ## (around the large differences before and after) n1 &lt;- 290 wind1 &lt;- c(n1-300,n1+300) dat1 &lt;- data.frame(t=n_index, y=random_walk1, d=1*(n_index &gt; n1)) dat1_sub &lt;- dat1[ n_index&gt;wind1[1] &amp; n_index &lt; wind1[2],] ## Then find your big break reg0 &lt;- lm(y~t, data=dat1_sub[dat1_sub$d==0,]) reg1 &lt;- lm(y~t, data=dat1_sub[dat1_sub$d==1,]) ## The evidence should show openly (it&#39;s just science) plot(random_walk1, pch=16, col=rgb(0,0,1,.25), xlim=wind1, xlab=&#39;Time&#39;, ylab=&#39;Random Value&#39;) abline(v=n1, lty=2) lines(reg0$model$t, reg0$fitted.values, col=1) lines(reg1$model$t, reg1$fitted.values, col=1) ## Dress with some statistics for added credibility rdd_sub &lt;- lm(y~d+t+d*t, data=dat1_sub) rdd_full &lt;- lm(y~d+t+d*t, data=dat1) stargazer::stargazer(rdd_sub, rdd_full, type=&#39;html&#39;, title=&#39;Recipe RDD&#39;, header=F, omit=c(&#39;Constant&#39;), notes=c(&#39;First column uses a dataset around the discontinuity.&#39;, &#39;Smaller windows are more causal, and where the effect is bigger.&#39;)) Recipe RDD Dependent variable: y (1) (2) d -13.169*** -9.639*** (0.569) (0.527) t 0.011*** 0.011*** (0.001) (0.002) d:t 0.009*** 0.004* (0.002) (0.002) Observations 589 1,000 R2 0.771 0.447 Adjusted R2 0.770 0.446 Residual Std. Error 1.764 (df = 585) 3.081 (df = 996) F Statistic 658.281*** (df = 3; 585) 268.763*** (df = 3; 996) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 First column uses a dataset around the discontinuity. Smaller windows are more causal, and where the effect is bigger. DID Third, find a change in the data that you can associate with a policy where the control group has parallel trends. This also provides a causal estimate of \\(X_{2}\\) on \\(X_{1}\\). ## Find a reversal of fortune ## (A good story always goes well with a nice pre-trend) n2 &lt;- 318 wind2 &lt;- c(n2-20,n2+20) plot(random_walk2, pch=16, col=rgb(0,0,1,.5), xlim=wind2, ylim=c(-15,15), xlab=&#39;Time&#39;, ylab=&#39;Random Value&#39;) points(random_walk1, pch=16, col=rgb(1,0,0,.5)) abline(v=n2, lty=2) ## Knead out any effects that are non-causal ## (or even just correlation) dat2A &lt;- data.frame(t=n_index, y=random_walk1, d=1*(n_index &gt; n2), RWid=1) dat2B &lt;- data.frame(t=n_index, y=random_walk2, d=0, RWid=2) dat2 &lt;- rbind(dat2A, dat2B) dat2$RWid &lt;- as.factor(dat2$RWid) dat2$tid &lt;- as.factor(dat2$t) dat2_sub &lt;- dat2[ dat2$t&gt;wind2[1] &amp; dat2$t &lt; wind2[2],] ## Report the stars for all to enjoy ## (what about the intercept?) ## (stable coefficients are the good ones?) did_fe1 &lt;- lm(y~d+tid, data=dat2_sub) did_fe2 &lt;- lm(y~d+RWid, data=dat2_sub) did_fe3 &lt;- lm(y~d*RWid+tid, data=dat2_sub) stargazer::stargazer(did_fe1, did_fe2, did_fe3, type=&#39;html&#39;, title=&#39;Recipe DID&#39;, header=F, omit=c(&#39;tid&#39;,&#39;RWid&#39;, &#39;Constant&#39;), notes=c( &#39;Fixed effects for time in column 1, for id in column 2, and both in column 3.&#39;, &#39;Fixed effects control for most of your concerns.&#39;, &#39;Anything else creates a bias in the opposite direction.&#39;)) Recipe DID Dependent variable: y (1) (2) (3) d 1.804* 1.847*** 5.851*** (0.892) (0.652) (0.828) Observations 78 78 78 R2 0.227 0.164 0.668 Adjusted R2 -0.566 0.142 0.309 Residual Std. Error 2.750 (df = 38) 2.035 (df = 75) 1.827 (df = 37) F Statistic 0.287 (df = 39; 38) 7.379*** (df = 2; 75) 1.860** (df = 40; 37) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Fixed effects for time in column 1, for id in column 2, and both in column 3. Fixed effects control for most of your concerns. Anything else creates a bias in the opposite direction. "],["why.html", " 11 Why? 11.1 An example workflow 11.2 An alternative workflow 11.3 Replicable 11.4 R and R-Markdown", " 11 Why? Before hopping into reproducible programming, lets think about why. My main sell to you is that it is in your own self-interest. 11.1 An example workflow Taking First Steps … Step 1: Some Ideas and Data \\(X_{1} \\to Y_{1}\\) You copy some data into a spreadsheet, manually aggregate do some calculations and tables the same spreadsheet some other analysis from here and there, using this software and that. Step 2: Pursuing the lead for a week or two you extend your dataset with more observations copy in a spreadsheet data, manually aggregate do some more calculations and tables, same as before Then, a Little Way Down the Road … 1 month later, someone asks about another factor: \\(X_{2}\\) you download some other type of data You repeat Step 2 with some data on \\(X_{2}\\). The details from your “point and click” method are a bit fuzzy. It takes a little time, but you successfully redo the analysis. 4 months later, someone asks about another factor: \\(X_{3}\\to Y_{1}\\) You again repeat Step 2 with some data on \\(X_{3}\\). You’re pretty sure none of tables your tried messed up the order of the rows or columns. It takes more time and effort. The data processing was not transparent, but you eventually redo the analysis. 6 months later, you want to explore: \\(X_{2} \\to Y_{2}\\). You found out Excel had some bugs in it’s statistical calculations (see e.g., https://biostat.app.vumc.org/wiki/pub/Main/TheresaScott/StatsInExcel.TAScot.handout.pdf). You now use a new version of the spreadsheet You’re not sure you merged everything correctly. After much time and effort, most (but not all) of the numbers match exactly. 2 years later, you want to replicate: \\(\\{ X_{1}, X_{2}, X_{3} \\} \\to Y_{1}\\) A rival has proposed something new. Their idea doesn’t actually make any sense, but their figures and statistics look better. You don’t even use that computer anymore and a collaborator who handled the data on \\(X_{2}\\) has moved on. 11.2 An alternative workflow Suppose you decided to code what you did beginning with Step 2. It does not take much time to update or replicate your results. Your computer runs for 2 hours and reproduces the figures and tables. You also rewrote your big calculations to use multiple cores, this took two hours to do but saved 6 hours each time you rerun your code. You add some more data. It adds almost no time to see whether much has changed. Your results are transparent and easier to build on. You see the exact steps you took and found an error glad you found it before publication! See https://retractionwatch.com/ and https://econjwatch.org/ Google “worst excell errors” and note the frequency they arise from copy/paste via the “point-and-click” approach. Future economists should also read https://core.ac.uk/download/pdf/300464894.pdf. You try out a new plot you found in The Visual Display of Quantitative Information, by Edward Tufte. It’s not a standard plot, but google answers most of your questions. Tutorials help avoid bad practices, such as plotting 2D data as a 3D object (see e.g., https://clauswilke.com/dataviz/no-3d.html). You try out an obscure statistical approach that’s hot in your field. it doesn’t make the paper, but you have some confidence that candidate issue isn’t a big problem 11.3 Replicable You should make your work reproducible, and we will cover some of the basics of how to do this in R. You also want your work to be replicable Replicable: someone collecting new data comes to the same results. Reproducibile: someone reusing your data comes to the same results. You can read more about the distinction in many places, including https://www.annualreviews.org/doi/10.1146/annurev-psych-020821-114157 https://nceas.github.io/sasap-training/materials/reproducible_research_in_r_fairbanks/ 11.4 R and R-Markdown We will use R Markdown for reproducible research, which is a good choice: http://www.r-bloggers.com/the-reproducibility-crisis-in-science-and-prospects-for-r/ http://fmwww.bc.edu/GStat/docs/pointclick.html https://github.com/qinwf/awesome-R\\#reproducible-research A Guide to Reproducible Code in Ecology and Evolution https://biostat.app.vumc.org/wiki/pub/Main/TheresaScott/ReproducibleResearch.TAScott.handout.pdf Note that R and R markdown are both languages: R studio interprets R code to produce statistics, R studio interprets R markdown code to produce pretty documents which contain both writing and statistics. (You should already be a bit familiar with R, but not necessarily R Markdown.) Altogether, your project will use R is our software Rstudio is our GUI R Markdown is our document Both are good for teaching https://doi.org/10.1080/00220485.2019.1618765 https://doi.org/10.1002/jae.657 Homework reports are the smallest and probably first document you create. We will create little homework reports using R markdown that are almost entirely self-contained (showing both code and output). To do this, you will need to install Pandoc on your computer. Install any required packages ## Packages for Rmarkdown install.packages(&quot;knitr&quot;) install.packages(&quot;rmarkdown&quot;) install.packages(&quot;bookdown&quot;) ## Other packages used in this primer install.packages(&quot;plotly&quot;) install.packages(&quot;sf&quot;) To get started with R Markdown, you can first read and work through https://jadamso.github.io/Rbooks/small-scale-projects.html, and then recreate https://jadamso.github.io/Rbooks/small-scale-projects.html#a-homework-example yourself. "],["small-projects.html", " 12 Small Projects 12.1 Code Chunks 12.2 Reports 12.3 Posters and Slides 12.4 More Literature", " 12 Small Projects 12.1 Code Chunks Save the following code as CodeChunk.R ## Define New Function sum_squared &lt;- function(x1, x2) { y &lt;- (x1 + x2)^2 return(y) } ## Test New Function x &lt;- c(0,1,3,10,6) sum_squared(x[1], x[3]) sum_squared(x, x[2]) sum_squared(x, x[7]) sum_squared(x, x) message(&#39;Chunk Completed&#39;) Clean the workspace In the right panels, manually cleanup save the code as MyFirstCode.R clear the environment and history (use the broom in top right panel) clear unsaved plots (use the broom in bottom right panel) Replicate either in another tab or directly in console on the bottom left, enter source(&#39;MyFirstCode.R&#39;) Note that you may first need to setwd() so your computer knows where you saved your code.6 After you get this working * add a the line print(sum_squared(y, y)) to the bottom of MyFirstCode.R. * apply the function to a vectors specified outside of that script ## Pass Objects/Functions *to* Script y &lt;- c(3,1,NA) source(&#39;MyFirstCode.R&#39;) ## Pass Objects/Functions *from* Script z &lt;- sqrt(y)/2 sum_squared(z,z) CLI Alternatives (Skippable) There are also alternative ways to replicate via the command line interface (CLI) after opening a terminal. ## Method 1 Rscript -e &quot;source(&#39;CodeChunk.R&#39;)&quot; ## Method 2 Rscript CodeChunk.R Note that you can open a new terminal in RStudio in the top bar by clicking ‘tools &gt; terminal &gt; new terminal’ 12.2 Reports We will create reproducible reports via R Markdown. Example 1: Data Scientism See DataScientism.html and then create it by Clicking the “Code” button in the top right and then “Download Rmd” Opening with Rstudio, change the name to your own, change the title to “Data Scientism Replication” then point-and-click “knit” Alternatively, * download the source file from DataScientism.Rmd * use the console to run rmarkdown::render(&#39;DataScientism.Rmd&#39;) Example 2: Homework Assignment Below is a template of what homework questions (and answers) look like. Create an .Rmd from scratch that produces a similar looking .html file. Question 1: Simulate 100 random observations of the form \\(y=x\\beta+\\epsilon\\) and plot the relationship. Plot and explore the data interactively via plotly, https://plotly.com/r/line-and-scatter/. Then play around with different styles, https://www.r-graph-gallery.com/13-scatter-plot.html, to best express your point. Answer I simulate \\(400\\) observations for \\(\\epsilon \\sim 2\\times N(0,1)\\) and \\(\\beta=4\\), as seen in this single chunk. Notice an upward trend. n &lt;- 100 E &lt;- rnorm(n) X &lt;- seq(n) Y &lt;- 4*X + 2*E library(plotly) plot_ly( data=data.frame(X=X,Y=Y), x=~X, y=~Y) Question 2: Verify the definition of a line segment for points \\(A=(0,3), B=(1,5)\\) using a \\(101 \\times 101\\) grid. Recall a line segment is all points \\(s\\) that have \\(d(s, A) + d(s, B) = d(A, B)\\). Answer library(sf) s_1 &lt;- c(0,3) s_2 &lt;- c(1,5) Seg1 &lt;- st_linestring( rbind(s_1,s_2) ) grid_pts &lt;- expand.grid( x=seq(s_1[1],s_2[1], length.out=101), y=seq(s_1[2],s_2[2], length.out=101) ) Seg1_dist &lt;- dist( Seg1 ) grid_pts$dist &lt;- apply(grid_pts, 1, function(s){ dist( rbind(s,s_1) ) + dist( rbind(s,s_2) ) }) grid_pts$seg &lt;- grid_pts$dist == Seg1_dist D_point_seg &lt;- st_multipoint( as.matrix(grid_pts[grid_pts$seg==T,1:2]) ) D_point_notseg &lt;- st_multipoint( as.matrix(grid_pts[grid_pts$seg==F,1:2]) ) plot(Seg1) points(D_point_notseg, col=2, pch=&#39;.&#39;) points(D_point_seg, pch=16) box() 12.3 Posters and Slides Posters and presentations are another important type of scientific document. R markdown is good at creating both of these, and actually very good with some additional packages. So we will also use flexdashboard for posters and beamer for presentations. Poster See DataScientism_Poster.html and recreate from the source file DataScientism_Poster.Rmd. Simply change the name to your own, and knit the document. Slides See DataScientism_Slides.pdf Since beamer is a pdf output, you will need to install Latex. Alternatively, you can install a lightweight version TinyTex from within R install.packages(&#39;tinytex&#39;) tinytex::install_tinytex() # install TinyTeX Then download source file DataScientism_Slides.Rmd, change the name to your own, and knit the document. If you cannot install Latex, then you must specify a different output. For example, change output: beamer_presentation to output: ioslides_presentation on line 6 of the source file. 12.4 More Literature For more guidance on how to create Rmarkdown documents, see https://github.com/rstudio/cheatsheets/blob/main/rmarkdown.pdf https://cran.r-project.org/web/packages/rmarkdown/vignettes/rmarkdown.html http://rmarkdown.rstudio.com https://bookdown.org/yihui/rmarkdown/ https://bookdown.org/yihui/rmarkdown-cookbook/ https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/rmarkdown.html An Introduction to the Advanced Theory and Practice of Nonparametric Econometrics. Raccine 2019. Appendices B &amp; D. https://rmd4sci.njtierney.com/using-rmarkdown.html https://alexd106.github.io/intro2R/Rmarkdown_intro.html If you are still lost, try one of the many online tutorials (such as these) https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet https://www.neonscience.org/resources/learning-hub/tutorials/rmd-code-intro https://m-clark.github.io/Introduction-to-Rmarkdown/ https://www.stat.cmu.edu/~cshalizi/rmarkdown/ http://math.wsu.edu/faculty/xchen/stat412/HwWriteUp.Rmd http://math.wsu.edu/faculty/xchen/stat412/HwWriteUp.html https://holtzy.github.io/Pimp-my-rmd/ https://ntaback.github.io/UofT_STA130/Rmarkdownforclassreports.html https://crd150.github.io/hw_guidelines.html https://r4ds.had.co.nz/r-markdown.html http://www.stat.cmu.edu/~cshalizi/rmarkdown http://www.ssc.wisc.edu/sscc/pubs/RFR/RFR_RMarkdown.html http://kbroman.org/knitr_knutshell/pages/Rmarkdown.html Some other good packages for posters/presenting you should be aware of https://github.com/mathematicalcoffee/beamerposter-rmarkdown-example https://github.com/rstudio/pagedown https://github.com/brentthorne/posterdown https://odeleongt.github.io/postr/ https://wytham.rbind.io/post/making-a-poster-in-r/ https://www.animateyour.science/post/How-to-design-an-award-winning-conference-poster You can also use GUI: point-click click ‘Source &gt; Source as a local job’ on top right↩︎ "],["large-projects.html", " 13 Large Projects 13.1 Logging/Sinking 13.2 Class Projects 13.3 Debugging 13.4 Optimizing 13.5 More Literature", " 13 Large Projects As you scale up a project, then you will have to be more organized. Medium and large sized projects should have their own Project folder on your computer with files, subdirectories with files, and subsubdirectories with files. It should look like this Project └── README.txt └── /Code └── MAKEFILE.R └── RBLOCK_001_DataClean.R └── RBLOCK_002_Figures.R └── RBLOCK_003_ModelsTests.R └── RBLOCK_004_Robust.R └── /Logs └── MAKEFILE.Rout └── /Data └── /Raw └── Source1.csv └── Source2.shp └── Source3.txt └── /Clean └── AllDatasets.Rdata └── MainDataset1.Rds └── MainDataset2.csv └── /Output └── MainFigure.pdf └── AppendixFigure.pdf └── MainTable.tex └── AppendixTable.tex └── /Writing └── /TermPaper └── TermPaper.tex └── TermPaper.bib └── TermPaper.pdf └── /Slides └── Slides.Rmd └── Slides.html └── Slides.pdf └── /Poster └── Poster.Rmd └── Poster.html └── Poster.pdf └── /Proposal └── Proposal.Rmd └── Proposal.html └── Proposal.pdf There are two main meta-files README.txt overviews the project structure and what the codes are doing MAKEFILE explicitly describes and executes all codes (and typically logs the output). MAKEFILE If all code is written with the same program (such as R) the makefile can be written in a single language. For us, this looks like ### Project Structure home_dir &lt;- path.expand(&quot;~/Desktop/Project/&quot;) data_dir &lt;- paste0(home_dir, &quot;Data/&quot;) data_dir_r &lt;- paste0(data_dir, &quot;Raw/&quot;) data_dir_c &lt;- paste0(data_dir, &quot;Clean/&quot;) out_dir &lt;- paste0(hdir, &quot;Output/&quot;) code_dir &lt;- paste0(hdir, &quot;Code/&quot;) ### Execute Codes ### libraries are loaded within each RBLOCK set.wd( code_dir ) source( &quot;RBLOCK_001_DataClean.R&quot; ) source( &quot;RBLOCK_002_Figures.R&quot; ) source( &quot;RBLOCK_003_ModelsTests.R&quot; ) source( &quot;RBLOCK_004_Robust.R&quot; ) Notice there is a lot of documentation ### like this, which is crucial for large projects. Also notice that anyone should be able to replicate the entire project by downloading a zip file and simply changing home_dir. If some folders or files need to be created, you can do this within R # list the files and directories list.files(recursive=TRUE, include.dirs=TRUE) # create directory called &#39;Data&#39; dir.create(&#39;Data&#39;) 13.1 Logging/Sinking When executing the makefile, you can also log the output. Either by Inserting some code into the makefile that logs/sinks the output ### Project Structure home_dir &lt;- path.expand(&quot;~/Desktop/Project/&quot;) data_dir &lt;- paste0(home_dir, &quot;Data/&quot;) data_dir_r &lt;- paste0(data_dir, &quot;Raw/&quot;) data_dir_c &lt;- paste0(data_dir, &quot;Clean/&quot;) out_dir &lt;- paste0(hdir, &quot;Output/&quot;) code_dir &lt;- paste0(hdir, &quot;Code/&quot;) ### Log Output set.wd( code_dir ) sink(&quot;MAKEFILE.Rout&quot;, append=TRUE, split=TRUE) ### Execute Codes source( &quot;RBLOCK_001_DataClean.R&quot; ) source( &quot;RBLOCK_002_Figures.R&quot; ) source( &quot;RBLOCK_003_ModelsTests.R&quot; ) source( &quot;RBLOCK_004_Robust.R&quot; ) ### Stop Logging Output sink() Starting a session that logs/sinks you sourcing the makefile sink(&quot;MAKEFILE.Rout&quot;, append=TRUE, split=TRUE) source(&quot;MAKEFILE.R&quot;) sink() Execute the makefile via the commandline R CMD BATCH MAKEFILE.R MAKEFILE.Rout 13.2 Class Projects Zip your project into a single file that is easy for others to identify: Class_Project_LASTNAME_FIRSTNAME.zip Your code should be readable and error free. For code writing guides, see https://google.github.io/styleguide/Rguide.html https://style.tidyverse.org/ https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/codestyle.html http://adv-r.had.co.nz/Style.html https://www.burns-stat.com/pages/Tutor/R_inferno.pdf For organization guidelines, see https://guides.lib.berkeley.edu/c.php?g=652220&amp;p=4575532 https://kbroman.org/steps2rr/pages/organize.html https://drivendata.github.io/cookiecutter-data-science/ https://ecorepsci.github.io/reproducible-science/project-organization.html For additional logging capabilities, see https://cran.r-project.org/web/packages/logr/ For very large projects, there are many more tools available at https://cran.r-project.org/web/views/ReproducibleResearch.html For larger scale projects, use scripts https://kbroman.org/steps2rr/pages/scripts.html https://kbroman.org/steps2rr/pages/automate.html 13.3 Debugging In R, you use multiple functions on different types of data objects. Moreover, you “typically solve complex problems by decomposing them into simple functions, not simple objects.” (H. Wickham) We can use the following packages to help deal with various problems that may arise library(microbenchmark) library(compiler) library(profvis) library(parallel) library(Rcpp) Problems print to the console message(&quot;This is what a message looks like&quot;) ## This is what a message looks like warning(&quot;This is what a warning looks like&quot;) ## Warning: This is what a warning looks like stop(&quot;This is what an error looks like&quot;) ## Error in eval(expr, envir, enclos): This is what an error looks like Nonproblems also print to the console cat(&#39;cat\\n&#39;) ## cat print(&#39;print&#39;) ## [1] &quot;print&quot; 13.3.1 Tracing Consider this example of an error process (originally taken from https://adv-r.hadley.nz/ ). ## Let i() check if its argument is numeric i &lt;- function(i0) { if ( !is.numeric(i0) ) { stop(&quot;`d` must be numeric&quot;, call.=FALSE) } i0 + 10 } ## Let f() call g() call h() call i() h &lt;- function(i0) i(i0) g &lt;- function(h0) h(h0) f &lt;- function(g0) g(g0) ## Observe Error f(&quot;a&quot;) ## Error: `d` must be numeric First try simple print debugging f2 &lt;- function(g0) { cat(&quot;f2 calls g2()\\n&quot;) g2(g0) } g2 &lt;- function(h0) { cat(&quot;g2 calls h2() \\n&quot;) cat(&quot;b =&quot;, h0, &quot;\\n&quot;) h2(h0) } h2 &lt;- function(i0) { cat(&quot;h2 call i() \\n&quot;) i(i0) } f2(&quot;a&quot;) ## f2 calls g2() ## g2 calls h2() ## b = a ## h2 call i() ## Error: `d` must be numeric If that fails, try traceback debugging traceback() ## No traceback available And if that fails, try an Interactive approach g3 &lt;- function(h0) { browser() h(h0) } f3 &lt;- function(g0){ g3(g0) } f3(&quot;a&quot;) ## Called from: g3(g0) ## debug at &lt;text&gt;#3: h(h0) ## Error: `d` must be numeric 13.3.2 Isolating To inspect objects is.object(f) is.object(c(1,1)) class(f) class(c(1,1)) ## Storage Mode Type typeof(f) typeof(c(1,1)) storage.mode(f) storage.mode(c(1,1)) To check for valid inputs/outputs x &lt;- c(NA, NULL, NaN, Inf, 0) cat(&quot;Vector to inspect: &quot;) x cat(&quot;NA: &quot;) is.na(x) cat(&quot;NULL: &quot;) is.null(x) cat(&quot;NaN: &quot;) is.nan(x) cat(&quot;Finite: &quot;) is.finite(x) cat(&quot;Infinite: &quot;) is.infinite(x) ## Many others To check for values all( x &gt; -2 ) any( x &gt; -2 ) ## Check Matrix Rows rowAny &lt;- function(x) rowSums(x) &gt; 0 rowAll &lt;- function(x) rowSums(x) == ncol(x) 13.3.3 Handling Simplest Example x &lt;- &#39;A&#39; tryCatch( expr = log(x), error = function(e) { message(&#39;Caught an error but did not break&#39;) print(e) return(NA) }) Simple Example x &lt;- -2 tryCatch( expr = log(x), error = function(e) { message(&#39;Caught an error but did not break&#39;) print(e) return(NA) }, warning = function(w){ message(&#39;Caught a warning!&#39;) print(w) return(NA) }, finally = { message(&quot;Returned log(x) if successfull or NA if Error or Warning&quot;) } ) ## Caught a warning! ## &lt;simpleWarning in log(x): NaNs produced&gt; ## Returned log(x) if successfull or NA if Error or Warning ## [1] NA Safe Functions ## Define log_safe &lt;- function(x){ lnx &lt;- tryCatch( expr = log(x), error = function(e){ cat(&#39;Error Caught: \\n\\t&#39;) print(e) return(NA) }, warning = function(w){ cat(&#39;Warning Caught: \\n\\t&#39;) print(w) return(NA) }) return(lnx) } ## Test log_safe( 1) ## [1] 0 log_safe(-1) ## Warning Caught: ## &lt;simpleWarning in log(x): NaNs produced&gt; ## [1] NA log_safe(&#39; &#39;) ## Error Caught: ## &lt;simpleError in log(x): non-numeric argument to mathematical function&gt; ## [1] NA ## Further Tests s &lt;- sapply(list(&quot;A&quot;,Inf, -Inf, NA, NaN, 0, -1, 1), log_safe) ## Error Caught: ## &lt;simpleError in log(x): non-numeric argument to mathematical function&gt; ## Warning Caught: ## &lt;simpleWarning in log(x): NaNs produced&gt; ## Warning Caught: ## &lt;simpleWarning in log(x): NaNs produced&gt; s ## [1] NA Inf NA NA NaN -Inf NA 0 13.4 Optimizing In General: Clean code is often faster and less error prone Repetitive tasks can be optimized You end up with code that is cleaner, faster, and more general can be easily parallelized Computers have big memories and are really good at math. First try vectors Then try apply functions See https://uscbiostats.github.io/software-dev-site/handbook-slow-patterns.html Don’t waste time on code that is not holding you back. Your code may break, be slow, or incorrect. Look at what has already done. 13.4.1 Benchmarking The simplest approach is to time how long a code-chunk runs system.time({ x &lt;- runif(1e5) sqrt(x) }) ## user system elapsed ## 0.005 0.000 0.005 For identifying bottlenecks ## Generate Large Random Dataset n &lt;- 2e6 x &lt;- runif(n) y &lt;- runif(n) z &lt;- runif(n) XYZ &lt;- cbind(x,y,z) ## Inspect 4 equivalent `row mean` calculations profvis::profvis({ m &lt;- rowSums(XYZ)/ncol(XYZ) m &lt;- rowMeans(XYZ) m &lt;- apply(XYZ, 1, mean) m &lt;- rep(NA, n); for(i in 1:n){ m[i] &lt;- (x[i] + y[i] + z[i]) / 3 } }) ## rowSums(), colSums(), rowMeans(), and colMeans() are vectorised and fast. ## for loop is not the slowest, but the ugliest. For more systematic speed comparisons, use the microbenchmark package ## 3 Equivalent calculations of the mean of a vector mean1 &lt;- function(x,p=1) mean(x^p) mean2 &lt;- function(x,p=1) sum(x^p) / length(x) mean3 &lt;- function(x,p=1) mean.default(x^p) ## Time them x &lt;- runif(1e6) microbenchmark::microbenchmark( mean1(x,.5), mean2(x,.5), mean3(x,.5) ) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## mean1(x, 0.5) 19.86670 21.01193 25.23672 23.08634 26.53421 54.19951 100 a ## mean2(x, 0.5) 18.93285 20.15900 24.98203 21.96553 27.34243 50.92795 100 a ## mean3(x, 0.5) 19.84720 20.66372 24.37547 22.86381 25.45524 41.98796 100 a For memory leaks, first free up space and use the bench package for timing gc() ## garbage cleanup bench::mark( mean1(x,.5), mean2(x,.5), mean3(x,.5)) 13.4.2 Speed-Ups vectorize Vector operations are generally faster and easier to read than loops x &lt;- runif(1e6) ## Compare 2 moving averages ## First Try ma1 &lt;- function(y){ z &lt;- y*NA for(i in 2:length(y)){ z[i] &lt;- (y[i]-y[i-1])/2 } return(z) } # Optimized using diff diff( c(2,2,10,9) ) ## [1] 0 8 -1 ma2 &lt;- function(y){ z2 &lt;- diff(y)/2 z2 &lt;- c(NA, z2) return(z2) } all.equal(ma1(y),ma2(y)) ## [1] TRUE microbenchmark::microbenchmark( ma1(y), ma2(y) ) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## ma1(y) 174.87259 187.94205 198.74114 192.49076 200.30609 340.8298 100 a ## ma2(y) 19.66996 21.73871 26.71302 24.20618 27.26824 186.1022 100 b Likewise, matrix operations are often faster than vector operations. Packages Before creating your own program, check if there is a faster or more memory efficient version. E.g., data.table or Rfast2 for basic data manipulation. X &lt;- cbind(1, runif(1e6)) Y &lt;- X %*% c(1,2) + rnorm(1e6) DAT &lt;- as.data.frame(cbind(Y,X)) system.time({.lm.fit(X, Y) }) ## user system elapsed ## 0.106 0.000 0.034 system.time({ lm(Y~X, data=DAT) }) ## user system elapsed ## 0.403 0.019 0.155 Note that quicker codes tend to have fewer checks and return less information. So you must know exactly what you are putting in and getting out. 13.4.3 Bottlenecks Sometimes there will still be a problematic bottleneck. Your next step should be parallelism: Write the function as a general vectorized function. Apply the same function to every element in a list at the same time ## lapply in parallel on {m}ultiple {c}ores x &lt;- c(10,20,30,40,50) f &lt;- function(element) { element^element } parallel::mclapply( x, mc.cores=2, FUN=f) ## [[1]] ## [1] 1e+10 ## ## [[2]] ## [1] 1.048576e+26 ## ## [[3]] ## [1] 2.058911e+44 ## ## [[4]] ## [1] 1.208926e+64 ## ## [[5]] ## [1] 8.881784e+84 More power is often not the solution ## vectorize and compile e_power_e_fun &lt;- compiler::cmpfun( function(vector){ vector^vector} ) ## base R x &lt;- 0:1E6 s_vc &lt;- system.time( e_power_e_vec &lt;- e_power_e_fun(x) ) s_vc ## user system elapsed ## 0.026 0.002 0.028 ## brute power x &lt;- 0:1E6 s_bp &lt;- system.time({ e_power_e_mc &lt;- unlist( parallel::mclapply(x, mc.cores=2, FUN=e_power_e_fun)) }) s_bp ## user system elapsed ## 0.903 0.159 0.605 ## Same results all(e_power_e_vec==e_power_e_mc) ## [1] TRUE Parallelism does not go great with a GUI. For identifying bottlenecks on a cluster without a GUI, try Rprof() Rprof( interval = 0.005) # Create Data x &lt;- runif(2e6) y &lt;- sqrt(x) ## Loop Format Data z &lt;- y*NA for(i in 2:length(y)){ z[i] &lt;- (y[i]-y[i-1])/2 } ## Regression X &lt;- cbind(1,x)[-1,] Z &lt;- z[-1] reg_fast &lt;- .lm.fit(X, Z) Rprof(NULL) summaryRprof() If you still are stuck, you can try Amazon Web Server for more brute-power rewrite bottlenecks with a working C++ compiler or Fortran compiler. Before doing that, however, look into https://cran.r-project.org/web/views/HighPerformanceComputing.html 13.4.3.0.1 Compiled Code To get C++ On Windows, install Rtools. On Mac, install Xcode from the app store. On Linux, sudo apt-get install r-base-dev or similar. To call C++ from R use package Rcpp Rcpp::cppFunction(&#39; int add(int x, int y, int z) { int sum = x + y + z; return sum; }&#39; ) add(1, 2, 3) ## [1] 6 For help getting started with Rcpp, see https://cran.r-project.org/web/packages/Rcpp/vignettes/Rcpp-quickref.pdf First try to use C++ (or Fortran) code that others have written .C .Fortran For a tutorial, see https://masuday.github.io/fortran_tutorial/r.html 13.5 More Literature Advanced Programming https://rmd4sci.njtierney.com/ https://smac-group.github.io/ds/high-performance-computing.html https://www.stat.umn.edu/geyer/3701/notes/arithmetic.Rmd For debugging tips https://cran.r-project.org/doc/manuals/R-lang.html#Debugging https://cran.r-project.org/doc/manuals/R-exts.html#Debugging https://adv-r.hadley.nz/debugging.html https://adv-r.hadley.nz/conditions.html https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/debugging.html https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/functions.html For optimization tips https://cran.r-project.org/doc/manuals/R-exts.html#Tidying-and-profiling-R-code https://cran.r-project.org/doc/manuals/R-lang.html#Exception-handling https://adv-r.hadley.nz/perf-measure.html.libPaths() https://adv-r.hadley.nz/perf-improve.html https://cran.r-project.org/doc/manuals/R-exts.html#System-and-foreign-language-interfaces https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/profiling.html https://adv-r.hadley.nz/rcpp.html https://bookdown.dongzhuoer.com/hadley/adv-r/ For parallel programming https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/parallel.html https://bookdown.org/rdpeng/rprogdatascience/parallel-computation.html https://grantmcdermott.com/ds4e/parallel.html https://psu-psychology.github.io/r-bootcamp-2018/talks/parallel_r.html For general tips https://github.com/christophergandrud/Rep-Res-Book Efficient R programming. C. Gillespie R. Lovelace. 2021. https://csgillespie.github.io/efficientR/ Data Science at the Command Line, 1e. Janssens J. 2020. https://www.datascienceatthecommandline.com/1e/ R Programming for Data Science. Peng R. 2020. https://bookdown.org/rdpeng/rprogdatascience/ Advanced R. H. Wickham 2019. https://adv-r.hadley.nz/ Econometrics in R. Grant Farnsworth. 2008. http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf The R Inferno. https://www.burns-stat.com/documents/books/the-r-inferno/ "],["web-applications.html", " 14 Web Applications 14.1 More Literature", " 14 Web Applications Shiny is an R packag to build web applications. Shiny Flexdashboards are nicely formatted Shiny Apps. While it is possible to use Shiny without the Flexdashboard formatting, I think it is easier to remember .R files are codes for statistical analysis .Rmd files are for communicating: reports, slides, posters, and apps Example: Histogram Download the source file TrialApp1_Histogram_Dashboard.Rmd and open it with rstudio. Then run it with rmarkdown::run(&#39;TrialApp1_Histogram_Dashboard.Rmd&#39;) Within the app, experiment with how larger sample sizes change the distribution. Edit the app to let the user specify the number of breaks in the histogram. If you are having difficulty, you can try working first with the barebones shiny code. To do this, download TrialApp0_Histogram.Rmd and edit it in Rstudio. You can run the code with rmarkdown::run('TrialApp0_Histogram.Rmd'). 14.1 More Literature Overview https://bookdown.org/yihui/rmarkdown/shiny-documents.html https://shiny.rstudio.com/tutorial/ https://shiny.rstudio.com/articles/ https://shiny.rstudio.com/gallery/ https://rstudio.github.io/leaflet/shiny.html https://mastering-shiny.org/ More Help with Shiny Apps https://shiny.rstudio.com/tutorial/written-tutorial/lesson1/ https://mastering-shiny.org/basic-app.html https://towardsdatascience.com/beginners-guide-to-creating-an-r-shiny-app-1664387d95b3 https://shiny.rstudio.com/articles/interactive-docs.html https://bookdown.org/yihui/rmarkdown/shiny-documents.html https://shiny.rstudio.com/gallery/plot-interaction-basic.html https://www.brodrigues.co/blog/2021-03-02-no_shiny_dashboard/ https://bookdown.org/yihui/rmarkdown/shiny.html https://shinyserv.es/shiny/ https://bookdown.org/egarpor/NP-UC3M/kre-i-kre.html#fig:kreg https://engineering-shiny.org/ "],["software.html", " 15 Software 15.1 Latest versions 15.2 General Workflow 15.3 Sweave 15.4 Stata", " 15 Software 15.1 Latest versions Make sure your packages are up to date update.packages() After reinstalling, you can update all packages stored in all .libPaths() with the following command install.packages(old.packages(checkBuilt=T)[,&quot;Package&quot;]) To find broken packages after an update library(purrr) set_names(.libPaths()) %&gt;% map(function(lib) { .packages(all.available = TRUE, lib.loc = lib) %&gt;% keep(function(pkg) { f &lt;- system.file(&#39;Meta&#39;, &#39;package.rds&#39;, package = pkg, lib.loc = lib) tryCatch({readRDS(f); FALSE}, error = function(e) TRUE) }) }) ## https://stackoverflow.com/questions/31935516/installing-r-packages-error-in-readrdsfile-error-reading-from-connection/55997765 To remove packages duplicated in multiple libraries ## Libraries i &lt;- installed.packages() libs &lt;- .libPaths() ## Find Duplicated Packages i1 &lt;- i[ i[,&#39;LibPath&#39;]==libs[1], ] i2 &lt;- i[ i[,&#39;LibPath&#39;]==libs[2], ] dups &lt;- i2[,&#39;Package&#39;] %in% i1[,&#39;Package&#39;] all( dups ) ## Remove remove.packages( i2[,&#39;Package&#39;], libs[2] ) 15.2 General Workflow If you want to go further down the reproducibility route (recommended, but not required for our class), consider making your entire workflow use Free Open Source Software Linux: An alternative to windows and mac operating systems. Used in computing clusters, big labs, and phones. E.g., Ubuntu and Fedora are popular brands https://www.r-bloggers.com/linux-data-science-virtual-machine-new-and-upgraded-tools/, http://www.howtogeek.com/249966/how-to-install-and-use-the-linux-bash-shell-on-windows-10/ On Fedora, you can open RStudio on the commandline with rstudio Alternatively, you are encouraged to try using R without a GUI. E.g., on Fedora, this document can be created directly via Rscript -e &quot;rmarkdown::render(&#39;RMarkown.Rmd&#39;)&quot; Latex: An alternative to Microsoft Word. Great for writing many equations and typesetting. Easy to integrate Figures, Tables, and References. Steep learning curve. easiest to get started online with Overleaf can also download yourself via Tex Live and GUI TexStudio To begin programming, see https://biostat.app.vumc.org/wiki/pub/Main/TheresaScott/Intro.to.LaTeX.TAScott.pdf https://www.tug.org/begin.html 15.3 Sweave Sweave: is an alternative to Rmarkdown for integrating latex and R. While Rmarkdown “writes R and latex within markdown”, Sweave “write R in latex”. Sweave files end in “.Rnw” and can be called within R Sweave(&#39;Sweave_file.Rnw&#39;) or directly from the command line R CMD Sweave Sweave_file.Rnw In both cases, a latex file Sweave_file.tex is produced, which can then be converted to Sweave_file.pdf. For more on Sweave, https://rpubs.com/YaRrr/SweaveIntro https://support.rstudio.com/hc/en-us/articles/200552056-Using-Sweave-and-knitr https://www.statistik.lmu.de/~leisch/Sweave/Sweave-manual.pdf Knitr: You can also produce a pdf from an .Rnw file via knitr Rscript -e &quot;knitr::Sweave2knitr(&#39;Sweave_file.Rnw&#39;)&quot; Rscript -e &quot;knitr::knit2pdf(&#39;Sweave_file-knitr.Rnw&#39;)&quot; For background on knitr https://yihui.org/knitr/ https://kbroman.org/knitr_knutshell/pages/latex.html https://sachsmc.github.io/knit-git-markr-guide/knitr/knit.html 15.4 Stata For those transitioning from Stata or replicating others’ Stata work, you can work with Stata data and code within R. Translations of common procedures is provided by https://stata2r.github.io/. See also the textbook “R for Stata Users” by Robert A. Muenchen and Joseph M. Hilbe. Many packages allows you to read data created by different programs. As of right now, haven is a particularly useful for reading in Stata files library(haven) read_dta() ## See also foreign::read.dta You can also execute stata commands directly in R via package Rstata. (Last time I checked, Rstata requires you to have purchased a non-student version of Stata.) Moreover, you can include stata in the markdown reports via package Statamarkdown: library(Rstata) library(Statamarkdown) There are many R packages to replicate or otherwise directly copy what Stata does. For example, see the margins package https://cran.r-project.org/web/packages/margins/vignettes/Introduction.html For more information on R and Stata, see https://github.com/lbraglia/RStata https://ignacioriveros1.github.io/r/2020/03/22/r_and_stata.html https://bookdown.org/yihui/rmarkdown-cookbook/eng-stata.html https://rpubs.com/quarcs-lab/stata-from-Rstudio https://clanfear.github.io/Stata_R_Equivalency/docs/r_stata_commands.html https://libguides.bates.edu/c.php?g=209169&amp;p=7233333 You can also use other software (such as Python) within R. You can also use R within Stata, or both within Python. With R, you can easily import many different data types https://cran.r-project.org/doc/manuals/R-data.html https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-import.pdf "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
