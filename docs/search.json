[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introductory Economic Statistics: A Data-Driven Approach using R",
    "section": "",
    "text": "Preface\nThis Rbook introduces students to econometrics without parametric assumptions and formulas. In many ways, it is a modern version of “Introductory Econometrics: Using Monte Carlo Simulation with Microsoft Excel” by Barreto and Howland, updated to adhere to modern statistics teaching guidelines and give econometrics students the best tools for their labor market. Altogether, students learn to produce statistical analyses of economic data relevant to both the private and public sector, as well as an intuitive foundation for more advanced courses, including both nonparametric statistics and structural econometrics.\nStudents are introduced to the basics of statistical programming using R alongside the theoretical analysis of economic data using R. This teaches applied statistics relevant to general students, leaving the classical statistics program to mathematics departments and poor applied practices in the dust.\nThis Rbook is organized into three substantive parts: univariate, bivariate, and multivariate data analysis. The first part has three notable differences from a typical intro to statistics book.\n1: students more deeply learn to use and interpret the Histogram, ECDF, and Boxplot (and to avoid 3D pie charts and other chart junk). They work with actual data before abstract probability theory (initially limited to simple events or intervals, with sums of random variable and transformations available optionally later)\n2: students learn the basics of probability theory with real world data and computer simulations. I aimed to replace mathematical proofs with simulations whenever possible. This allows less emphasis on classical probability theory mechanics as well as fewer “t and z drills”. Confidence intervals and hypothesis tests are covered via boostrapping, for example, so students learn the conceptual approach rather than a formula.\n3: students learn the theory and practice of univariate statistics before moving to bivariate statistics, rather than mixing uni-and-bivariate content. Business textbooks often introduce both types of data, then cover univariate statistics, and return to bivariate statistics much later. Math textbooks typically introduce students to probability theory long before concrete applications. This textbooks includes many practical examples, including on how to analyze data interactively and communicate results.\nParts II and III refines material from several introductory econometrics textbooks and covers linear models only from a “minimum distance” perspective. (We operate under the maxim “All models are wrong” and do not prove unbiasedness.) Also included is a novel chapter on “Data scientism” that more clearly illustrates the ways that simplistic approaches can mislead rather than illuminate. (I stress “gun safety” instead of “pull to shoot”, which is missing from many econometrics textbooks that start with “Assume \\(Y=X\\beta+E\\)”.) Overall, there is a more humble view towards what we can infer from linear regressions that opens the door towards more advanced courses in model development and interpretation. We also cover statistical reporting using R + markdown, which research suggests is a good combination for students 1 2.\n\nAlthough any interested reader may find it useful, this Rbook is primarily developed for my students.\nIf you use this Rbook, please cite\n@book{Adamson2025_Rbook,\n  title={Introductory Economic Statistics: A Data-Driven Approach using R},\n  author={Adamson, Jordan},\n  year={2025},\n  publisher={Bookdown},\n  url={https://jadamso.github.io/Rbooks/}\n}\nPlease also report any errors or issues at https://github.com/Jadamso/Rbooks/issues.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-00-GettingStarted.html",
    "href": "00-00-GettingStarted.html",
    "title": "Getting Started",
    "section": "",
    "text": "We will use R statistical software, which you will be introduced to in the first chapters.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "01_01_FirstSteps.html",
    "href": "01_01_FirstSteps.html",
    "title": "1  First Steps",
    "section": "",
    "text": "1.1 Why Program in R?\nYou should program your statistical analysis, and we will cover some of the basics of how to do this in R. You also want your work to be replicable\nYou can read more about the distinction in many places, including\nWe focus on R because it is good for complex stats, concise figures, and coherent organization. It is built and developed by applied statisticians for statistics, and used by many in academia and industry. For students, think about labor demand and what may be good for getting a job. Do some of your own research to best understand how much to invest.\nMy main sell to you is that being reproducible is in your own self-interest.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First Steps</span>"
    ]
  },
  {
    "objectID": "01_01_FirstSteps.html#why-program-in-r",
    "href": "01_01_FirstSteps.html#why-program-in-r",
    "title": "1  First Steps",
    "section": "",
    "text": "Replicable: someone collecting new data comes to the same results.\nReproducibile: someone reusing your data comes to the same results.\n\n\n\nhttps://www.annualreviews.org/doi/10.1146/annurev-psych-020821-114157\nhttps://nceas.github.io/sasap-training/materials/reproducible_research_in_r_fairbanks/\n\n\n\n\nAn example workflow.\nFirst Steps…\nStep 1: Some ideas and data about how variable \\(X_{1}\\) affects variable \\(Y_{1}\\), which we denote as \\(X_{1}\\to Y_{1}\\)\n\nYou copy some data into a spreadsheet, manually aggregate\ndo some calculations and tables the same spreadsheet\nsome other analysis from here and there, using this software and that.\n\nStep 2: Pursuing the lead for a week or two\n\nyou extend your dataset with more observations\ncopy in a spreadsheet data, manually aggregate\ndo some more calculations and tables, same as before\n\nA Little Way Down the Road …\n1 month later: someone asks about another factor: \\(X_{2} \\to Y\\).\n\nyou download some other type of data\nYou repeat Step 2 with some data on \\(X_{2}\\).\nThe details from your “point and click” method are a bit fuzzy.\nIt takes a little time, but you successfully redo the analysis.\n\n4 months later: someone asks about yet another factor: \\(X_{3}\\to Y_{1}\\).\n\nYou again repeat Step 2 with some data on \\(X_{3}\\).\nYou’re pretty sure none of tables your tried messed up the order of the rows or columns.\nIt takes more time and effort. The data processing was not transparent, but you eventually redo the analysis.\n\n6 months later: you want to explore another outcome: \\(X_{2} \\to Y_{2}\\).\n\nYou found out Excel had some bugs in it’s statistical calculations (see e.g., https://biostat.app.vumc.org/wiki/pub/Main/TheresaScott/StatsInExcel.TAScot.handout.pdf). You now use a new version of the spreadsheet\nYou’re not sure you merged everything correctly. After much time and effort, most (but not all) of the numbers match exactly.\n\n2 years later: your boss wants you to replicate your work: \\(X_{1}, X_{2}, X_{3} \\to Y_{1}\\).\n\nA rival has proposed something new. Their idea doesn’t actually make any sense, but their figures and statistics look better.\nYou don’t even use that computer anymore and a collaborator who handled the data on \\(X_{2}\\) has moved on.\n\n\n\nAn alternative workflow.\nSuppose you decided to code what you did beginning with Step 2.\nIt does not take much time to update or replicate your results.\n\nYour computer runs for 2 hours and reproduces the figures and tables.\nYou also rewrote your big calculations to use multiple cores, this took two hours to do but saved 6 hours each time you rerun your code.\nYou add some more data. It adds almost no time to see whether much has changed.\n\nYour results are transparent and easier to build on.\n\nYou see the exact steps you took and found an error\n\nGoogle “worst Excel errors” and note the frequency they arise from copy/paste via the “point-and-click” approach. E.g., Fidelity’s $2.6 Billion Dividend Error.\n\nGlad you found a problem before sending your research out! See https://retractionwatch.com/ and https://econjwatch.org/. Future economists should also read https://core.ac.uk/download/pdf/300464894.pdf.\n\n\nYou try out a new plot you found in The Visual Display of Quantitative Information, by Edward Tufte.\n\nIt’s not a standard plot, but google answers most of your questions.\nTutorials help avoid bad practices, such as plotting 2D data as a 3D object (see e.g., https://clauswilke.com/dataviz/no-3d.html).\n\nYou try out an obscure statistical approach that’s hot in your field.\n\nit doesn’t make the report, but you have some confidence that candidate issue isn’t a big problem",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First Steps</span>"
    ]
  },
  {
    "objectID": "01_01_FirstSteps.html#first-steps",
    "href": "01_01_FirstSteps.html#first-steps",
    "title": "1  First Steps",
    "section": "1.2 First Steps",
    "text": "1.2 First Steps\n\nInstall R.\nFirst Install R. Then Install Rstudio.\nFor help setting up, see any of the following links\n\nhttps://learnr-examples.shinyapps.io/ex-setup-r/\nhttps://rstudio-education.github.io/hopr/starting.html\nhttps://a-little-book-of-r-for-bioinformatics.readthedocs.io/en/latest/src/installr.html\nhttps://cran.r-project.org/doc/manuals/R-admin.html\nhttps://courses.edx.org/courses/UTAustinX/UT.7.01x/3T2014/56c5437b88fa43cf828bff5371c6a924/\nhttps://owi.usgs.gov/R/training-curriculum/installr/\nhttps://www.earthdatascience.org/courses/earth-analytics/document-your-science/setup-r-rstudio/\n\nFor Fedora users, note that you need to first enable the repo and then install\n\n\nCode\nsudo dnf install 'dnf-command(copr)'\nsudo dnf copr enable iucar/rstudio\nsudo dnf install rstudio-desktop\n\n\nMake sure you have the latest version of R and Rstudio for class. If not, then reinstall.\n\n\nInterfacing with R Studio.\nRstudio is perhaps the easiest to get going with. (There are other GUI’s.)\nIn Rstudio, there are 4 panes. (If you do not see 4, click “file &gt; new file &gt; R script” on the top left of the toolbar.)\n\n\n\n\n\n\n\n\n\nThe top left pane is where you write your code. For example, type\n\n1+1\n\nThe pane below is where your code is executed. Keep you mouse on the same line as your code, and then click “Run”. You should see\n&gt; 1+1\n[1] 2\nIf you click “Run” again, you should see that same output printed again.\nYou should add comments to your codes, and you do this with hashtags. For example\n# This is my first comment!\n1+1 # The simplest calculation I could think of\nYou can execute each line one-at-a-time. Or you can highlight them both, to take advantage of how R executes commands line-by-line.\n\n\nReading This Textbook.\nAs we proceed, you can see both my source code and output like this:\n\n\nCode\n1+1\n## [1] 2\n\n\nThere are also special boxes\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis box contains need to know examples. Such as\n\n\nCode\n1+1\n## [1] 2\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThis box contains test yourself examples and questions. Such as\n\n\nCode\n2+7\n## [1] 9\n2/7\n## [1] 0.2857143\n\n\n\n\n\n\n\nAssignment.\nYou can create “variables” that store values. For example,\n\n\nCode\nx &lt;- 1 # Make your first variable\nx + 1 # The simplest calculation I could think of\n## [1] 2\n\n\n\n\nCode\nx &lt;- 23 #Another example\nx + 1\n## [1] 24\n\n\n\n\nCode\ny &lt;- x + 1 #Another example\ny\n## [1] 24\n\n\nYour variables must be defined in order to use them. Otherwise you get an error. For example,\n\n\nCode\nX +   1 # notice that R is sensitive to capitalization \n## Error: object 'X' not found\n\n\nYour variable names do not matter technically, but they should be informative\n\n\nCode\none &lt;- 1 # good variable name\none\n## [1] 1\n\none &lt;- 43 # bad variable name\none\n## [1] 43\n\n\nGood names avoid confusion later\n\n\nCode\nx &lt;- 43\nx_plus_two &lt;- x + 2 # better\nx_plus_two\n## [1] 45\n\n\n\n\nScripting.\n\nCreate a folder on your computer to save your scripts\nSave your R Script file as My_First_Script.R in your folder\nClose Rstudio\nOpen your script and re-run it\n\nAs you work through the material, make sure to both execute and save your scripts. Add lots of commentary to your scripts. Name your scripts systematically.\nThere are often many ways to accomplish the same goal. You first scripts will be very basic and rough, but you can edit them later based on what you learn. And you can always ask R for help\n\n\nCode\nsum(x, 2) # x + 2\n?sum\n\n\nWe write script in the top left so that we can edit common mistakes.\n\n\nCode\n# Mistake 1: using undefined objects\nY\n\n#  Mistake 2: spelling and spacing\nY &lt; - 43\nY_plus_z &lt;- Y + z\n\n# Mistake 3: half-completed code\nx + y + \nx_plus_y_plus_z &lt;- x + y + z\n# Seeing \"+\" in the bottom console?\n# press \"Escape\" and try again",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First Steps</span>"
    ]
  },
  {
    "objectID": "01_01_FirstSteps.html#further-reading",
    "href": "01_01_FirstSteps.html#further-reading",
    "title": "1  First Steps",
    "section": "1.3 Further Reading",
    "text": "1.3 Further Reading\nThere are many good and free programming materials online.\nThe most common tasks can be found https://github.com/rstudio/cheatsheets/blob/main/rstudio-ide.pdf\nSome of my programming examples originally come from https://r4ds.had.co.nz/ and I recommend https://intro2r.com.\nI have also used online material from many places over the years, as there are many good yet free-online tutorials and courses specifically on R programming. See e.g.,\n\nhttps://cran.r-project.org/doc/manuals/R-intro.html\nR Graphics Cookbook, 2nd edition. Winston Chang. 2021. https://r-graphics.org/\nR for Data Science. H. Wickham and G. Grolemund. 2017. https://r4ds.had.co.nz/index.html\nAn Introduction to R. W. N. Venables, D. M. Smith, R Core Team. 2017. https://colinfay.me/intro-to-r/\nIntroduction to R for Econometrics. Kieran Marray. https://bookdown.org/kieranmarray/intro_to_r_for_econometrics/\nWollschläger, D. (2020). Grundlagen der Datenanalyse mit R: eine anwendungsorientierte Einführung. http://www.dwoll.de/rexrepos/\nSpatial Data Science with R: Introduction to R. Robert J. Hijmans. 2021. https://rspatial.org/intr/index.html\nhttps://www.econometrics-with-r.org/1.2-a-very-short-introduction-to-r-and-rstudio.html\nhttps://rafalab.github.io/dsbook/\nhttps://moderndive.com/foreword.html\nhttps://rstudio.cloud/learn/primers/1.2\nhttps://cran.r-project.org/manuals.html\nhttps://stats.idre.ucla.edu/stat/data/intro_r/intro_r_interactive_flat.html\nhttps://cswr.nrhstat.org/app-r\n\nFor more on why to program in R, see\n\nhttp://www.r-bloggers.com/the-reproducibility-crisis-in-science-and-prospects-for-r/\nhttp://fmwww.bc.edu/GStat/docs/pointclick.html\nhttps://github.com/qinwf/awesome-R\\#reproducible-research\nA Guide to Reproducible Code in Ecology and Evolution\nhttps://biostat.app.vumc.org/wiki/pub/Main/TheresaScott/ReproducibleResearch.TAScott.handout.pdf",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First Steps</span>"
    ]
  },
  {
    "objectID": "01_02_Mathematics.html",
    "href": "01_02_Mathematics.html",
    "title": "2  Mathematics",
    "section": "",
    "text": "2.1 Objects\nIn R: scalars, vectors, and matrices are different kinds of “objects”.\nThese objects are used extensively in data analysis\nVectors are probably your most common object in R, but we will start with scalars.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mathematics</span>"
    ]
  },
  {
    "objectID": "01_02_Mathematics.html#objects",
    "href": "01_02_Mathematics.html#objects",
    "title": "2  Mathematics",
    "section": "",
    "text": "scalars: summary statistics (average household income).\nvectors: single variables in data sets (the household income of each family in Vancouver).\nmatrices: two variables in data sets (the age and education level of every person in class).\n\n\n\nScalars.\nMake your first scalar\n\n\nCode\nxs &lt;- 2 # Make your first scalar\nxs  # Print the scalar\n## [1] 2\n\n\nPerform simple calculations and see how R is doing the math for you\n\n\nCode\nxs + 2\n## [1] 4\nxs*2 # Perform and print a simple calculation\n## [1] 4\n(xs+1)^2 # Perform and print a simple calculation\n## [1] 9\nxs + NA # often used for missing values\n## [1] NA\n\n\nNow change xs, predict what will happen, then re-run the code.\n\n\nVectors.\nMake your first vector\n\n\nCode\nx &lt;- c(0,1,3,10,6) # Your First Vector\nx # Print the vector\n## [1]  0  1  3 10  6\nx[2] # Print the 2nd Element; 1\n## [1] 1\nx+2 # Print simple calculation; 2,3,5,8,12\n## [1]  2  3  5 12  8\nx*2\n## [1]  0  2  6 20 12\nx^2\n## [1]   0   1   9 100  36\n\n\nApply mathematical calculations elementwise\n\n\nCode\nx+x\n## [1]  0  2  6 20 12\nx*x\n## [1]   0   1   9 100  36\nx^x\n## [1] 1.0000e+00 1.0000e+00 2.7000e+01 1.0000e+10 4.6656e+04\n\n\nIn R, scalars are treated as a vector with one element.\n\n\nCode\nc(1)\n## [1] 1\n\n\nSometimes, we will use vectors that are entirely ordered.\n\n\nCode\n1:7\n## [1] 1 2 3 4 5 6 7\nseq(0,1,by=.1)\n##  [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\n# Ordering data\nsort(x)\n## [1]  0  1  3  6 10\nx[order(x)]\n## [1]  0  1  3  6 10\n\n\n\n\nMatrices.\nMatrices are also common objects\n\n\nCode\nx1 &lt;- c(1,4,9)\nx2 &lt;- c(3,0,2)\nx_mat &lt;- rbind(x1, x2)\n\nx_mat       # Print full matrix\n##    [,1] [,2] [,3]\n## x1    1    4    9\n## x2    3    0    2\nx_mat[2,]   # Print Second Row\n## [1] 3 0 2\nx_mat[,2]   # Print Second Column\n## x1 x2 \n##  4  0\nx_mat[2,2]  # Print Element in Second Column and Second Row\n## x2 \n##  0\n\n\nThere are elementwise calculations\n\n\nCode\nx_mat+2\n##    [,1] [,2] [,3]\n## x1    3    6   11\n## x2    5    2    4\nx_mat*2\n##    [,1] [,2] [,3]\n## x1    2    8   18\n## x2    6    0    4\nx_mat^2\n##    [,1] [,2] [,3]\n## x1    1   16   81\n## x2    9    0    4\n\nx_mat + x_mat\n##    [,1] [,2] [,3]\n## x1    2    8   18\n## x2    6    0    4\nx_mat*x_mat #NOT classical matrix multiplication\n##    [,1] [,2] [,3]\n## x1    1   16   81\n## x2    9    0    4\nx_mat^x_mat\n##    [,1] [,2]      [,3]\n## x1    1  256 387420489\n## x2   27    1         4\n\n\nAnd you can also use matrix algebra\n\n\nCode\nx_mat1 &lt;- matrix(2:7,2,3)\nx_mat1\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n\nx_mat2 &lt;- matrix(4:-1,2,3)\nx_mat2\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\ntcrossprod(x_mat1, x_mat2) #x_mat1 %*% t(x_mat2)\n##      [,1] [,2]\n## [1,]   16    4\n## [2,]   22    7\n\ncrossprod(x_mat1, x_mat2)\n##      [,1] [,2] [,3]\n## [1,]   17    7   -3\n## [2,]   31   13   -5\n## [3,]   45   19   -7",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mathematics</span>"
    ]
  },
  {
    "objectID": "01_02_Mathematics.html#functions",
    "href": "01_02_Mathematics.html#functions",
    "title": "2  Mathematics",
    "section": "2.2 Functions",
    "text": "2.2 Functions\n\nSimple Functions.\nFunctions are applied to objects\n\n\nCode\n# Define a function that adds two to any vector\nadd_two &lt;- function(input_vector) { #input_vector is a placeholder\n    output_vector &lt;- input_vector + 2 # new object defined locally \n    return(output_vector) # return new object \n}\n# Apply that function to a vector\nx &lt;- c(0,1,3,10,6)\nadd_two(input_vector=x) #same as add_two(x)\n## [1]  2  3  5 12  8\n\n\nCommon mistakes:\n\n\nCode\nprint(output_vector)\n# This is not available globally\n\n# Double check your spelling\nx &lt; - add_two(input_vector=X) \n\n# Seeing \"+\" in the bottom console\n# often means you forgot to close the function with \"}\" \n# press \"Escape\" and try again\nadd_two &lt;- function(input_vector) { \n    output_vector &lt;- input_vector + 2 \n    return(output_vector)\nx &lt;- c(0,1,3,10,6)\nadd_two(x)\n\n\nThere are many different functions\n\n\nCode\nadd_vec &lt;- function(input_vector1, input_vector2) {\n    output_vector &lt;- input_vector1 + input_vector2\n    return(output_vector)\n}\nadd_vec(x,3)\n## [1]  3  4  6 13  9\nadd_vec(x,x)\n## [1]  0  2  6 20 12\n\nsum_squared &lt;- function(x1, x2) {\n    y &lt;- (x1 + x2)^2\n    return(y)\n}\n\nsum_squared(1, 3)\n## [1] 16\nsum_squared(x, 2)\n## [1]   4   9  25 144  64\nsum_squared(x, NA) \n## [1] NA NA NA NA NA\nsum_squared(x, x)\n## [1]   0   4  36 400 144\nsum_squared(x, 2*x)\n## [1]   0   9  81 900 324\n\n\nFunctions can take functions as arguments. Note that a statistic is defined as a function of data.\n\n\nCode\nstatistic &lt;- function(x, f){\n    y &lt;- f(x)\n    return(y)\n}\nstatistic(x, sum)\n## [1] 20\n\n\nThere are many possible functions you can make and use. More complicated functions often have defaults.\n\n\nCode\nfun_of_seq &lt;- function(f, constant=2){\n    x1 &lt;- seq(1,3, length.out=12)\n    x2 &lt;- x1+constant\n    x &lt;- cbind(x1,x2)\n    y &lt;- f(x)\n    return(y)\n}\nfun_of_seq(sum)\n## [1] 72\nfun_of_seq(sum, 3)\n## [1] 84\nfun_of_seq(prod)\n## [1] 30799645993\nfun_of_seq(prod, 3)\n## [1] 473621744988\n\n\nYou can also apply functions to matrices\n\n\nCode\nsum_squared(x_mat, x_mat)\n##    [,1] [,2] [,3]\n## x1    4   64  324\n## x2   36    0   16\n\n# Apply function to each matrix row\ny &lt;- apply(x_mat, 1, sum)^2 \n# ?apply  #checks the function details\n\n\n\n\nLoops.\nApplying the same function over and over again\n\n\nCode\n#Create empty vector\nexp_vector &lt;- vector(length=3)\n#Fill empty vector\nfor(i in 1:3){\n    exp_vector[i] &lt;- exp(i)\n}\n\n# Compare\nexp_vector\n## [1]  2.718282  7.389056 20.085537\nc( exp(1), exp(2), exp(3))\n## [1]  2.718282  7.389056 20.085537\n\n\nA more complicated example\n\n\nCode\ncomplicated_fun &lt;- function(i, j=0){\n    x &lt;- i^(i-1)\n    y &lt;- x + mean( j:i )\n    z &lt;- log(y)/i\n    return(z)\n}\ncomplicated_vector &lt;- vector(length=10)\nfor(i in 1:10){\n    complicated_vector[i] &lt;- complicated_fun(i)\n}\n\n\nA recursive example\n\n\nCode\nx &lt;- vector(length=4)\nx[1] &lt;- 1\nfor(i in 2:4){\n    x[i] &lt;- (x[i-1]+1)^2\n}\nx\n## [1]   1   4  25 676",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mathematics</span>"
    ]
  },
  {
    "objectID": "01_02_Mathematics.html#logic-and-counting",
    "href": "01_02_Mathematics.html#logic-and-counting",
    "title": "2  Mathematics",
    "section": "2.3 Logic and Counting",
    "text": "2.3 Logic and Counting\n\nBasic Logic.\nTRUE/FALSE\n\n\nCode\nx &lt;- c(1,2,3,NA)\nx &gt; 2\n## [1] FALSE FALSE  TRUE    NA\nx==2\n## [1] FALSE  TRUE FALSE    NA\n\nany(x==2)\n## [1] TRUE\nall(x==2)\n## [1] FALSE\n2 %in% x\n## [1] TRUE\n\n2==TRUE\n## [1] FALSE\n2==FALSE\n## [1] FALSE\n \nis.numeric(x)\n## [1] TRUE\nis.na(x)\n## [1] FALSE FALSE FALSE  TRUE\n\n\nThe “&” and “|” commands are logical calculations that compare vectors to the left and right.\n\n\nCode\nx &lt;- 1:3\n(x &gt;= 1) & (x &lt; 2)\n## [1]  TRUE FALSE FALSE\n(x &gt;= 1) | (x &lt; 2)\n## [1] TRUE TRUE TRUE\n\nif( all(x &gt;= 1) ){\n    print(\"ok\")\n} else {\n    print(\"not ok\")\n}\n## [1] \"ok\"\n\nlogic_fun &lt;- function(x){\n    if( all(x &gt;= 1) ){\n        print(\"ok\")\n    } else {\n        print(\"not ok\")\n    }\n}\nlogic_fun(1:3)\n## [1] \"ok\"\nlogic_fun(0:2)\n## [1] \"not ok\"\n\n\n\n\nBasic Counting.\nFactorials are used for counting the different ways of arranging \\(n\\) distinct objects into a sequence: \\(n!=1\\times2\\times3 ... (n-2)\\times(n-1)\\times n\\).\n\n\nCode\nfactorial(3)\n## [1] 6\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nHow many ways are there to order the numbers \\(\\{1, 2, 3\\}\\)?\n{1,2,3} {1,3,2}\n{2,1,3} {2,3,1}\n{3,2,1} {3,1,2}\n\n\n\nThe binomial coefficient \\(\\tbinom {n}{k}\\) counts the subsets of \\(k\\) elements from a set with \\(n\\) elements.\n\n\nCode\n#Ways to draw k=2 from a set with n=4 \nchoose(4,2)\n## [1] 6\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nHow many subsets with \\(k=2\\) are there for the set \\(\\{1,2,3,4\\}\\)\n{1,2} {1,3}, {3,4}\n{2,3} {2,4}\n{3,4}\n\n\n\nThe exponential function is \\(e^{x}=1+x/1+2^2/2+x^3/6....=\\sum_{k=0}^{\\infty} x^k/k!\\) and the \\(log\\) function is it’s inverse.\n\n\nCode\nx &lt;- exp(4)\nx\n## [1] 54.59815\n\ny &lt;- log(x)\ny\n## [1] 4",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mathematics</span>"
    ]
  },
  {
    "objectID": "01-00-UnivariateData.html",
    "href": "01-00-UnivariateData.html",
    "title": "Univariate Data",
    "section": "",
    "text": "This section introduces basic univariate statistics from a computational rather than mathematical approach. We will also use some basic probability theory, which you can review at the high-school level if this is not already familiar to you.\n\nhttps://www.khanacademy.org/math/cc-sixth-grade-math/cc-6th-data-statistics\nhttps://www.khanacademy.org/math/grade-6-fl-best/x9def9752caf9d75b:data-and-statistics\nhttps://www.khanacademy.org/math/cc-seventh-grade-math/cc-7th-probability-statistics\nhttps://www.khanacademy.org/math/grade-7-virginia/x1e291b30c04dacab:probability-statistics",
    "crumbs": [
      "Univariate Data"
    ]
  },
  {
    "objectID": "01_03_Data.html",
    "href": "01_03_Data.html",
    "title": "3  Data",
    "section": "",
    "text": "3.1 Types",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "01_03_Data.html#types",
    "href": "01_03_Data.html#types",
    "title": "3  Data",
    "section": "",
    "text": "Basic Types.\nThe two basic types of data are cardinal (aka numeric) data and factor data. We can further distinguish between whether cardinal data are discrete or continuous. We can also further distinguish between whether factor data are ordered or not\n\nCardinal (Numeric): the difference between elements always means the same thing.\n\nDiscrete: E.g. \\(\\{ 1,2,3\\}\\) and notice that \\(2-1=3-2\\).\nContinuous: E.g., \\(\\{1.4348, 2.4348, 2.9, 3.9 \\}\\) and notice that \\(2.9-1.4348=3.9-2.4348\\)\n\nFactor: the difference between elements does not always mean the same thing.\n\nOrdered: E.g., \\(\\{1^{st}, 2^{nd}, 3^{rd}\\}\\) place in a race and notice that \\(1^{st}\\) - \\(2^{nd}\\) place does not equal \\(2^{nd}\\) - \\(3^{rd}\\) place for a very competitive person who cares only about winning.\nUnordered (categorical): E.g., \\(\\{Amanda, Bert, Charlie\\}\\) and notice that \\(Amanda - Bert\\) never makes sense.\n\n\nHere are some examples\n\n\nCode\ndat_card1 &lt;- 1:3 # Cardinal data (Discrete)\ndat_card1\n## [1] 1 2 3\n\ndat_card2 &lt;- c(1.1, 2/3, 3) # Cardinal data (Continuous)\ndat_card2\n## [1] 1.1000000 0.6666667 3.0000000\n\ndat_fact1 &lt;- factor( c('A','B','C'), ordered=T) # Factor data (Ordinal)\ndat_fact1\n## [1] A B C\n## Levels: A &lt; B &lt; C\n\ndat_fact2 &lt;- factor( c('Leipzig','Los Angeles','Logan'), ordered=F) # Factor data (Categorical)\ndat_fact2\n## [1] Leipzig     Los Angeles Logan      \n## Levels: Leipzig Logan Los Angeles\n\ndat_fact3 &lt;- factor( c(T,F), ordered=F) # Factor data (Categorical)\ndat_fact3\n## [1] TRUE  FALSE\n## Levels: FALSE TRUE\n\n# Explicitly check the data types:\n#class(dat_card1)\n#class(dat_card2)\n\n\nNote that for theoretical analysis, the types are sometimes grouped differently as\n\nDiscrete (discrete cardinal, ordered factor, and unordered factor data). You can count the potential values. E.g., the set \\(\\{A,B,C\\}\\) has three potential values.\nContinuous (continuous cardinal data). There are uncountably infinite potential values. E.g., Try counting the numbers between \\(0\\) and \\(1\\) including decimal points, and notice that any two numbers have another potential number between them.\n\nIn any case, data are often computationally analyzed as data.frame objects, discussed below.\n\n\nStrings.\nNote that R allows for unstructured plain text, called character strings, which we can then format as factors\n\n\nCode\nc('A','B','C')  # character strings\n## [1] \"A\" \"B\" \"C\"\nc('Leipzig','Los Angeles','Logan')  # character strings\n## [1] \"Leipzig\"     \"Los Angeles\" \"Logan\"\n\n\nAlso note that strings are encounter in a variety of settings, and you often have to format them after reading them into R.1\n\n\nCode\n# Strings\npaste( 'hi', 'mom')\n## [1] \"hi mom\"\npaste( c('hi', 'mom'), collapse='--')\n## [1] \"hi--mom\"\n\nkingText &lt;- \"The king infringes the law on playing curling.\"\ngsub(pattern=\"ing\", replacement=\"\", kingText)\n## [1] \"The k infres the law on play curl.\"\n# advanced usage\n#gsub(\"[aeiouy]\", \"_\", kingText)\n#gsub(\"([[:alpha:]]{3})ing\\\\b\", \"\\\\1\", kingText)",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "01_03_Data.html#datasets",
    "href": "01_03_Data.html#datasets",
    "title": "3  Data",
    "section": "3.2 Datasets",
    "text": "3.2 Datasets\nDatasets can be stored in a variety of formats on your computer. But they can be analyzed in R in three basic ways.\n\nLists.\nLists are probably the most basic type\n\n\nCode\nx &lt;- 1:10\ny &lt;- 2*x\nlist(x, y)  # list of vectors\n## [[1]]\n##  [1]  1  2  3  4  5  6  7  8  9 10\n## \n## [[2]]\n##  [1]  2  4  6  8 10 12 14 16 18 20\n\nx_mat1 &lt;- matrix(2:7,2,3)\nx_mat2 &lt;- matrix(4:-1,2,3)\nlist(x_mat1, x_mat2)  # list of matrices\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\n\nLists are useful for storing unstructured data\n\n\nCode\nlist(list(x_mat1), list(x_mat2))  # list of lists\n## [[1]]\n## [[1]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## \n## [[2]]\n## [[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\nlist(x_mat1, list(x_mat1, x_mat2)) # list of different objects\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]]\n## [[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]][[2]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\n# ...inception...\nlist(x_mat1,\n    list(x_mat1, x_mat2), \n    list(x_mat1, list(x_mat2)\n    )) \n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]]\n## [[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]][[2]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n## \n## \n## [[3]]\n## [[3]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[3]][[2]]\n## [[3]][[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\n\n\n\nData.frames.\nA data.frame looks like a matrix but each column is actually a list rather than a vector. This allows you to combine different data types into a single object for analysis, which is why it might be your most common object.\n\n\nCode\n# data.frames: your most common data type\n    # matrix of different data-types\n    # well-ordered lists\ndata.frame(x, y)  # list of vectors\n##     x  y\n## 1   1  2\n## 2   2  4\n## 3   3  6\n## 4   4  8\n## 5   5 10\n## 6   6 12\n## 7   7 14\n## 8   8 16\n## 9   9 18\n## 10 10 20\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nCreate a data.frame storing two different types of data. Then show print only the second column\n\n\nCode\nd0 &lt;- data.frame(x=dat_fact2, y=dat_card2)\nd0\n##             x         y\n## 1     Leipzig 1.1000000\n## 2 Los Angeles 0.6666667\n## 3       Logan 3.0000000\n\nd0[,'y']\n## [1] 1.1000000 0.6666667 3.0000000\n\n\n\n\n\n\n\nArrays.\nArrays are generalization of matrices to multiple dimensions. They are a very efficient way to store well-formatted numeric data, and are often used in spatial econometrics and time series (often in the form of “data cubes”).\n\n\nCode\n# data square (matrix)\narray(data = 1:24, dim = c(3,8))\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n## [1,]    1    4    7   10   13   16   19   22\n## [2,]    2    5    8   11   14   17   20   23\n## [3,]    3    6    9   12   15   18   21   24\n\n# data cube\na &lt;- array(data = 1:24, dim = c(3, 2, 4))\na\n## , , 1\n## \n##      [,1] [,2]\n## [1,]    1    4\n## [2,]    2    5\n## [3,]    3    6\n## \n## , , 2\n## \n##      [,1] [,2]\n## [1,]    7   10\n## [2,]    8   11\n## [3,]    9   12\n## \n## , , 3\n## \n##      [,1] [,2]\n## [1,]   13   16\n## [2,]   14   17\n## [3,]   15   18\n## \n## , , 4\n## \n##      [,1] [,2]\n## [1,]   19   22\n## [2,]   20   23\n## [3,]   21   24\n\n\n\n\nCode\na[1, , , drop = FALSE]  # Row 1\n#a[, 1, , drop = FALSE]  # Column 1\n#a[, , 1, drop = FALSE]  # Layer 1\n\na[ 1, 1,  ]  # Row 1, column 1\n#a[ 1,  , 1]  # Row 1, \"layer\" 1\n#a[  , 1, 1]  # Column 1, \"layer\" 1\na[1 , 1, 1]  # Row 1, column 1, \"layer\" 1\n\n\nApply extends to arrays\n\n\nCode\napply(a, 1, mean)    # Row means\n## [1] 11.5 12.5 13.5\napply(a, 2, mean)    # Column means\n## [1] 11 14\napply(a, 3, mean)    # \"Layer\" means\n## [1]  3.5  9.5 15.5 21.5\napply(a, 1:2, mean)  # Row/Column combination \n##      [,1] [,2]\n## [1,]   10   13\n## [2,]   11   14\n## [3,]   12   15\n\n\nOuter products yield arrays\n\n\nCode\nx &lt;- c(1,2,3)\nx_mat1 &lt;- outer(x, x) # x %o% x\nx_mat1\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    2    4    6\n## [3,]    3    6    9\nis.array(x_mat1) # Matrices are arrays\n## [1] TRUE\n\nx_mat2 &lt;- matrix(6:1,2,3)\nouter(x_mat2, x)\n## , , 1\n## \n##      [,1] [,2] [,3]\n## [1,]    6    4    2\n## [2,]    5    3    1\n## \n## , , 2\n## \n##      [,1] [,2] [,3]\n## [1,]   12    8    4\n## [2,]   10    6    2\n## \n## , , 3\n## \n##      [,1] [,2] [,3]\n## [1,]   18   12    6\n## [2,]   15    9    3\n# outer(x_mat2, matrix(x))\n# outer(x_mat2, t(x))\n# outer(x_mat1, x_mat2)",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "01_03_Data.html#densities-and-distributions",
    "href": "01_03_Data.html#densities-and-distributions",
    "title": "3  Data",
    "section": "3.3 Densities and Distributions",
    "text": "3.3 Densities and Distributions\n\nInitial Data Inspection.\nRegardless of the data types you have, you typically begin by inspecting your data by examining the first few observations.\nConsider, for example, historical data on crime in the US.\n\n\nCode\nhead(USArrests) # Actual Data\n##            Murder Assault UrbanPop Rape\n## Alabama      13.2     236       58 21.2\n## Alaska       10.0     263       48 44.5\n## Arizona       8.1     294       80 31.0\n## Arkansas      8.8     190       50 19.5\n## California    9.0     276       91 40.6\n## Colorado      7.9     204       78 38.7\n\n# Check NA values\nX &lt;- c(3,3.1,NA,0.02) #Small dataset we will use in numerical examples\nsum(is.na(X))\n## [1] 1\n\n\nTo further examine a particular variable, we look at its distribution. In what follows, we will often work with data as vector \\(\\hat{X}=(\\hat{X}_{1}, \\hat{X}_{2}, ....\\hat{X}_{n})\\), where there are \\(n\\) observations and \\(\\hat{X}_{i}\\) is the value of the \\(i\\)th one. We often analyze observations in comparison to some value \\(x\\).\n\n\nCode\nX &lt;- c(3,3.1,0.02) # Data: \"big X\"\nx &lt;- 2 # particular value: \"little x\"\nsum(X &lt;= x)\n## [1] 1\nsum(X == x)\n## [1] 0\n\n\n\n\nHistogram Density Estimate.\nThe histogram measures the proportion of the data in different bins. It does so by dividing the range of the data into exclusive bins of equal-width \\(h\\), and count the number of observations within each bin. We often rescale the counts, dividing by the number of observations \\(n\\) multiplied by bin-width \\(h\\), so that the total area of the histogram sums to one, which allows us to interpret the numbers as a density.\nMathematically, for an exclusive bin \\(\\left(x-\\frac{h}{2}, x+\\frac{h}{2} \\right]\\) defined by their midpoint \\(x\\) and width \\(h\\), we compute \\[\\begin{eqnarray}\n\\widehat{f}(x) &=& \\frac{  \\sum_{i=1}^{n} \\mathbf{1}\\left( \\hat{X}_{i} \\in \\left(x-\\frac{h}{2}, x+\\frac{h}{2} \\right] \\right) }{n h},\n\\end{eqnarray}\\] where \\(\\mathbf{1}()\\) is an indicator function that equals \\(1\\) if the expression inside is TRUE and \\(0\\) otherwise. E.g., if \\(\\hat{X}_{i}=3.8\\) and \\(h=1\\), then for \\(x=1\\) we have \\(\\mathbf{1}\\left( \\hat{X}_{i} \\in \\left(1-\\frac{h}{2}, 1+\\frac{h}{2} \\right] \\right)=\\mathbf{1}\\left( 3.8 \\in \\left(0.5, 1.5\\right] \\right)=0\\) and for \\(x=4\\) \\(\\mathbf{1}\\left( \\hat{X}_{i} \\in \\left(4-\\frac{h}{2}, 4+\\frac{h}{2} \\right] \\right)=\\mathbf{1}\\left(  3.8 \\in \\left(3.5, 4.5\\right] \\right)=1\\).\nNote that the area of the rectangle is “base x height”, which is \\(h \\times \\widehat{f}(x)= \\sum_{i=1}^{n} \\mathbf{1}\\left( \\hat{X}_{i} \\in \\left(x-\\frac{h}{2}, x+\\frac{h}{2} \\right] \\right) /n\\). This means the rectangle area equals the proportion of data in the bin. We compute \\(\\widehat{f}(x)\\) for each bin midpoint \\(x\\), and the area of all rectangles sums to one.2\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor example, consider the dataset \\(\\{3,3.1,0.02\\}\\) and use bins \\((0,1], (1,2], (2,3], (3,4]\\). In this case, the midpoints are \\(x=(0.5,1.5,2.5,3.5)\\) and \\(h=1\\). Then the counts at each midpoints are \\((1,0,1,1)\\). Since \\(\\frac{1}{nh}=\\frac{1}{3\\times1}=\\frac{1}{3}\\), we can rescale the counts to compute the density as \\(\\widehat{f}(x)=(1,0,1,1) \\frac{1}{3}=(1/3,0,1/3,1/3)\\).\n\n\nCode\n# Intuitive Examples\nX &lt;- c(3,3.1,0.02)\nXhist &lt;- hist(X, breaks=c(0,1,2,3,4), plot=F)\nXhist\n## $breaks\n## [1] 0 1 2 3 4\n## \n## $counts\n## [1] 1 0 1 1\n## \n## $density\n## [1] 0.3333333 0.0000000 0.3333333 0.3333333\n## \n## $mids\n## [1] 0.5 1.5 2.5 3.5\n## \n## $xname\n## [1] \"X\"\n## \n## $equidist\n## [1] TRUE\n## \n## attr(,\"class\")\n## [1] \"histogram\"\n\n# base x height\nbase &lt;- 1\nheight &lt;- Xhist$density\nsum(base*height)\n## [1] 1\n\n\nFor another example, use the bins \\((0,2]\\) and \\((2,4]\\). So the midpoints are \\(1\\) and \\(3\\), and the bin width is \\(2\\). Only one observation, \\(0.02\\), falls in the bin \\((0,2]\\). The other two observations, \\(3\\) and \\(3.1\\), fall into the bin \\((2,4]\\). The scaling factor is \\(\\frac{1}{nh}=\\frac{1}{3\\times 2}=\\frac{1}{6}\\). So the first bin has density \\(f(1)=1\\frac{1}{6}=1/6\\) and the second bin has density \\(f(3)=2\\frac{1}{6}=2/6\\). The area of the first bin’s rectangle is \\(2 \\times f(1)=2/6=1/3\\) and the area of the second rectangle is \\(2 \\times f(3)=4/6=2/3\\).\nNow intuitively work through an example with three bins instead of four. Compute the areas\n\n\nCode\nXhist &lt;- hist(X, breaks=c(0,4/3,8/3,4), plot=F)\nbase &lt;- 4/3\nheight &lt;- Xhist$density\nsum(base*height)\n## [1] 1\n\n\n# as a default, R uses bins (,] instead of [,)\n# but you can change that with \"right=F\"\n# hist(X, breaks=c(0,4/3,8/3,4), plot=F, right=F)\n\n\n\n\n\n\n\nCode\n# Practical Example\nX &lt;- USArrests[,'Murder']\nhist(X,\n    breaks=seq(0,20,by=1), #bin width=1\n    freq=F,\n    border=NA, \n    main='',\n    xlab='Murder Arrests')\n# Raw Observations\nrug(USArrests[,'Murder'], col=grey(0,.5))\n\n\n\n\n\n\n\n\n\nCode\n\n# Since h=1, the density equals the proportion of states in each bin\n# Redo this example with h=2\n\n\nNote that if you your data are factor data, or discrete cardinal data, you can directly plot the proportions in a bar plot. For each unique outcome \\(x\\) we compute \\[\\begin{eqnarray}\n\\widehat{p}_{x}=\\sum_{i=1}^{n}\\mathbf{1}\\left(\\hat{X}_{i}=x\\right)/n.\n\\end{eqnarray}\\] where \\(n\\) is the number of observations and \\(\\sum_{i=1}^{n}\\mathbf{1}\\left(\\hat{X}_{i}=x\\right)\\) counts the number of observations equal to \\(x\\). The height of each line equals the proportion of data with a specific value.\n\n\nCode\n# Discretized data\nX &lt;- USArrests[,'Murder']\nXr &lt;- floor(X) #rounded down\n#table(Xr)\nproportions &lt;- table(Xr)/length(Xr)\nplot(proportions, col=grey(0,.5),\n    xlab='Murder Arrests (Discretized)',\n    ylab='Proportion of States with each value')\n\n\n\n\n\n\n\n\n\n\n\nEmpirical Cumulative Distribution Function.\nThe ECDF counts the proportion of observations whose values are less than or equal to \\(x\\); \\[\\begin{eqnarray}\n\\widehat{F}(x) = \\frac{1}{n} \\sum_{i}^{n} \\mathbf{1}( \\hat{X}_{i} \\leq x).\n\\end{eqnarray}\\] Typically, we compute this for each unique value of \\(x\\) in the dataset. The ECDF jumps up by \\(1/n\\) at each of the \\(n\\) data points.\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor example, let \\(X=(3,3.1,0.02)\\). We reorder the observations as \\((0.02, 3, 3.1)\\), so that there are discrete jumps of \\(1/n=1/3\\) at each value. Consider the points \\(x \\in \\{0.5,2.5,3.5\\}\\). At \\(x=0.5\\), \\(F(0.5)\\) measures the proportion of the data \\(\\leq 0.5\\). Since only one observations, \\(0.02\\), of three is \\(\\leq 0.5\\), we can compute \\(F(0.5)=1/3\\). Similarly, since only one observations, \\(0.02\\), of three is \\(\\leq 2.5\\), we can compute \\(F(2.5)=1/3\\). Since all observations are \\(\\leq 3.5\\), we can compute \\(F(3.5)=1\\).\n\n\nCode\nX &lt;- c(3,3.1,0.02)\nFhat &lt;- ecdf(X)\n\n# Visualized\nplot(Fhat)\n\n\n\n\n\n\n\n\n\nCode\n\n# Evaluated at the data\nx &lt;- X\nFhat(X)\n## [1] 0.6666667 1.0000000 0.3333333\n#sum(X&lt;=3)/length(X)\n#sum(X&lt;=3.1)/length(X)\n#sum(X&lt;=0.02)/length(X)\n\n# Evaluated at other points\nx &lt;- c(0.5, 2.5, 3.5)\nFhat(x)\n## [1] 0.3333333 0.3333333 1.0000000\n#sum(X&lt;=0.5)/length(X)\n#sum(X&lt;=2.5)/length(X)\n#sum(X&lt;=3.5)/length(X)\n\n\n\n\n\n\n\nCode\nF_murder &lt;- ecdf(USArrests[,'Murder'])\n# proportion of murders &lt;= 10\nF_murder(10)\n## [1] 0.7\n# proportion of murders &lt;= x, for all x\nplot(F_murder, main='', \n    xlab='Murder Arrests (x)',\n    ylab='Proportion of States with Murder Arrests &lt;= x',\n    pch=16, col=grey(0,.5))\nrug(USArrests[,'Murder'])\n\n\n\n\n\n\n\n\n\n\n\nQuantiles.\nYou can summarize the distribution of data using quantiles: the \\(p\\)th quantile is a value of \\(x\\) where \\(p\\) percent of the data are below \\(x\\) and (\\(1-p\\)) percent are above \\(x\\).\n\nThe min is the smallest value (or the most negative value if there are any), where \\(0%\\) of the data has lower values.\nThe median is the middle value, where one half of the data has lower values and the other half has higher values.\nThe max is the smallest value (or the most negative value if there are any), where \\(100%\\) of the data has lower values.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor example, if \\(X=(0,0,0.02,3,5)\\) then the median is \\(0.02\\), the lower quartile is \\(0\\), and the upper quartile is \\(3\\). (The number \\(0\\) is also special: the most frequent observation is called the mode.)\n\n\nCode\nX &lt;-  c(3.1, 3, 0.02)\nquantile(X, probs=c(0,.5,1))\n##   0%  50% 100% \n## 0.02 3.00 3.10\n\n\nNow work through an intuitive example with observations \\(\\{1,2,...,13\\}\\). Hint: split the ordered observations into four groups.\n\n\n\nWe are often also interested in quartiles: where \\(25\\%\\) of the data fall below \\(x\\) (lower quartile) and where \\(75\\%\\) of the data fall below \\(x\\) (upper quartile). Sometimes we are also interested in deciles: where \\(10\\%\\) of the data fall below \\(x\\) (lower decile) and where \\(90\\%\\) of the data fall below \\(x\\) (upper decile). In general, we can use the empirical quantile function \\[\\begin{eqnarray}\n\\widehat{Q}(p) = \\widehat{F}^{-1}(p),\n\\end{eqnarray}\\] to compute a quantile for any probability \\(p\\in [0,1]\\). The median corresponds to \\(p=0.5\\), upper quartile to \\(p=0.75\\), and upper decile to \\(p=0.9\\).\n\n\nCode\n# common quantiles\nX &lt;- USArrests[,'Murder']\nquantile(X)\n##     0%    25%    50%    75%   100% \n##  0.800  4.075  7.250 11.250 17.400\n\n# All deciles are quantiles\nquantile(X, probs=seq(0,1, by=.1))\n##    0%   10%   20%   30%   40%   50%   60%   70%   80%   90%  100% \n##  0.80  2.56  3.38  4.75  6.00  7.25  8.62 10.12 12.12 13.32 17.40\n\n# Visualized: Inverting the Empirical Distribution\nFX_hat &lt;- ecdf(X)\nplot(FX_hat, lwd=2, xlim=c(0,20),\n    pch=16, col=grey(0,.5), main='')\n# Two Examples of Quantiles \np &lt;- c(.25, .9) # Lower Quartile, Upper Decile\ncols &lt;- c(2,4) \nQX_hat &lt;- quantile(X, p, type=1)\nQX_hat\n##  25%  90% \n##  4.0 13.2\nsegments(QX_hat, p, -10, p, col=cols)\nsegments(QX_hat, p, QX_hat, 0, col=cols)\nmtext( round(QX_hat,2), 1, at=QX_hat, col=cols)\n\n\n\n\n\n\n\n\n\nThere are some issues with quantiles with smaller datasets. E.g., to compute the median of \\(\\{3,3.1,0,1\\}\\), we need some ways to break ties (of which there are many options). Similar issues arise for other quantiles, so quantiles are not used for such small datasets.\n\n\n\n\n\n\nTip\n\n\n\n\n\nTo calculate quantiles, the computer sorts the observations from smallest to largest as \\(\\hat{X}_{(1)}, \\hat{X}_{(2)},... \\hat{X}_{(N)}\\), and then computes quantiles as \\(\\hat{X}_{ (q \\times N) }\\). Note that \\((q \\times N)\\) is rounded and there are different ways to break ties.\n\n\nCode\nX &lt;- USArrests[,'Murder']\nXo &lt;- sort(X)\nXo\n##  [1]  0.8  2.1  2.1  2.2  2.2  2.6  2.6  2.7  3.2  3.3  3.4  3.8  4.0  4.3  4.4\n## [16]  4.9  5.3  5.7  5.9  6.0  6.0  6.3  6.6  6.8  7.2  7.3  7.4  7.9  8.1  8.5\n## [31]  8.8  9.0  9.0  9.7 10.0 10.4 11.1 11.3 11.4 12.1 12.2 12.7 13.0 13.2 13.2\n## [46] 14.4 15.4 15.4 16.1 17.4\n\n# median\nXo[length(Xo)*.5]\n## [1] 7.2\nquantile(X, probs=.5, type=4) #tie break rule #4\n## 50% \n## 7.2\n\n# min\nXo[1]\n## [1] 0.8\nmin(Xo)\n## [1] 0.8\nquantile(Xo, probs=0)\n##  0% \n## 0.8\n\n\n\n\n\n\n\nBoxplots.\nBoxplots also summarize the distribution. The boxplot shows the median (solid black line) and interquartile range (\\(IQR=\\) upper quartile \\(-\\) lower quartile; filled box).3 As a default, whiskers are shown as \\(1.5\\times IQR\\) and values beyond that are highlighted as outliers (so whiskers do not typically show the data range). You can alternatively show all the raw data points instead of whisker+outliers.\n\n\nCode\nboxplot(USArrests[,'Murder'],\n    main='', ylab='Murder Arrests',\n    whisklty=0, staplelty=0, outline=F)\n# Raw Observations\nstripchart(USArrests[,'Murder'],\n    pch='-', col=grey(0,.5), cex=2,\n    vert=T, add=T)",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "01_03_Data.html#further-reading",
    "href": "01_03_Data.html#further-reading",
    "title": "3  Data",
    "section": "3.4 Further Reading",
    "text": "3.4 Further Reading\nECDF\n\nhttps://library.virginia.edu/data/articles/understanding-empirical-cumulative-distribution-functions\n\nHandling Strings\n\nhttps://meek-parfait-60672c.netlify.app/docs/M1_R-intro_03_text.html\nhttps://raw.githubusercontent.com/rstudio/cheatsheets/main/regex.pdf",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "01_03_Data.html#footnotes",
    "href": "01_03_Data.html#footnotes",
    "title": "3  Data",
    "section": "",
    "text": "We will not cover the statistical analysis of text in this course, but strings are amenable to statistical analysis.↩︎\nIf \\(L\\) distinct bins exactly span the range, then \\(h=[\\text{max}(\\hat{X}_{i}) - \\text{min}(\\hat{X}_{i})]/L\\) and \\(x\\in \\left\\{ \\frac{\\ell h}{2} + \\text{min}(\\hat{X}_{i}) \\right\\}_{\\ell=1}^{L}\\).↩︎\nTechnically, the upper and lower hinges use two different versions of the first and third quartile. See https://stackoverflow.com/questions/40634693/lower-and-upper-quartiles-in-boxplot-in-r.↩︎",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "01_04_RandomVariables.html",
    "href": "01_04_RandomVariables.html",
    "title": "4  Random Variables",
    "section": "",
    "text": "Probability.\nIn the last section we computed a distribution given the data, whereas now we generate individual data points given the distribution.\nRandom variables are vectors whose values occur according to a frequency distribution. As such, random variables have a\nWe think of each observation \\(\\hat{X}_{i}\\) before it is actually observed, potentially taking on specific values \\(x\\) from the sample space with known probabilities. For example, we consider flipping a coin before knowing whether it lands on heads or tails. We denote the random variable, in this case the unflipped coin, as \\(X_{i}\\).\nThere are two basic types of sample spaces: discrete (encompassing cardinal-discrete, factor-ordered, and factor-unordered data) and continuous. This leads to two types of random variables: discrete and continuous. However, each type has many different probability distributions.\nThe most common random variables are easily accessible and can be described using the Cumulative Distribution Function (CDF) \\[\\begin{eqnarray}\nF(x) &=& Prob(X_{i} \\leq x).\n\\end{eqnarray}\\] Note that this is just like the Empirical Cumulative Distribution Function (ECDF), \\(\\widehat{F}(x)\\), except that it is now theoretically known. You can think of \\(F(x)\\) as the ECDF for a dataset with an infinite number of observations. Equivalently, the ECDF is an empirical version of the CDF that is applied to observed data.\nAfter introducing different random variables, we will also cover some basic implications of their CDF. Intuitively, probabilities must sum up to one. So we can compute \\(Prob(X_{i} &gt; x) = 1- F(x)\\). We also have two “in” and “out” probabilities.\nThe probability of \\(X_{i}\\leq b\\) and \\(X_{i}\\geq a\\) can be written in terms of falling into a range \\(Prob(X_{i} \\in [a,b])=Prob(a \\leq X_{i} \\leq b) = F(b) - F(a)\\).\nThe opposite probability of \\(X_{i} &gt; b\\) or \\(X_{i} &lt; a\\) is \\(Prob(X_{i} &lt; a \\text{ or } X_{i} &gt; b) = F(a) + [1- F(b)]\\). Notice that this opposite probability \\(F(a) + [1- F(b)] =1 - [F(b) - F(a)]\\), so that \\(Prob(X_{i} \\text{ out of } [a,b]) = 1 - Prob( X_{i} \\in [a,b])\\)",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "01_04_RandomVariables.html#discrete",
    "href": "01_04_RandomVariables.html#discrete",
    "title": "4  Random Variables",
    "section": "4.1 Discrete",
    "text": "4.1 Discrete\nA discrete random variable can take one of several values in a set. E.g., any number in \\(\\{1,2,3,...\\}\\) or any letter in \\(\\{A,B,C,...\\}\\). Theoretical proportions are referred to as a probability mass function, which can be thought of as a proportions bar plot for an infinitely large dataset. Equivalently, the bar plot is an empirical version of the probability mass function that is applied to observed data.\n\nBernoulli.\nThink of a Coin Flip: Heads or Tails with equal probability. In general, a Bernoulli random variable denotes Heads as the event \\(X_{i}=1\\) and Tails as the event \\(X_{i}=0\\), and allows the probability of Heads to vary. \\[\\begin{eqnarray}\nX_{i} &\\in& \\{0,1\\} \\\\\nProb(X_{i} =0) &=& 1-p \\\\\nProb(X_{i} =1) &=& p \\\\\nF(x) &=& \\begin{cases}\n    0   & x&lt;0 \\\\\n    1-p & x \\in [0,1) \\\\\n    1   & x\\geq 1\n\\end{cases}\n\\end{eqnarray}\\]\nHere is an example of the Bernoulli distribution. While you might get all heads (or all tails) in the first few coin flips, the ratios level out to their theoretical values after many flips.\n\n\nCode\nx &lt;- c(0,1)\nx_probs &lt;- c(3/4, 1/4)\n\nsample(x, 1, prob=x_probs, replace=T) # 1 Flip\n## [1] 0\nsample(x, 4, prob=x_probs, replace=T) # 4 Flips \n## [1] 0 0 0 0\nX0 &lt;- sample(x, 400, prob=x_probs, replace=T)\n\n# Plot Cumulative Proportion\nX0_t &lt;- seq_len(length(X0)) #head(X0_t)\nX0_mt &lt;- cumsum(X0)/X0_t #head(X0_mt)\npar(mar=c(4,4,1,4))\nplot(X0_t, X0_mt, type='l',\n    ylab='Cumulative Proportion (p)',\n    xlab='Flip #', \n    ylim=c(0,1), \n    lwd=2)\n# Show individual flip outcomes\npoints(X0_t, X0, col=grey(0,.5),\n    pch='|', cex=.3)\n# Show theoretical proportions\nabline(h=0.25, col='blue')\n\n\n\n\n\n\n\n\n\nCode\n\n# Plot Long run proportions\nproportions &lt;- table(X0)/length(X0)\nplot(proportions, \n    col=grey(0, 0.5),\n    xlab='Flip Outcome', \n    ylab='Proportion',\n    main=NA)\npoints(c(0,1), c(.75, .25), pch=16, col='blue') # Theoretical values\n\n\n\n\n\n\n\n\n\nCode\n\n# Plot CDF\nplot( ecdf(X0), col=grey(0,.5),\n    pch=16, main=NA) #Empirical\n\n\n\n\n\n\n\n\n\nCode\n#points(c(0,1), c(.75, 1), pch=16, col='blue') # Theoretical\n\n\n\n\nDiscrete Uniform.\nDiscrete numbers with equal probability, such as a die with \\(K\\) sides. \\[\\begin{eqnarray}\nX_{i} &\\in& \\{1,...K\\} \\\\\nProb(X_{i} =1) &=& Prob(X_{i} =2) = ... = 1/K\\\\\nF(x) &=& \\begin{cases}\n    0   & x&lt;1 \\\\\n    1/K & x \\in [1,2) \\\\\n    2/K & x \\in [2,3) \\\\\n    \\vdots & \\\\\n    1   & x\\geq K\n\\end{cases}\n\\end{eqnarray}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nHere is an example with \\(K=4\\). E.g., rolling a four-sided die.\nThe probability of a value smaller than or equal to \\(3\\) is \\(Prob(X_{i} \\leq 3)=1/4 + 1/4 + 1/4 = 3/4\\).\nThe probability of a value larger than \\(3\\) is \\(Prob(X_{i} &gt; 3) = 1-Prob(X_{i} \\leq 3)=1/4\\).\nThe probability of a value \\(&gt;\\) 1 and \\(\\leq 3\\) is \\(Prob(1 &lt; X_{i} \\leq 3) = Prob(X_{i} \\leq 3) - \\left[ 1- Prob(X_{i} \\leq 1) \\right] = 3/4 - 1/4 = 2/4\\).1\nThe probability of a value \\(\\leq\\) 1 or \\(&gt; 3\\) is \\(Prob(X_{i} \\leq 1 \\text{ or } X_{i} &gt; 3) =  Prob(X_{i} \\leq 1) +  \\left[ 1- Prob(X_{i} \\leq 3) \\right] = 1/4 + [1 - 3/4]=2/4\\).\n\n\n\n\n\nCode\nx &lt;- c(1,2,3,4)\nx_probs &lt;- c(1/4, 1/4, 1/4, 1/4)\n# sample(x, 1, prob=x_probs, replace=T) # 1 roll\nX1 &lt;- sample(x, 2000, prob=x_probs, replace=T) # 2000 rolls\n\n# Plot Long run proportions\nproportions &lt;- table(X1)/length(X1)\nplot(proportions, col=grey(0,.5),\n    xlab='Outcome', ylab='Proportion', main=NA)\npoints(x, x_probs, pch=16, col='blue') # Theoretical values\n\n\n\n\n\n\n\n\n\nCode\n\n# Hist w/ Theoretical Counts\n# hist(X1, breaks=50, border=NA, main=NA, ylab='Count')\n# points(x, x_probs*length(X1), pch='-') \n\n# Alternative Plot\nplot( ecdf(X1), pch=16, col=grey(0,.5), main=NA)\n\n\n\n\n\n\n\n\n\nCode\n\n# Alternative Plot 2\n#props &lt;- table(X1)\n#barplot(props, ylim = c(0, 0.35), ylab = \"Proportion\", xlab = \"Value\")\n#abline(h = 1/4, lty = 2)\n\n\nNote that the Discrete Uniform distribution generalizes to arbitrary intervals, although we will not exploit the generalization in this class.\n\n\nMultinoulli (aka Categorical).\nNumbers \\(1,...K\\) with unequal probabilities. \\[\\begin{eqnarray}\nX_{i} &\\in& \\{1,...K\\} \\\\\nProb(X_{i} =1) &=& p_{1} \\\\\nProb(X_{i} =2) &=& p_{2} \\\\\n        &\\vdots& \\\\\np_{1} + p_{2} + ... &=& 1\\\\\nF(x) &=& \\begin{cases}\n    0   & x&lt;1 \\\\\n    p_{1} & x \\in [1,2) \\\\\n    p_{1} + p_{2} & x \\in [2,3) \\\\\n    \\vdots & \\\\\n    1   & x\\geq K\n\\end{cases}\n\\end{eqnarray}\\]\nWe can also replace numbers with letters \\((A,...Z)\\) or names \\((John, Jamie, ...)\\) although we must be careful with the CDF when there is no longer a natural ordering. Here is an empirical example with three outcomes\n\n\nCode\nx &lt;- c('A', 'B', 'C')\nx_probs &lt;- c(3/10, 1/10, 6/10)\nsum(x_probs)\n## [1] 1\nX2 &lt;- sample(x, 2000, prob=x_probs, replace=T) # sample of 2000\n\n# Plot Long run proportions\nproportions &lt;- table(X2)/length(X2)\nplot(proportions, col=grey(0,.5),\n    xlab='Outcome', ylab='Proportion', main=NA)\npoints(x_probs, pch=16, col='blue') # Theoretical values\n\n\n\n\n\n\n\n\n\nCode\n\n# Histogram version\n# X2_alt &lt;- X1\n# X2_alt[X2_alt=='A'] &lt;- 1\n#X2_alt[X2_alt=='B'] &lt;- 2\n#X2_alt[X2_alt=='C'] &lt;- 3\n#X2_alt &lt;- as.numeric(X1_alt)\n#hist(X2_alt, breaks=50, border=NA, \n#    main=NA, ylab='Count')\n#points(x, x_probs*length(X2_alt), pch=16) ## Theoretical Counts\n\n# Alternative Plot\n# plot( ecdf(X2), pch=16, col=grey(0,.5), main=NA)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nSuppose there is an experiment with three possible outcomes, \\(\\{A, B, C\\}\\). It was repeated \\(50\\) times and discovered that \\(A\\) occurred \\(10\\) times, \\(B\\) occurred \\(13\\) times, and \\(C\\) occurred \\(27\\) times. The estimated probability of each outcome is found via the bar plot \\(\\hat{p}_{A} = 10/50\\), \\(\\hat{p}_{B} = 13/50\\), \\(\\hat{p}_{A} = 27/50\\). We can also estimate the “in” probabilities as \\(\\widehat{Prob}(A \\text{ or } B)=10/50+13/50=23/50\\) and \\(\\widehat{Prob}(B \\text{ or } C)=13/50+27/50=40/50\\), as well as the “out” probability as \\(\\widehat{Prob}(A \\text{ or } C)=13/50+27/50=37/50\\).\nSuppose there are three possible outcomes of an experiment, \\(\\{\\text{my car dies}, \\text{it rains next Tuesday},\\text{a cat is born}\\}\\), which have corresponding probabilities \\(\\{3/10, 1/10, 6/10 \\}\\) that are known theoretically. Compute the probability that my car dies or a cat is born.",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "01_04_RandomVariables.html#continuous",
    "href": "01_04_RandomVariables.html#continuous",
    "title": "4  Random Variables",
    "section": "4.2 Continuous",
    "text": "4.2 Continuous\nA continuous random variable can take one value out of an uncountably infinite number. E.g., any number between \\(0\\) and \\(1\\) with any number of decimal points. With a continuous random variable, the probability of any individual point is zero, so we describe these variables with the cumulative distribution function (CDF), \\(F\\), or the probability density function (PDF), \\(f\\). Just as \\(F\\) can be thought of as the ECDF, \\(\\widehat{F}\\), with an infinite amount of data, \\(f\\) can be thought of as a histogram, \\(\\widehat{f}\\), with an infinite amount of data. Equivalently, the histogram is an empirical version of the PDF that is applied to observed data.\nOften, the PDF helps you intuitively understand a random variable whereas the CDF helps you calculate numerical values. This is because probabilities are depicted as areas in the PDF and the CDF accumulates those areas: \\(F(x)\\) equals the area under the PDF from \\(-\\infty\\) to \\(x\\). For example, \\(Prob(X_{i} \\leq 1)\\) is depicted by the PDF as the area under \\(f(x)\\) from the lowest possible value until \\(x=1\\), which is numerically calculated simply as \\(F(1)\\).\n\nContinuous Uniform.\nAny number on a unit interval allowing for any number of decimal points, with every interval of the same size having the same probability. \\[\\begin{eqnarray}\nX_{i} &\\in& [0,1] \\\\\nf(x) &=& \\begin{cases}\n    1 & x \\in [0,1] \\\\\n    0 & \\text{Otherwise}\n\\end{cases}\\\\\nF(x) &=& \\begin{cases}\n    0 & x &lt; 0 \\\\\n    x & x \\in [0,1] \\\\\n    1 & x &gt; 1.\n\\end{cases}\n\\end{eqnarray}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe probability of a value being exactly \\(0.25\\) is \\(Prob(X_{i} =0.25)=0\\).\nThe probability of a value smaller than \\(0.25\\) is \\(F(0.25)=0.25\\).\nThe probability of a value larger than \\(0.25\\) is \\(1-F(0.25)=0.75\\).\nThe probability of a value in \\((0.25,0.75]\\) is \\(Prob(0.25 &lt; X_{i} \\leq 0.75) = Prob(X_{i} \\leq 0.75) - \\left[ 1- Prob(X_{i} \\leq 0.25) \\right] = 0.75 - 0.25 = 0.5\\).\nThe probability of a value in \\((0.2,0.7]\\) is \\(Prob(0.2 &lt; X_{i} \\leq 0.7) = Prob(X_{i} \\leq 0.7) - \\left[ 1- Prob(X_{i} \\leq 0.2) \\right] = 0.7 - 0.2 = 0.5\\).\nThe probability of a value outside of \\((0.2,0.7]\\) is \\(Prob(X_{i} \\leq 0.2 \\text{ or } x &gt; 0.7) = 0.2 + [1-0.7]=0.5\\). Alternatively, you can compute \\(1- Prob(0.2 &lt; X_{i} \\leq 0.7)=1-0.5=0.5\\).\n\n\nCode\n# Prob. &lt; 0.25\npunif(0.25)\n## [1] 0.25\n\n# Prob. &gt; 0.25\n1-punif(0.25)\n## [1] 0.75\n\n# Prob. in (0.2,0.7]\nF_two &lt;- punif( c(0.2, 0.7) )\nF_in &lt;- F_two[2] - F_two[1]\nF_in\n## [1] 0.5\n\n# Prob. out of (0.2,0.7]\nF_out &lt;- 1- F_in\nF_out\n## [1] 0.5\n\n\n\n\n\n\n\nCode\nrunif(3) # 3 draws\n## [1] 0.1239967 0.9376580 0.2329165\n\n# Empirical Density \nX3 &lt;- runif(2000)\nhist(X3, breaks=20, border=NA, main=NA, freq=F)\n# Theoretical Density\nx &lt;- seq(-0.1,1.1,by=.001)\nfx &lt;- dunif(x)\nlines(x, fx, col='blue')\n\n\n\n\n\n\n\n\n\nCode\n\n# CDF example 1\nP_low &lt;- punif(0.25)\nP_low\n## [1] 0.25\n# Uncomment to show via PDF\n# x_low &lt;- seq(0,0.25,by=.001)\n# fx_low &lt;- dunif(x_low)\n# polygon( c(x_low, rev(x_low)), c(fx_low,fx_low*0),\n#    col=rgb(0,0,1,.25), border=NA)\n    \n# CDF example 2\nP_high &lt;- 1-punif(0.25)\nP_high\n## [1] 0.75\n# Uncomment to show via PDF\n# x_high &lt;- seq(0.25,1,by=.001)\n# fx_high &lt;- dunif(x_high)\n# polygon( c(x_high, rev(x_high)), c(fx_high,fx_high*0),\n#    col=rgb(0,0,1,.25), border=NA)\n    \n# CDF example 3\nP_mid &lt;- punif(0.75) - punif(0.25)\nP_mid\n## [1] 0.5\n# Uncomment to show via PDF\n# x_mid &lt;-  seq(0.25,0.75,by=.001)\n# fx_mid &lt;- dunif(x_mid)\n# polygon( c(x_mid, rev(x_mid)), c(fx_mid,fx_mid*0),\n#    col=rgb(0,0,1,.25), border=NA)\n\n\nNote that the Continuous Uniform distribution generalizes to an arbitrary interval, \\(X_{i} \\in [a,b]\\). In this case, \\(f(x)=1/[b-a]\\) if \\(x \\in [a,b]\\) and \\(F(x)=[x-a]/[b-a]\\) if \\(x \\in [a,b]\\).\n\n\n\n\n\n\nNote\n\n\n\n\n\nSuppose \\(X_{i}\\) is a random variable continuously distributed over \\(a=-2\\) and \\(b=2\\). What is the probability of a value larger than \\(0.25\\)? First use the computer to suggest an answer: simulate \\(1000\\) draws and then make a histogram and an ECDF. Then find the answer mathematically using the CDF. Finally, verify the answer is intuitively correct in a figure of the PDF. You should draw by hand both the CDF and the PDF with correct axes labels and marking clearly the probability of a value larger than \\(0.25\\).\n\n\nCode\n# Simulation\nX &lt;- runif(1000, -2, 2)\n# ... \n\n# Draw by computer the CDF, F(x)\nx &lt;- seq(-3,3,by=0.005)\nFx &lt;- punif(x, -2, 2)\nplot(x, Fx, type='l', col='blue')\n# Answer\nFstar &lt;- punif(0.25, -2, 2)\n# Visualize Answer\nsegments(0.25, 0, 0.25, Fstar,\n    col=rgb(0,0,1, 0.25))\n\n\n\n\n\n\n\n\n\nCode\n\n# Draw by computer the PDF, f(x)\nfx &lt;- dunif(x, -2, 2)\nplot(x, fx, type='l', col='blue')\n# Visualize Answer\nfstar &lt;- dunif(0.25, -2, 2)\npolygon(\n    c(-2, 0.25, 0.25, -2), \n    c(0, 0, fstar,fstar),\n    col=rgb(0,0,1, 0.25), border=NA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nSuppose the flight time between Calgary and Kamloops is Uniformly distributed between \\(68\\) and \\(78\\) minutes. According to Air Canada the flight takes \\(70\\) minutes. What is the probability that the flight will be late?\n\n\n\n\n\nBeta.\nThe sample space is any number on the unit interval, \\(X_{i} \\in [0,1]\\), but with non-uniform probabilities.\n\n\nCode\nX4 &lt;- rbeta(2000,2,2) ## two shape parameters\nhist(X4, breaks=20, border=NA, main=NA, freq=F)\n\n#See the underlying probabilities\n#f_25 &lt;- dbeta(.25, 2, 2)\n\nx &lt;- seq(0,1,by=.01)\nfx &lt;- dbeta(x, 2, 2)\nlines(x, fx, col='blue')\n\n\n\n\n\n\n\n\n\nThe Beta distribution is mathematically complicated to write, and so we omit it. However, we can find the probability graphically using either the probability density function or cumulative distribution function.\n\n\n\n\n\n\nTip\n\n\n\n\n\nSuppose \\(X_{i}\\) is a random variable with a beta distribution. Intuitively depict \\(Prob(X_{i} \\in [0.2, 0.8])\\) by drawing an area under the density function. Numerically estimate that same probability using the CDF.\n\n\nCode\nplot( ecdf(X4), main=NA) # Empirical\n\nx &lt;- seq(0,1,by=.01) # Theoretical\nFx &lt;- pbeta(x, 2, 2)\nlines(x, Fx, col='blue')\n\n# Middle Interval Example \nF2 &lt;- pbeta(0.2, 2, 2)\nF8 &lt;- pbeta(0.8, 2, 2)\nF_2_8 &lt;- F8 - F2\nF_2_8\n## [1] 0.792\n\n# Visualize\ntitle('Middle between 0.2 and 0.8')\nsegments( 0.2, F2, -1, F2, col='red')\nsegments( 0.8, F8, -1, F8, col='red')\n\n\n\n\n\n\n\n\n\n\n\n\nThis distribution is often used, as the probability density function has two parameters that allow it to take many different shapes.\n\n\n\n\n\n\nTip\n\n\n\n\n\nFor each example below, intuitively depict \\(Prob(X_{i} \\leq 0.5)\\) using the PDF. Repeat the exercise using a CDF instead of a PDF to calculate a numerical value.\n\n\nCode\nop &lt;- par(no.readonly = TRUE); on.exit(par(op), add = TRUE)\nx &lt;- seq(0,1,by=.01)\npars &lt;- expand.grid( c(.5,1,2), c(.5,1,2) )\npar(mfrow=c(3,3))\napply(pars, 1, function(p){\n    fx &lt;- dbeta( x,p[1], p[2])\n    plot(x, fx, type='l', xlim=c(0,1), ylim=c(0,4), lwd=2, col='blue')\n    #hist(rbeta(2000, p[1], p[2]), breaks=50, border=NA, main=NA, freq=F)\n})\ntitle('Beta densities', outer=T, line=-1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExponential.\nThe sample space is any positive number.2 An Exponential random variable has a single parameter, \\(\\lambda&gt;0\\), that governs its shape \\[\\begin{eqnarray}\nX_{i} &\\in& [0,\\infty) \\\\\nf(x) &=& \\lambda exp\\left\\{ -\\lambda x \\right\\} \\\\\nF(x) &=& \\begin{cases}\n    0 & x &lt; 0 \\\\\n    1-  exp\\left\\{ -\\lambda x \\right\\} & x \\geq 0.\n\\end{cases}\n\\end{eqnarray}\\]\n\n\nCode\nrexp(3) # 3 draws\n## [1] 0.8151625 0.4720302 1.1547771\n\nX5 &lt;- rexp(2000)\nhist(X5, breaks=20,\n    border=NA, main=NA,\n    freq=F, ylim=c(0,1), xlim=c(0,10))\n    \nx &lt;- seq(0,10,by=.1)\nfx &lt;- dexp(x)\nlines(x, fx, col='blue')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nSuppose the lifetime of a battery is an exponential random variable with \\(\\lambda=1/50\\). Using the computer, find the probability that the lifetime is \\(&lt; 10\\) hours. Find the probability that the lifetime is \\(\\geq 100\\) hours. Use the computer to find the probability that the lifetime is between \\(10\\) and \\(100\\) hours.\n\n\nCode\npexp(10, 1/50)\n## [1] 0.1812692\n\n\n\n\n\n\n\nNormal (Gaussian).\nThis distribution is for any number on the real line, with bell shaped probabilities. The Normal distribution is mathematically complex and sometimes called the Gaussian distribution. We call it “Normal” because we will encounter it again and again and again. The probability density function \\(f\\) has two parameters \\(\\mu \\in (\\infty,\\infty)\\) and \\(\\sigma &gt; 0\\). \\[\\begin{eqnarray}\nX_{i} &\\in& (\\infty,\\infty) \\\\\nf(x) &=& \\frac{1}{\\sqrt{2\\pi \\sigma^2}} exp\\left\\{ \\frac{-(x-\\mu)^2}{2\\sigma^2} \\right\\}\n\\end{eqnarray}\\]\n\n\nCode\nrnorm(3) # 3 draws\n## [1]  0.9224901  1.5357070 -1.1047990\n\nX6 &lt;- rnorm(2000)\nhist(X6, breaks=20,\n    border=NA, main=NA,\n    freq=F, ylim=c(0,.4), xlim=c(-4,4))\n\nx &lt;- seq(-10,10,by=.025)\nfx &lt;- dnorm(x)\nlines(x, fx, col='blue')\n\n\n\n\n\n\n\n\n\nEven thought the distribution function is complex, we can compute CDF values using the computer.\n\n\nCode\npnorm( c(-1.645, 1.645) ) # 10%\n## [1] 0.04998491 0.95001509\npnorm( c(-2.576, 2.576) ) #  1%\n## [1] 0.004997532 0.995002468\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nSuppose \\(X_{i}\\) is a random variable with a normal distribution with \\(\\mu=0\\) and \\(\\sigma=1\\). Intuitively depict \\(Prob(X_{i} \\in [0.2, 0.8])\\) by drawing an area under the density function. Numerically estimate that same probability using the CDF.\n\n\nCode\nplot( ecdf(X6), main=NA) # Empirical\n\nx &lt;- seq(-4,4,by=.01) # Theoretical\nFx &lt;- pnorm(x, 0, 1)\nlines(x, Fx, col='blue')\n\n# Middle Interval Example \nF2 &lt;- pnorm(0.2, 0, 1)\nF8 &lt;- pnorm(0.8, 0, 1)\nF_2_8 &lt;- F8 - F2\nF_2_8\n## [1] 0.2088849\n\n# Visualize\ntitle('Middle between 0.2 and 0.8')\nsegments( 0.2, F2, -5, F2, col='red')\nsegments( 0.8, F8, -5, F8, col='red')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nSuppose that your health status is a normally distributed random variable with \\(\\mu=2\\) and \\(\\sigma=3\\). If we randomly sample one person, what is the probability there health status is higher than \\(4\\)?\n\n\nCode\n# Start with a simulation of 1000 people to build intuition\nX &lt;- rnorm(1000,2,3)\nhist(X, freq=F, border=NA, main=NA)\n\n\n\n\n\n\n\n\n\nCode\nsum(X&gt;4)/1000\n## [1] 0.242\n\n# Do an exact calculation\n1-pnorm(4,2,3)\n## [1] 0.2524925\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nDraw the Middle \\(90\\%\\) of a normal distribution\n\n\nCode\n# PDF\nx &lt;- seq(-10,10,by=.025)\nfx &lt;- dnorm(x)\nplot(x, fx,\n    col='blue', type='l',\n    ylab='Theoretical Density: f(x)',\n    main='Middle 90%')\n# Show Middle 90%\nx_90 &lt;- seq(-1.645,1.645,by=.025)\nfx_90 &lt;- dnorm(x_90)\npolygon( c(x_90, rev(x_90)), c(fx_90,fx_90*0),\n    col=rgb(0,0,1,.25), border=NA)\n\n\n\n\n\n\n\n\n\nCode\npnorm(1.645)-pnorm(-1.645)\n## [1] 0.9000302\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nSuppose scores in math class are approximately normally distributed with \\(\\mu=50, \\sigma=1\\). If you selected one student randomly, what is the probability their score is higher than \\(90\\). Is \\(Prob(X_{i}\\geq 90)\\) higher if \\(\\mu=25, \\sigma=2\\)? What about \\(\\mu=10, \\sigma=5\\)?",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "01_04_RandomVariables.html#further-reading",
    "href": "01_04_RandomVariables.html#further-reading",
    "title": "4  Random Variables",
    "section": "4.3 Further Reading",
    "text": "4.3 Further Reading\nNote that many random variables are related to each other\n\nhttps://en.wikipedia.org/wiki/Relationships_among_probability_distributions\nhttps://www.math.wm.edu/~leemis/chart/UDR/UDR.html\nhttps://qiangbo-workspace.oss-cn-shanghai.aliyuncs.com/2018-11-11-common-probability-distributions/distab.pdf\n\nAlso note that numbers randomly generated on your computer cannot be truly random, they are “Pseudorandom”.",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "01_04_RandomVariables.html#footnotes",
    "href": "01_04_RandomVariables.html#footnotes",
    "title": "4  Random Variables",
    "section": "",
    "text": "This is the general formula using CDFs, and you can verify it works in this instance by directly adding the probability of each 2 or 3 event: \\(Prob(X_{i} = 2) +  Prob(X_{i} = 3) = 1/4 + 1/4 = 2/4\\).↩︎\nIn other classes, you may further distinguish types of random variables based on whether their maximum value is theoretically finite or infinite.↩︎",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html",
    "href": "01_05_Statistics.html",
    "title": "5  Statistics",
    "section": "",
    "text": "5.1 Mean and Variance\nWe often summarize distributions with statistics: functions of data. The most basic way to do this is with summary, whose values can all be calculated individually.\nTogether, the mean and variance statistics summarize the central tendency and dispersion of a distribution. In some special cases, such as with the normal distribution, they completely describe the distribution. Other distributions are better described with other statistics, either as an alternative or in addition to the mean and variance. After discussing those other statistics, we will return to the two most basic statistics in theoretical detail.\nThe mean and variance are the two most basic statistics that summarize the center and how spread apart the values are. As before, we represent data as vector \\(\\hat{X}=(\\hat{X}_{1}, \\hat{X}_{2}, ....\\hat{X}_{n})\\), where there are \\(n\\) observations and \\(\\hat{X}_{i}\\) is the value of the \\(i\\)th one.",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html#mean-and-variance",
    "href": "01_05_Statistics.html#mean-and-variance",
    "title": "5  Statistics",
    "section": "",
    "text": "Mean.\nPerhaps the most common statistic is the mean, which is the [sum of all values] divided by [number of values]; \\[\\begin{eqnarray}\n\\hat{M} &=& \\frac{\\sum_{i=1}^{n}\\hat{X}_{i}}{n},\n\\end{eqnarray}\\] where \\(\\hat{X}_{i}\\) denotes the value of the \\(i\\)th observation.\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor example, a dataset of \\(\\{1,4,10\\}\\) has a mean of \\([1+4+10]/3=5\\).\n\n\nCode\nX &lt;- c(1,4,10)\nsum(X)/length(X)\n## [1] 5\nmean(X)\n## [1] 5\n\n\n\n\n\n\n\nCode\n# compute the mean of a random sample\nX1 &lt;- USArrests[,'Murder']\nX1_mean &lt;- mean(X1)\nX1_mean\n## [1] 7.788\n\n# visualize on a histogram\nhist(X1, border=NA, main=NA)\nabline(v=X1_mean, col=2, lwd=2)\ntitle(paste0('mean= ', round(X1_mean,2)), font.main=1)\n\n\n\n\n\n\n\n\n\n\n\nVariance.\nPerhaps the second most common statistic is the variance: the average squared deviation from the mean \\[\\begin{eqnarray}\n\\hat{V} &=&\\frac{\\sum_{i=1}^{n} [\\hat{X}_{i} - \\hat{M}]^2}{n}.\n\\end{eqnarray}\\] The standard deviation is simply \\(\\hat{S} = \\sqrt{\\hat{V} }\\), which can be interpreted as the average distance from the mean.\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor example, a dataset of \\(\\{1,4,10\\}\\) has a mean of \\([1+4+10]/3=5\\). The variance is \\([(1-5)^2+(4-5)^2+(10-5)^2]/3=14\\) and the standard deviation is \\(\\sqrt{14}\\).\n\n\nCode\nX &lt;- c(1,4,10)\nX_mean &lt;- mean(X)\nX_var &lt;- mean( (X - X_mean)^2 )\nsqrt(X_var)\n## [1] 3.741657\n\n\n\n\n\n\n\nCode\nX1_s &lt;- sd(X1) # sqrt(var(X))\nhist(X1, border=NA, main=NA, freq=F)\nX1_s_lh &lt;- c(X1_mean - X1_s,  X1_mean + X1_s)\nabline(v=X1_s_lh, col=4)\ntext(X1_s_lh, -.02,\n    c( expression(bar(X)-s[X]), expression(bar(X)+s[X])),\n    col=4, adj=0)\ntitle(paste0('sd= ', round(X1_s,2)), font.main=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nNote that a “corrected version” is used by R and many statisticians: \\(\\hat{V}' =\\frac{\\sum_{i=1}^{n} [\\hat{X}_{i} - \\hat{M}]^2}{n-1}\\) and \\(\\hat{S}' = \\sqrt{\\hat{V}'}\\). In this class, we use the “unocorrect version” as defined previously. There is hardly any difference when \\(n\\) is large: e.g., \\(\\frac{1}{n}\\approx \\frac{1}{n-1}\\) for \\(n=100,000\\).\n\n\nCode\nX &lt;- c(1,4,10)\n\nX_mean &lt;- mean(X)\nX_var &lt;- mean( (X - X_mean)^2 )\nX_var\n## [1] 14\n\n# Corrected Version\nn &lt;- length(X)  \nX_var2 &lt;- sum( (X - X_mean)^2 )/(n-1)\nX_var2\n## [1] 21\n\nvar(X) # R-Version\n## [1] 21",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html#other-centerspread-statistics",
    "href": "01_05_Statistics.html#other-centerspread-statistics",
    "title": "5  Statistics",
    "section": "5.2 Other Center/Spread Statistics",
    "text": "5.2 Other Center/Spread Statistics\nA general rule of applied statistics is that there are multiple ways to measure something. Mean and Variance are measurements of Center and Spread, but there are others that have different theoretical properties and may be better suited for your dataset.\n\nMedians and Absolute Deviations.\nWe can use the Median as a “robust alternative” to means that is especially useful for data with asymmetric distributions and extreme values. Recall that the \\(q\\)th quantile is the value where \\(q\\) percent of the data are below and (\\(1-q\\)) percent are above. The median (\\(q=.5\\)) is the point where half of the data is lower values and the other half is higher. This means that median is not sensitive to extreme values (whereas the mean is).\n\n\nCode\nX &lt;- rexp(50, 0.4)\nX\n##  [1] 0.91016272 4.88260863 1.58679507 3.24193262 0.04782469 2.66774217\n##  [7] 1.84790458 2.46855913 1.03367074 2.34591347 5.94956770 5.11509418\n## [13] 2.26841729 1.84824413 2.75500751 1.52930723 3.29390866 0.69037997\n## [19] 1.76877226 2.29951790 1.70335870 1.09934813 0.98335784 1.29420585\n## [25] 0.37392728 0.05250515 3.07122349 0.15815237 0.49980370 1.25843519\n## [31] 0.33435859 0.40147074 4.17070184 1.42378013 3.34881907 6.48139804\n## [37] 1.23036662 2.37269204 0.57109060 0.33470121 1.61252818 7.41661318\n## [43] 1.31081089 1.20169949 0.92360120 0.92177694 0.33916287 0.03108448\n## [49] 3.28943487 3.35773088\nhist(X, freq=F, breaks=seq(0,16),\n    border=NA, main='')\n\n\n\n\n\n\n\n\n\nCode\n\n# For discrete numbers\n# X &lt;- rgeom(50, .4)\n#proportions &lt;- table(X)/length(X)\n#plot(proportions, ylab='proportion')\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nExamine robustness to an extreme value\n\n\nCode\n\nX_extreme &lt;- c(X, 1000) # add one extreme value\n#par(mfrow=c(1,2)) # visualize side-by-side\n#hist(X)\n#hist(X_extreme)\n\n\n# Which measures of central tendency are robust\n# to a single extreme value?\nmean(X)\n## [1] 2.002389\nmean( X_extreme )\n## [1] 21.57097\n\nquantile(X, prob=0.5)\n##      50% \n## 1.558051\nquantile(X_extreme, prob=0.5)\n##      50% \n## 1.586795\n\n\n\n\n\nWe can also use the Interquartile Range or Median Absolute Deviation as an alternative to variance. The difference between the first and third quartiles (\\(q=.25\\) and \\(q=.75\\)) measure is range of the middle \\(50%\\) of the data, which is how the boxplot measures “spread”. The median absolute deviation is another statistic that also measures “spread”. \\[\\begin{eqnarray}\n\\tilde{M} &=& Med( \\hat{X}_{i}) \\\\\n\\hat{MAD} &=& Med\\left( | \\hat{X}_{i} - \\tilde{M} | \\right).\n\\end{eqnarray}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nCompute the IQR for the dataset \\(\\{-100,1,4,10,10\\}\\). Upper quartile - Lower quartile \\(= 10 - 1 = 9\\).\n\n\nCode\nX &lt;- c(-100,1,4,10,10)\n# An intuitive alternative to sd(X), used in the boxplot\nquants &lt;- quantile(X, probs=c(.25,.75))\nquants[2]-quants[1]\n## 75% \n##   9\nIQR(X)\n## [1] 9\n\n\nCompute the MAD for the dataset \\(\\{1,4,10\\}\\). First compute Median\\((1,4,10)=1\\). Then compute Median\\(( |1-4|, |4-4|, |10-4| )=\\)Median\\(( 3,0,6 ) = 3\\).\n\n\nCode\nX &lt;- c(1,4,10)\n#Another alternative to sd(X)\nmad(X, constant=1)\n## [1] 3\n\n# Computationally equivalent\n# x_med &lt;- quantile(X, probs=.5)\n# x_absdev &lt;- abs(X -x_med)\n# quantile(x_absdev, probs=.5)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nExamine robustness to an extreme value\n\n\nCode\nX &lt;- rgeom(50, .4)\nX_extreme &lt;- c(X, 1000) # add one extreme value\n\nsd(X)\n## [1] 1.912472\nsd(X_extreme)\n## [1] 139.8532\n\nmad(X, constant=1)\n## [1] 1\nmad(X_extreme, constant=1)\n## [1] 1\n\nIQR(X)\n## [1] 2\nIQR(X_extreme)\n## [1] 2\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nNote that there other “absolute deviation” statistics\n\n\nCode\n# sometimes seen elsewhere\nmean( abs(X - mean(X)) )\nmean( abs(X - median(X)) )\nmedian( abs(X - mean(X)) )\n\n\n\n\n\n\n\nMode and Share Concentration.\nSometimes, none of the above work well. With categorical data, for example, distributions are easier to describe with other statistics. The mode is the most common observation: the value with the highest observed frequency. We can also measure the spread of the frequencies or concentration at the mode vs elsewhere.\n\n\nCode\n# Draw 3 Random Letters\nx &lt;- LETTERS\nx_probs &lt;- 1:length(x)\nx_probs &lt;- x_probs/sum(x_probs) # probs must sum to 1\nX &lt;- sample(x, 1, prob=x_probs, replace=T)\nX\n## [1] \"T\"\n\n# Draw Random Letters 100 Times\nX &lt;- sample(x, 1000, prob=x_probs, replace=T)\nX &lt;- factor(unlist(X), levels=LETTERS, ordered=F)\n\nproportions &lt;- table(X)/length(X)\nplot(proportions, col=grey(0,0.5))\n\n\n\n\n\n\n\n\n\nCode\n\n# mode(s)\nmode_id &lt;- which(proportions==max(proportions))\nnames(proportions)[ mode_id ]\n## [1] \"Z\"\n\n# freq. spreads\nsd(proportions)\n## [1] 0.02173335\nsum(proportions^2)\n## [1] 0.05027\n# freq. concentration at mode\nmax(proportions)/mean(proportions)\n## [1] 2.028",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html#shape-statistics",
    "href": "01_05_Statistics.html#shape-statistics",
    "title": "5  Statistics",
    "section": "5.3 Shape Statistics",
    "text": "5.3 Shape Statistics\nCentral tendency and dispersion are often insufficient to describe a distribution. To further describe shape, we can compute skew and kurtosis to measure asymmetry and extreme values. There are many other statistics we could compute on an ad-hoc basis.\n\nSkewness.\nThe skew statistic captures how asymmetric the distribution is by measuring the average cubed deviation from the mean, normalized the standard deviation cubed \\[\\begin{eqnarray}\n\\frac{\\sum_{i=1}^{n} [\\hat{X}_{i} - \\hat{M}]^3 / n}{ s^3 }\n\\end{eqnarray}\\]\n\n\nCode\nX &lt;- rweibull(1000, shape=1)\nhist(X, border=NA, main=NA, freq=F, breaks=20)\n\n\n\n\n\n\n\n\n\nCode\n\nskewness &lt;-  function(X){\n X_mean &lt;- mean(X)\n m3 &lt;- mean((X - X_mean)^3)\n s3 &lt;- sd(X)^3\n skew &lt;- m3/s3\n return(skew)\n}\n\nskewness( rweibull(1000, shape=1))\n## [1] 1.99457\nskewness( rweibull(1000, shape=10) )\n## [1] -0.6524973\n\n\nWe can automatically compare against the normal distribution, which has a skew of 0.\n\n\nKurtosis.\nThis statistic captures how many “outliers” there are like skew but looking at quartic deviations instead of cubed ones. \\[\\begin{eqnarray}\n\\frac{\\sum_{i=1}^{n} [\\hat{X}_{i} - \\hat{M}]^4 / n}{ s^4 }.\n\\end{eqnarray}\\] Some authors further subtract \\(3\\) to explicitly compare against the normal distribution (the normal distribution has a kurtosis of \\(3\\)).\n\n\nCode\nX &lt;- rweibull(1000, shape=1)\nboxplot(X, main=NA)\n\n\n\n\n\n\n\n\n\nCode\n\nkurtosis &lt;- function(X){  \n X_mean &lt;- mean(X)\n m4 &lt;- mean((X - X_mean)^4) \n s4 &lt;- sd(X)^4\n kurt &lt;- m4/s4\n excess_kurt &lt;- kurt - 3 # compare against normal\n return(kurt)\n}\n\nkurtosis( rweibull(1000, shape=1) )\n## [1] 8.668162\nkurtosis( rweibull(1000, shape=10) )\n## [1] 4.035707\n\n\n\n\nClusters/Gaps.\nYou can also describe distributions in terms of how clustered the values are, including the number of modes, bunching, and many other statistics. But remember that “a picture is worth a thousand words”.\n\n\nCode\n# Complicated Random Variable\nr_ugly0 &lt;- function(n, populationMean=c(-2, 3), populationVar=c(1, 4), prob=c(.5,.5)){\n  # Which distribution is each observation coming from\n  di &lt;- sample(1:2, size=n, replace=TRUE, prob=prob)\n  rnorm(n, mean=populationMean[di], sd=sqrt(populationVar)[di])\n}\n\nX &lt;- r_ugly0(6000)\nhist(X, breaks=60,\n    freq=F, border=NA,\n    xlab=\"x\", main='')\n\n\n\n\n\n\n\n\n\nCode\n\n#hist( rbeta(1000, .6, .6), border=NA, main=NA, freq=F, breaks=20)\n\n\n\n\nCode\n# Another Complicated Random Variable\nr_ugly1 &lt;- function(n, theta1=c(-8,-1), theta2=c(-2,2), rho=.25){\n    omega   &lt;- rbinom(n, size=1, rho)\n    epsilon &lt;- omega * runif(n, theta1[1], theta2[1]) +\n        (1-omega) * rnorm(n, theta1[2], theta2[2])\n    return(epsilon)\n}\n# Very Large Sample\nX &lt;- r_ugly1(1000000)\nhist(X, breaks=1000,  freq=F, border=NA,\n    xlab=\"x\", main='')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Show True Density\nd_ugly1 &lt;- function(x, theta1=c(-8,-1), theta2=c(-2,2), rho=.25){\n    rho     * dunif(x, theta1[1], theta2[1]) +\n    (1-rho) * dnorm(x, theta1[2], theta2[2]) }\nx &lt;- seq(-12,6,by=.001)\ndx &lt;- d_ugly1(x)\nlines(x, dx, col=1)",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html#probability-theory",
    "href": "01_05_Statistics.html#probability-theory",
    "title": "5  Statistics",
    "section": "5.4 Probability Theory",
    "text": "5.4 Probability Theory\nYou were already introduced to this with random variables. We will now dig a little deeper theoretically into the statistics we compute. When we know how the data are generated theoretically, we can often compute the theoretical value of the two most basic and often-used statistics: the mean and variance. To see this, we separately analyze how they are computed for discrete and continuous random variables.\nHere, we denote \\(X_{i}\\) as a random variable that can take on specific values \\(x\\) from the sample space with known probabilities.\n\nDiscrete Random Variables.\nIf the sample space is discrete, we can compute the theoretical mean (or expected value) as \\[\n\\mathbb{E}[X_{i}] = \\sum_{x} x Prob(X_{i}=x),\n\\] where \\(Prob(X_{i}=x)\\) is the probability the random variable \\(X_{i}\\) takes the particular value \\(x\\). Similarly, we can compute the theoretical variance as \\[\n\\mathbb{V}[X_{i}] = \\sum_{x} \\left(x - \\mathbb{E}[X_{i}] \\right)^2 Prob(X_{i}=x).\n\\] The theoretical standard deviation is \\(\\mathbb{s}[X_{i}] = \\sqrt{\\mathbb{V}[X_{i}]}\\).\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor example, consider an unfair coin with a \\(3/4\\) probability of heads (\\(x=1\\)) and a \\(1/4\\) probability of tails (\\(x=0\\)) has a theoretical mean of \\[\n\\mathbb{E}[X_{i}] = 1\\frac{3}{4} + 0\\frac{1}{4} = \\frac{3}{4}\n\\] and a theoretical variance of \\[\n\\mathbb{V}[X_{i}] = [1 - \\frac{3}{4}]^2 \\frac{3}{4} + [0 - \\frac{3}{4}]^2 \\frac{1}{4}\n= \\frac{3}{64} + \\frac{9}{64}\n= \\frac{12}{64}\n= \\frac{3}{16}.\n\\]\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nFor a more general example, consider an unfair coin with a \\(p\\) probability of heads (\\(x=1\\)) and a \\(1-p\\) probability of tails (\\(x=0\\)), where \\(p\\) is a parameter between \\(0\\) and \\(1\\). The theoretical mean is \\[\n\\mathbb{E}[X_{i}] = 1[p] + 0[1-p] = p\n\\] and the theoretical variance is \\[\n\\mathbb{V}[X_{i}]\n= [1 - p]^2 p + [0 - p]^2 [1-p] = [1 - p]^2 p + p^2 [1-p]\n= [1-p]p\\left( [1-p] + p\\right) = [1-p] p.\n\\] So the theoretical standard deviation is \\(\\mathbb{s}[X_{i}]=\\sqrt{[1-p] p}\\).\n\n\nCode\n# A simulation of many coin flips\nx &lt;- c(0,1)\nx_probs &lt;- c(1/4, 3/4)\nX &lt;- sample(x, 1000, prob=x_probs, replace=T)\n\nround( mean(X), 4)\n## [1] 0.736\n\nround( var(X), 4)\n## [1] 0.1945\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nConsider a four-sided die with outcomes \\(\\{1,2,3,4 \\}\\) and corresponding probabilities \\(\\{ 1/8, 2/8, 1/8, 4/8 \\}\\) . What is the mean? \\[\n\\mathbb{E}[X_{i}] = 1 \\frac{1}{8} + 2 \\frac{2}{8} + 3 \\frac{1}{8} + 4 \\frac{4}{8} = 3\n\\] What is the variance? Verify both the mean and variance by simulation.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nConsider a six-sided die with equal probability of landing on each side. I.e., \\(X_{i}\\) is a Discrete Uniform random variable with outcomes \\(x \\in \\{1,2,3,4,5,6\\}\\). What is the theoretical mean?\n\n\nCode\n# Manual Way\n1*1/6 + 2*1/6 + 3*1/6 + 4*1/6 + 5*1/6 + 6*1/6\n## [1] 3.5\n\n# Another Manual Way (only works with equal probabilities)\n(1+2+3+4+5+6)/6\n## [1] 3.5\n\n# Computerized Way\nx &lt;- c(1,2,3,4,5,6)\nx_probs &lt;- c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6)\nX_mean &lt;- sum(x*x_probs)\nX_mean\n## [1] 3.5\n\n# Verified by simulation\nXsim &lt;- sample(x, prob=x_probs,\n          size=10000, replace=T)\nmean(Xsim)\n## [1] 3.4691\n\n\nWhat is the variance and standard deviation?\n\n\nCode\n# Manual Way\nXvar &lt;- (1-X_mean)^2*1/6 +\n  (2 - X_mean)^2*1/6 +\n  (3 - X_mean)^2*1/6 +\n  (4 - X_mean)^2*1/6 +\n  (5 - X_mean)^2*1/6 + \n  (6 - X_mean)^2*1/6\nXvar\n## [1] 2.916667\nXsd &lt;- sqrt(Xvar)\nXsd\n## [1] 1.707825\n\n# Verified by simulation\nsd(Xsim)\n## [1] 1.706791\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nSuppose \\(X_{i}\\) is a discrete random variable with this probability mass function: \\[\\begin{eqnarray}\nX_{i} &=& \\begin{cases}\n-1 &  \\text{ with probability } \\frac{1}{2 \\lambda^2} \\\\\n0  & \\text{ with probability } 1-\\frac{1}{\\lambda^2} \\\\\n+1 & \\text{ with probability } \\frac{1}{2\\lambda^2}\n\\end{cases},\n\\end{eqnarray}\\] where \\(\\lambda&gt;0\\) is a parameter. What is the theoretical standard deviation?\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nFor a discrete uniform random variable with this sample space: \\({1,2,3,4,5}\\), calculate the theoretical median and IQR.\n\n\n\n\n\nWeighted Data.\nSometimes, you may have a dataset of values and probability weights. Othertimes, you can calculate them yourself. In either case, you can explicitly do the computations for discrete data. Given data on unique outcome \\(x\\) and their frequency \\(\\hat{p}_{x}=\\sum_{i=1}^{n}\\mathbf{1}\\left(X_{i}=x\\right)/n\\), we compute \\[\\begin{eqnarray}\n\\hat{M} &=& \\sum_{x} x \\hat{p}_{x}.\n\\end{eqnarray}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nSuppose we flipped a coin \\(100\\) times and found that \\(76\\) were Heads and \\(23\\) were Tails. The estimated probabilities are \\(76/100\\) for the outcome \\(X_{1}=1\\) and \\(24/100\\) for the outcome \\(X_{i}=0\\). We compute the mean as \\(\\hat{M}=1\\times 0.76 + 0 \\times 0.24 = 0.76\\).\n\n\nCode\n# Compute a sample estimate using probability weights\nP  &lt;- table(X) #table of counts\np &lt;- c(P)/length(X) #frequencies (must sum to 1)\nx &lt;- as.numeric(names(p)) #unique values\ncbind(x,p)\n##   x     p\n## 0 0 0.264\n## 1 1 0.736\n\n# Sample Mean Estimate\nX_mean &lt;- sum(x*p)\nX_mean\n## [1] 0.736\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nTry estimating the sample mean the two different ways for another random sample\n\n\nCode\nx &lt;- c(0,1,2)\nx_probs &lt;- c(1/3,1/3,1/3)\nX  &lt;-  sample(x, prob=x_probs, 1000, replace=T)\n\n# First Way (Computerized)\nmean(X)\n## [1] 1.033\n\n# Second Way (Explicit Calculations)\n# start with a table of counts like the previous example\n\n\n\n\n\nThis idea generalizes the mean to a weighted mean: an average where different values contribute to the final result with varying levels of importance. For each outcome \\(x\\) we have a weight \\(W_{x}\\) and compute \\[\\begin{eqnarray}\n\\hat{M} &=& \\frac{\\sum_{x} x W_{x}}{\\sum_{x} W_{x}} = \\sum_{x} x w_{x},\n\\end{eqnarray}\\] where \\(w_{x}=\\frac{W_{x}}{\\sum_{x'} W_{x'}}\\) is normalized version of \\(W_{x}\\) that implies \\(\\sum_{x}w_{x}=1\\).1\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor example, suppose a student has these scores\n\n\nCode\nHomework1 &lt;- c(score=88, weight=0.25)\nHomework2 &lt;- c(score=92, weight=0.25)\nExam1 &lt;- c(score=67, weight=0.2)\nExam2 &lt;- c(score=90, weight=0.3)\n\nGrades &lt;- rbind(Homework1, Homework2, Exam1, Exam2)\nGrades\n##           score weight\n## Homework1    88   0.25\n## Homework2    92   0.25\n## Exam1        67   0.20\n## Exam2        90   0.30\n\n\nWe can compute the final grade as a weighted mean\n\n\nCode\n# Manual Way\n88*0.25 + 92*0.25 + 67*0.2 + 90*0.3\n## [1] 85.4\n\n# Computerized Way\nValues &lt;- Grades[,'score'] * Grades[,'weight']\nFinalGrade &lt;- sum(Values)\nFinalGrade\n## [1] 85.4\n\n\n\n\n\nThe weighting idea generalizes to other statistics. E.g., we can also computed a weighted variance or weighted quantile.\n\n\n\n\n\n\nTip\n\n\n\n\n\nProvide an example of computing a weighted variance building on this code below\n\n\nCode\nx_diff &lt;- (x - x_mean)^2\np &lt;- P/sum(P)\nx_var &lt;- sum(p * x_diff)\n\n\nSee that we can also compute weighted quantiles\n\n\nCode\nweighted.quantile &lt;- function(x, w, probs){\n    #See spatstat.univar::weighted.quantile\n    oo &lt;- order(x)\n    x &lt;- x[oo]\n    w &lt;- w[oo]\n    Fx &lt;- cumsum(w)/sum(w)\n    quantile_id &lt;- max(which(Fx &lt;= probs))+1\n    xq &lt;- x[quantile_id] \n    return(xq)\n}\n\n## Unweighted\nquantile(X, probs=.5)\n## 50% \n##   1\nweights &lt;- rep(1, length(X))\nweighted.quantile(x=X, w=weights, probs=.5)\n## [1] 1\n\n## Weighted\nweights &lt;- seq(X)\nweights &lt;- weights/sum(weights) # normalize\nweighted.quantile(x=X, w=weights, probs=.5)\n## [1] 1\n\n\n\n\n\n\n\nContinuous Random Variables.\nMany continuous random variables are parameterized by their means and variances. For example, the exponential distribution has parameter \\(\\lambda\\), which corresponds to the theoretical mean \\(1/\\lambda\\) and variance \\(1/\\lambda^2\\). For another example, the two parameters of the normal distribution are \\(\\mu\\) and \\(\\sigma\\), which corresponds to the theoretical mean and variance.\n\n\nCode\n# Exponential Random Variable\nX &lt;- rexp(5000, 2)\nm &lt;- mean(X)\nround(m, 2)\n## [1] 0.51\n\n# Normal Random Variable\nX &lt;- rnorm(5000, 1, 2)\nround(mean(X), 2)\n## [1] 1.02\nround(var(X), 2)\n## [1] 3.91\n\n\n\n\nAdvanced and Optional\n\n\nIf the sample space is continuous, we can compute the theoretical mean (or expected value) as \\[\n\\mathbb{E}[X] = \\int x f(x) d x,\n\\] where \\(f(x)\\) is the probability the random variable takes the particular value \\(x\\). Similarly, we can compute the theoretical variance as \\[\n\\mathbb{V}[X_{i}]= \\int \\left(x - \\mathbb{E}[X_{i}] \\right)^2 f(x) d x,\n\\]\nFor example, consider a random variable with a continuous uniform distribution over \\([-1, 1]\\). In this case, \\(f(x)=1/[1 - (-1)]=1/2\\) for each \\(x \\in [-1, 1]\\) and \\[\n\\mathbb{E}[X_{i}] = \\int_{-1}^{1} \\frac{x}{2} d x = \\int_{-1}^{0} \\frac{x}{2} d x + \\int_{0}^{1} \\frac{x}{2} d x = 0\n\\] and \\[\n\\mathbb{V}[X_{i}]= \\int_{-1}^{1} x^2 \\frac{1}{2} d x = \\frac{1}{2} \\frac{x^3}{3}|_{-1}^{1} = \\frac{1}{6}[1 - (-1)] = 2/6 =1/3\n\\]\n\n\nCode\nX &lt;- USArrests[,'Murder']\nround( mean(X), 4)\n## [1] 7.788\nround( var(X), 4)\n## [1] 18.9705\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nYou can estimate means and variances for continuous random variables with weights, but here we have an additional approximation error\n\n\nCode\n# values and probabilities\nh  &lt;- hist(X, plot=F)\nwt &lt;- h[['counts']]/length(X) \nxt &lt;- h[['mids']]\n# Weighted mean\nX_mean &lt;- sum(wt*xt)\nX_mean\n## [1] 7.8\n\n# Compare to \"mean(x)\"\n\n\nTry it yourself with\n\n\nCode\nX &lt;- runif(2000, -1, 1)",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html#further-reading",
    "href": "01_05_Statistics.html#further-reading",
    "title": "5  Statistics",
    "section": "5.5 Further Reading",
    "text": "5.5 Further Reading\nProbability Theory\n\n[Refresher] https://www.khanacademy.org/math/statistics-probability/probability-library/basic-theoretical-probability/a/probability-the-basics\nhttps://book.stat420.org/probability-and-statistics-in-r.html\nhttps://bookdown.org/speegled/foundations-of-statistics/\nhttps://math.dartmouth.edu/~prob/prob/prob.pdf\nhttps://bookdown.org/probability/beta/discrete-random-variables.html\nhttps://www.econometrics-with-r.org/2.1-random-variables-and-probability-distributions.html\nhttps://probability4datascience.com/ch02.html\nhttps://statsthinking21.github.io/statsthinking21-R-site/probability-in-r-with-lucy-king.html\nhttps://bookdown.org/probability/statistics/\nhttps://www.atmos.albany.edu/facstaff/timm/ATM315spring14/R/IPSUR.pdf\nhttps://rc2e.com/probability\nhttps://bookdown.org/probability/beta/\nhttps://bookdown.org/a_shaker/STM1001_Topic_3/\nhttps://bookdown.org/fsancier/bookdown-demo/\nhttps://bookdown.org/kevin_davisross/probsim-book/\nhttps://bookdown.org/machar1991/ITER/2-pt.html\nhttps://www.atmos.albany.edu/facstaff/timm/ATM315spring14/R/IPSUR.pdf\nhttps://math.dartmouth.edu/~prob/prob/prob.pdf\n\nFor weighted statistics, see\n\nhttps://seismo.berkeley.edu/~kirchner/Toolkits/Toolkit_12.pdf\nhttps://www.bookdown.org/rwnahhas/RMPH/survey-desc.html",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html#footnotes",
    "href": "01_05_Statistics.html#footnotes",
    "title": "5  Statistics",
    "section": "",
    "text": "Note that if there are \\(K\\) unique outcomes and \\(W_{x}=1\\) then \\(\\sum_{x}W_{x}=K\\) and \\(w_{x}=1/K\\). This means \\(\\hat{M} = \\sum_{x} x w_{x} = \\sum_{x} x /K\\), which is just a simple mean.↩︎",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html",
    "href": "01_06_Sampling.html",
    "title": "6  (Re)Sampling",
    "section": "",
    "text": "6.1 Sample Distributions\nA sample is a subset of the population. A simple random sample is a sample where each possible sample of size n has the same probability of being selected.\nOften, we think of the population as being infinitely large. This is an approximation that makes mathematical and computational work much simpler.\nThe sampling distribution of a statistic shows us how much a statistic varies from sample to sample.\nFor example, see how the mean varies from sample to sample to sample.\nCode\n# Three Sample Example w/ Visual\npar(mfrow=c(1,3))\nfor(b in 1:3){\n    x &lt;- runif(100) \n    m &lt;-  mean(x)\n    hist(x,\n        breaks=seq(0,1,by=.1), #for comparability\n        freq=F, main=NA, border=NA)\n    abline(v=m, col=2, lwd=2)\n    title(paste0('mean= ', round(m,2)),  font.main=1)\n}\nExamine the sampling distribution of the mean\nCode\n# Many sample example\nsample_means &lt;- vector(length=500)\nfor(i in seq(sample_means)){\n    x &lt;- runif(1000)\n    m &lt;- mean(x)\n    sample_means[i] &lt;- m\n}\nhist(sample_means, \n    breaks=seq(0.45,0.55,by=.001),\n    border=NA, freq=F,\n    col=2, font.main=1, \n    main='Sampling Distribution of the mean')\nIn this figure, you see two the most profound results known in statistics",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html#sample-distributions",
    "href": "01_06_Sampling.html#sample-distributions",
    "title": "6  (Re)Sampling",
    "section": "",
    "text": "Note\n\n\n\n\n\n\n\nCode\n# Three Sample Example\nx_one &lt;- runif(100)\nmean_one &lt;- mean(x_one)\nx_two &lt;- runif(100)\nmean_two &lt;- mean(x_two)\nx_three &lt;- runif(100)\nmean_three &lt;- mean(x_three)\nsample_means &lt;- c(mean_one, mean_two, mean_three)\nsample_means\n## [1] 0.5293868 0.4740400 0.5048282\n\n# An Equivalent Approach: fill vector in a loop\nsample_means &lt;- vector(length=3)\nfor(i in seq(sample_means)){\n    x &lt;- runif(100)\n    m &lt;- mean(x)\n    sample_means[i] &lt;- m\n}\nsample_means\n## [1] 0.4842352 0.5486234 0.4405253\n\n\n\n\n\n\n\n\n\n\nLaw of Large Numbers: the sample mean is centered around the true mean.\nCentral Limit Theorem: the sampling distribution of the mean is approximately standard normal.\n\n\nLaw of Large Numbers.\nThere are different variants of the Law of Large Numbers (LLN), but they all say some version of “the sample mean is centered around the true mean”.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nCode\n# LLLN example\nm_LLLN &lt;- mean(sample_means)\nround(m_LLLN, 3)\n## [1] 0.5\n\n\n\n\n\n\n\nCentral Limit Theorem.\nThere are different variants of the central limit theorem (CLT), but they all say some version of “the sampling distribution of a statistic is approximately standard normal”. For example, the sampling distribution of the mean, shown above, is approximately standard normal.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nCode\nhist(sample_means,\n    breaks=seq(0.45,0.55,by=.001),\n    border=NA, freq=F,\n    col=2, font.main=1, \n    main='Sampling Distribution of the mean')\n    \n## Approximately normal?\nmu &lt;- mean(sample_means)\nmu_sd &lt;- sd(sample_means)\nx &lt;- seq(0.1, 0.9, by=0.001)\nfx &lt;- dnorm(x, mu, mu_sd)\nlines(x, fx, col='red')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nFor an example with another statistic, let’s the sampling distribution of the standard deviation.\n\n\nCode\n# CLT example of the \"sd\" statistic\nsample_sds &lt;- vector(length=1000)\nfor(i in seq(sample_sds)){\n    x &lt;- runif(100) # same distribution\n    s &lt;- sd(x) # different statistic\n    sample_sds[i] &lt;- s\n}\nhist(sample_sds,\n    breaks=seq(0.2,0.4,by=.01),\n    border=NA, freq=F,\n    col=4, font.main=1, \n    main='Sampling Distribution of the sd')\n\n## Approximately normal?\nmu &lt;- mean(sample_sds)\nmu_sd &lt;- sd(sample_sds)\nx &lt;- seq(0.1, 0.9, by=0.001)\nfx &lt;- dnorm(x, mu, mu_sd)\nlines(x, fx, col='blue')\n\n\n\n\n\n\n\n\n\nCode\n\n# Try another function, such as\nmy_function &lt;- function(x){ diff(range(exp(x))) }\n\n# try another random variable, such as rexp(100) instead of runif(100)\n\n\n\n\n\nIt is beyond this class to prove this result mathematically, but you should know that not all sampling distributions are standard normal. The CLT approximation is better for “large \\(n\\)” datasets and data with “well behaved” variances. The CLT does not apply to “extreme” statistics.\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor example of “extreme” statistics, examine the sampling distribution of min, median, max statistics.\n\n\nCode\n# Create 300 samples, each with 1000 random uniform variables\nx_samples &lt;- matrix(nrow=300, ncol=1000)\nfor(i in seq(nrow(x_samples))){\n    x_samples[i,] &lt;- runif(1000)\n}\n# Each row is a new sample\nlength(x_samples[1,])\n## [1] 1000\n\n# Compute min, median, and max for each sample\nx_mins &lt;- apply(x_samples, 1, quantile, probs=0)\nx_meds &lt;- apply(x_samples, 1, quantile, probs=.5)\nx_maxs &lt;- apply(x_samples, 1, quantile, probs=1)\n\n# Plot the sampling distributions of min, median, and max\n# Median looks normal. Maximum and Minumum do not!\npar(mfrow=c(1,3))\nhist(x_mins, breaks=100, main='Min', font.main=1, border=NA, freq=F)\nhist(x_meds, breaks=100, main='Med', font.main=1, border=NA, freq=F)\nhist(x_maxs, breaks=100, main='Max', font.main=1, border=NA, freq=F)\ntitle('Sampling Distributions', outer=T, line=-1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nHere is an example where variance is not “well behaved” .\n\n\nCode\nsample_means &lt;- vector(length=1000)\nfor(i in seq(sample_means)){\n    x &lt;- rcauchy(1000,0,10)\n    m &lt;- mean(x)\n    sample_means[i] &lt;- m\n} )\nhist(sample_means, breaks=100, border=NA, freq=F) # Tails look too \"fat\"",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html#resampling",
    "href": "01_06_Sampling.html#resampling",
    "title": "6  (Re)Sampling",
    "section": "6.2 Resampling",
    "text": "6.2 Resampling\nOften, we only have one sample. How then can we estimate the sampling distribution of a statistic?\n\n\nCode\nsample_dat &lt;- USArrests[,'Murder']\nsample_mean &lt;- mean(sample_dat)\nsample_mean\n## [1] 7.788\n\n\nWe can “resample” our data. Hesterberg (2015) provides a nice illustration of the idea. The two most basic versions are the jackknife and the bootstrap, which are discussed below.\n\n\n\n\n\n\n\n\n\nNote that we do not use the mean of the resampled statistics as a replacement for the original estimate. This is because the resampled distributions are centered at the observed statistic, not the population parameter. (The bootstrapped mean is centered at the sample mean, for example, not the population mean.) This means that we cannot use resampling to improve on \\(\\hat{M}\\). We use resampling to estimate sampling variability.\n\nJackknife Distribution.\nHere, we compute all “leave-one-out” estimates. Specifically, for a dataset with \\(n\\) observations, the jackknife uses \\(n-1\\) observations other than \\(i\\) for each unique subsample. Taking the mean, for example, we have\n\n\nCode\nsample_dat &lt;- USArrests[,'Murder']\nsample_mean &lt;- mean(sample_dat)\n\n# Jackknife Estimates\njackknife_means &lt;- vector(length=length(sample_dat))\nfor(i in seq_along(jackknife_means)){\n    dat_noti &lt;- sample_dat[-i]\n    mean_noti &lt;- mean(dat_noti)\n    jackknife_means[i] &lt;- mean_noti\n}\nhist(jackknife_means, breaks=25,\n    border=NA, freq=F,\n    main='', xlab=expression(hat(M)[-i]))\nabline(v=sample_mean, col='red', lty=2)\n\n\n\n\n\n\n\n\n\n\n\nBootstrap Distribution.\nHere, we draw \\(n\\) observations with replacement from the original data to create a bootstrap sample and calculate a statistic. Each bootstrap sample \\(b=1...B\\) uses a random subset of observations to compute a statistic. We repeat that many times, say \\(B=9999\\), to estimate the sampling distribution.\nConsider the sample mean as an example;\n\n\nCode\n# Bootstrap estimates\nbootstrap_means &lt;- vector(length=9999)\nfor(b in seq_along(bootstrap_means)){\n    dat_b &lt;- sample(sample_dat, replace=T) # c.f. jackknife\n    mean_b &lt;- mean(dat_b)\n    bootstrap_means[b] &lt;-mean_b\n}\n\nhist(bootstrap_means, breaks=25,\n    border=NA, freq=F,\n    main='', xlab=expression(hat(M)[b]))\nabline(v=sample_mean, col='red', lty=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nWhy does this work? The sample: \\(\\{\\hat{X}_{1}, \\hat{X}_{2}, ... \\hat{X}_{n}\\}\\) is drawn from a CDF \\(F\\). Each bootstrap sample: \\(\\{\\hat{X}_{1}^{(b)}, \\hat{X}_{2}^{(b)}, ... \\hat{X}_{n}^{(b)}\\}\\) is drawn from the ECDF \\(\\hat{F}\\). With \\(\\hat{F} \\approx F\\), each bootstrap sample is approximately a random sample. So when we compute a statistic on each bootstrap sample, we approximate the sampling distribution of the statistic.\n\n\nCode\n# theoretical probabilities\nx &lt;- c(0,1)\nx_probs &lt;- c(1/4, 3/4)\n# sample draws\ncoin_sample &lt;- sample(x, prob=x_probs, 1000, replace=T)\nFhat &lt;- ecdf(coin_sample)\n\nx_probs_boot &lt;- c(Fhat(0), 1-Fhat(0))\nx_probs_boot # approximately the theoretical value\n## [1] 0.248 0.752\ncoin_resample &lt;- sample(x, prob=x_probs_boot, 999, replace=T)\n# any draw from here is almost the same as the original process\n\n\n\n\n\nNote that both resampling methods provide imperfect estimates, and can give different numbers. Percentiles of jackknife resamples are systematically less variable than they should be. Until you know more, a conservative approach is to take the larger estimate (typically the bootstrap).\n\n\nCode\n# Boot CI\nboot_ci &lt;- quantile(bootstrap_means, probs=c(.025, .975))\nboot_ci\n##   2.5%  97.5% \n## 6.6139 8.9980\n\n# Jack CI\njack_ci &lt;- quantile(jackknife_means, probs=c(.025, .975))\njack_ci\n##     2.5%    97.5% \n## 7.621582 7.904082\n\n# more conservative estimate\nci &lt;- boot_ci",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html#intervals",
    "href": "01_06_Sampling.html#intervals",
    "title": "6  (Re)Sampling",
    "section": "6.3 Intervals",
    "text": "6.3 Intervals\nUsing either the bootstrap or jackknife distribution for subsamples, or across multiple samples if we can get them, we can calculate\n\nConfidence Interval: range your statistic varies across different samples.\nStandard Error: variance of your statistic across different samples (square rooted).\n\n\n\nCode\n# Create 300 samples, each with 1000 random uniform variables\nx_samples &lt;- matrix(nrow=300, ncol=1000)\nfor(i in seq(nrow(x_samples))){\n    x_samples[i,] &lt;- runif(1000)\n}\n# Each row is a new sample\nsample_dat1 &lt;- x_samples[1,]\nsd(sample_dat1) # standard deviation\n## [1] 0.2868836\n\n# Compute means for each row (for each sample)\nsample_means &lt;- apply(x_samples, 1, mean)\nsd(sample_means) # standard error\n## [1] 0.009191805\n\n\n\nPercentile Intervals.\nThis type of confidence interval is simply the upper and lower quantiles of the sampling distribution.\nFor example, consider the sample mean. We simulate the sampling distribution of the sample mean and construct a \\(90\\%\\) confidence interval by taking the \\(5^{th}\\) and \\(95^{th}\\) percentiles of the simulated means.\n\n\nCode\n\n# Middle 90%\nmq &lt;- quantile(sample_means, probs=c(.05,.95))\npaste0('we are 90% confident that the mean is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\n## [1] \"we are 90% confident that the mean is between 0.48 and 0.52\"\n\nhist(sample_means,\n    breaks=seq(.4,.6, by=.001), \n    border=NA, freq=F,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='90% Confidence Interval for the Mean')\nabline(v=mq)\n\n\n\n\n\n\n\n\n\nFor another example, consider an extreme statistic. We now repeat the above process to estimate the median for each sample, instead of the mean. We then construct a \\(95\\%\\) confidence interval using the \\(2.5^{th}\\) and \\(97.5^{th}\\) quantiles of these estimates.\n\n\nCode\n## Sample Quantiles (medians)\nsample_quants &lt;- apply(x_samples, 1, quantile, probs=0.5)\n\n# Middle 90% of estimates\nmq &lt;- quantile(sample_quants, probs=c(.05,.95))\npaste0('we are 90% confident that the median is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\n## [1] \"we are 90% confident that the median is between 0.48 and 0.53\"\n\nhist(sample_quants,\n    breaks=seq(.4,.6, by=.001),\n    border=NA, freq=F,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='90% Confidence Interval for the Median')\nabline(v=mq)\n\n\n\n\n\n\n\n\n\nNote that \\(Z\\%\\) confidence intervals do not generally cover \\(Z\\%\\) of the data (those types of intervals are covered later). In the examples above, notice the confidence interval for the mean differs from the confidence interval of the median, and so both cannot cover \\(90\\%\\) of the data. The confidence interval for the mean is roughly \\([0.48, 0.52]\\), which theoretically covers only a \\(0.52-0.48=0.04\\) proportion of uniform random data, much less than \\(90%\\).",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html#probability-theory",
    "href": "01_06_Sampling.html#probability-theory",
    "title": "6  (Re)Sampling",
    "section": "6.4 Probability Theory",
    "text": "6.4 Probability Theory\n\nMeans.\nThe LLN follows from a famous theoretical result in statistics, Linearity of Expectations: the expected value of a sum of random variables equals the sum of their individual expected values. To be concrete, suppose we take \\(n\\) random variables, each one denoted as \\(X_{i}\\). Then, for constants \\(a,b,1/n\\), we have \\[\\begin{eqnarray}\n\\mathbb{E}[a X_{1}+ b X_{2}] &=& a \\mathbb{E}[X_{1}]+ b \\mathbb{E}[X_{2}]\\\\\n\\mathbb{E}\\left[M\\right] &=& \\mathbb{E}\\left[ \\sum_{i=1}^{n} X_{i}/n \\right] = \\sum_{i=1}^{n} \\mathbb{E}[X_{i}]/n\n\\end{eqnarray}\\] Assuming each data point has identical means; \\(\\mathbb{E}[X_{i}]=\\mu\\), the expected value of the sample average is the mean; \\(\\mathbb{E}\\left[M\\right] = \\sum_{i=1}^{n} \\mu/n = \\mu\\).\nNote that the estimator \\(M\\) differs from the particular estimate you calculated for your sample, \\(\\hat{M}\\). For example, consider flipping a coin three times: \\(M\\) corresponds to a theoretical value before you flip the coins and \\(\\hat{M}\\) corresponds to a specific value after you flip the coins.\n\n\n\n\n\n\nTip\n\n\n\n\n\nFor example, consider a coin flip with Heads \\(X_{i}=1\\) having probability \\(p\\) and Tails \\(X_{i}=0\\) having probability \\(1-p\\). First notice that \\(\\mathbb{E}[X_{i}]=p\\). Then notice we can first \\[\\begin{align*}\n\\mathbb{E}[X_{1}+X_{2}]\n&= [1+1][p \\times p] + [1+0][p \\times (1-p)] + [0+1][(1-p) \\times p] + [0+0][(1-p) \\times (1-p)] &  \\text{``HH + HT + TH + TT''} \\\\\n&= [1][p \\times p] + [1][p \\times (1-p)] + [0][(1-p) \\times p] + [0][(1-p) \\times (1-p)] &  \\text{first outcomes times prob.} \\\\\n&+ [1][p \\times p] + [0][p \\times (1-p)] + [1][(1-p) \\times p] + [0][(1-p) \\times (1-p)] &\n\\text{+second outcomes times prob.} \\\\\n&= [1][p \\times p] + [1][p \\times (1-p)] + [1][p \\times p] + [1][(1-p) \\times p] & \\text{drop zeros}\\\\\n&= 1p (p + [1-p]) +  1p (p + [1-p]) = p+p & \\text{algebra}\\\\\n&= \\mathbb{E}[X_{1}] +  \\mathbb{E}[X_{2}] .\n\\end{align*}\\] The theoretical mean is \\(\\mathbb{E}[\\frac{X_{1}+X_{2}}{2}]=\\frac{p+p}{2}=p\\).\n\n\n\n\n\nVariances.\nAnother famous theoretical result in statistics is that if we have independent and identical data (i.e., that each random variable \\(X_{i}\\) has the same mean \\(\\mu\\) and same variance \\(\\sigma^2\\) and is drawn without any dependence on the previous draws), then the standard error of the sample mean is “root n” proportional to the theoretical standard error. Intuitively, this follows from thinking of the variance as a type of mean (the theoretical mean squared deviation from \\(\\mu\\)). \\[\\begin{eqnarray}\n\\mathbb{V}\\left( M \\right)\n&=& \\mathbb{V}\\left( \\frac{\\sum_{i}^{n} X_{i}}{n} \\right)\n= \\sum_{i}^{n} \\mathbb{V}\\left(\\frac{X_{i}}{n}\\right)\n= \\sum_{i}^{n} \\frac{\\sigma^2}{n^2}\n= \\sigma^2/n\\\\\n\\mathbb{s}\\left(M\\right) &=& \\sqrt{\\mathbb{V}\\left( M \\right) } = \\sqrt{\\sigma^2/n} = \\sigma/\\sqrt{n}.\n\\end{eqnarray}\\]\nNote that the standard deviation refers to variance within a single sample, and is hence different from the standard error. Nonetheless, it can be used to estimate the variability of the mean: we can estimate \\(\\mathbb{s}\\left(M\\right)\\) with \\(\\hat{S}/\\sqrt{n}\\), where \\(\\hat{S}\\) is the standard deviation of the sample. This estimate is often a little different from than the bootstrap estimate, as it is based on idealistic theoretical assumptions whereas the bootstrap estimate is driven by data that are often not ideal.\n\n\nCode\nboot_se &lt;- sd(bootstrap_means)\n\ntheory_se &lt;- sd(sample_dat)/sqrt(n)\n\nc(boot_se, theory_se)\n## [1] 0.6137248 0.8711020\n\n\nAlso note that each additional data point you have provides more information, which ultimately decreases the standard error of your estimates. This is why statisticians will often recommend that you to get more data. However, the improvement in the standard error increases at a diminishing rate. In economics, this is known as diminishing returns and why economists may recommend you do not get more data.\n\n\nCode\nB &lt;- 1000 # number of bootstrap samples\nNseq &lt;- seq(1,100, by=1) # different sample sizes\n\nSE &lt;- vector(length=length(Nseq))\nfor(n in Nseq){\n    sample_statistics_n &lt;- vector(length=B)\n    for(b in seq(sample_statistics_n)){\n        x_b &lt;- rnorm(n) # Sample of size n\n        x_stat_b &lt;- quantile(x_b, probs=.4) # Stat of interest\n        sample_statistics_n[b] &lt;- x_stat_b\n    }\n    se_n &lt;- sd(sample_statistics_n) # How much the stat varies across samples\n    SE[n] &lt;- se_n\n}\n\n\nplot(Nseq, SE, pch=16, col=grey(0,.5),\n    main='Absolute Gain', font.main=1,\n    ylab='standard error', xlab='sample size')\n\n\n\n\n\n\n\n\n\nCode\n\n#plot(Nseq[-1], abs(diff(SE)), pch=16, col=grey(0,.5),\n#    main='Marginal Gain', font.main=1,\n#    ylab='decrease in standard error', xlab='sample size')\n\n\n\n\nShape.\nSometimes, the sampling distribution is approximately normal (according to the CLT). In this case, you can use a standard error and the normal distribution to get a confidence interval.\n\n\nCode\n# Standard Errors\nsd_theory &lt;- sd(sample_dat1)/sqrt(length(sample_dat1))\n## Normal CI\nspread_theory &lt;- qnorm(c(0.025, 0.975))\nci &lt;- mean(sample_dat1) + spread_theory*sd_theory\n\n\n\n\nCI Coverage.\n\n\nAdvanced and Optional\n\n\nIn many cases, we want a \\(Z\\%\\) interval to mean that \\(Z\\%\\) of the intervals we generate will contain the true mean. E.g., when repeatedly sampling \\(M\\), \\(50\\%\\) of constructed confidence intervals are expected to contain the theoretical mean \\(\\mu\\).\nGiven the sample mean, \\(M\\), and the sample size, \\(n\\), is large enough for the mean to be approximately normally distributed, we can construct a symmetric confidence interval \\([M - E, M + E]\\), where \\(E\\) is some “margin of error” on either side of \\(M\\). A coverage level of \\(1-\\alpha\\) means \\(Prob( M - E &lt; \\mu &lt; M + E)=1-\\alpha\\). I.e., if the same sampling procedure were repeated \\(100\\) times from the same population, approximately \\(95\\) of the resulting intervals would be expected to contain the true population mean.1 Note that a \\(95%\\) coverage level does not imply a 95% probability that the true parameter lies within a particular calculated interval. E.g., if you compute \\(\\hat{M}=9\\) for your particular sample, a coverage level of \\(1-\\alpha=95%\\) does not mean \\(Prob(9 - E &lt; \\mu &lt; 9 + E)=95%\\).\n\n\n\n\n\n\nTip\n\n\n\n\n\nGiven the sample mean, \\(M\\), and the sample size, \\(n\\), is large enough for the mean to be approximately normally distributed. What confidence interval satisfies the following: the theoretical mean \\(\\mu\\) is inside of the interval with probability \\((1 - 0.05)%\\) (i.e., for \\((1 - 0.05)%\\) of samples).\n\n\n\nFor a fixed sample size \\(n\\), there is a trade-off between Precision: the width of a confidence interval, Accuracy: the probability that a confidence interval contains the theoretical value.\n\n\nCode\n# Confidence Interval for each sample\nxq &lt;- apply(x_samples, 1, function(r){ #theoretical se's \n    mean(r) + c(-1,1)*sd(r)/sqrt(length(r))\n})\n# First 4 interval estimates\nxq[,1:4]\n##           [,1]      [,2]      [,3]      [,4]\n## [1,] 0.4990344 0.4984030 0.4841576 0.4756585\n## [2,] 0.5171785 0.5164427 0.5020911 0.4938873\n\n# Explicit calculation\nmu_true &lt;- 0.5 # theoretical result for x_samples ~ uniform\n# Logical vector: whether the true mean is in each CI\ncovered &lt;- mu_true &gt;= xq[1, ] & mu_true &lt;= xq[2, ]\n# Empirical coverage rate\ncoverage_rate &lt;- mean(covered)\ncat(sprintf(\"Estimated coverage probability: %.2f%%\\n\", 100 * coverage_rate))\n## Estimated coverage probability: 70.33%\n\n# Theoretically: [-1 sd, +1 sd] has 2/3 coverage\n# Change to [-2 sd, +2 sd] to see Precision-Accuracy tradeoff.\n\n\n\n\nCode\n# Visualize first 100 confidence intervals\nN &lt;- 100\nplot.new()\nplot.window(xlim = range(xq), ylim = c(0, N))\nfor (i in 1:N) {\n  col_i &lt;- if (covered[i]) rgb(0, 0, 0, 0.3) else rgb(1, 0, 0, 0.5)\n  segments(xq[1, i], i, xq[2, i], i, col = col_i, lwd = 2)\n}\nabline(v = mu_true, col = \"blue\", lwd = 2)\naxis(1)\ntitle(\"Visualizing CI Coverage (Red = Missed)\")",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html#further-reading",
    "href": "01_06_Sampling.html#further-reading",
    "title": "6  (Re)Sampling",
    "section": "6.5 Further Reading",
    "text": "6.5 Further Reading\nSee\n\nhttps://www.r-bloggers.com/2025/02/bootstrap-vs-standard-error-confidence-intervals/\n\nSee\n\nhttps://dlsun.github.io/probability/linearity.html",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html#footnotes",
    "href": "01_06_Sampling.html#footnotes",
    "title": "6  (Re)Sampling",
    "section": "",
    "text": "Notice that \\(Prob( M - E &lt; \\mu &lt; M + E) = Prob( - E &lt; \\mu - M &lt; + E) = Prob( E + mu &gt; M &gt; mu - E)\\). So if the interval \\([\\mu - 10, \\mu + 10]\\) contains \\(95%\\) of all \\(M\\), then the interval \\([M-10, M+10]\\) will also contain \\(\\mu\\) \\(95%\\) of the time because whenever \\(M\\) is within \\(10\\) of \\(\\mu\\), \\(\\mu\\) is also within \\(10\\) of \\(M\\).↩︎",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_07_HypothesisTests.html",
    "href": "01_07_HypothesisTests.html",
    "title": "7  Hypothesis Tests",
    "section": "",
    "text": "7.1 Basic Ideas\nIn this section, we test hypotheses using data-driven methods that assume much less about the data generating process. There are two main ways to conduct a hypothesis test to do so: inverting a confidence interval and imposing the null. The first treats the distribution of estimates directly; the second explicitly enforces the null hypothesis to evaluate how unusual the observed statistic is. Both approaches rely on the bootstrap: resampling the data to approximate sampling variability. The most typical case is hypothesizing about about the mean, and the bootstrap idea here is to approximate \\(M-\\mu\\), the difference between the sample mean \\(M\\) and the unknown theoretical mean \\(\\mu\\), with the difference between the bootstrap mean \\(M^{\\text{boot}}\\) and the sample mean, \\(M^{\\text{boot}}-M\\).",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "01_07_HypothesisTests.html#basic-ideas",
    "href": "01_07_HypothesisTests.html#basic-ideas",
    "title": "7  Hypothesis Tests",
    "section": "",
    "text": "Invert a CI.\nOne main way to conduct hypothesis tests is to examine whether a confidence interval contains a hypothesized value. We then use this decision rule\n\nreject the null if value falls outside of the interval\nfail to reject the null if value falls inside of the interval\n\nWe typically use a 95% confidence interval to create a rejection region.\nE.g., suppose you hypothesize the mean is \\(9\\). You then construct a bootstrap distribution with \\(95\\%\\) confidence interval, and find your hypothesized value falls outside of the confidence interval. Then, after accounting for sampling variability (which you estimate), it still seems extremely unlikely that the theoretical mean actually equals \\(9\\), so you reject that that hypothesis. (If the theoretical value landed in the interval, you would “fail to reject” the theoretical mean equals \\(9\\).)\n\n\nCode\nsample_dat &lt;- USArrests[,'Murder']\nsample_mean &lt;- mean(sample_dat)\n\nset.seed(1) # to be replicable\nbootstrap_means &lt;- vector(length=999)\nfor(b in seq_along(bootstrap_means)){\n    dat_b &lt;- sample(sample_dat, replace=T) \n    mean_b &lt;- mean(dat_b)\n    bootstrap_means[b] &lt;- mean_b\n}\nhist(bootstrap_means, breaks=25,\n    border=NA,\n    main='',\n    xlab='Bootstrap Samples')\n# CI\nci_95 &lt;- quantile(bootstrap_means, probs=c(.025, .975))\nabline(v=ci_95, lwd=2)\n# H0: mean=9\nabline(v=9, col=2, lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nImpose the Null.\nWe can also compute a null distribution: the sampling distribution of the statistic under the null hypothesis (assuming your null hypothesis was true). We use the bootstrap to loop through a large number of “resamples”. In each iteration of the loop, we impose the null hypothesis and re-estimate the statistic of interest. We then calculate the range of the statistic across all resamples and compare how extreme the original value we observed is.\nE.g., suppose you hypothesize the mean is \\(9\\). You then construct a 95% confidence interval around the null bootstrap distribution (resamples centered around \\(9\\)). If your sample mean falls outside of that interval, then even after accounting for sampling variability (which you estimate), it seems extremely unlikely that the theoretical mean actually equals \\(9\\), so you reject that that hypothesis. (If the sample mean landed in the interval, you would “fail to reject” the theoretical mean equals \\(9\\).)\n\n\nCode\nsample_dat &lt;- USArrests[,'Murder']\nsample_mean &lt;- mean(sample_dat)\n\n# Bootstrap NULL: mean=9\n# Bootstrap shift: center each bootstrap resample so that the distribution satisfies the null hypothesis on average.\nset.seed(1)\nmu &lt;- 9\nbootstrap_means_null &lt;- vector(length=999)\nfor(b in seq_along(bootstrap_means_null)){\n    dat_b &lt;- sample(sample_dat, replace=T) \n    mean_b &lt;- mean(dat_b) + (mu - sample_mean) # impose the null via Bootstrap shift\n    bootstrap_means_null[b] &lt;- mean_b\n}\nhist(bootstrap_means_null, breaks=25, border=NA,\n    main='',\n    xlab='Null Bootstrap Samples')\nci_95 &lt;- quantile(bootstrap_means_null, probs=c(.025, .975)) # critical region\nabline(v=ci_95, lwd=2)\nabline(v=sample_mean, lwd=2, col=4)",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "01_07_HypothesisTests.html#p-values",
    "href": "01_07_HypothesisTests.html#p-values",
    "title": "7  Hypothesis Tests",
    "section": "7.2 p-values",
    "text": "7.2 p-values\nA p-value is the frequency you see something as extreme as your statistic when sampling from the null distribution. There are three tests associated with p-values: the two-sided test (observed statistic is either extremely high or low) or one of the one-sided tests (observed statistic is extremely low, observed statistic is extremely high).\nFor a concrete example, consider whether the mean statistic, \\(M\\), is centered on a theoretical value of \\(\\mu=9\\) for the population. If your null hypothesis is that the theoretical mean is eight, \\(H_{0}: \\mu =9\\), and you calculated the mean for your sample as \\(\\hat{M}\\), then you can consider any one of these three alternative hypotheses:\n\n\\(H_{A​}: \\mu &gt; 9\\), a right-tail test, \\(Prob( M &gt; \\hat{M} \\mid \\mu = 9 )\\).\n\\(H_{A}: \\mu &lt; 9\\), a left-tail test, \\(Prob( M &lt; \\hat{M} \\mid \\mu = 9 )\\).\n\\(H_{A}​: \\mu \\neq 9\\), a two-tail test, depicted in the previous section.\n\nA one-sided test is straightforward to implement via a bootstrap null distribution. For a left-tail test, we examine \\[\\begin{eqnarray}\nProb( M &lt; \\hat{M} \\mid \\mu = 9 )\n    &\\approx& Prob( M^{\\text{boot}} &lt; \\hat{M} \\mid  \\mu^{\\text{boot}} = 9 ) = \\hat{F}^{\\text{boot}}_{0}(\\hat{M}),\n\\end{eqnarray}\\] where \\(\\hat{F}^{\\text{boot}}_{0}\\) is the ECDF of the bootstrap null distribution. For a right-tail test, we examine \\(Prob( M &gt; \\hat{M} \\mid \\mu = 9 ) \\approx 1-\\hat{F}^{\\text{boot}}_{0}(\\hat{M})\\).\n\n\nCode\n# One-Sided Test, ALTERNATIVE: mean &gt; 9\npar(mfrow=c(1,2))\n# Visualize One Sided Prob. & reject region boundary\nhist(bootstrap_means_null, border=NA,\n    freq=F, main=NA, xlab='Null Bootstrap')\nabline(v=sample_mean, col=4)\n# Equivalent Visualization\nFhat0 &lt;- ecdf(bootstrap_means_null) # Look at right tail\nplot(Fhat0,\n    main='',\n    xlab='Null Bootstrap')\nabline(v=sample_mean, col=4)\n\n\n\n\n\n\n\n\n\nCode\n\n# Numerically Compute Two Sided Probability\np1 &lt;- 1- Fhat0(sample_mean) #Compute right Tail\np1\n## [1] 0.986987\n\n\nA two sided test is slightly more complicated to compute. We want the probability mass in both tails, for the random variable \\(M\\) that is at least as far from the null mean of \\(9\\) as our observed sample mean \\(\\hat{M}\\). \\[\\begin{eqnarray}\nProb( |M - \\mu| \\geq |\\hat{M} - \\mu| \\mid \\mu = 9 )\n&\\approx& Prob( |M^{\\text{boot}}- \\mu^{\\text{boot}}| \\geq |\\hat{M}- \\mu^{\\text{boot}}|  \\mid \\mu^{\\text{boot}} = 9) \\\\\n&=& 1-\\hat{F}^{|\\text{boot}|}_{0}(|\\hat{M}-9|),\n\\end{eqnarray}\\] where \\(\\hat{F}^{|\\text{boot}|}_{0}\\) is the ECDF of \\(|M^{\\text{boot}}- \\mu^{\\text{boot}}|\\).\n\n\nCode\n# Two-Sided Test, ALTERNATIVE: mean &lt; 9 or mean &gt;9\nmu &lt;- 9\n# Visualize Two Sided Prob. & reject region boundary\npar(mfrow=c(1,2))\nhist(abs(bootstrap_means_null-mu),\n    freq=F, breaks=20,\n    border=NA, main='', xlab='Null Bootstrap')\nabline(v=abs(sample_mean-mu), col=4)\n\n# Equivalent Visualization\nFhat_abs0 &lt;- ecdf( abs(bootstrap_means_null-mu) )\nplot(Fhat_abs0,\n    main='',\n    xlab='Null Bootstrap')\nabline(v=abs(sample_mean-mu), col=4)\n\n\n\n\n\n\n\n\n\nCode\n\n\n# Numerically Compute Two Sided Probability\np2 &lt;- 1 - Fhat_abs0( abs(sample_mean-mu) )\np2\n## [1] 0.03303303\n\n\n\nStatistical significance.\nOften, one may see or hear “p&lt;.05: statistically significant” and “p&gt;.05: not statistically significant”. That is decision making on purely statistical grounds, and it may or may not be suitable for your context. You simply need to know that whoever says those things is using \\(5\\%\\) as a critical value to reject an alternative hypothesis.\n\n\nCode\n# Purely-Statistical Decision Making Examples.\n\n# One Sided Test\nif(p1 &gt;.05){\n    print('fail to reject the null that sample_mean=9, at the 5% level')\n} else {\n    print('reject the null that sample_mean=9 in favor of &gt;9, at the 5% level')\n}\n## [1] \"fail to reject the null that sample_mean=9, at the 5% level\"\n\n# Two Sided Test\nif(p2 &gt;.05){\n    print('fail to reject the null that sample_mean=9, at the 5% level')\n} else {\n    print('reject the null that sample_mean=9 in favor of either &lt;9 or &gt;9, at the 5% level')\n}\n## [1] \"reject the null that sample_mean=9 in favor of either &lt;9 or &gt;9, at the 5% level\"\n\n\nBeware that a common misreading of the p-value as “the probability the null is true”. That is false.\n\n\nCaveat.\nAlso note that the p-value is itself a function of data, and hence a random variable that changes from sample to sample. Given that the \\(5\\%\\) level is somewhat arbitrary, and that the p-value both varies from sample to sample and is often misunderstood, it makes sense to give p-values a limited role in decision making.\n\n\nCode\np_values &lt;- vector(length=300)\nfor(b2 in seq(p_values)){\n    bootstrap_means_null &lt;- vector(length=999)\n    for(b in seq_along(bootstrap_means_null)){\n        dat_b &lt;- sample(sample_dat, replace=T) \n        mean_b &lt;- mean(dat_b) + (mu - sample_mean) # impose the null\n        bootstrap_means_null[b] &lt;- mean_b\n    }\n    Fhat_abs0 &lt;- ecdf( abs(bootstrap_means_null-mu) )\n    p2 &lt;- 1- Fhat_abs0( abs(sample_mean-mu) )\n    p_values[b2] &lt;- p2\n}\n\nhist(p_values, freq=F,\n    border=NA, main='')",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "01_07_HypothesisTests.html#other-statistics",
    "href": "01_07_HypothesisTests.html#other-statistics",
    "title": "7  Hypothesis Tests",
    "section": "7.3 Other Statistics",
    "text": "7.3 Other Statistics\n\nt-values.\nA t-value standardizes the approach for hypothesis tests of the mean. For any specific sample, we compute the estimate \\[\\begin{eqnarray}\n\\hat{t}=(\\hat{M}-\\mu)/\\hat{S},\n\\end{eqnarray}\\] which corresponds to the estimator \\(t = (M - \\mu) / \\mathbb{s}(M)\\), which varies from sample to sample.\n\n\nCode\n# t statistic estimate\njackknife_means &lt;- vector(length=length(sample_dat))\nfor(i in seq_along(jackknife_means)){\n    jackknife_means[i] &lt;- mean(dat_b[-i])\n}\nmu &lt;- 9\nsample_t &lt;- (sample_mean - mu)/sd(jackknife_means)\n\n\nThere are several benefits to this:\n\nuses the same statistic for different hypothesis tests\nmakes the statistic comparable across different studies\nremoves dependence on unknown parameters by normalizing with a standard error\nmakes the null distribution theoretically known asymptotically (approximately)\n\nFor the first point, notice that the recentering adjustment affects two-sided tests (because they depend on distance from the null mean) but not one-sided tests (because adding a constant does not change rank order).\n\n\nCode\nset.seed(1)\nbootstrap_means_null &lt;- vector(length=999)\nfor(b in seq_along(bootstrap_means_null)){\n    dat_b &lt;- sample(sample_dat, replace=T) \n    mean_b &lt;- mean(dat_b) + (mu - sample_mean) # impose the null\n    bootstrap_means_null[b] &lt;- mean_b\n}\n\n# See that the \"recentering\" matters for two-sided tests\necdf( abs(bootstrap_means_null-mu) )( abs(sample_mean-mu) )\n## [1] 0.966967\necdf( abs(bootstrap_means_null) )( abs(sample_mean) )\n## [1] 0.01301301\n\n# See that the \"recentering\" doesn't matter for one-sided ones\necdf( bootstrap_means_null-mu)( sample_mean-mu)\n## [1] 0.01301301\necdf( bootstrap_means_null )( sample_mean)\n## [1] 0.01301301\n\n\nThe last point implies we are typically dealing with a normal distribution that is well-studied, or another well-studied distribution derived from it.1\n\n\nCode\n# Boostrap Null Distribution\nbootstrap_t_null &lt;- vector(length=999)\nfor(b in seq_along(bootstrap_t_null)){\n    dat_b &lt;- sample(sample_dat, replace=T) \n    mean_b &lt;- mean(dat_b) + (mu - sample_mean) # impose the null by recentering\n    # Compute t-stat using jackknife ses (same as above)\n    jackknife_means_b &lt;- vector(length=length(dat_b))\n    for(i in seq_along(jackknife_means_b)){\n        jackknife_means_b[i] &lt;- mean(dat_b[-i])\n    }\n    jackknife_se_b &lt;- sd( jackknife_means_b )\n    jackknife_t_b &lt;- (mean_b - mu)/jackknife_se_b\n    bootstrap_t_null[b] &lt;- jackknife_t_b\n}\n\n# Two Sided Test\nFhat0 &lt;- ecdf(abs(bootstrap_t_null))\nplot(Fhat0, \n    xlim=range(bootstrap_t_null, sample_t),\n    xlab='Null Bootstrap Distribution for |t|',\n    main='')\nabline(v=abs(sample_t), col=4)\n\n\n\n\n\n\n\n\n\nCode\np &lt;- 1 - Fhat0( abs(sample_t) ) \np\n## [1] 0.04204204\n\n\n\n\nQuantiles and Shape Statistics.\nBootstrap allows hypothesis tests for any statistic, not just the mean, without relying on parametric theory. For example, the above procedures generalize from differences in means to statistics like medians and other quantiles.\n\n\nCode\n# Test for Median Differences (Impose the Null)\n# Bootstrap Null Distribution for the median\n# Each Bootstrap shifts medians so that median = q_null\n\nq_obs &lt;- quantile(sample_dat, probs=.5)\nq_null &lt;- 7.8\nbootstrap_quantile_null &lt;- vector(length=999)\nfor(b in seq_along(bootstrap_quantile_null)){\n    x_b &lt;- sample(sample_dat, replace=T) #bootstrap sample\n    q_b &lt;- quantile(x_b, probs=.5) # median\n    d_b &lt;- q_b - (q_obs-q_null) #impose the null\n    bootstrap_quantile_null[b] &lt;- d_b \n}\n\n# 2-Sided Test for Median Difference\nhist(bootstrap_quantile_null-q_null, \n    border=NA, freq=F, xlab='Null Bootstrap',\n    font.main=1, main='Medians (Impose Null)')\nmedian_ci &lt;- quantile(bootstrap_quantile_null-q_null, probs=c(.025, .975))\nabline(v=median_ci, lwd=2)\nabline(v=q_obs-q_null, lwd=2, col=4)\n\n\n\n\n\n\n\n\n\nCode\n\n# 2-Sided Test for Median Difference\n## Null: No Median Difference\n1 - ecdf( abs(bootstrap_quantile_null-q_null))( abs(q_obs-q_null) ) \n## [1] 0.5485485\n\n\nThe above procedure generalizes to differences in many other statistics. Perhaps the most informative are differences in shape. E.g., you can test for differences in spread, skew, or kurtosis.\n\n\nCode\n# Test for SD Differences (Invert CI)\nsd_obs &lt;- sd(sample_dat)\nsd_null &lt;- 3.6\nbootstrap_sd &lt;- vector(length=999)\nfor(b in seq_along(bootstrap_sd)){\n    x_b &lt;- sample(sample_dat, replace=T)\n    sd_b &lt;- sd(x_b)\n    bootstrap_sd[b] &lt;- sd_b\n}\n\nhist(bootstrap_sd, freq=F,\n    border=NA, xlab='Bootstrap', font.main=1,\n    main='Standard Deviations (Invert CI)')\nsd_ci &lt;- quantile(bootstrap_sd, probs=c(0.25,.975) )\nabline(v=sd_ci, lwd=2)\nabline(v=sd_null, lwd=2, col=2)\n\n\n\n\n\n\n\n\n\nCode\n\n\n# Try any function!\n# IQR(x_b)/median(x_b)",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "01_07_HypothesisTests.html#further-reading",
    "href": "01_07_HypothesisTests.html#further-reading",
    "title": "7  Hypothesis Tests",
    "section": "7.4 Further Reading",
    "text": "7.4 Further Reading\n\nhttps://learningstatisticswithr.com/book/hypothesistesting.html\nhttps://okanbulut.github.io/rbook/part5.html",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "01_07_HypothesisTests.html#footnotes",
    "href": "01_07_HypothesisTests.html#footnotes",
    "title": "7  Hypothesis Tests",
    "section": "",
    "text": "In another statistics class, you will learn the math behind the null t-distribution. In this class, we skip this because we can simply bootstrap the t-statistic too.↩︎",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "01_08_MiscTopics.html",
    "href": "01_08_MiscTopics.html",
    "title": "8  Misc. Univariate Topics",
    "section": "",
    "text": "8.1 Functions for Data Analysis",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Misc. Univariate Topics</span>"
    ]
  },
  {
    "objectID": "01_08_MiscTopics.html#functions-for-data-analysis",
    "href": "01_08_MiscTopics.html#functions-for-data-analysis",
    "title": "8  Misc. Univariate Topics",
    "section": "",
    "text": "Data Handling.\nYou can find a value by a particular criterion\n\n\nCode\ny &lt;- 1:10\n\n# Return Y-value with minimum absolute difference from 3\nabs_diff_y &lt;- abs( y - 3 ) \nabs_diff_y # is this the luckiest number?\n##  [1] 2 1 0 1 2 3 4 5 6 7\n\nmin(abs_diff_y)\n## [1] 0\nwhich.min(abs_diff_y)\n## [1] 3\ny[ which.min(abs_diff_y) ]\n## [1] 3\n\n\nThere are also some useful built in functions for standardizing data\n\n\nCode\nm &lt;- matrix(c(1:3,2*(1:3)),byrow=TRUE,ncol=3)\nm\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    2    4    6\n\n# normalize rows\nm/rowSums(m)\n##           [,1]      [,2] [,3]\n## [1,] 0.1666667 0.3333333  0.5\n## [2,] 0.1666667 0.3333333  0.5\n\n# normalize columns\nt(t(m)/colSums(m))\n##           [,1]      [,2]      [,3]\n## [1,] 0.3333333 0.3333333 0.3333333\n## [2,] 0.6666667 0.6666667 0.6666667\n\n# de-mean rows\nsweep(m,1,rowMeans(m), '-')\n##      [,1] [,2] [,3]\n## [1,]   -1    0    1\n## [2,]   -2    0    2\n\n# de-mean columns\nsweep(m,2,colMeans(m), '-')\n##      [,1] [,2] [,3]\n## [1,] -0.5   -1 -1.5\n## [2,]  0.5    1  1.5\n\n\n\n\nBinning and Aggregating.\n\n\nCode\nx &lt;- 1:10\ncut(x, 4)\n##  [1] (0.991,3.25] (0.991,3.25] (0.991,3.25] (3.25,5.5]   (3.25,5.5]  \n##  [6] (5.5,7.75]   (5.5,7.75]   (7.75,10]    (7.75,10]    (7.75,10]   \n## Levels: (0.991,3.25] (3.25,5.5] (5.5,7.75] (7.75,10]\nsplit(x, cut(x, 4))\n## $`(0.991,3.25]`\n## [1] 1 2 3\n## \n## $`(3.25,5.5]`\n## [1] 4 5\n## \n## $`(5.5,7.75]`\n## [1] 6 7\n## \n## $`(7.75,10]`\n## [1]  8  9 10\n\n\n\n\nCode\nxs &lt;- split(x, cut(x, 4))\nsapply(xs, mean)\n## (0.991,3.25]   (3.25,5.5]   (5.5,7.75]    (7.75,10] \n##          2.0          4.5          6.5          9.0\n\n# shortcut\naggregate(x, list(cut(x,4)), mean)\n##        Group.1   x\n## 1 (0.991,3.25] 2.0\n## 2   (3.25,5.5] 4.5\n## 3   (5.5,7.75] 6.5\n## 4    (7.75,10] 9.0\n\n\nSee also https://bookdown.org/rwnahhas/IntroToR/logical.html",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Misc. Univariate Topics</span>"
    ]
  },
  {
    "objectID": "01_08_MiscTopics.html#transformations",
    "href": "01_08_MiscTopics.html#transformations",
    "title": "8  Misc. Univariate Topics",
    "section": "8.2 Transformations",
    "text": "8.2 Transformations\nTransformations can stabilize variance, reduce skewness, and make model errors closer to Gaussian.\nPerhaps the most common examples are power transformations: \\(y= x^\\lambda\\), which includes \\(\\sqrt{x}\\) and \\(x^2\\).\nOther examples include the exponential transformation: \\(y=exp(x)\\) for any \\(x\\in (-\\infty, \\infty)\\) and logarithmic transformation: \\(y=\\log(x)\\) for any \\(x&gt;0\\).\nThe Box–Cox Transform nests many cases. For \\(x&gt;0\\) and parameter \\(\\lambda\\), \\[\\begin{eqnarray}\ny=\\begin{cases}\n\\dfrac{x^\\lambda-1}{\\lambda}, & \\lambda\\neq 0,\\\\\n\\log x, & \\lambda=0.\n\\end{cases}\n\\end{eqnarray}\\] This function is continuous over \\(\\lambda\\).\n\n\nCode\n# Box–Cox transform and inverse\nbc_transform &lt;- function(x, lambda) {\n  if (any(x &lt;= 0)) stop(\"Box-Cox requires x &gt; 0\")\n  if (abs(lambda) &lt; 1e-8) log(x) else (x^lambda - 1)/lambda\n}\nbc_inverse &lt;- function(t, lambda) {\n  if (abs(lambda) &lt; 1e-8) exp(t) else (lambda*t + 1)^(1/lambda)\n}\n\nX &lt;- USArrests$Murder\nhist(X, main='', border=NA, freq=F)\n\n\n\n\n\n\n\n\n\nCode\n\npar(mfrow=c(1,3))\nfor(lambda in c(-1,0,1)){\n    Y &lt;- bc_transform(X, lambda)\n    hist(Y, \n        main=bquote(paste(lambda,'=',.(lambda))),\n        border=NA, freq=F)\n}\n\n\n\n\n\n\n\n\n\n\nLaw of the Unconscious Statistician (LOTUS).\nAs before, we will represent the random variable as \\(X_{i}\\), which can take on values \\(x\\) from the sample space. If \\(X_{i}\\) is a discrete random variable (a random variable with a discrete sample space) and \\(g\\) is a function, then \\(\\mathbb E[g(X)] = \\sum_x g(x)Prob(X_{i}=x)\\).\n\n\n\n\n\n\nNote\n\n\n\n\n\nLet \\(X_{i}\\) take values \\(\\{1,2,3\\}\\) with \\[\nPr(X_{i}=1)=0.2,\\quad Prob(X_{i}=2)=0.5,\\quad Prob(X_{i}=3)=0.3.\n\\] Let \\(g(x)=x^2+1\\). Then \\(g(1)=1^2+1=2\\), \\(g(2)=2^2+1=5\\), \\(g(3)=3^2+1=10\\).\nThen, by LOTUS, \\[\\begin{eqnarray}\n\\mathbb E[g(X_{i})]=\\sum_x g(x)Prob(X_{i}=x)\n&=& g(1)\\cdot 0.2 + g(2)\\cdot 0.5 + g(3)\\cdot 0.3 \\\\\n&=& 2 \\cdot 0.2 + 5 \\cdot 0.5 + 10 \\cdot 0.3 \\\\\n&=& 0.4 + 2.5 + 3 = 5.9.\n\\end{eqnarray}\\]\n\n\nCode\nx  &lt;- c(1,2,3)\nx_probs &lt;- c(0.2,0.5,0.3)\ng  &lt;- function(x) x^2 + 1\nsum(g(x) * x_probs) \n## [1] 5.9\n\n\n\n\n\n\n\nCode\ng  &lt;- function(x) x^2 + 1\n\n# A theoretical example\nx &lt;- c(1,2,3,4)\nx_probs &lt;- c(1/4, 1/4, 1/4, 1/4)\nsum(g(x) * x_probs) \n## [1] 8.5\n\n# A simulation example\nX &lt;- sample(x, x_probs, size=1000, replace=T)\nmean(g(X))\n## [1] 8.369\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nIf \\(X_{i}\\) is a continuous random variable (a random variable with a continuous sample space) and \\(g\\) is a function, then \\(\\mathbb E[g(X_{i})] = \\int_{-\\infty}^{\\infty} g(x)f(x) dx\\).\n\n\nCode\nx &lt;- rexp(5e5, rate = 1)           # X ~ Exp(1)\nmean(sqrt(x))                      # LOTUS Simulation\n## [1] 0.8860904\nsqrt(pi) / 2                       # Exact via LOTUS integral\n## [1] 0.8862269\n\n\n\n\n\nNote that you have already seen the special case where \\(g(X_{i})=\\left(X_{i}-\\mathbb{E}[X_{i}]\\right)^2\\).\n\n\nJensen’s inequality.\nConcave functions curve inwards, like the inside of a cave. Convex functions curve outward, the opposite of concave.\nIf \\(g\\) is a concave function, then \\(g(\\mathbb E[X_{i}]) \\geq \\mathbb E[g(X_{i})]\\).\n\n\nCode\n# Continuous Example 1\nmean( sqrt(x) )\n## [1] 0.8860904\nsqrt( mean(x) ) \n## [1] 0.9998538\n\n# Continuous Example 2\nmean( log(x) )\n## [1] -0.5774324\nlog( mean(x) ) \n## [1] -0.0002925044\n\n\n\n\nCode\n## Discrete Example\nx  &lt;- c(1,2,3)\npx &lt;- c(0.2,0.5,0.3)\nEX &lt;- sum(x * px)\nEX\n## [1] 2.1\n\ng  &lt;- sqrt\ngEX &lt;- g(EX)\nEgX &lt;- sum(g(x) * px)\nc(gEX, EgX)\n## [1] 1.449138 1.426722\n\n\nIf \\(g\\) is a convex function, then the inequality reverses: \\(g(\\mathbb E[X_{i}]) \\leq \\mathbb E[g(X_{i})]\\).\n\n\nCode\nmean( exp(x) )\n## [1] 10.06429\nexp( mean(x) )  \n## [1] 7.389056",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Misc. Univariate Topics</span>"
    ]
  },
  {
    "objectID": "01_08_MiscTopics.html#drawing-samples",
    "href": "01_08_MiscTopics.html#drawing-samples",
    "title": "8  Misc. Univariate Topics",
    "section": "8.3 Drawing Samples",
    "text": "8.3 Drawing Samples\nTo generate a random variable from known distributions, you can use some type of physical machine. E.g., you can roll a fair die to generate Discrete Uniform data or you can roll weighted die to generate Categorical data.\nThere are also several ways to computationally generate random variables from a probability distribution. Perhaps the most common one is “inverse sampling”. To generate a random variable using inverse sampling, first sample \\(p\\) from a uniform distribution and then find the associated quantile quantile function \\(\\widehat{F}^{-1}(p)\\).1\n\nUsing Data.\nYou can generate a random variable from a known empirical distribution. Inverse sampling randomly selects observations from the dataset with equal probabilities. To implement this, we\n\norder the data and associate each observation with an ECDF value\ndraw \\(p \\in [0,1]\\) as a uniform random variable\nfind the associated data point on the ECDF\n\n\n\nCode\n# Empirical Distribution\nX &lt;- USArrests$Murder\nFX_hat &lt;- ecdf(X)\nplot(FX_hat, lwd=2, xlim=c(0,20),\n    pch=16, col=grey(0,.5), main='')\n\n\n\n\n\n\n\n\n\nCode\n\n# Generating a random variable\np &lt;- runif(3000) ## Multiple Draws\nQX_hat &lt;- quantile(X, p, type=1)\nQX_hat[1:5]\n## 15.08271161% 29.10842544% 47.01485662% 25.34648341% 47.24773357% \n##          2.7          4.4          6.8          4.0          6.8\n\n\n\n\nUsing Math.\nIf you know the distribution function that generates the data, then you can derive the quantile function and do inverse sampling. Here is an in-depth example of the Dagum distribution. The distribution function is \\(F(x)=(1+(x/b)^{-a})^{-c}\\). For a given probability \\(p\\), we can then solve for the quantile as \\(F^{-1}(p)=\\frac{ b p^{\\frac{1}{ac}} }{(1-p^{1/c})^{1/a}}\\). Afterwhich, we sample \\(p\\) from a uniform distribution and then find the associated quantile.\n\n\nCode\n# Theoretical Quantile Function (from VGAM::qdagum)\nqdagum &lt;- function(p, scale.b=1, shape1.a, shape2.c) {\n  # Quantile function (theoretically derived from the CDF)\n  ans &lt;- scale.b * (expm1(-log(p) / shape2.c))^(-1 / shape1.a)\n  # Special known cases\n  ans[p == 0] &lt;- 0\n  ans[p == 1] &lt;- Inf\n  # Safety Checks\n  ans[p &lt; 0] &lt;- NaN\n  ans[p &gt; 1] &lt;- NaN\n  if(scale.b &lt;= 0 | shape1.a &lt;= 0 | shape2.c &lt;= 0){ ans &lt;- ans*NaN }\n  # Return\n  return(ans)\n}\n\n# Generate Random Variables (VGAM::rdagum)\nrdagum &lt;-function(n, scale.b=1, shape1.a, shape2.c){\n    p &lt;- runif(n) # generate random probabilities\n    x &lt;- qdagum(p, scale.b=scale.b, shape1.a=shape1.a, shape2.c=shape2.c) #find the inverses\n    return(x)\n}\n\n# Example\nset.seed(123)\nX &lt;- rdagum(3000,1,3,1)\nX[1:5]\n## [1] 0.7390476 1.5499868 0.8845006 1.9616251 2.5091656",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Misc. Univariate Topics</span>"
    ]
  },
  {
    "objectID": "01_08_MiscTopics.html#footnotes",
    "href": "01_08_MiscTopics.html#footnotes",
    "title": "8  Misc. Univariate Topics",
    "section": "",
    "text": "Drawing random uniform samples with computers is actually quite complex and beyond the scope of this course.↩︎",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Misc. Univariate Topics</span>"
    ]
  },
  {
    "objectID": "02-00-BivariateData.html",
    "href": "02-00-BivariateData.html",
    "title": "Bivariate Data",
    "section": "",
    "text": "This section introduces basic bivariate statistics, building on the approach in the previous part.",
    "crumbs": [
      "Bivariate Data"
    ]
  },
  {
    "objectID": "02_01_BivariateDistributions.html",
    "href": "02_01_BivariateDistributions.html",
    "title": "9  Bivariate Distributions",
    "section": "",
    "text": "9.1 Types of Distributions\nWe will now study two variables. The data for each observation data can be grouped together as a vector \\((\\hat{X}_{i}, \\hat{Y}_{i})\\).\nThe vector \\((\\hat{X}_{i}, \\hat{Y}_{i})\\) has a joint distribution that describes the relationship between \\(\\hat{X}_{i}\\) and \\(\\hat{Y}_{i}\\).",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate Distributions</span>"
    ]
  },
  {
    "objectID": "02_01_BivariateDistributions.html#types-of-distributions",
    "href": "02_01_BivariateDistributions.html#types-of-distributions",
    "title": "9  Bivariate Distributions",
    "section": "",
    "text": "Joint Distributions.\nScatterplots are used frequently to summarize the joint relationship between two variables, multiple observations of \\((\\hat{X}_{i}, \\hat{Y}_{i})\\). They can be enhanced in several ways. As a default, use semi-transparent points so as not to hide any points (and perhaps see if your observations are concentrated anywhere). You can also add other features that help summarize the relationship, although I will defer this until later.\n\n\nCode\nplot(Murder~UrbanPop, USArrests, pch=16, col=grey(0.,.5))\n\n\n\n\n\n\n\n\n\nIf you have many points, you can also use a 2D histogram instead. https://plotly.com/r/2D-Histogram/.\n\n\nCode\nlibrary(plotly)\nfig &lt;- plot_ly(\n    USArrests, x = ~UrbanPop, y = ~Assault)\nfig &lt;- add_histogram2d(fig, nbinsx=25, nbinsy=25)\nfig\n\n\n\n\nMarginal Distributions.\nYou can also show the distributions of each variable along each axis.\n\n\nCode\n# Setup Plot\nlayout( matrix(c(2,0,1,3), ncol=2, byrow=TRUE),\n    widths=c(9/10,1/10), heights=c(1/10,9/10))\n\n# Scatterplot\npar(mar=c(4,4,1,1))\nplot(Murder~UrbanPop, USArrests, pch=16, col=rgb(0,0,0,.5))\n\n# Add Marginals\npar(mar=c(0,4,1,1))\nxhist &lt;- hist(USArrests[,'UrbanPop'], plot=FALSE)\nbarplot(xhist[['counts']], axes=FALSE, space=0, border=NA)\n\npar(mar=c(4,0,1,1))\nyhist &lt;- hist(USArrests[,'Murder'], plot=FALSE)\nbarplot(yhist[['counts']], axes=FALSE, space=0, horiz=TRUE, border=NA)\n\n\n\n\n\n\n\n\n\n\n\nConditional Distributions.\nWe can show how distributions and densities change according to a second (or even third) variable using data splits. E.g.,\n\n\nCode\n# Tailored Histogram \nylim &lt;- c(0,8)\nxbks &lt;-  seq(min(USArrests[,'Murder'])-1, max(USArrests[,'Murder'])+1, by=1)\n\n# Also show more information\n# Split Data by Urban Population above/below mean\npop_mean &lt;- mean(USArrests[,'UrbanPop'])\npop_cut &lt;- USArrests[,'UrbanPop']&lt; pop_mean\nmurder_lowpop &lt;- USArrests[pop_cut,'Murder']\nmurder_highpop &lt;- USArrests[!pop_cut,'Murder']\ncols &lt;- c(low=rgb(0,0,1,.75), high=rgb(1,0,0,.75))\n\npar(mfrow=c(1,2))\nhist(murder_lowpop,\n    breaks=xbks, col=cols[1],\n    main='Urban Pop &gt;= Mean', font.main=1,\n    xlab='Murder Arrests',\n    border=NA, ylim=ylim)\n\nhist(murder_highpop,\n    breaks=xbks, col=cols[2],\n    main='Urban Pop &lt; Mean', font.main=1,\n    xlab='Murder Arrests',\n    border=NA, ylim=ylim)\n\n\n\n\n\n\n\n\n\nIt is sometimes it is preferable to show the ECDF instead. And you can glue various combinations together to convey more information all at once\n\n\nCode\npar(mfrow=c(1,2))\n# Full Sample Density\nhist(USArrests[,'Murder'], \n    main='Density Function Estimate', font.main=1,\n    xlab='Murder Arrests',\n    breaks=xbks, freq=F, border=NA)\n\n# Split Sample Distribution Comparison\nF_lowpop &lt;- ecdf(murder_lowpop)\nplot(F_lowpop, col=cols[1],\n    pch=16, xlab='Murder Arrests',\n    main='Distribution Function Estimates',\n    font.main=1, bty='n')\nF_highpop &lt;- ecdf(murder_highpop)\nplot(F_highpop, add=T, col=cols[2], pch=16)\n\nlegend('bottomright', col=cols,\n    pch=16, bty='n', inset=c(0,.1),\n    title='% Urban Pop.',\n    legend=c('Low (&lt;= Mean)','High (&gt;= Mean)'))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Simple Interactive Scatter Plot\n# plot(Assault~UrbanPop, USArrests, col=grey(0,.5), pch=16,\n#    cex=USArrests[,'Murder']/diff(range(USArrests[,'Murder']))*2,\n#    main='US Murder arrests (per 100,000)')\n\n\nYou can also split data into grouped boxplots in the same way\n\n\nCode\nlayout( t(c(1,2,2)))\nboxplot(USArrests[,'Murder'], main='',\n    xlab='All Data', ylab='Murder Arrests')\n\n# K Groups with even spacing\nK &lt;- 3\nUSArrests[,'UrbanPop_Kcut'] &lt;- cut(USArrests[,'UrbanPop'],K)\nKcols &lt;- hcl.colors(K,alpha=.5)\nboxplot(Murder~UrbanPop_Kcut, USArrests,\n    main='', col=Kcols,\n    xlab='Urban Population', ylab='')\n\n\n\n\n\n\n\n\n\nCode\n\n# 4 Groups with equal numbers of observations\n#Qcuts &lt;- c(\n#    '0%'=min(USArrests[,'UrbanPop'])-10*.Machine[['double.eps']],\n#    quantile(USArrests[,'UrbanPop'], probs=c(.25,.5,.75,1)))\n#USArrests[,'UrbanPop']_cut &lt;- cut(USArrests[,'UrbanPop'], Qcuts)\n#boxplot(Murder~UrbanPop_cut, USArrests, col=hcl.colors(4,alpha=.5))",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate Distributions</span>"
    ]
  },
  {
    "objectID": "02_01_BivariateDistributions.html#probability-theory",
    "href": "02_01_BivariateDistributions.html#probability-theory",
    "title": "9  Bivariate Distributions",
    "section": "9.2 Probability Theory",
    "text": "9.2 Probability Theory\nWe now consider a bivariate random vector \\((X_{i}, Y_{i})\\), which is a theoretical version of the bivariate observations \\((\\hat{X}_{i}, \\hat{Y}_{i})\\). E.g., If we are going to flip two coins, then \\((X_{i}, Y_{i})\\) corresponds to the unflipped coins and \\((\\hat{X}_{i}, \\hat{Y}_{i})\\) corresponds to concrete values after they are flipped.\n\nDefinitions for Discrete Data.\nThe joint distribution is defined as \\[\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y)\n\\end{eqnarray}\\] Variables are statistically independent if \\(Prob(X_{i} = x, Y_{i} = y)= Prob(X_{i} = x) Prob(Y_{i} = y)\\) for all \\(x, y\\). Independence is sometimes assumed for mathematical simplicity, not because it generally fits data well.1\nThe conditional distributions are defined as \\[\\begin{eqnarray}\nProb(X_{i} = x | Y_{i} = y) = \\frac{ Prob(X_{i} = x, Y_{i} = y)}{ Prob( Y_{i} = y )}\\\\\nProb(Y_{i} = y | X_{i} = x) = \\frac{ Prob(X_{i} = x, Y_{i} = y)}{ Prob( X_{i} = x )}\n\\end{eqnarray}\\] The marginal distributions are then defined as \\[\\begin{eqnarray}\nProb(X_{i} = x) = \\sum_{y} Prob(X_{i} = x | Y_{i} = y) Prob( Y_{i} = y ) \\\\\nProb(Y_{i} = y) = \\sum_{x} Prob(Y_{i} = y | X_{i} = x) Prob( X_{i} = x ),\n\\end{eqnarray}\\] which is also known as the law of total probability.\n\n\nFair Coin Flips Example.\nFor one example, Consider flipping two coins, where we mark whether “heads” is face up with a \\(1\\) and “tail” with a \\(0\\). E.g., the first coin has \\(X_{i}=1\\) if Heads and \\(=0\\) if Tails. Suppose both coins are “fair”: \\(Prob(X_{i}=1)= 1/2\\) and \\(Prob(Y_{i}=1|X_{i})=1/2\\), then the four potential outcomes have equal probabilities. \\[\\begin{eqnarray}\nProb(X_{i} = 0, Y_{i} = 0) &=& 1/2 \\times 1/2 = 1/4 \\\\\nProb(X_{i} = 0, Y_{i} = 1) &=& 1/4 \\\\\nProb(X_{i} = 1, Y_{i} = 0) &=& 1/4 \\\\\nProb(X_{i} = 1, Y_{i} = 1) &=& 1/4 .\n\\end{eqnarray}\\] The joint distribution is written generally as \\[\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y) &=& Prob(X_{i} = x) Prob(Y_{i} = y).\n\\end{eqnarray}\\]\nThe marginal distribution of the second coin is \\[\\begin{eqnarray}\nProb(Y_{i} = 0) &=& Prob(Y_{i} = 0 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 0 | X_{i} = 1) Prob(X_{i}=1)\\\\\n&=& 1/2 \\times 1/2 + 1/2 \\times 1/2 = 1/2\\\\\nProb(Y_{i} = 1) &=& Prob(Y_{i} = 1 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 1 | X_{i} = 1) Prob(X_{i}=1)\\\\\n&=& 1/2 \\times 1/2 + 1/2 \\times 1/2 = 1/2\n\\end{eqnarray}\\]\n\n\nCode\n# Create a 2x2 matrix for the joint distribution.\n# Rows correspond to X1 (coin 1), and columns correspond to X2 (coin 2).\nP_fair &lt;- matrix(1/4, nrow = 2, ncol = 2)\nrownames(P_fair) &lt;- c(\"X1=0\", \"X1=1\")\ncolnames(P_fair) &lt;- c(\"X2=0\", \"X2=1\")\nP_fair\n##      X2=0 X2=1\n## X1=0 0.25 0.25\n## X1=1 0.25 0.25\n\n# Compute the marginal distributions.\n# Marginal for X1: sum across columns.\nP_X1 &lt;- rowSums(P_fair)\nP_X1\n## X1=0 X1=1 \n##  0.5  0.5\n# Marginal for X2: sum across rows.\nP_X2 &lt;- colSums(P_fair)\nP_X2\n## X2=0 X2=1 \n##  0.5  0.5\n\n# Compute the conditional probabilities Prob(X2 | X1).\ncond_X2_given_X1 &lt;- matrix(0, nrow = 2, ncol = 2)\nfor (j in 1:2) {\n  cond_X2_given_X1[, j] &lt;- P_fair[, j] / P_X1[j]\n}\nrownames(cond_X2_given_X1) &lt;- c(\"X2=0\", \"X2=1\")\ncolnames(cond_X2_given_X1) &lt;- c(\"given X1=0\", \"given X1=1\")\ncond_X2_given_X1\n##      given X1=0 given X1=1\n## X2=0        0.5        0.5\n## X2=1        0.5        0.5\n\n\n\n\nUnFair Coin Flips Example.\nConsider a second example, where the second coin is “Completely Unfair”, so that it is always the same as the first. The outcomes generated with a Completely Unfair coin are the same as if we only flipped one coin. \\[\\begin{eqnarray}\nProb(X_{i} = 0, Y_{i} = 0) &=& 1/2 \\\\\nProb(X_{i} = 0, Y_{i} = 1) &=& 0 \\\\\nProb(X_{i} = 1, Y_{i} = 0) &=& 0 \\\\\nProb(X_{i} = 1, Y_{i} = 1) &=& 1/2 .\n\\end{eqnarray}\\] The joint distribution is written generally as \\[\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y) &=& Prob(X_{i} = x) \\mathbf{1}( x=y ),\n\\end{eqnarray}\\] where \\(\\mathbf{1}(X_{i}=1)\\) means \\(X_{i}= 1\\) and \\(0\\) if \\(X_{i}\\neq0\\). The marginal distribution of the second coin is \\[\\begin{eqnarray}\nProb(Y_{i} = 0)\n&=& Prob(Y_{i} = 0 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 0 | X_{i} = 1) Prob(X_{i} = 1)\\\\\n&=& 1/2 \\times 1 + 0 \\times 1/2 = 1/2 .\\\\\nProb(Y_{i} = 1)\n&=& Prob(Y_{i} = 1 | X_{i} =0) Prob( X_{i} = 0) + Prob(Y_{i} = 1 | X_{i} = 1) Prob( X_{i} = 1)\\\\\n&=& 0\\times 1/2 + 1 \\times 1/2 = 1/2 .\n\\end{eqnarray}\\] which is the same as in the first example! Different joint distributions can have the same marginal distributions.\n\n\nCode\n# Create the joint distribution matrix for the unfair coin case.\nP_unfair &lt;- matrix(c(0.5, 0, 0, 0.5), nrow = 2, ncol = 2, byrow = TRUE)\nrownames(P_unfair) &lt;- c(\"X1=0\", \"X1=1\")\ncolnames(P_unfair) &lt;- c(\"X2=0\", \"X2=1\")\nP_unfair\n##      X2=0 X2=1\n## X1=0  0.5  0.0\n## X1=1  0.0  0.5\n\n# Compute the marginal distribution for X2 in the unfair case.\nP_X2_unfair &lt;- colSums(P_unfair)\nP_X1_unfair &lt;- rowSums(P_unfair)\n\n# Compute the conditional probabilities Prob(X1 | X2) for the unfair coin.\ncond_X2_given_X1_unfair &lt;- matrix(NA, nrow = 2, ncol = 2)\nfor (j in 1:2) {\n  if (P_X1_unfair[j] &gt; 0) {\n    cond_X2_given_X1_unfair[, j] &lt;- P_unfair[, j] / P_X1_unfair[j]\n  }\n}\nrownames(cond_X2_given_X1_unfair) &lt;- c(\"X2=0\", \"X2=1\")\ncolnames(cond_X2_given_X1_unfair) &lt;- c(\"given X1=0\", \"given X1=1\")\ncond_X2_given_X1_unfair\n##      given X1=0 given X1=1\n## X2=0          1          0\n## X2=1          0          1\n\n\n\n\nDefinitions for Continuous Data.\nThe joint distribution is defined as \\[\\begin{eqnarray}\nF(x, y) &=& Prob(X_{i} \\leq x, Y_{i} \\leq y)\n\\end{eqnarray}\\] The marginal distributions are then defined as \\[\\begin{eqnarray}\nF_{X}(x) &=& F(x, \\infty)\\\\\nF_{Y}(y) &=& F(\\infty, y).\n\\end{eqnarray}\\] which is also known as the law of total probability. Variables are statistically independent if \\(F(x, y) = F_{X}(x)F_{Y}(y)\\) for all \\(x, y\\).\nFor example, suppose \\((X_{i},Y_{i})\\) is bivariate normal with means \\((\\mu_{X}, \\mu_{Y})\\), variances \\((\\sigma_{X}, \\sigma_{Y})\\) and covariance \\(\\rho\\).\n\n\nCode\n# Simulate Bivariate Data\nN &lt;- 10000\nMu &lt;- c(2,2) ## Means\n\nSigma1 &lt;- matrix(c(2,-.8,-.8,1),2,2) ## CoVariance Matrix\nMVdat1 &lt;- mvtnorm::rmvnorm(N, Mu, Sigma1)\n\nSigma2 &lt;- matrix(c(2,.4,.4,1),2,2) ## CoVariance Matrix\nMVdat2 &lt;- mvtnorm::rmvnorm(N, Mu, Sigma2)\n\npar(mfrow=c(1,2))\n## Different diagonals\nplot(MVdat2, col=rgb(1,0,0,0.02), pch=16,\n    main='Joint', font.main=1,\n    ylim=c(-4,8), xlim=c(-4,8), xlab='X1', ylab='X2')\npoints(MVdat1,col=rgb(0,0,1,0.02),pch=16)\n## Same marginal distributions\nxbks &lt;- seq(-4,8,by=.2)\nhist(MVdat2[,2], col=rgb(1,0,0,0.5),\n    breaks=xbks, border=NA, xlab='X2',\n    main='Marginal', font.main=1)\nhist(MVdat1[,2], col=rgb(0,0,1,0.5),\n    add=T, breaks=xbks, border=NA)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# See that independent data are a special case\nn &lt;- 2e4\n## 2 Indepenant RV\nXYiid &lt;- cbind( rnorm(n),  rnorm(n))\n## As a single Joint Draw\nXYjoint &lt;- mvtnorm::rmvnorm(n, c(0,0))\n## Plot\npar(mfrow=c(1,2))\nplot(XYiid, xlab=\n    col=grey(0,.05), pch=16, xlim=c(-5,5), ylim=c(-5,5))\nplot(XYjoint,\n    col=grey(0,.05), pch=16, xlim=c(-5,5), ylim=c(-5,5))\n\n# Compare densities\n#d1 &lt;- dnorm(XYiid[,1],0)*dnorm(XYiid[,2],0)\n#d2 &lt;- mvtnorm::dmvnorm(XYiid, c(0,0))\n#head(cbind(d1,d2))\n\n\nThe multivariate normal is a workhorse for analytical work on multivariate random variables, but there are many more. See e.g., https://cran.r-project.org/web/packages/NonNorMvtDist/NonNorMvtDist.pdf\n\n\nImportant Applications.\nNote Simpson’s Paradox:\nAlso note Bayes’ Theorem: \\[\\begin{eqnarray}\nProb(X_{i} = x | Y_{i} = y)  Prob( Y_{i} = y)\n    &=& Prob(X_{i} = x, Y_{i} = y) = Prob(Y_{i} = y | X_{i} = x) Prob(X_{i} = x).\\\\\nProb(X_{i} = x | Y_{i} = y)\n    &=& \\frac{ Prob(Y_{i} = y | X_{i} = x) Prob(X_{i}=x) }{ Prob( Y_{i} = y) }.\n\\end{eqnarray}\\]\n\n\nCode\n# Verify Bayes' theorem for the unfair coin case:\n# Compute Prob(X1=1 | X2=1) using the formula:\n#   Prob(X1=1 | X2=1) = [Prob(X2=1 | X1=1) * Prob(X1=1)] / Prob(X2=1)\n\nP_X1_1 &lt;- 0.5\nP_X2_1_given_X1_1 &lt;- 1  # Since coin 2 copies coin 1.\nP_X2_1 &lt;- P_X2_unfair[\"X2=1\"]\n\nbayes_result &lt;- (P_X2_1_given_X1_1 * P_X1_1) / P_X2_1\nbayes_result\n## X2=1 \n##    1",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate Distributions</span>"
    ]
  },
  {
    "objectID": "02_01_BivariateDistributions.html#further-reading",
    "href": "02_01_BivariateDistributions.html#further-reading",
    "title": "9  Bivariate Distributions",
    "section": "9.3 Further Reading",
    "text": "9.3 Further Reading\nFor plotting histograms and marginal distributions, see\n\nhttps://www.r-bloggers.com/2011/06/example-8-41-scatterplot-with-marginal-histograms/\nhttps://r-graph-gallery.com/histogram.html\nhttps://r-graph-gallery.com/74-margin-and-oma-cheatsheet.html\nhttps://jtr13.github.io/cc21fall2/tutorial-for-scatter-plot-with-marginal-distribution.html\n\nMany introductory econometrics textbooks have a good appendix on probability and statistics. There are many useful statistical texts online too\nSee the Further reading about Probability Theory in the Statistics chapter.\n\nhttps://www.r-bloggers.com/2024/03/calculating-conditional-probability-in-r/",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate Distributions</span>"
    ]
  },
  {
    "objectID": "02_01_BivariateDistributions.html#footnotes",
    "href": "02_01_BivariateDistributions.html#footnotes",
    "title": "9  Bivariate Distributions",
    "section": "",
    "text": "The same can be said about assuming normally distributed errors, although at least that can be motivated by the Central Limit Theorems.↩︎",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate Distributions</span>"
    ]
  },
  {
    "objectID": "02_02_BivariateStatistics.html",
    "href": "02_02_BivariateStatistics.html",
    "title": "10  Bivariate Statistics",
    "section": "",
    "text": "10.1 Statistics of Association\nAll of the univariate statistics we have covered apply to marginal distributions. For joint distributions, there are several ways to statistically describe the relationship between two variables. The major differences surround whether the data are cardinal or an ordered/unordered factor.",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate Statistics</span>"
    ]
  },
  {
    "objectID": "02_02_BivariateStatistics.html#statistics-of-association",
    "href": "02_02_BivariateStatistics.html#statistics-of-association",
    "title": "10  Bivariate Statistics",
    "section": "",
    "text": "Two Cardinals.\nPearson (Linear) Correlation. Suppose you have two vectors, \\(\\hat{X}\\) and \\(\\hat{Y}\\), that are both cardinal data. As such, you can compute the most famous measure of association, the covariance. Letting \\(\\hat{M}_{X}\\) and \\(\\hat{M}_{Y}\\) denote the mean of \\(X\\) and \\(Y\\), we have \\[\n\\hat{C}_{XY} =  \\sum_{i=1}^{n} [\\hat{X}_{i} - \\hat{M}_{X}] [\\hat{Y}_i - \\hat{M}_{Y}] / n\n\\]\n\n\nCode\nxy &lt;- USArrests[,c('Murder','UrbanPop')]\n#plot(xy, pch=16, col=grey(0,.25))\ncov(xy)\n##             Murder   UrbanPop\n## Murder   18.970465   4.386204\n## UrbanPop  4.386204 209.518776\n\n\nNote that \\(\\hat{C}_{XX}=\\hat{V}_{X}\\). For ease of interpretation and comparison, we rescale this statistic to always lay between \\(-1\\) and \\(1\\) \\[\n\\hat{R}_{XY} = \\frac{ \\hat{C}_{XY} }{ \\sqrt{\\hat{V}_X} \\sqrt{\\hat{V}_Y}}\n\\]\n\n\nCode\ncor(xy)[2]\n## [1] 0.06957262\n\n\nFalk Codeviance. The Codeviance, \\(\\tilde{C}_{XY}\\), is a robust alternative to Covariance. Instead of relying on means (which can be sensitive to outliers), it uses medians.1 We can also scale the Codeviance by the median absolute deviation to compute the median correlation, which typically lies in \\([-1,1]\\) but not always. Letting \\(\\tilde{M}_{X}\\) and \\(\\tilde{M}_{Y}\\) denote the median of \\(X\\) and \\(Y\\), we have \\[\\begin{eqnarray}\n\\tilde{C}_{XY} = \\text{Med}\\left\\{ |\\hat{X}_{i} - \\tilde{M}_{X}| |\\hat{Y}_i - \\tilde{M}_{Y}| \\right\\} \\\\\n\\tilde{R}_{XY} = \\frac{ \\tilde{C}_{XY} }{ \\hat{\\text{MAD}}_{X} \\hat{\\text{MAD}}_{Y}}.\n\\end{eqnarray}\\]\n\n\nCode\ncodev &lt;- function(xy) {\n  # Compute medians for each column\n  med &lt;- apply(xy, 2, median)\n  # Subtract the medians from each column\n  xm &lt;- sweep(xy, 2, med, \"-\")\n  # Compute CoDev\n  CoDev &lt;- median(xm[, 1] * xm[, 2])\n  # Compute the medians of absolute deviation\n  MadProd &lt;- prod( apply(abs(xm), 2, median) )\n  # Return the robust correlation measure\n  return( CoDev / MadProd)\n}\ncodev(xy)\n## [1] 0.005707763\n\n\n\n\nTwo Ordered Factors.\nSuppose now that \\(X\\) and \\(Y\\) are both ordered variables. Kendall’s rank correlation coefficient measures the strength and direction of association by counting the number of concordant pairs (where the ranks agree) versus discordant pairs (where the ranks disagree). A value of \\(1\\) implies perfect agreement in rankings, a value of \\(-1\\) indicates perfect disagreement, and a value of \\(0\\) suggests no association in the ordering. \\[\n\\hat{KT} = \\frac{2}{n(n-1)} \\sum_{i} \\sum_{j &gt; i} \\text{sgn} \\Bigl( (\\hat{X}_{i} - \\hat{X}_{j})(\\hat{Y}_i - \\hat{Y}_j) \\Bigr),\n\\] where the sign function is: \\[\n\\text{sgn}(z) =\n\\begin{cases}\n+1 & \\text{if } z &gt; 0\\\\\n0  & \\text{if } z = 0 \\\\\n-1 & \\text{if} z &lt; 0\n\\end{cases}.\n\\]\n\n\nCode\nxy &lt;- USArrests[,c('Murder','UrbanPop')]\nxy[,1] &lt;- rank(xy[,1] )\nxy[,2] &lt;- rank(xy[,2] )\n# plot(xy, pch=16, col=grey(0,.25))\nKT &lt;- cor(xy[, 1], xy[, 2], method = \"kendall\")\nround(KT, 3)\n## [1] 0.074\n\n\nKendall’s rank correlation coefficient can also be used for non-linear relationships, where Pearson’s correlation coefficient often falls short.\n\n\nCode\n## https://commons.wikimedia.org/wiki/File:Correlation_examples2.svg\n\nMyPlot &lt;- function(xy, xlim = c(-4, 4), ylim = c(-3,3)) {\n   plot(xy, main ='', xlab = \"\", ylab = \"\",\n        col = grey(0,.25), pch=16, cex=.5,\n        xaxt = \"n\", yaxt = \"n\", bty = \"n\",\n        xlim = xlim, ylim = ylim)\n    box(lwd=.1)\n    \n    cor1 &lt;- cor(xy[,1], xy[,2])\n    cor2 &lt;- codev(xy)\n    cor3 &lt;- cor(xy[,1], xy[,2], method='kendall')\n    cors &lt;- c(cor1, cor2, cor3)\n    \n    #cor3 &lt;- abs(generalCorr::gmcmtx0( xy[,1:2])[2])\n    #cor4 &lt;- Rfast::dcor( xy[,1], xy[,2])[[4]]\n    #cor5 &lt;- XICOR::xicor( xy[,1], xy[,2]) ## cor( xy[,1], xy[,2], method='kendall') \n\n    fm &lt;- cors*0 + 1\n    fm[which.max(abs(cors))] &lt;- 2\n    title(paste0('Pearson:  ',  formatC(cor1, digits=2, format='f')),\n        line=3, adj=0, font.main=fm[1])\n    title(paste0('Falk:     ',  formatC(cor2, digits=2, format='f')),\n        line=2, adj=0, font.main=fm[2])    \n    title(paste0('Kendall:  ',  formatC(cor3, digits=2, format='f')),\n        line=1, adj=0, font.main=fm[3])\n}\n\nMvNormal &lt;- function(n = 1000, cor, f) {\n   for (i in cor) {\n      sd = matrix(c(1, i, i, 1), ncol = 2)\n      x = mvtnorm::rmvnorm(n, c(0, 0), sd)\n      x[,2] &lt;- f(x[,2])\n      MyPlot(x)\n   }\n}\n\noutput &lt;- function() {\n   par(mfrow = c(3, 2), oma = c(0,0,0,0), mar=c(3,2,6,1))\n   cor &lt;- c(0.99, 0.9)\n   MvNormal(800, cor, function(y){y});\n   MvNormal(800, cor, function(y){(y/2)^3});\n   MvNormal(800, cor, function(y){log( (y-min(y)+.Machine$double.eps)/(max(y)-min(y)) ) });\n}\noutput()\n\n\n\n\n\n\n\n\n\n\n\nTwo Unordered Factors.\nSuppose \\(X\\) and \\(Y\\) are both categorical variables; the value of \\(X\\) is one of \\(1...K\\) categories and the value of \\(Y\\) is one of \\(1...J\\) categories. Cramer’s V quantifies the strength of association by adjusting a “chi-squared” statistic to provide a measure that ranges from \\(0\\) to \\(1\\); \\(0\\) indicates no association while a value closer to \\(1\\) signifies a strong association.\nFirst, consider a contingency table for \\(X\\) and \\(Y\\) with \\(I\\) rows and \\(J\\) columns. The chi-square statistic is then defined as:\n\\[\n\\hat{\\chi}^2 = \\sum_{k=1}^{K} \\sum_{j=1}^{J} \\frac{(\\hat{O}_{kj} - \\hat{E}_{kj})^2}{\\hat{E}_{kj}}.\n\\]\nwhere\n\n\\(\\hat{O}_{kj}\\) denote the observed frequency in cell \\((k, j)\\),\n\\(\\hat{E}_{kj} = \\hat{RF}_{k} \\cdot \\hat{CF}_j / n\\) is the expected frequency for each cell if \\(\\hat{X}\\) and \\(\\hat{Y}\\) are independent\n\\(\\hat{RF}_{k}\\) denotes the total frequency for row \\(k\\) (i.e., \\(\\hat{RF}_i = \\sum_{j=1}^{J} \\hat{O}_{kj}\\)),\n\\(\\hat{CF}_{j}\\) denotes the total frequency for column \\(j\\) (i.e., \\(\\hat{CF}_{j} = \\sum_{k=1}^{K} \\hat{O}_{kj}\\)),\n\nSecond, normalize the chi-square statistic with the sample size and the degrees of freedom to compute Cramer’s V. Recalling that \\(I\\) is the number of categories for \\(X\\), and \\(J\\) is the number of categories for \\(Y\\), the statistic is \\[\n\\hat{CV} = \\sqrt{\\frac{\\hat{\\chi}^2 / n}{\\min(J - 1, \\, K - 1)}},\n\\]\n\n\nCode\nxy &lt;- USArrests[,c('Murder','UrbanPop')]\nxy[,1] &lt;- cut(xy[,1],3)\nxy[,2] &lt;- cut(xy[,2],4)\ntable(xy)\n##               UrbanPop\n## Murder         (31.9,46.8] (46.8,61.5] (61.5,76.2] (76.2,91.1]\n##   (0.783,6.33]           4           5           8           5\n##   (6.33,11.9]            0           4           7           6\n##   (11.9,17.4]            2           4           2           3\n\nCV &lt;- function(xy){\n    # Create a contingency table from the categorical variables\n    tbl &lt;- table(xy)\n    # Compute the chi-square statistic (without Yates' continuity correction)\n    chi2 &lt;- chisq.test(tbl, correct=FALSE)[['statistic']]\n    # Total sample size\n    n &lt;- sum(tbl)\n    # Compute the minimum degrees of freedom (min(rows-1, columns-1))\n    df_min &lt;- min(nrow(tbl) - 1, ncol(tbl) - 1)\n    # Calculate Cramer's V\n    V &lt;- sqrt((chi2 / n) / df_min)\n    return(V)\n}\nCV(xy)\n## X-squared \n## 0.2307071\n\n# DescTools::CramerV( table(xy) )",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate Statistics</span>"
    ]
  },
  {
    "objectID": "02_02_BivariateStatistics.html#mixed-data",
    "href": "02_02_BivariateStatistics.html#mixed-data",
    "title": "10  Bivariate Statistics",
    "section": "10.2 Mixed Data",
    "text": "10.2 Mixed Data\nFor mixed data, \\(\\hat{Y}_{i}\\) is a cardinal variable and \\(\\hat{X}_{i}\\) is a factor variable (typically unordered). For such data, we analyze associations via group comparisons. The basic idea is seen in a comparison of two samples, which corresponds to an \\(\\hat{X}_{i}\\) with two categories.\nSuppose we have two samples of data. For example, the heights of men and women in Canada. For another example, homicide rates in two different American states. For another example, the wages for people with and without completing a degree.\n\n\nCode\nlibrary(wooldridge)\nx1 &lt;- wage1[wage1$educ == 15, 'wage']\nx2 &lt;- wage1[wage1$educ == 16, 'wage']\n\n\n\n\nCode\n# Sample 1 (e.g., males)\nn1 &lt;- 100\nx1 &lt;- rnorm(n1, 0, 2)\n# Sample 2 (e.g., females)\nn2 &lt;- 80\nx2 &lt;- rnorm(n2, 1, 1)\n\npar(mfrow=c(1,2))\nbks &lt;- seq(-7,7, by=.5)\nhist(x1, border=NA, breaks=bks,\n    main='Sample 1', font.main=1)\n\nhist(x2, border=NA, breaks=bks, \n    main='Sample 2', font.main=1)\n\n\n\n\n\n\n\n\n\nThere may be several differences between these samples. Often, the first summary statistic we investigate is the difference in means.\n\nMean Differences.\nWe often want to know if the means of different sample are different in . To test this hypothesis, we compute the means separately for each sample and then examine the differences term \\[\\begin{eqnarray}\n\\hat{D} = \\hat{M}_{X1} - \\hat{M}_{X2},\n\\end{eqnarray}\\] with a null hypothesis of \\(D=0\\).\n\n\nCode\n# Differences between means\nm1 &lt;- mean(x1)\nm2 &lt;- mean(x2)\nd &lt;- m1-m2\n    \n# Bootstrap Distribution\nbootstrap_diff &lt;- vector(length=9999)\nfor(b in seq(bootstrap_diff) ){\n    x1_b &lt;- sample(x1, replace=T)\n    x2_b &lt;- sample(x2, replace=T)\n    m1_b &lt;- mean(x1_b)\n    m2_b &lt;- mean(x2_b)\n    d_b &lt;- m1_b - m2_b\n    bootstrap_diff[b] &lt;- d_b\n}\nhist(bootstrap_diff,\n    border=NA, font.main=1,\n    main='Difference in Means')\n\n# 2-Sided Test\nboot_ci &lt;- quantile(bootstrap_diff, probs=c(.025, .975))\nabline(v=boot_ci, lwd=2)\nabline(v=0, lwd=2, col=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# p-value\n1 - ecdf(bootstrap_diff)(0)\n## [1] 0\n\n\nJust as with one sample tests, we can compute a standardized differences, where \\(D\\) is converted into a \\(t\\) statistic. Note, however, that we have to compute the standard error for the difference statistic, which is a bit more complicated. However, this allows us to easily conduct one or two sided hypothesis tests using a standard normal approximation.\n\n\nCode\nse_hat &lt;- sqrt(var(x1)/n1 + var(x2)/n2);\nt_obs &lt;- d/se_hat\n\n\n\n\nOther Differences.\nThe above procedure generalized from differences in means to other statistics like quantiles or standard deviations.\n\n\nCode\n# Bootstrap Distribution Function\nboot_fun &lt;- function( fun, B=9999, ...){\n    bootstrap_diff &lt;- vector(length=B)\n    for(b in seq(bootstrap_diff)){\n        x1_b &lt;- sample(x1, replace=T)\n        x2_b &lt;- sample(x2, replace=T)\n        f1_b &lt;- fun(x1_b, ...)\n        f2_b &lt;- fun(x2_b, ...)\n        d_b &lt;- f1_b - f2_b\n        bootstrap_diff[b] &lt;- d_b\n    }\n    return(bootstrap_diff)\n}\n\n# 2-Sided Test for Median Differences\n# d &lt;- median(x2) - median(x1)\nboot_d &lt;- boot_fun(median)\nhist(boot_d, border=NA, font.main=1,\n    main='Difference in Medians')\nabline(v=quantile(boot_d, probs=c(.025, .975)), lwd=2)\nabline(v=0, lwd=2, col=2)\n\n\n\n\n\n\n\n\n\nCode\n1 - ecdf(boot_d)(0)\n## [1] 0.00070007\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\nCode\n# 2-Sided Test for SD Differences\n#d &lt;- sd(x2) - sd(x1)\nboot_d &lt;- boot_fun(sd)\nhist(boot_d, border=NA, font.main=1,\n    main='Difference in Standard Deviations')\nabline(v=quantile(boot_d, probs=c(.025, .975)), lwd=2)\nabline(v=0, lwd=2, col=2)\n1 - ecdf(boot_d)(0)\n\n\n# Try any function!\n# boot_fun( function(xs) { IQR(xs)/median(xs) } )\n\n\nNote that these estimates suffer from a finite-sample bias, which we can correct for. Also note that bootstrap tests can perform poorly with highly unequal variances or skewed data.\n\n\n\n\n\nDistributional Comparisons.\nWe can also examine whether there are any differences between the entire distributions\n\n\nCode\n# Sample Wage Data\nlibrary(wooldridge)\nx1 &lt;- sort( wage1[wage1$educ == 15,  'wage'])  \nx2 &lt;- sort( wage1[wage1$educ == 16,  'wage'] )\nx &lt;- sort(c(x1, x2))\n\n# Compute Quantiles\nquants &lt;- seq(0,1,length.out=101)\nQ1 &lt;- quantile(x1, probs=quants)\nQ2 &lt;- quantile(x2, probs=quants)\n\n# Compare Distributions via Quantiles\nrx &lt;- range(c(x1, x2))\npar(mfrow=c(1,2))\nplot(rx, c(0,1), type='n', font.main=1,\n    main='Distributional Comparison',\n    xlab=expression(Q[s]),\n    ylab=expression(F[s]))\nlines(Q1, quants, col=2)\nlines(Q2, quants, col=4)\nlegend('bottomright', col=c(2,4), lty=1,\nlegend=c('F1', 'F2'))\n\n# Compare Quantiles\nplot(Q1, Q2, xlim=rx, ylim=rx,\n    main='Quantile-Quantile Plot', font.main=1,\npch=16, col=grey(0,.25))\nabline(a=0,b=1,lty=2)\n\n\n\n\n\n\n\n\n\nThe starting point for hypothesis testing is the Kolmogorov-Smirnov Statistic: the maximum absolute difference between two CDF’s over all sample data \\(x \\in \\{X_1\\} \\cup \\{X_2\\}\\). \\[\\begin{eqnarray}\n\\hat{KS} &=& \\max_{x} |\\hat{F}_{1}(x)- \\hat{F}_{2}(x)|^{p},\n\\end{eqnarray}\\] where \\(p\\) is an integer (typically 1). An intuitive alternative is the Cramer-von Mises Statistic: the sum of absolute differences (raised to an integer, typically 2) between two CDF’s. \\[\\begin{eqnarray}\n\\hat{CVM} &=& \\sum_{x} | \\hat{F}_{1}(x)- \\hat{F}_{2}(x)|^{p}.\n\\end{eqnarray}\\]\n\n\nCode\n# Distributions\nF1 &lt;- ecdf(x1)(x)\nF2 &lt;- ecdf(x2)(x)\n\nlibrary(twosamples)\n\n# Kolmogorov-Smirnov\nKSq &lt;- which.max(abs(F2 - F1))\nKSqv &lt;- round(twosamples::ks_stat(x1, x2),2)\n\n# Cramer-von Mises Statistic (p=2)\nCVMqv &lt;- round(twosamples::cvm_stat(x1, x2, power=2), 2) \n\n# Visualize Differences\nplot(range(x), c(0,1), type=\"n\", xlab='x', ylab='ECDF')\nlines(x, F1, col=2, lwd=2)\nlines(x, F2, col=4, lwd=2)\n# CVM\nsegments(x, F1, x, F2, lwd=.5, col=grey(0,.2))\n# KS\nsegments(x[KSq], F1[KSq], x[KSq], F2[KSq], lwd=1.5, col=grey(0,.75), lty=2)\n\n\n\n\n\n\n\n\n\nJust as before, you use bootstrapping for hypothesis testing.\n\n\nCode\ntwosamples::cvm_test(x1, x2)\n## Test Stat   P-Value \n##  2.084253  0.078000\n\n\n\n\nComparing Multiple Groups.\nFor multiple groups, we can tests the equality of all distributions (whether at least one group is different). The Kruskal-Wallis test examines \\(H_0:\\; F_1 = F_2 = \\dots = F_G\\) versus \\(H_A:\\; \\text{at least one } F_g \\text{ differs}\\), where \\(F_g\\) is the continuous distribution of group \\(g=1,...G\\). This test does not tell us which group is different.\nTo conduct the test, first denote individuals \\(i=1,...n\\) with overall ranks \\(\\hat{r}_1,....\\hat{r}_{n}\\). Each individual belongs to group \\(g=1,...G\\), and each group \\(g\\) has \\(n_{g}\\) individuals with average rank \\(\\bar{r}_{g} = \\sum_{i} \\hat{r}_{i} /n_{g}\\). The Kruskal Wallis statistic is \\[\\begin{eqnarray}\n\\hat{KW} &=& (N-1) \\frac{\\sum_{g=1}^{G} n_{g}( \\bar{r}_{g} - \\bar{r}  )^2  }{\\sum_{i=1}^{N} ( \\hat{r}_{i} - \\bar{r}  )^2},\n\\end{eqnarray}\\] where \\(\\bar{r} = \\frac{n+1}{2}\\) is the grand mean rank.\nIn the special case with only two groups, \\(G=2\\), the Kruskal Wallis test reduces to the Mann–Whitney U-test (also known as the ). In this case, we can write the hypotheses in terms of individual outcomes in each group, \\(Y_i\\) in one group \\(Y_j\\) in the other; \\(H_0: Prob(Y_i &gt; Y_j)=Prob(Y_i &gt; Y_i)\\) versus \\(H_A: Prob(Y_i &gt; Y_j) \\neq Prob(Y_i &gt; Y_j)\\). The corresponding test statistic is \\[\\begin{eqnarray}\n\\hat{U}   &=& \\min(\\hat{U}_1, \\hat{U}_2) \\\\\n\\hat{U}_g &=& \\sum_{i\\in g}\\sum_{j\\in -g}\n           \\Bigl[\\mathbf 1( \\hat{Y}_{i} &gt; \\hat{Y}_{j}) + \\tfrac12\\mathbf 1(\\hat{Y}_{i} = \\hat{Y}_{j})\\Bigr].\n\\end{eqnarray}\\]\n\n\nCode\nlibrary(AER)\ndata(CASchools)\nCASchools$stratio &lt;- CASchools$students/CASchools$teachers\n\n# Do student/teacher ratio differ for at least 1 county?\n# Single test of multiple distributions\nkruskal.test(CASchools$stratio, CASchools$county)\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  CASchools$stratio and CASchools$county\n## Kruskal-Wallis chi-squared = 161.18, df = 44, p-value = 2.831e-15\n\n# Multiple pairwise tests\n# pairwise.wilcox.test(CASchools$stratio, CASchools$county)",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate Statistics</span>"
    ]
  },
  {
    "objectID": "02_02_BivariateStatistics.html#further-reading",
    "href": "02_02_BivariateStatistics.html#further-reading",
    "title": "10  Bivariate Statistics",
    "section": "10.3 Further Reading",
    "text": "10.3 Further Reading\nOther Statistics\n\nhttps://cran.r-project.org/web/packages/qualvar/vignettes/wilcox1973.html",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate Statistics</span>"
    ]
  },
  {
    "objectID": "02_02_BivariateStatistics.html#footnotes",
    "href": "02_02_BivariateStatistics.html#footnotes",
    "title": "10  Bivariate Statistics",
    "section": "",
    "text": "See also Theil-Sen Estimator, which may be seen as a precursor.↩︎",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate Statistics</span>"
    ]
  },
  {
    "objectID": "02_03_BasicRegression.html",
    "href": "02_03_BasicRegression.html",
    "title": "11  Simple Regression",
    "section": "",
    "text": "11.1 Simple Linear Regression\nSuppose we have some bivariate data. First, we inspect it as in Part I.\nNow we will assess the association between variables by fitting a line through the data points using a “regression”.\nThis refers to fitting a linear model to bivariate data. Specifically, our model is \\[\\begin{eqnarray}\n\\hat{Y}_{i}=\\beta_{0}+\\beta_{1} \\hat{X}_{i}+e_{i}\n\\end{eqnarray}\\] and our objective function is \\[\\begin{eqnarray}\nmin_{\\beta_{0}, \\beta_{1}} \\sum_{i=1}^{N} \\left( e_{i} \\right)^2 =  min_{\\beta_{0}, \\beta_{1}} \\sum_{i=1} \\left( \\hat{Y}_{i} - [\\beta_{0}+\\beta_{1} \\hat{X}_{i}] \\right)^2.\n\\end{eqnarray}\\] Minimizing the sum of squared errors yields parameter estimates \\[\\begin{eqnarray}\n\\hat{\\beta_{0}}=\\bar{Y}-\\hat{\\beta_{1}}\\bar{X} \\\\\n\\hat{\\beta_{1}}=\\frac{\\sum_{i}^{}(\\hat{X}_{i}-\\bar{X})(\\hat{Y}_{i}-\\bar{Y})}{\\sum_{i}^{}(\\hat{X}_{i}-\\bar{X})^2} = \\frac{C_{XY}}{V_{X}}\n\\end{eqnarray}\\] and predictions \\[\\begin{eqnarray}\n\\hat{y}_{i} &=& \\hat{\\beta}_{0}+\\hat{\\beta}\\hat{X}_{i}\\\\\n\\hat{e}_i &=& \\hat{Y}_{i}-\\hat{y}_i\n\\end{eqnarray}\\]\nCode\n# Run a Regression Coefficients\nreg &lt;- lm(y~x, dat=xy)\n# predict(reg)\n# resid(reg)\n# coef(reg)",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Regression</span>"
    ]
  },
  {
    "objectID": "02_03_BasicRegression.html#simple-linear-regression",
    "href": "02_03_BasicRegression.html#simple-linear-regression",
    "title": "11  Simple Regression",
    "section": "",
    "text": "Goodness of Fit.\nFirst, we qualitatively analyze the ‘’Goodness of fit’’ of our model, we plot our predictions for a qualitative analysis\n\n\nCode\n# Plot Data and Predictions\nlibrary(plotly)\nxy$ID &lt;- rownames(USArrests)\nxy$pred &lt;- predict(reg)\nxy$resid &lt;- resid(reg)\nfig &lt;- plotly::plot_ly(\n  xy, x=~x, y=~y,\n  mode='markers',\n  type='scatter',\n  hoverinfo='text',\n  marker=list(color=grey(0,.25), size=10),\n  text=~paste('&lt;b&gt;', ID, '&lt;/b&gt;',\n              '&lt;br&gt;Urban  :', x,\n              '&lt;br&gt;Murder :', y,\n              '&lt;br&gt;Predicted Murder :', round(pred,2),\n              '&lt;br&gt;Residual :', round(resid,2)))              \n# Add Legend\nfig &lt;- plotly::layout(fig,\n          showlegend=F,\n          title='Crime and Urbanization in America 1975',\n          xaxis = list(title='Percent of People in an Urban Area'),\n          yaxis = list(title='Homicide Arrests per 100,000 People'))\n# Plot Model Predictions\nadd_trace(fig, x=~x, y=~pred,\n    inherit=F, hoverinfo='none',\n    mode='lines+markers', type='scatter',\n    color=I('black'),\n    line=list(width=1/2),\n    marker=list(symbol=134, size=5))\n\n\n\n\n\n\nFor a quantitative summary, we can also compute the linear correlation between the predictions and the data \\[\nR = Cor( \\hat{y}_i, y)\n\\] With linear models, we typically compute \\(R^2\\), known as the “coefficient of determination”, using the sums of squared errors (Total, Explained, and Residual) \\[\\begin{eqnarray}\n\\underbrace{\\sum_{i}(\\hat{Y}_{i}-\\bar{Y})^2}_\\text{TSS}\n=\\underbrace{\\sum_{i}(\\hat{y}_i-\\bar{Y})^2}_\\text{ESS}+\\underbrace{\\sum_{i}\\hat{e}_{i}^2}_\\text{RSS}\\\\\nR^2 = \\frac{ESS}{TSS}=1-\\frac{RSS}{TSS}\n\\end{eqnarray}\\]\n\n\nCode\n# Manually Compute R2\nEhat &lt;- resid(reg)\nRSS  &lt;- sum(Ehat^2)\nY &lt;- xy$y\nTSS  &lt;- sum((Y-mean(Y))^2)\nR2 &lt;- 1 - RSS/TSS\nR2\n## [1] 0.00484035\n\n# Check R2\nsummary(reg)$r.squared\n## [1] 0.00484035\n\n# Double Check R2\nR &lt;- cor(xy$y, predict(reg))\nR^2\n## [1] 0.00484035",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Regression</span>"
    ]
  },
  {
    "objectID": "02_03_BasicRegression.html#variability-estimates",
    "href": "02_03_BasicRegression.html#variability-estimates",
    "title": "11  Simple Regression",
    "section": "11.2 Variability Estimates",
    "text": "11.2 Variability Estimates\nA regression coefficient is a statistic. And, just like all statistics, we can calculate\n\nstandard deviation: variability within a single sample.\nstandard error: variability across different samples.\nconfidence interval: range your statistic varies across different samples.\n\nNote that values reported by your computer do not necessarily satisfy this definition. To calculate these statistics, we will estimate variability using data-driven methods. (For some theoretical background, see, e.g., https://www.sagepub.com/sites/default/files/upm-binaries/21122_Chapter_21.pdf.)\n\nJackknife.\nWe first consider the simplest, the jackknife. In this procedure, we loop through each row of the dataset. And, in each iteration of the loop, we drop that observation from the dataset and reestimate the statistic of interest. We then calculate the standard deviation of the statistic across all ``subsamples’’.\n\n\nCode\n# Jackknife Standard Errors for OLS Coefficient\njack_regs &lt;- lapply(1:nrow(xy), function(i){\n    xy_i &lt;- xy[-i,]\n    reg_i &lt;- lm(y~x, dat=xy_i)\n})\njack_coefs &lt;- sapply(jack_regs, coef)['x',]\njack_se &lt;- sd(jack_coefs)\n# classic_se &lt;- sqrt(diag(vcov(reg)))[['x']]\n\n\n# Jackknife Sampling Distribution\nhist(jack_coefs, breaks=25,\n    main=paste0('SE est. = ', round(jack_se,4)),\n    font.main=1, border=NA,\n    xlab=expression(beta[-i]))\n# Original Estimate\nabline(v=coef(reg)['x'], lwd=2)\n# Jackknife Confidence Intervals\njack_ci_percentile &lt;- quantile(jack_coefs, probs=c(.025,.975))\nabline(v=jack_ci_percentile, lty=2)\n\n\n\n\n\n\n\n\n\nCode\n\n\n# Plot Normal Approximation\n# jack_ci_normal &lt;- jack_mean+c(-1.96, +1.96)*jack_se\n# abline(v=jack_ci_normal, col=\"red\", lty=3)\n\n\n\n\nBootstrap.\nThere are several resampling techniques. The other main one is the bootstrap, which resamples with replacement for an arbitrary number of iterations. When bootstrapping a dataset with \\(n\\) observations, you randomly resample all \\(n\\) rows in your data set \\(B\\) times. Random subsampling is one of many hybrid approaches that tries to combine the best of both worlds.\n\n\n\n\nSample Size per Iteration\nNumber of Iterations\nResample\n\n\n\n\nBootstrap\n\\(n\\)\n\\(B\\)\nWith Replacement\n\n\nJackknife\n\\(n-1\\)\n\\(n\\)\nWithout Replacement\n\n\nRandom Subsample\n\\(m &lt; n\\)\n\\(B\\)\nWithout Replacement\n\n\n\n\n\nCode\n# Bootstrap\nboot_regs &lt;- lapply(1:399, function(b){\n    b_id &lt;- sample( nrow(xy), replace=T)\n    xy_b &lt;- xy[b_id,]\n    reg_b &lt;- lm(y~x, dat=xy_b)\n})\nboot_coefs &lt;- sapply(boot_regs, coef)['x',]\nboot_se &lt;- sd(boot_coefs)\n\nhist(boot_coefs, breaks=25,\n    main=paste0('SE est. = ', round(boot_se,4)),\n    font.main=1, border=NA,\n    xlab=expression(beta[b]))\nboot_ci_percentile &lt;- quantile(boot_coefs, probs=c(.025,.975))\nabline(v=boot_ci_percentile, lty=2)\nabline(v=coef(reg)['x'], lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Random Subsamples\nrs_regs &lt;- lapply(1:399, function(b){\n    b_id &lt;- sample( nrow(xy), nrow(xy)-10, replace=F)\n    xy_b &lt;- xy[b_id,]\n    reg_b &lt;- lm(y~x, dat=xy_b)\n})\nrs_coefs &lt;- sapply(rs_regs, coef)['x',]\nrs_se &lt;- sd(rs_coefs)\n\nhist(rs_coefs, breaks=25,\n    main=paste0('SE est. = ', round(rs_se,4)),\n    font.main=1, border=NA,\n    xlab=expression(beta[b]))\nabline(v=coef(reg)['x'], lwd=2)\nrs_ci_percentile &lt;- quantile(rs_coefs, probs=c(.025,.975))\nabline(v=rs_ci_percentile, lty=2)\n\n\n\n\n\n\n\n\n\nWe can also bootstrap other statistics, such as a t-statistic or \\(R^2\\). We do such things to test a null hypothesis, which is often ``no relationship’’. We are rarely interested in computing standard errors and conducting hypothesis tests for two variables. However, we work through the ideas in the two-variable case to better understand the multi-variable case.",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Regression</span>"
    ]
  },
  {
    "objectID": "02_03_BasicRegression.html#hypothesis-tests",
    "href": "02_03_BasicRegression.html#hypothesis-tests",
    "title": "11  Simple Regression",
    "section": "11.3 Hypothesis Tests",
    "text": "11.3 Hypothesis Tests\n\nInvert a CI.\nOne main way to conduct hypothesis tests is to examine whether a confidence interval contains a hypothesized value. Does the slope coefficient equal \\(0\\)? For reasons we won’t go into in this class, we typically normalize the coefficient by its standard error: \\[ \\hat{t} = \\frac{\\hat{\\beta}}{\\hat{\\sigma}_{\\hat{\\beta}}} \\]\n\n\nCode\ntvalue &lt;- coef(reg)['x']/jack_se\n\njack_t &lt;- sapply(jack_regs, function(reg_b){\n    # Data\n    xy_b &lt;- reg_b$model\n    # Coefficient\n    beta_b &lt;- coef(reg_b)[['x']]\n    t_hat_b &lt;- beta_b/jack_se\n    return(t_hat_b)\n})\n\nhist(jack_t, breaks=25,\n    main='Jackknife t Density',\n    font.main=1, border=NA,\n    xlab=expression(hat(t)[b]), \n    xlim=range(c(0, jack_t)) )\nabline(v=quantile(jack_t, probs=c(.025,.975)), lty=2)\nabline(v=0, col=\"red\", lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nImpose the Null.\nWe can also compute a null distribution. We focus on the simplest: bootstrap simulations that each impose the null hypothesis and re-estimate the statistic of interest. Specifically, we compute the distribution of t-values on data with randomly reshuffled outcomes (imposing the null), and compare how extreme the observed value is.\n\n\nCode\n# Null Distribution for Beta\nboot_t0 &lt;- sapply( 1:399, function(b){\n    xy_b &lt;- xy\n    xy_b$y &lt;- sample( xy_b$y, replace=T)\n    reg_b &lt;- lm(y~x, dat=xy_b)\n    beta_b &lt;- coef(reg_b)[['x']]\n    t_hat_b &lt;- beta_b/jack_se\n    return(t_hat_b)\n})\n\n# Null Bootstrap Distribution\nboot_ci_percentile0 &lt;- quantile(boot_t0, probs=c(.025,.975))\nhist(boot_t0, breaks=25,\n    main='Null Bootstrap Density',\n    font.main=1, border=NA,\n    xlab=expression(hat(t)[b]),\n    xlim=range(boot_t0))\nabline(v=boot_ci_percentile0, lty=2)\nabline(v=tvalue, col=\"red\", lwd=2)\n\n\n\n\n\n\n\n\n\nAlternatively, you can impose the null by recentering the sampling distribution around the theoretical value; \\[\\hat{t} = \\frac{\\hat{\\beta} - \\beta_{0} }{\\hat{\\sigma}_{\\hat{\\beta}}}.\\] Under some assumptions, the null distribution follows a t-distribution. (For more on parametric t-testing based on statistical theory, see https://www.econometrics-with-r.org/4-lrwor.html.)\nIn any case, we can calculate a p-value: the probability you would see something as extreme as your statistic under the null (assuming your null hypothesis was true). We can always calculate a p-value from an explicit null distribution.\n\n\nCode\n# One Sided Test for P(t &gt; boot_t | Null) = 1 - P(t &lt; boot_t | Null)\nThat_NullDist1 &lt;- ecdf(boot_t0)\nPhat1  &lt;- 1-That_NullDist1(jack_t)\n\n# Two Sided Test for P(t &gt; jack_t or t &lt; -jack_t | Null)\nThat_NullDist2 &lt;- ecdf(abs(boot_t0))\nplot(That_NullDist2, xlim=range(boot_t0, jack_t),\n    xlab=expression( abs(hat(t)[b]) ),\n    main='Null Bootstrap Distribution', font.main=1)\nabline(v=tvalue, col='red')\n\n\n\n\n\n\n\n\n\nCode\n\nPhat2  &lt;-  1-That_NullDist2( abs(tvalue))\nPhat2\n## [1] 0.6265664",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Regression</span>"
    ]
  },
  {
    "objectID": "02_04_KernelIntro.html",
    "href": "02_04_KernelIntro.html",
    "title": "12  Local Regression",
    "section": "",
    "text": "12.1 Local Relationships",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Local Regression</span>"
    ]
  },
  {
    "objectID": "02_04_KernelIntro.html#local-relationships",
    "href": "02_04_KernelIntro.html#local-relationships",
    "title": "12  Local Regression",
    "section": "",
    "text": "The Effect.\nThe interpretation of regression coefficients as ``the effect’’ assumes the linear model is true. If you fit a line to a non-linear relationship then you will get back a number, but there is no singular the effect if the true relationship is non-linear! Consider a classic example, Anscombe’s Quartet, which shows four very different datasets that give the same linear regression coefficient. You understand the problem because we used scatterplots to visual the data.1\n\n\nCode\n##################\n# Anscombe\n##################\n\npar(mfrow=c(2,2))\nfor(i in 1:4){\n    xi &lt;- anscombe[,paste0('x',i)]\n    yi &lt;- anscombe[,paste0('y',i)]\n    plot(xi, yi, ylim=c(4,13), xlim=c(4,20),\n        pch=16, col=grey(0,.6))\n    reg &lt;- lm(yi~xi)\n    b &lt;- round(coef(reg)[2],2)\n    p &lt;- round(summary(reg)$coefficients[2,4],4)\n    abline(reg, col='orange')\n    title(paste0(\"Slope=\", b,', p=',p), font.main=1)\n}\n\n\n\n\n\n\n\n\n\nCode\n\n## For an even better example, see `Datasaurus Dozen'\n#browseURL(\n#'https://bookdown.org/paul/applied-data-visualization/\n#why-look-the-datasaurus-dozen.html')\n\n\nIt is true that “OLS is the best linear predictor of the nonlinear regression function if the mean-squared error is used as the loss function.” . But this is not a carte-blanche justification for OLS—the best of the bad predictors is still a bad predictor. For many economic applications, it is more helpful to think and speak of “dose response curves” instead of “the effect”.\nWhile adding interaction terms or squared terms allows one incorporate heterogeneity and non-linearity, they change several features of the model—most of which are not intended. Often, there are nonsensical predicted values. For example, if the most of your age data are between \\([23,65]\\), a quadratic term can imply silly things for people aged \\(10\\) or \\(90\\).\nNonetheless, OLS provides an important piece of quantitative information that is understood by many. All models are an approximation, and sometimes only unimportant nuances are missing from a vanilla linear model. Other times, that model can be seriously misleading. (This is especially true if your making policy recommondations based on a universal ``the effect’’.) As an exploratory tool, OLS is a good guess but one whose point estimates should not be taken too seriously (in which case, the standard errors are also much less important). Before trying to find a regression specification that makes sense for the entire dataset, explore local relationships.\n\n\nLocal Relationships.\nScatterplots are a great and simplest plot for bivariate data that simply plots each observation. There are many extensions and similar tools. The example below shows two ways of summarizing the information; in both cases helping you understand how the central tendency and dispersion change.\n\n\nCode\n##################\n# Application: Summarizing wages\n##################\nlibrary(wooldridge)\n\n## Plot 1\nplot(wage~educ, data=wage1, pch=16, col=grey(0,.1))\neduc_means &lt;- aggregate(wage1[,c(\"wage\",\"educ\")], list(wage1$educ), mean)\npoints(wage~educ, data=educ_means, pch=17, col='blue', type='b')\ntitle(\"Grouped Means and Scatterplot\", font.main=1)\n\n\n\n\n\n\n\n\n\nCode\n\n## Plot 2 (Less informative!)\n#barplot(wage~educ, data=educ_means)\n#title(\"Bar Plot of Grouped Means\")\n\n\n\n\nRegressograms.\nJust as we use histograms to describe the distribution of a random variable, we can use a regressogram for conditional relationships. Specifically, we can use dummies for exclusive intervals or bins to estimate how the average value of \\(Y\\) varies with \\(X\\).\nAfter dividing \\(X\\) into \\(1,...L\\) exclusive bins of width \\(h\\). Each bin has a midpoint, \\(x\\), and an associated dummy variable \\(D(\\hat{X}_{i},x,h)\\). Then conduct a dummy variable regression \\[\\begin{eqnarray}\n\\hat{Y}_{i} &=& \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} \\alpha(x) D(\\hat{X}_{i},x,h)  + e_{i}.\n\\end{eqnarray}\\] Notice that each bin has \\(N(x,h) = \\sum_{i}^{N}D(\\hat{X}_{i},x,h)\\) observations. We can split the dataset into parts associated with each bin; \\[\\begin{eqnarray}\n\\label{eqn:regressogram1}\n\\sum_{i}^{N}\\left[e_{i}\\right]^2\n&=& \\sum_{i}^{N}\\left[\\hat{Y}_{i}- \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} \\alpha(x,h) D(\\hat{X}_{i},x,h) \\right]^2 \\\\\n&=& \\sum_{i}^{N(x_{1},h)}\\left[\\hat{Y}_{i}- \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} \\alpha(x,h) D(\\hat{X}_{i},x,h) \\right]^2 + ...  \\nonumber  \\\\\n& & \\sum_{i}^{N(x_{L},h)}\\left[\\hat{Y}_{i}- \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} \\alpha(x,h) D(\\hat{X}_{i},x,h) \\right]^2 \\\\\n&=& \\sum_{i}^{N(x_{1},h)}\\left[\\hat{Y}_{i}- \\alpha\\left(x_1,h\\right) \\right]^2 + ... \\sum_{i}^{N(x_L,h)}\\left[\\hat{Y}_{i}-\\alpha\\left(x_L,h\\right) \\right]^2 % +~ (N-1)\\sum_{i}\\hat{Y}_{i}\n\\end{eqnarray}\\] Then notice that we can optimize each bin separately; \\[\\begin{eqnarray}\n\\label{eqn:regressogram2}\n\\text{argmin}_{ \\left\\{ \\alpha(x,h) \\right\\} } \\sum_{i}^{N}\\left[e_{i}\\right]^2\n&=& \\text{argmin}_{ \\left\\{ \\alpha(x,h) \\right\\} } \\sum_{i}^{N(x,h)}\\left[\\hat{Y}_{i}- \\alpha\\left(x,h\\right) \\right]^2,\n\\end{eqnarray}\\] since, in either case, minimizing yields \\[\\begin{eqnarray}\n0 &=& -2 \\sum_{i}^{N(x,h)}\\left[ \\hat{Y}_{i} - \\alpha(x,h)  \\right] \\\\\n\\widehat{\\alpha}(x,h) &=& \\frac{\\sum_{i}^{N(x,h)} \\hat{Y}_{i}}{ N(x,h) } .\n\\end{eqnarray}\\] As such, the OLS regression yields coefficients that are intepreted as the conditional mean of \\(Y\\). We can directly compute the same statistic directly by simply takes the average value of \\(Y\\) for all \\(i\\) observations in a particular bin.\nInterestingly, we can obtain the same statistic from weighted least squares regression. For some specific design point, \\(x\\), we can find \\(\\widehat{\\alpha}(x, h)\\) by minimizing \\[\\begin{eqnarray}\n& & \\sum_{i}^{N}\\left[ \\hat{Y}_{i}- \\alpha(x,h) \\right]^2  D(\\hat{X}_{i},x,h) \\\\\n&=& \\sum_{i}^{N(x_{1},h)}\\left[ \\hat{Y}_{i}- \\alpha(x,h) \\right]^2  D(\\hat{X}_{i},x,h) + ... \\sum_{i}^{N(x_{L},h)}\\left[ \\hat{Y}_{i}- \\alpha(x,h) \\right]^2  D(\\hat{X}_{i},x,h) \\\\\n&=& \\sum_{i}^{N(x,h)}\\left[\\hat{Y}_{i}- \\alpha\\left(x,h\\right) \\right]^2\n\\end{eqnarray}\\]\nFinally, predicted values are \\(\\widehat{Y}_{i} = \\sum_{x} \\widehat{\\alpha}(x,h) D(\\hat{X}_{i},x,h)\\).\nConsider this three-bin example of how age affects wage. \\[\\begin{eqnarray}\n\\text{Wage} &=& \\alpha_{1} \\mathbf{1}\\left(\\text{Age} &lt; 23\\right) + \\alpha_{2} \\mathbf{1}\\left(\\text{Age} \\in [23,65) \\right) +  \\alpha_{2} \\mathbf{1}\\left( \\text{Age} \\geq 65) \\right) + \\epsilon \\nonumber\n\\end{eqnarray}\\] I.e., the main effect on wages is whether your the right age to work (not in school or retired). But you could also look at yearly bins and see if that tri-part grouping emerges naturally or not (e.g., whether we shouldn’t group all working-age people together). For example,\n\n\nCode\n##################\n# Regressogram\n##################\n\n## Ages\nXmx &lt;- 70\nXmn &lt;- 15\n\n##Generate N Observations\ndat_sim &lt;- function(n=1000){\n    n  &lt;- 1000\n    X &lt;- seq(Xmn,Xmx,length.out=n)\n    ## Random Productivity\n    e &lt;- runif(n, 0, 1E6)\n    beta &lt;-  1E-10*exp(1.4*X -.015*X^2)\n    Y    &lt;-  (beta*X + e)/10\n    return(data.frame(Y,X))\n}\n\n\ndat &lt;- dat_sim(1000)\nX &lt;- dat$X\n## Plot\nplot(Y~X, data=dat, pch=16, col=grey(0,.1),\n    ylab='Yearly Productivity ($)', xlab='Age' )\n\n## Regression Estimates\nreg1  &lt;- lm(Y~X, data=dat) ## OLS\npred1 &lt;- cbind( Y=predict(reg1), X)[order(X),]\n\ndat$xcc   &lt;- cut(X, seq(Xmn-1,Xmx,length.out=6)) ## Course Age Bins\nreg2  &lt;- lm(Y~xcc, data=dat)\npred2 &lt;- cbind( Y=predict(reg2), X)[order(X),]\n\ndat$xcf   &lt;- cut(X, seq(Xmn-1, Xmx, length.out=31)) ## Fine Age Bins\nreg3  &lt;- lm(Y~xcf, data=dat)\npred3 &lt;- cbind( Y=predict(reg3), X)[order(X),]\n\n## Compare Models\nlines(Y~X, data=pred1, lwd=2, col=2)\nlines(Y~X, data=pred2, lwd=2, col=3)\nlines(Y~X, data=pred3, lwd=2, col=4)\nlegend('topleft',\n    legend=c('Linear Regression','Regressogram (5)','Regressogram (30)'),\n    lty=1, col=2:4, cex=.8)\n\n\n\n\n\n\n\n\n\n\n\nPiecewise Regression.\nThe regressogram depicts locally constant relationships. We can also included slope terms within each bin to allow for locally linear relationships. This is often called segmented/piecewise regression, which runs a separate regression for different subsets of the data.\n\\[\\begin{eqnarray}\n\\hat{Y}_{i} &=& \\sum_{x} \\left[\\alpha(x) + \\hat{X}_{i}\\beta(x) \\right] D(\\hat{X}_{i},x,h) + \\epsilon_{i}.\n\\end{eqnarray}\\]\n\n\nCode\n##################\n# Regressogram w/ Slopes\n##################\n\n## Plot\ndat &lt;- dat_sim(1000)\nX &lt;- dat$X\nplot(Y~X, data=dat, pch=16, col=grey(0,.1),\n    ylab='Yearly Productivity ($)', xlab='Age' )\n\n## Course Age Bins\n#### Single Regression\ndat$xcc   &lt;- cut(X, seq(Xmn-1,Xmx,length.out=6)) ## Course Age Bins\npred4 &lt;- cbind( Y=predict( lm(Y~xcc*X, data=dat) ), X)[order(X),]\n#### Split Sample Regressions\ndat4 &lt;- split( dat, dat$xcc)\npred4_B &lt;- lapply(dat4, function(d){\n    pred_d &lt;- cbind( Y=predict( lm(Y~X, d)), X=d$X)\n})\npred4_B &lt;- as.data.frame(do.call('rbind', pred4_B))\npred4_B &lt;- pred4_B[order(pred4_B$X),]\n\nlines(Y~X, data=pred4, lwd=2, col=5, lty=1)\nlines(Y~X, data=pred4_B, lwd=4, col=5, lty=3)\n\n\n## Fine Age Bins\n#### Single Regression\ndat$xcf  &lt;- cut(X, seq(Xmn-1,Xmx,length.out=31)) ## Course Age Bins\npred5 &lt;- cbind( Y=predict(lm(Y~xcf*X,data=dat)), X)[order(X),]\n#### Split Sample Regressions\ndat5 &lt;- split(dat, dat$xcf)\npred5_B &lt;- lapply(dat5, function(d){\n    pred_d &lt;- cbind( Y=predict(lm(Y~X, d)), X=d$X)\n})\npred5_B &lt;- as.data.frame(do.call('rbind', pred5_B))\npred5_B &lt;- pred5_B[order(pred5_B$X),]\n\n## Compare Models\nlines(Y~X, data=pred5, lwd=2, col=6, lty=1)\nlines(Y~X, data=pred5_B, lwd=4, col=6, lty=3)\nlegend('topleft', \n    legend=c('5 bins','30 bins'),\n    lty=1, col=5:6, cex=.8)\n\n\n\n\n\n\n\n\n\nHere is another example with a real dataset\n\n\nCode\nxy &lt;- USArrests[,c('Murder','UrbanPop')]\ncolnames(xy) &lt;- c('y','x')\n\n# Globally Linear\nreg &lt;- lm(y~x, data=xy)\n\n# Diagnose Fit\n#plot( fitted(reg), resid(reg), pch=16, col=grey(0,.5))\n#plot( xy$x, resid(reg), pch=16, col=grey(0,.5))\n\n# Linear in 2 Pieces (subsets)\nxcut2 &lt;- cut(xy$x,2)\nxy_list2 &lt;- split(xy, xcut2)\nregs2 &lt;- lapply(xy_list2, function(xy_s){\n    lm(y~x, data=xy_s)\n})\nsapply(regs2, coef)\n##             (31.9,61.5] (61.5,91.1]\n## (Intercept)  -0.2836303  4.15337509\n## x             0.1628157  0.04760783\n\n# Linear in 3 Pieces (subsets or bins)\nxcut3 &lt;- cut(xy$x, seq(32,92,by=20)) # Finer Bins\nxy_list3 &lt;- split(xy, xcut3)\nregs3 &lt;- lapply(xy_list3, function(xy_s){\n    lm(y~x, data=xy_s)\n})\nsapply(regs3, coef)\n##                (32,52]    (52,72]      (72,92]\n## (Intercept) 4.60313390 2.36291848  8.653829140\n## x           0.08233618 0.08132841 -0.007174454\n\n\nCompare Predictions\n\n\nCode\npred1 &lt;- data.frame(yhat=predict(reg), x=reg$model$x)\npred1 &lt;- pred1[order(pred1$x),]\n\npred2 &lt;- lapply(regs2, function(reg){\n    data.frame(yhat=predict(reg), x=reg$model$x)\n})\npred2 &lt;- do.call(rbind,pred2)\npred2 &lt;- pred2[order(pred2$x),]\n\npred3 &lt;- lapply(regs3, function(reg){\n    data.frame(yhat=predict(reg), x=reg$model$x)\n})\npred3 &lt;- do.call(rbind,pred3)\npred3 &lt;- pred3[order(pred3$x),]\n\n# Compare Predictions\nplot(y ~ x, pch=16, col=grey(0,.5), dat=xy)\nlines(yhat~x, pred1, lwd=2, col=2)\nlines(yhat~x, pred2, lwd=2, col=4)\nlines(yhat~x, pred3, lwd=2, col=3)\nlegend('topleft',\n    legend=c('Globally Linear', 'Peicewise Linear (2)','Peicewise Linear (3)'),\n    lty=1, col=c(2,4,3), cex=.8)\n\n\n\n\n\n\n\n\n\nFor many things, this pseudo-smoothing is “good enough” or even “great”. It really depends on the question. (It is also super computationally efficient.) But sometimes we want to (a) smooth estimates or (b) to make predictions or estimate derivatives at the data. To cover more advanced regression methods, we will need to first learn about kernel density estimation.",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Local Regression</span>"
    ]
  },
  {
    "objectID": "02_04_KernelIntro.html#kernel-density-estimation",
    "href": "02_04_KernelIntro.html#kernel-density-estimation",
    "title": "12  Local Regression",
    "section": "12.2 Kernel Density Estimation",
    "text": "12.2 Kernel Density Estimation\nA kernel density is generally a “smooth” version of a histogram. We estimate the density at many points (e.g., all unique values \\(x\\) in the dataset), not just the midpoints of exclusive bins. The uniform kernel and density estimator is \\[\\begin{eqnarray}\n\\label{eqn:uniform}\n\\widehat{f}_{unif}(x) &=& \\frac{1}{N} \\sum_{i}^{N} \\frac{k_{U}(\\hat{X}_{i}, x, h) }{2h} \\\\\nk_{U}\\left( \\hat{X}_{i}, x, h \\right) &=& \\mathbf{1}\\left(\\frac{|\\hat{X}_{i}-x|}{h}&lt;1\\right)\n= \\mathbf{1}\\left( \\hat{X}_{i} \\in \\left( x-h, x + h\\right) \\right).\n\\end{eqnarray}\\] Comparing equations \\(\\ref{eqn:uniform}\\) to \\(\\ref{eqn:indicator}\\), we can see the uniform kernel is essentially the histogram but without the restriction that \\(x\\) must be a midpoint of exclusive bins. Typically, the points \\(x\\) are chosen to be either the unique observations or some equidistant set of “design points”.\nWe can also replace the uniform kernel with a more general kernel function \\(k\\left( \\hat{X}_{i}, x, h \\right)= K\\left( \\frac{|\\hat{X}_{i}-x|}{h} \\right)\\). (We normalize \\(k\\), which is easier to program, so that the kernel function \\(K\\) take a single argument that is easier to read.) The general idea behind kernel density is to use windows around each \\(x\\) that potentially overlap, rather than partitioning the range of \\(X\\) into exclusive bins.\nWe define a general kernel function as a non-negative real-valued function \\(K\\) that integrates to unity: \\[\\begin{eqnarray}\n\\int_{-\\infty}^{\\infty} K(v) dv &=& 1\n\\end{eqnarray}\\] We also only examine symmetric kernels, as some texts also include symmetric in the definition of a kernel; \\(K(v) = K(-v)\\).\nFor examples of some common kernels, see https://en.wikipedia.org/wiki/Kernel_(statistics)#In_non-parametric_statistics. In my view, these are the most intuitive and common.\n\n\nCode\n##################\n# Kernel Density Functions\n##################\n\nX &lt;- seq(-2,2, length.out=1001)\n\nplot.new()\nplot.window(xlim=c(-1,1), ylim=c(0,1))\n\nh &lt;- 1\nlines( dunif(X,-h,h)~X, col=1, lty=1)\n\nh &lt;- 1/2\nlines( dnorm(X,0,h)~X, col=2, lty=1)\n\ndtricub &lt;- function(X, x=0, h){\n    u &lt;- abs(X-x)/h\n    fu &lt;- 70/81*(1-u^3)^3/h*(u &lt;= 1)\n    return(fu)\n}\nh &lt;- 1\nlines( dtricub(X,0,h)~X, col=3, lty=1)\n\nh &lt;- 1/2\nlines(density(x=0, bw=h, kernel=\"epanechnikov\"), col=4, lty=1)\n## Note that \"density\" defines h slightly differently\n\nsegments(0,1,0,0, col=grey(0,1), lwd=2, lend=2)\naxis(1)\naxis(2)\n\nlegend('topright', lty=1, col=1:4,\n    legend=c('uniform(1)', 'gaussian(1/2)', 'tricubic(1)', 'epanechnikov(1)'))\n\n\n\n\n\n\n\n\n\nCode\n\n\n## Try others:\n## lines(density(x=0, bw=1/2, kernel=\"triangular\"),col=4, lty=1)\n\n\nOnce we have picked a kernel (which particular one is not particularly important) we can use it to compute density estimates.\n\n\nCode\n##################\n# Kernel Density Estimation\n##################\n\nN &lt;- 1000\ne &lt;- rweibull(N,100,100)\nebins &lt;- seq(floor(min(e)), ceiling(max(e)), length.out=12)\n\n## Histogram Estimates at 12 points\nxbks &lt;- c(ebins[1]-diff(ebins)[1]/2, ebins+diff(ebins)[1]/2)\nhist(e, freq=F, main='', breaks=xbks, ylim=c(0,.4), border=NA)\nrug(e, lwd=.07, col=grey(0,.5))  ## Sample\n\n\n## Manually Compute Uniform Estimate at X=100 with h=2\n# w100 &lt;- (e &lt; 101)*(e &gt; 99)\n# sum(w100)/(N*2)\n\n## Gaussian Estimates at same points as histogram\nF_hat &lt;- sapply(ebins, function(x,h=.5){\n    kx &lt;- dnorm( abs(e-x)/h )\n    fx &lt;- sum(kx,na.rm=T)/(h*N)\n    fx\n})\nlines(ebins, F_hat, col=1, lty=1, type='o')\n## Verify the same\nfhat1 &lt;- density(e, n=12, from=min(ebins), to=max(ebins), bw=.5)\npoints(fhat1$x, fhat1$y, pch=16, col=rgb(0,0,1,.5), cex=1.5)\n\n## Gaussian Estimates at all sample points\nfhat2 &lt;- density(e, n=1000, from=min(ebins), to=max(ebins), bw=.5)\npoints(fhat2$x, fhat2$y, pch=16, col=rgb(1,0,0,.25), cex=.5)\n\nlegend('topleft', pch=c(15,16,16),\n    col=c(grey(0,.5),rgb(0,0,1,.5), rgb(1,0,0,.25)),\n    title='Type (# Design Points)', bty='n',\n    legend=c('Histogram (12)',\n    'Gaussian-Kernel (12)',\n    'Gaussian-Kernel (1000)'))",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Local Regression</span>"
    ]
  },
  {
    "objectID": "02_04_KernelIntro.html#local-linear-regression",
    "href": "02_04_KernelIntro.html#local-linear-regression",
    "title": "12  Local Regression",
    "section": "12.3 Local Linear Regression",
    "text": "12.3 Local Linear Regression\nIt is safer to assume that you could be analyzing data with nonlinear relationships. A general nonparametric model is written as \\[\\begin{eqnarray}\n\\hat{Y}_{i} = m(\\hat{X}_{i}) + \\epsilon_{i}\n\\end{eqnarray}\\] where \\(m\\) is an unknown continuous function and \\(\\epsilon\\) is white noise. (As such, the linear model is a special case.) You can estimate the model (the mean of \\(Y\\) conditional on \\(X=x\\)) with a regressogram or a variety of other least-squares procedures.\n\nLocally Constant.\nConsider a point \\(x\\) and suppose \\(\\hat{Y}_{i} = \\alpha(x) + e_{i}\\) locally. Then notice a weighted OLS estimator with uniform kernel weights yields \\[\\begin{eqnarray} \\label{eqn:lcls}\n& & \\min_{\\alpha(x)}~ \\sum_{i}^{N}\\left[e_{i} \\right]^2 k_{U}\\left( \\hat{X}_{i}, x, h \\right) \\\\\n\\Rightarrow & & -2 \\sum_{i}^{N}\\left[\\hat{Y}_{i}- \\alpha(x) \\right] k_{U}\\left(\\hat{X}_{i}, x, h\\right) = 0\\\\\n\\label{eqn:lcls1}\n\\Rightarrow & & \\widehat{\\alpha_{U}}(x)\n= \\frac{\\sum_{i} \\hat{Y}_{i} k_{U} \\left( \\hat{X}_{i}, x, h \\right) }{ \\sum_{i} k_{U}\\left( \\hat{X}_{i}, x, h \\right) }\n= \\sum_{i} \\hat{Y}_{i} \\left[ \\frac{ k_{U} \\left( \\hat{X}_{i}, x, h \\right) }{ \\sum_{i} k_{U}\\left( \\hat{X}_{i}, x, h \\right)} \\right] =  \\sum_{i} \\hat{Y}_{i} w_{i},\n\\end{eqnarray}\\] where weight \\(w_{i} = \\mathbf{1}\\left( |\\hat{X}_{i} - x| &lt; h \\right)/N\\). The last equality is derived analogously to equation \\(\\ref{eqn:sum}\\); where \\(k_{U} \\left( \\hat{X}_{i}, x, h \\right)\\) is either one or zero, and \\(\\sum_{i} k_{U} \\left( \\hat{X}_{i}, x, h \\right) = N(x)\\).\nWhen \\(N\\) is small, \\(\\widehat{\\alpha_U}(x)\\) is typically estimated for each observed value: \\(x \\in \\{ x_{1},...x_{N} \\}\\). For large datasets, you can select a subset or evenly spaced values of \\(x\\) for which to estimate \\(\\widehat{\\alpha_{U}}(x)\\). If we use exclusive bins, then equation \\(\\ref{eqn:regressogram1}\\) equals \\(\\ref{eqn:lcls1}\\), which shows the regressogram is a kernel regression weights that recovers the conditional mean. This regressogram is more crude but can be estimated with OLS.\n\n\nCode\n##################\n# LCLS\n##################\n## Generate Sample Data\nx &lt;- 1:5\ny &lt;- runif(length(x))\n## plot(x,y)\n\n## Manually Compute Estimate at X=3\nw3 &lt;- dunif(x-3,-1,1) #(x &lt; 4)*(x &gt; 2)\nyhat_3 &lt;- sum(w3*y)\nyhat_3\n## [1] 0.8262743\n\n\nThe basic idea also generalizes other kernels. As such, a kernel regression using uniform weights is often called a ``naive kernel regression’’. Typically, kernel regressions use kernels that weight nearby observations more heavily. We can also add a slope term to improve the fit.\nIf \\(X\\) represents time, then the local constant regressions is also called a moving average. With non-uniform kernel weights, we have a weighted moving average.\n\n\nLocally Linear.\nA less simple case is a local linear regression which conducts a linear regression for each data point using a subsample of data around it. Consider a point \\(x\\) and suppose \\(\\hat{Y}_{i} = \\alpha(x) + \\beta(x) \\hat{X}_{i} + e_{i}\\) for data near \\(x\\). The weighted OLS estimator with kernel weights is \\[\\begin{eqnarray}\n& & \\min_{\\alpha(x),\\beta(x)}~ \\sum_{i}^{N}\\left[\\hat{Y}_{i}- \\alpha(x) - \\beta(x) \\hat{X}_{i} \\right]^2 K\\left(\\frac{|\\hat{X}_{i}-x|}{h}\\right)\n\\end{eqnarray}\\] Deriving the optimal values \\(\\widehat{\\alpha}(x)\\) and \\(\\widehat{\\beta}(x)\\) for \\(k_{U}\\) is left as a homework exercise.2\n\n\nCode\n# ``Naive\" Smoother\npred_fun &lt;- function(x0, h, xy){\n    # Assign equal weight to observations within h distance to x0\n    # 0 weight for all other observations\n    ki   &lt;- dunif(xy$x, x0-h, x0+h) \n    llls &lt;- lm(y~x, data=xy, weights=ki)\n    yhat_i &lt;- predict(llls, newdata=data.frame(x=x0))\n}\n\nX0 &lt;- sort(unique(xy$x))\npred_lo1 &lt;- sapply(X0, pred_fun, h=2, xy=xy)\npred_lo2 &lt;- sapply(X0, pred_fun, h=20, xy=xy)\n\nplot(y~x, pch=16, data=xy, col=grey(0,.5),\n    ylab='Murder Rate', xlab='Population Density')\ncols &lt;- c(rgb(.8,0,0,.5), rgb(0,0,.8,.5))\nlines(X0, pred_lo1, col=cols[1], lwd=1, type='o')\nlines(X0, pred_lo2, col=cols[2], lwd=1, type='o')\nlegend('topleft', title='Locally Linear',\n    legend=c('h=2 ', 'h=20'),\n    lty=1, col=cols, cex=.8)\n\n\n\n\n\n\n\n\n\nNote that there are more complex versions of local linear regressions (see https://shinyserv.es/shiny/kreg/ for a nice illustration.) An even more complex (and more powerful) version is loess, which uses adaptive bandwidths in order to have a similar number of data points in each subsample (especially useful when \\(X\\) is not uniform.)\n\n\nCode\n# Adaptive-width subsamples with non-uniform weights\nxy0 &lt;- xy[order(xy$x),]\nplot(y~x, pch=16, col=grey(0,.5), dat=xy0)\n\nreg_lo4 &lt;- loess(y~x, data=xy0, span=.4)\nreg_lo8 &lt;- loess(y~x, data=xy0, span=.8)\n\ncols &lt;- hcl.colors(3,alpha=.75)[-3]\nlines(xy0$x, predict(reg_lo4),\n    col=cols[1], type='o', pch=2)\nlines(xy0$x, predict(reg_lo8),\n    col=cols[2], type='o', pch=2)\n\nlegend('topleft', title='Loess',\n    legend=c('span=.4 ', 'span=.8'),\n    lty=1, col=cols, cex=.8)\n\n\n\n\n\n\n\n\n\n\n\nConfidence Bands.\nThe smoothed predicted values estimate the local means. So we can also construct confidence bands\n\n\nCode\n# Loess\nxy0 &lt;- xy[order(xy$x),]\nX0 &lt;- unique(xy0$x)\nreg_lo &lt;- loess(y~x, data=xy0, span=.8)\n\n# Jackknife CI\njack_lo &lt;- sapply(1:nrow(xy), function(i){\n    xy_i &lt;- xy[-i,]\n    reg_i &lt;- loess(y~x, dat=xy_i, span=.8)\n    predict(reg_i, newdata=data.frame(x=X0))\n})\njack_cb &lt;- apply(jack_lo,1, quantile,\n    probs=c(.025,.975), na.rm=T)\n\n# Plot\nplot(y~x, pch=16, col=grey(0,.5), dat=xy0)\npreds_lo &lt;- predict(reg_lo, newdata=data.frame(x=X0))\nlines(X0, preds_lo,\n    col=hcl.colors(3,alpha=.75)[2],\n    type='o', pch=2)\n# Plot CI\npolygon(\n    c(X0, rev(X0)),\n    c(jack_cb[1,], rev(jack_cb[2,])),\n    col=hcl.colors(3,alpha=.25)[2],\n    border=NA)",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Local Regression</span>"
    ]
  },
  {
    "objectID": "02_04_KernelIntro.html#footnotes",
    "href": "02_04_KernelIntro.html#footnotes",
    "title": "12  Local Regression",
    "section": "",
    "text": "The same principles holds when comparing two groups: http://www.stat.columbia.edu/~gelman/research/published/causal_quartet_second_revision.pdf↩︎\nNote that one general benefit of LLLS is with edge effects (see homework). Another is that it is theoretically motivated: assuming that \\(Y_{i}=m(X_{i}) + \\epsilon_{i}\\), we can then take a Taylor approximation: \\(m(X_{i}) + \\epsilon_{i} \\approx m(x) + m'(x)[X_{i}-x] + \\epsilon_{i} = [m(x) - m'(x)x ] + m'(x)X_{i} + \\epsilon_{i} = \\alpha(x) + \\beta(x) X_{i}\\). As such, a third benefit is that the estimated coefficient \\(\\widehat{\\beta}\\) can be interpreted as a gradient estimate at \\(x\\).↩︎",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Local Regression</span>"
    ]
  },
  {
    "objectID": "02_05_MiscTopics.html",
    "href": "02_05_MiscTopics.html",
    "title": "13  Misc. Bivariate Topics",
    "section": "",
    "text": "13.1 Predictions",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Misc. Bivariate Topics</span>"
    ]
  },
  {
    "objectID": "02_05_MiscTopics.html#predictions",
    "href": "02_05_MiscTopics.html#predictions",
    "title": "13  Misc. Bivariate Topics",
    "section": "",
    "text": "Describe vs. Explain vs. Predict.\n\n\nPrediction Intervals.\nIn addition to confidence intervals, we can also compute a prediction interval which estimate the variability of new data rather than a statistic\nIn this example, we consider a single variable and compute the frequency each value was covered.\n\n\nCode\nx &lt;- runif(1000)\n# Middle 90% of values\nxq0 &lt;- quantile(x, probs=c(.05,.95))\n\nbks &lt;- seq(0,1,by=.01)\nhist(x, breaks=bks, border=NA,\n    main='Prediction Interval', font.main=1)\nabline(v=xq0)\n\n\n\n\n\n\n\n\n\nCode\n\npaste0('we are 90% confident that the a future data point will be between ', \n    round(xq0[1],2), ' and ', round(xq0[2],2) )\n## [1] \"we are 90% confident that the a future data point will be between 0.05 and 0.95\"\n\n\nIn this example, we consider a range for \\(y_{i}(x)\\) rather than for \\(m(x)\\). These intervals also take into account the residuals — the variability of individuals around the mean.\n\n\nCode\n# Bivariate Data from USArrests\nxy &lt;- USArrests[,c('Murder','UrbanPop')]\ncolnames(xy) &lt;- c('y','x')\nxy0 &lt;- xy[order(xy$x),]\n\n\nFor a nice overview of different types of intervals, see https://www.jstor.org/stable/2685212. For an in-depth view, see “Statistical Intervals: A Guide for Practitioners and Researchers” or “Statistical Tolerance Regions: Theory, Applications, and Computation”. See https://robjhyndman.com/hyndsight/intervals/ for constructing intervals for future observations in a time-series context. See Davison and Hinkley, chapters 5 and 6 (also Efron and Tibshirani, or Wehrens et al.)\n\n\nCode\n# From \"Basic Regression\"\nxy0 &lt;- xy[order(xy$x),]\nX0 &lt;- unique(xy0$x)\nreg_lo &lt;- loess(y~x, data=xy0, span=.8)\npreds_lo &lt;- predict(reg_lo, newdata=data.frame(x=X0))\n\n\n# Jackknife CI\njack_lo &lt;- sapply(1:nrow(xy), function(i){\n    xy_i &lt;- xy[-i,]\n    reg_i &lt;- loess(y~x, dat=xy_i, span=.8)\n    predict(reg_i, newdata=data.frame(x=X0))\n})\n\nboot_regs &lt;- lapply(1:399, function(b){\n    b_id &lt;- sample( nrow(xy), replace=T)\n    xy_b &lt;- xy[b_id,]\n    reg_b &lt;- lm(y~x, dat=xy_b)\n})\n\nplot(y~x, pch=16, col=grey(0,.5),\n    dat=xy0, ylim=c(0, 20))\nlines(X0, preds_lo,\n    col=hcl.colors(3,alpha=.75)[2],\n    type='o', pch=2)\n\n# Estimate Residuals CI at design points\nres_lo &lt;- sapply(1:nrow(xy), function(i){\n    y_i &lt;- xy[i,'y']\n    preds_i &lt;- jack_lo[,i]\n    resids_i &lt;- y_i - preds_i\n})\nres_cb &lt;- apply(res_lo, 1, quantile,\n    probs=c(.025,.975), na.rm=T)\n\n# Plot\nlines( X0, preds_lo +res_cb[1,],\n    col=hcl.colors(3,alpha=.75)[2], lt=2)\nlines( X0, preds_lo +res_cb[2,],\n    col=hcl.colors(3,alpha=.75)[2], lty=2)\n\n\n\n# Smooth estimates \nres_lo &lt;- lapply(1:nrow(xy), function(i){\n    y_i &lt;- xy[i,'y']\n    x_i &lt;- xy[i,'x']\n    preds_i &lt;- jack_lo[,i]\n    resids_i &lt;- y_i - preds_i\n    cbind(e=resids_i, x=x_i)\n})\nres_lo &lt;- as.data.frame(do.call(rbind, res_lo))\n\nres_fun &lt;- function(x0, h, res_lo){\n    # Assign equal weight to observations within h distance to x0\n    # 0 weight for all other observations\n    ki &lt;- dunif(res_lo$x, x0-h, x0+h) \n    ei &lt;- res_lo[ki!=0,'e']\n    res_i &lt;- quantile(ei, probs=c(.025,.975), na.rm=T)\n}\nres_lo2 &lt;- sapply(X0, res_fun, h=15, res_lo=res_lo)\n\nlines( X0, preds_lo + res_lo2[1,],\n    col=hcl.colors(3,alpha=.75)[2], lty=1, lwd=2)\nlines( X0, preds_lo + res_lo2[2,],\n    col=hcl.colors(3,alpha=.75)[2], lty=1, lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Bootstrap Prediction Interval\nboot_resids &lt;- lapply(boot_regs, function(reg_b){\n    e_b &lt;- resid(reg_b)\n    x_b &lt;- reg_b$model$x\n    res_b &lt;- cbind(e_b, x_b)\n})\nboot_resids &lt;- as.data.frame(do.call(rbind, boot_resids))\n# Homoskedastic\nehat &lt;- quantile(boot_resids$e_b, probs=c(.025, .975))\nx &lt;- quantile(xy$x,probs=seq(0,1,by=.1))\nboot_pi &lt;- coef(reg)[1] + x*coef(reg)['x']\nboot_pi &lt;- cbind(boot_pi + ehat[1], boot_pi + ehat[2])\n\n# Plot Bootstrap PI\nplot(y~x, dat=xy, pch=16, main='Prediction Intervals',\n    ylim=c(-5,20), font.main=1)\npolygon( c(x, rev(x)), c(boot_pi[,1], rev(boot_pi[,2])),\n    col=grey(0,.2), border=NA)\n\n# Parametric PI (For Comparison)\n#pi &lt;- predict(reg, interval='prediction', newdata=data.frame(x))\n#lines( x, pi[,'lwr'], lty=2)\n#lines( x, pi[,'upr'], lty=2)\n\n\n\n\nCrossvalidation.\nPerhaps the most common approach to selecting a bandwidth is to minimize error. Leave-one-out Cross-validation minimizes the average “leave-one-out” mean square prediction errors: \\[\\begin{eqnarray}\nCV &=& \\min_{\\mathbf{H}} \\quad \\frac{1}{n} \\sum_{i=1}^{n} \\left[ Y_{i} - \\widehat{Y_{[i]}}(\\mathbf{X},\\mathbf{H}) \\right]^2,\n% \\widehat{Y_{[i]}}(\\mathbf{X},\\mathbf{H}) &=& \\sum_{j\\neq i} k(\\mathbf{X}_{j},\\mathbf{X}_{i},\\mathbf{H}) \\left[ \\widehat{\\alpha}(\\mathbf{X}_{j}) +  \\widehat{\\beta}(\\mathbf{X}_{j}) \\mathbf{X}_{i} \\right]\n\\end{eqnarray}\\] where \\(\\widehat{Y_{[i]}}(\\mathbf{X},\\mathbf{H})\\) is the predicted value at \\(\\mathbf{X}_{i}\\) based on a dataset that excludes \\(\\mathbf{X}_{i}\\).\nThere are many types of cross-validation . For example, one extension is , which splits \\(N\\) datapoints into \\(k=1...K\\) groups, each sized \\(B\\), and predicts values for the left-out group. adjusts for the degrees of freedom, whereas the function in R uses by default. You can refer to extensions on a case by case basis.\nMinimizing out-sample prediction error is perhaps the simplest computational approach to choose bandwidths, and it also addresses an issue that plagues observational studies in the social sciences of explanations without predictions. It is a problem if your model explains everything and predicts nothing, but minimizing prediction error is not necessarily “best”.\n\n\nCode\n##################                                         \n# Crossvalidated bandwidth for regression\n##################\ny &lt;- (CASchools$read + CASchools$math) / 2\nxy_mat &lt;- data.frame(y=y, x1=CASchools$income)\nlibrary(np)\n\n## Grid Search\nBWS &lt;- seq(1,10,length.out=20)\nBWS_CV &lt;- sapply(BWS, function(bw){\n    E_bw &lt;- sapply(1:nrow(xy_mat), function(i){\n        llls &lt;- npreg(y~x1, data=xy_mat[-i,], \n            bws=bw, regtype=\"ll\",\n            ckertype='epanechnikov', bandwidth.compute=F)\n        pred_i &lt;- predict(llls, newdata=xy_mat[i,])\n        e &lt;-  (pred_i- xy_mat[i,'y'])\n        return(e)\n    })\n    return( mean(E_bw^2) )\n})\n\n## Plot MSE\npar(mfrow=c(1,2))\nplot(BWS, BWS_CV, ylab='CV', pch=16, \n    xlab='bandwidth (h)',)\n## Plot Resulting Predictions\nbw &lt;- BWS[which.min(BWS_CV)]\nllls &lt;- npreg(y~x1, data=xy_mat, \n    ckertype='epanechnikov',\n    bws=bw, regtype=\"ll\")\nplot(xy_mat$x, predict(llls), pch=16, col=grey(0,.5),\n    xlab='X', ylab='Predictions')\nabline(a=0,b=1, lty=2)\n\n## Built in algorithmic Optimziation\nllls2 &lt;- npreg(y~x1, data=xy_mat, ckertype='epanechnikov', regtype=\"ll\")\npoints(xy_mat$x, predict(llls2), pch=2, col=rgb(1,0,0,.25))\n\n## Add legend\nadd_legend &lt;- function(...) {\n  opar &lt;- par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), \n              mar=c(0, 0, 0, 0), new=TRUE)\n  on.exit(par(opar))\n  plot(0, 0, type='n', bty='n', xaxt='n', yaxt='n')\n  legend(...)\n}\nadd_legend('topright',\n    col=c(grey(0,.5),rgb(1,0,0,.25)), \n    pch=c(16,2),\n    bty='n', horiz=T,\n    legend=c('Grid Search', 'NP-algorithm'))\n\n\n\n\nCode\n##################\n# CV Application\n##################\n\n## Smoothly Estimate X & Y Density\ny &lt;- sort(xy_mat$y)\nfy &lt;- npudens(y, bandwidth.compute=TRUE)\nx1 &lt;- sort(xy_mat$x1)\nfx &lt;- npudens(x1, bandwidth.compute=TRUE)\n## Smoothly Estimate How Y changes with X\nllls2 &lt;- npreg(y~x1,data=xy_mat,\n    ckertype='epanechnikov',\n    regtype=\"ll\", bandwidth.compute=TRUE)\n\n\nlayout( matrix(c(2,0,1,3), ncol=2, byrow=TRUE),\n    widths=c(4/5,1/5), heights=c(1/5,4/5))\n## Joint Distribution\npar(mar=c(4,4,1,1))\nplot(y~x1, data=xy_mat,\n    pch=16, col=grey(0,.25),\n    xlab=\"District Income (1000$)\", \n    ylab=\"Test Score\")\nlines( sort(xy_mat$x), predict(llls2)[order(xy_mat$x1)],\n    pch=16, col=1)\n## Marginal Distribution\npar(mar=c(0,4,1,1))\nplot(x1, predict(fx),\n    col=grey(0,1), type='l', axes=F,\n    xlab='', ylab='')\nrug(x1, col=grey(0,.25))\npar(mar=c(4,0,1,1))\nplot(predict(fy), y,\n    col=grey(0,1), type='l', axes=F,\n    xlab='', ylab='')\nrug(y, col=grey(0,.25), side=2)\n\n\n\n\nBias vs. Variance",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Misc. Bivariate Topics</span>"
    ]
  },
  {
    "objectID": "02_05_MiscTopics.html#decision-theory",
    "href": "02_05_MiscTopics.html#decision-theory",
    "title": "13  Misc. Bivariate Topics",
    "section": "13.2 Decision Theory",
    "text": "13.2 Decision Theory\n\nType II Errors and Statistical Power.\n\n\nQuality Control.\n\n\nOptimal Experiment Designs.",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Misc. Bivariate Topics</span>"
    ]
  },
  {
    "objectID": "02_06_DataAnalysis.html",
    "href": "02_06_DataAnalysis.html",
    "title": "14  Data Analysis",
    "section": "",
    "text": "14.1 Beyond Basics\nUse expansion “packages” for less common procedures and more functionality",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "02_06_DataAnalysis.html#beyond-basics",
    "href": "02_06_DataAnalysis.html#beyond-basics",
    "title": "14  Data Analysis",
    "section": "",
    "text": "CRAN.\nMost packages can be found on CRAN and can be easily installed\n\n\nCode\n# commonly used packages\ninstall.packages(\"stargazer\")\ninstall.packages(\"data.table\")\ninstall.packages(\"plotly\")\n# other statistical packages\ninstall.packages(\"extraDistr\")\ninstall.packages(\"twosamples\")\n# install.packages(\"purrr\")\n# install.packages(\"reshape2\")\n\n\nThe most common tasks also have cheatsheets you can use.\nFor example, to generate ‘exotic’ probability distributions\n\n\nCode\nlibrary(extraDistr)\n\npar(mfrow=c(1,2))\nfor(p in c(-.5,0)){\n    x &lt;- rgev(2000, mu=0, sigma=1, xi=p)\n    hist(x, breaks=50, border=NA, main=NA, freq=F)\n}\ntitle('GEV densities', outer=T, line=-1)\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(extraDistr)\n\npar(mfrow=c(1,3))\nfor(p in c(-1, 0,2)){\n    x &lt;- rtlambda(2000, p)\n    hist(x, breaks=100, border=NA, main=NA, freq=F)\n}\ntitle('Tukey-Lambda densities', outer=T, line=-1)\n\n\n\n\n\n\n\n\n\n\n\nUpdating.\nMake sure R and your packages are up to date. The current version of R and any packages used can be found (and recorded) with\n\n\nCode\nsessionInfo()\n\n\nTo update your R packages, use\n\n\nCode\nupdate.packages()\n\n\n\n\nBase.\nWhile additional packages can make your code faster, they also create dependancies that can lead to problems. So learn base R well before becoming dependent on other packages\n\nhttps://bitsofanalytics.org/posts/base-vs-tidy/\nhttps://jtr13.github.io/cc21fall2/comparison-among-base-r-tidyverse-and-datatable.html\n\n\n\nAdvanced Programming.\n\n\nAdvanced and Optional\n\n\nSometimes you will want to install a package from GitHub. For this, you can use devtools or its light-weight version remotes\n\n\nCode\ninstall.packages(\"devtools\")\ninstall.packages(\"remotes\")\n\n\nNote that to install devtools, you also need to have developer tools installed on your computer.\n\nWindows: Rtools\nMac: Xcode\n\nTo color terminal output on Linux systems, you can use the colorout package\n\n\nCode\nlibrary(remotes)\n# Install &lt;https://github.com/jalvesaq/colorout\n# to .libPaths()[1]\ninstall_github('jalvesaq/colorout')\nlibrary(colorout)\n\n\nNote that after updating R, you can update all packages stored in all .libPaths() with the following command\n\n\nCode\nupdate.packages(checkBuilt=T, ask=F)\n# install.packages(old.packages(checkBuilt=T)[,\"Package\"])\n\n\nSometimes there is a problem. To find specific broken packages after an update\n\n\nCode\nlibrary(purrr)\n\nset_names(.libPaths()) %&gt;%\n  map(function(lib) {\n    .packages(all.available = TRUE, lib.loc = lib) %&gt;%\n        keep(function(pkg) {\n            f &lt;- system.file('Meta', 'package.rds', package = pkg, lib.loc = lib)\n            tryCatch({readRDS(f); FALSE}, error = function(e) TRUE)\n        })\n  })\n# https://stackoverflow.com/questions/31935516/installing-r-packages-error-in-readrdsfile-error-reading-from-connection/55997765\n\n\nTo remove packages duplicated in multiple libraries\n\n\nCode\n# Libraries\ni &lt;- installed.packages()\nlibs &lt;- .libPaths()\n# Find Duplicated Packages\ni1 &lt;- i[ i[,'LibPath']==libs[1], ]\ni2 &lt;- i[ i[,'LibPath']==libs[2], ]\ndups &lt;- i2[,'Package'] %in% i1[,'Package']\nall( dups )\n# Remove\nremove.packages(  i2[,'Package'], libs[2] )",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "02_06_DataAnalysis.html#inputs",
    "href": "02_06_DataAnalysis.html#inputs",
    "title": "14  Data Analysis",
    "section": "14.2 Inputs",
    "text": "14.2 Inputs\n\nReading Data.\nThe first step in data analysis is getting data into R. There are many ways to do this, depending on your data structure. Perhaps the most common case is reading in a csv file.\n\n\nCode\n# Read in csv (downloaded from online)\n# download source 'http://www.stern.nyu.edu/~wgreene/Text/Edition7/TableF19-3.csv'\n# download destination '~/TableF19-3.csv'\nread.csv('~/TableF19-3.csv')\n \n# Can read in csv (directly from online)\n# dat_csv &lt;- read.csv('http://www.stern.nyu.edu/~wgreene/Text/Edition7/TableF19-3.csv')\n\n\nReading in other types of data can require the use of “packages”. For example, the “wooldridge” package contains datasets on crime. To use this data, we must first install the package on our computer. Then, to access the data, we must first load the package.\n\n\nCode\n# Install R Data Package and Load in\ninstall.packages('wooldridge') # only once\nlibrary(wooldridge) # anytime you want to use the data\n\ndata('crime2') \ndata('crime4')\n\n\nWe can use packages to access many different types of data. To read in a Stata data file, for example, we can use the “haven” package.\n\n\nCode\n# Read in stata data file from online\n#library(haven)\n#dat_stata &lt;- read_dta('https://www.ssc.wisc.edu/~bhansen/econometrics/DS2004.dta')\n#dat_stata &lt;- as.data.frame(dat_stata)\n\n# For More Introductory Econometrics Data, see \n# https://www.ssc.wisc.edu/~bhansen/econometrics/Econometrics%20Data.zip\n# https://pages.stern.nyu.edu/~wgreene/Text/Edition7/tablelist8new.htm\n# R packages: wooldridge, causaldata, Ecdat, AER, ....\n\n\n\n\nCleaning Data.\nData transformation is often necessary before analysis, so remember to be careful and check your code is doing what you want. (If you have large datasets, you can always test out the code on a sample.)\n\n\nCode\n# Function to Create Sample Datasets\nmake_noisy_data &lt;- function(n, b=0){\n    # Simple Data Generating Process\n    x &lt;- seq(1,10, length.out=n) \n    e &lt;- rnorm(n, mean=0, sd=10)\n    y &lt;- b*x + e \n    # Obervations\n    xy_mat &lt;- data.frame(ID=seq(x), x=x, y=y)\n    return(xy_mat)\n}\n\n# Two simulated datasets\ndat1 &lt;- make_noisy_data(6)\ndat2 &lt;- make_noisy_data(6)\n\n# Merging data in long format\ndat_merged_long &lt;- rbind(\n    cbind(dat1,DF=1),\n    cbind(dat2,DF=2))\n\n\nNow suppose we want to transform into wide format\n\n\nCode\n# Merging data in wide format, First Attempt\ndat_merged_wide &lt;- cbind( dat1, dat2)\nnames(dat_merged_wide) &lt;- c(paste0(names(dat1),'.1'), paste0(names(dat2),'.2'))\n\n# Merging data in wide format, Second Attempt\n# higher performance\ndat_merged_wide2 &lt;- merge(dat1, dat2,\n    by='ID', suffixes=c('.1','.2'))\n## CHECK they are the same.\nidentical(dat_merged_wide, dat_merged_wide2)\n## [1] FALSE\n# Inspect any differences\n\n# Merging data in wide format, Third Attempt with dedicated package\n# (highest performance but with new type of object)\nlibrary(data.table)\ndat_merged_longDT &lt;- as.data.table(dat_merged_long)\ndat_melted &lt;- melt(dat_merged_longDT, id.vars=c('ID', 'DF'))\ndat_merged_wide3 &lt;- dcast(dat_melted, ID~DF+variable)\n\n## CHECK they are the same.\nidentical(dat_merged_wide, dat_merged_wide3)\n## [1] FALSE\n\n\nOften, however, we ultimately want data in long format\n\n\nCode\n# Merging data in long format, Second Attempt with dedicated package \ndat_melted2 &lt;- melt(dat_merged_wide3, measure=c(\"1_x\",\"1_y\",\"2_x\",\"2_y\"))\nmelt_vars &lt;- strsplit(as.character(dat_melted2[['variable']]),'_')\ndat_melted2[,'DF'] &lt;- sapply(melt_vars, `[[`,1)\ndat_melted2[,'variable'] &lt;- sapply(melt_vars, `[[`,2)\ndat_merged_long2 &lt;- dcast(dat_melted2, DF+ID~variable)\ndat_merged_long2 &lt;- as.data.frame(dat_merged_long2)\n\n## CHECK they are the same.\nidentical( dat_merged_long2, dat_merged_long)\n## [1] FALSE\n\n# Further Inspect\ndat_merged_long2 &lt;- dat_merged_long2[,c('ID','x','y','DF')]\nmapply( identical, dat_merged_long2, dat_merged_long)\n##    ID     x     y    DF \n##  TRUE  TRUE  TRUE FALSE\n\n\nFor more tips, see https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-import.pdf and https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reshape.html",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "02_06_DataAnalysis.html#outputs",
    "href": "02_06_DataAnalysis.html#outputs",
    "title": "14  Data Analysis",
    "section": "14.3 Outputs",
    "text": "14.3 Outputs\n\nInteractive Figures.\nNotably, histograms and boxplots\n\n\nCode\npop_mean &lt;- mean(USArrests[,'UrbanPop'])\npop_cut &lt;- USArrests[,'UrbanPop'] &lt; pop_mean\nmurder_lowpop &lt;- USArrests[ pop_cut,'Murder']\nmurder_highpop &lt;- USArrests[ !pop_cut,'Murder']\n\nlibrary(plotly)\nfig &lt;- plot_ly(alpha=0.6, \n    hovertemplate=\"%{y}\")\nfig &lt;- fig %&gt;% add_histogram(murder_lowpop, name='Low Pop. (&lt; Mean)')\nfig &lt;- fig %&gt;% add_histogram(murder_highpop, name='High Pop (&gt;= Mean)')\nfig &lt;- fig %&gt;% layout(barmode=\"stack\") # barmode=\"overlay\"\nfig &lt;- fig %&gt;% layout(\n    title=\"Crime and Urbanization in America 1975\",\n    xaxis = list(title='Murders Arrests per 100,000 People'),\n    yaxis = list(title='Number of States'),\n    legend=list(title=list(text='&lt;b&gt; % Urban Pop. &lt;/b&gt;'))\n)\nfig\n\n\n\n\n\n\n\n\nCode\nUSArrests[,'ID'] &lt;- rownames(USArrests)\nfig &lt;- plot_ly(USArrests,\n    y=~Murder, color=~cut(UrbanPop,4),\n    alpha=0.6, type=\"box\",\n    pointpos=0, boxpoints = 'all',\n    hoverinfo='text',    \n    text = ~paste('&lt;b&gt;', ID, '&lt;/b&gt;',\n        \"&lt;br&gt;Urban  :\", UrbanPop,\n        \"&lt;br&gt;Assault:\", Assault,\n        \"&lt;br&gt;Murder :\", Murder))    \nfig &lt;- layout(fig,\n    showlegend=FALSE,\n    title='Crime and Urbanization in America 1975',\n    xaxis = list(title = 'Percent of People in an Urban Area'),\n    yaxis = list(title = 'Murders Arrests per 100,000 People'))\nfig\n\n\n\n\n\n\nFor further data exploration, your plots can also be made interactive via https://plotly.com/r/. For more details, see examples and then applications.\n\n\nCode\n#install.packages(\"plotly\")\nlibrary(plotly)\n\n\n\n\nInteractive Tables.\nYou can create an interactive table to explore raw data.\n\n\nCode\ndata(\"USArrests\")\nlibrary(reactable)\nreactable(USArrests, filterable=T, highlight=T)\n\n\n\n\n\n\nYou can create an interactive table that summarizes the data too.\n\n\nCode\n# Compute summary statistics\nvars &lt;- names(USArrests)\nstats_list &lt;- lapply(vars, function(v) {\n  x &lt;- USArrests[[v]]\n  c(\n    Variable = v,\n    N       = sum(!is.na(x)),\n    Mean    = mean(x, na.rm = TRUE),\n    SD      = sd(x, na.rm = TRUE),\n    Min     = min(x, na.rm = TRUE),\n    Q1      = as.numeric(quantile(x, 0.25, na.rm = TRUE)),\n    Median  = median(x, na.rm = TRUE),\n    Q3      = as.numeric(quantile(x, 0.75, na.rm = TRUE)),\n    Max     = max(x, na.rm = TRUE)\n  )\n})\n\n# Convert list to data frame with numeric columns \nstats_df &lt;- as.data.frame(do.call(rbind, stats_list), stringsAsFactors = FALSE)\nnum_cols &lt;- setdiff(names(stats_df), \"Variable\")\nstats_df[num_cols] &lt;- lapply(stats_df[num_cols], function(i){\n    round(as.numeric(i), 3)\n})\n\n# Display interactively\nreactable(stats_df)\n\n\n\n\n\n\n\n\nPolishing.\nYour first figures are typically standard, and probably not as good as they should be. Edit your plot to focus on the most useful information. For others to easily comprehend your work, you must also polish the plot. When polishing, you must do two things\n\nAdd details that are necessary to understand the figure\nRemove unnecessary details. See e.g., https://www.edwardtufte.com/notebook/chartjunk/ and https://www.biostat.wisc.edu/~kbroman/topten_worstgraphs/ and https://www.businessinsider.com/the-27-worst-charts-of-all-time-2013-6.\n\n\n\nCode\n# Random Data\nx &lt;- seq(1, 10, by=.0002)\ne &lt;- rnorm(length(x), mean=0, sd=1)\ny &lt;- .25*x + e \n\n# First Draft\n# plot(x, y)\n\n# Second Draft: Focus\n# (In this example: relationship magnitude)\nxs &lt;- scale(x)\nys &lt;- scale(y)\nplot(ys, xs, \n    xlab='', ylab='',\n    pch=16, cex=.5, col=grey(0,.2))\nmtext(expression('['~X-hat(M)[X]~'] /'~hat(S)[X]), 1, line=2)\nmtext(expression('['~Y-hat(M)[Y]~'] /'~hat(S)[Y]), 2, line=2.5)\n# Add a 45 degree line\nabline(a=0, b=1, lty=2, col='red')\nlegend('topleft', \n    legend=c('data point', '45 deg. line'),\n    pch=c(16,NA), lty=c(NA,2), col=c(grey(0,.2), 'red'), \n    bty='n')\ntitle('Standard Relationship')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Another Example\nxy_dat &lt;- data.frame(x=x, y=y)\npar(fig=c(0,1,0,0.9), new=F)\nplot(y~x, xy_dat, pch=16, col=rgb(0,0,0,.05), cex=.5,\n    xlab='', ylab='') # Format Axis Labels Seperately\nmtext( 'y=0.25 x + e\\n e ~ standard-normal', 2, line=2.2)\nmtext( expression(x%in%~'[0,10]'), 1, line=2.2)\n#abline( lm(y~x, data=xy_dat), lty=2)\ntitle('Plot with good features, but too excessive in several ways',\n    adj=0, font.main=1)\n\n# Outer Legend (https://stackoverflow.com/questions/3932038/)\nouter_legend &lt;- function(...) {\n  opar &lt;- par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), \n    mar=c(0, 0, 0, 0), new=TRUE)\n  on.exit(par(opar))\n  plot(0, 0, type='n', bty='n', xaxt='n', yaxt='n')\n  legend(...)\n}\nouter_legend('topright', legend='single data point',\n    title='do you see the normal distribution?',\n    pch=16, col=rgb(0,0,0,.1), cex=1, bty='n')\n\n\n\n\n\n\n\n\n\nWhich features are most informative depends on what you want to show, and you can always mix and match. Learn to edit your figures:\n\nhttps://websites.umich.edu/~jpboyd/eng403_chap2_tuftegospel.pdf\nhttps://jtr13.github.io/cc19/tuftes-principles-of-data-ink.html\nhttps://github.com/cxli233/FriendsDontLetFriends\n\nand be aware that each type has benefits and costs. E.g., see\n\nhttps://www.data-to-viz.com/caveats.html\nhttps://x.com/EdwardTufte/status/1092717905156993024/photo/1\nhttps://towardsdatascience.com/why-a-box-plot-should-not-be-used-alone-and-some-plots-to-use-it-with-23381f7e3cb6/\n\nFor small datasets, you can plot individual data points with a strip chart. For datasets with spatial information, a map is also helpful. Sometime tables are better than graphs (see https://www.edwardtufte.com/notebook/boxplots-data-test). For useful tips, see C. Wilke (2019) “Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures” https://clauswilke.com/dataviz/\nFor plotting math, see https://astrostatistics.psu.edu/su07/R/html/grDevices/html/plotmath.html and https://library.virginia.edu/data/articles/mathematical-annotation-in-r\n\n\nStatic Publishing.\n\n\nAdvanced and Optional\n\n\nYou can export figures with specific dimensions\n\n\nCode\npdf( 'Figures/plot_example.pdf', height=5, width=5)\n# plot goes here\ndev.off()\n\n\nFor exporting options, see ?pdf. For saving other types of files, see png(\"*.png\"), tiff(\"*.tiff\"), and jpeg(\"*.jpg\")\nYou can also export tables in a variety of formats, including many that other software programs can easily read\n\nCode\nlibrary(stargazer)\n# summary statistics\nstargazer(USArrests,\n    type='html', \n    summary=T,\n    title='Summary Statistics for USArrests')\n\n\n\nSummary Statistics for USArrests\n\n\n\n\n\n\n\nStatistic\n\n\nN\n\n\nMean\n\n\nSt. Dev.\n\n\nMin\n\n\nMax\n\n\n\n\n\n\n\n\nMurder\n\n\n50\n\n\n7.788\n\n\n4.356\n\n\n0.800\n\n\n17.400\n\n\n\n\nAssault\n\n\n50\n\n\n170.760\n\n\n83.338\n\n\n45\n\n\n337\n\n\n\n\nUrbanPop\n\n\n50\n\n\n65.540\n\n\n14.475\n\n\n32\n\n\n91\n\n\n\n\nRape\n\n\n50\n\n\n21.232\n\n\n9.366\n\n\n7.300\n\n\n46.000\n\n\n\n\n\n\n\nNote that many of the best plots are custom made (see https://www.r-graph-gallery.com/). Here are some ones that I have made over the years.",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "02_06_DataAnalysis.html#r-markdown-reports",
    "href": "02_06_DataAnalysis.html#r-markdown-reports",
    "title": "14  Data Analysis",
    "section": "14.4 R-Markdown Reports",
    "text": "14.4 R-Markdown Reports\nWe will use R Markdown for communicating results to each other. Note that R and R Markdown are both languages. R studio interprets R code make statistical computations and interprets R Markdown code to produce pretty documents that contain both writing and statistics. Altogether, your project will use\n\nR: does statistical computations\nR Markdown: formats statistical computations for sharing\nRstudio: graphical user interface that allows you to easily use both R and R Markdown.\n\nHomework reports are probably the smallest document you can create. These little reports are almost entirely self-contained (showing both code and output). To make them, you will need to\nFirst install Pandoc on your computer.\nThen install any required packages\n\n\nCode\n# Packages for Rmarkdown\ninstall.packages(\"knitr\")\ninstall.packages(\"rmarkdown\")\n\n# Other packages frequently used\n#install.packages(\"plotly\") #for interactive plots\n#install.packages(\"sf\") #for spatial data\n\n\nWe will create simple reproducible reports via R Markdown.\n\nExample 1: Simple Report.\nSee ReportTemplate_1Descriptive.html and then create it by\n\nClicking the “Code” button in the top right and then “Download Rmd”\nOpen with Rstudio\nChange the name and title to your own, make other edits\nThen point-and-click “knit” or use the console to run\n\n\n\nCode\nrmarkdown::render('DataScientism.Rmd')\n\n\nAlternatively, download the source file ReportTemplate_1Descriptive.Rmd and then follow the steps above.",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "02_06_DataAnalysis.html#further-reading",
    "href": "02_06_DataAnalysis.html#further-reading",
    "title": "14  Data Analysis",
    "section": "14.5 Further Reading",
    "text": "14.5 Further Reading\nFor more guidance on how to create Rmarkdown documents, see\n\nhttps://github.com/rstudio/cheatsheets/blob/main/rmarkdown.pdf\nhttps://cran.r-project.org/web/packages/rmarkdown/vignettes/rmarkdown.html\nhttp://rmarkdown.rstudio.com\nhttps://bookdown.org/yihui/rmarkdown/\nhttps://bookdown.org/yihui/rmarkdown-cookbook/\nhttps://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/rmarkdown.html\nAn Introduction to the Advanced Theory and Practice of Nonparametric Econometrics. Raccine 2019. Appendices B & D.\nhttps://rmd4sci.njtierney.com/using-rmarkdown.html\nhttps://alexd106.github.io/intro2R/Rmarkdown_intro.html\n\nIf you are still lost, try one of the many online tutorials (such as these)\n\nhttps://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf\nhttps://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\nhttps://www.neonscience.org/resources/learning-hub/tutorials/rmd-code-intro\nhttps://m-clark.github.io/Introduction-to-Rmarkdown/\nhttps://www.stat.cmu.edu/~cshalizi/rmarkdown/\nhttp://math.wsu.edu/faculty/xchen/stat412/HwWriteUp.Rmd\nhttp://math.wsu.edu/faculty/xchen/stat412/HwWriteUp.html\nhttps://holtzy.github.io/Pimp-my-rmd/\nhttps://ntaback.github.io/UofT_STA130/Rmarkdownforclassreports.html\nhttps://crd150.github.io/hw_guidelines.html\nhttps://r4ds.had.co.nz/r-markdown.html\nhttp://www.stat.cmu.edu/~cshalizi/rmarkdown\nhttp://www.ssc.wisc.edu/sscc/pubs/RFR/RFR_RMarkdown.html\nhttp://kbroman.org/knitr_knutshell/pages/Rmarkdown.html",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "03-00-MultivariateData.html",
    "href": "03-00-MultivariateData.html",
    "title": "Multivariate Data",
    "section": "",
    "text": "This section introduces linear regression models from the perspective that ``all models are wrong, but some are useful’’. All linear models are estimated via Ordinary Least Squares (OLS). This contrasts with many other introductory books that begin by assuming a linear data generating process.",
    "crumbs": [
      "Multivariate Data"
    ]
  },
  {
    "objectID": "03_01_IntermediateRegression.html",
    "href": "03_01_IntermediateRegression.html",
    "title": "15  Multivariate Data I",
    "section": "",
    "text": "15.1 Multiple Linear Regression\nFirst, note that you can summarize a dataset with multiple variables using the previous tools.\nYou can also use size, color, and shape to distinguish conditional relationships.\nSee also https://plotly.com/r/bubble-charts/\nWith \\(K\\) variables, the linear model is \\[\ny_i=\\beta_0+\\beta_1 x_{i1}+\\beta_2 x_{i2}+\\ldots+\\beta_K x_{iK}+\\epsilon_i = [1~~  x_{i1} ~~...~~ x_{iK}] \\beta + \\epsilon_i\n\\] and our objective is \\[\nmin_{\\beta} \\sum_{i=1}^{N} (\\epsilon_i)^2.\n\\]\nDenoting \\[\ny= \\begin{pmatrix}\ny_{1} \\\\ \\vdots \\\\ y_{N}\n\\end{pmatrix} \\quad\n\\textbf{X} = \\begin{pmatrix}\n1 & x_{11} & ... & x_{1K} \\\\\n& \\vdots & & \\\\\n1 & x_{N1} & ... & x_{NK}\n\\end{pmatrix},\n\\] we can also write the model and objective in matrix form \\[\ny=\\textbf{X}\\beta+\\epsilon\\\\\nmin_{\\beta} (\\epsilon' \\epsilon)\n\\]\nMinimizing the squared errors yields coefficient estimates \\[\n\\hat{\\beta}=(\\textbf{X}'\\textbf{X})^{-1}\\textbf{X}'y\n\\] and predictions \\[\n\\hat{y}=\\textbf{X} \\hat{\\beta} \\\\\n\\hat{\\epsilon}=y - \\hat{y} \\\\\n\\]\nCode\n# Manually Compute\nY &lt;- USArrests[,'Murder']\nX &lt;- USArrests[,c('Assault','UrbanPop')]\nX &lt;- as.matrix(cbind(1,X))\n\nXtXi &lt;- solve(t(X)%*%X)\nBhat &lt;- XtXi %*% (t(X)%*%Y)\nc(Bhat)\n## [1]  3.20715340  0.04390995 -0.04451047\n\n# Check\nreg &lt;- lm(Murder~Assault+UrbanPop, data=USArrests)\ncoef(reg)\n## (Intercept)     Assault    UrbanPop \n##  3.20715340  0.04390995 -0.04451047\nTo measure the ``Goodness of fit’’ of the model, we can again plot our predictions.\nCode\nplot(USArrests$Murder, predict(reg), pch=16, col=grey(0,.5))\nabline(a=0,b=1, lty=2)\nWe can also again compute sums of squared errors. Adding random data may sometimes improve the fit, however, so we adjust the \\(R^2\\) by the number of covariates \\(K\\). \\[\nR^2 = \\frac{ESS}{TSS}=1-\\frac{RSS}{TSS}\\\\\nR^2_{\\text{adj.}} = 1-\\frac{N-1}{N-K}(1-R^2)\n\\]\nCode\nksims &lt;- 1:30\nfor(k in ksims){ \n    USArrests[,paste0('R',k)] &lt;- runif(nrow(USArrests),0,20)\n}\nreg_sim &lt;- lapply(ksims, function(k){\n    rvars &lt;- c('Assault','UrbanPop', paste0('R',1:k))\n    rvars2 &lt;- paste0(rvars, collapse='+')\n    reg_k &lt;- lm( paste0('Murder~',rvars2), data=USArrests)\n})\nR2_sim &lt;- sapply(reg_sim, function(reg_k){  summary(reg_k)$r.squared })\nR2adj_sim &lt;- sapply(reg_sim, function(reg_k){  summary(reg_k)$adj.r.squared })\n\nplot.new()\nplot.window(xlim=c(0,30), ylim=c(0,1))\npoints(ksims, R2_sim)\npoints(ksims, R2adj_sim, pch=16)\naxis(1)\naxis(2)\nmtext(expression(R^2),2, line=3)\nmtext('Additional Random Covariates', 1, line=3)\nlegend('topleft', horiz=T,\n    legend=c('Undjusted', 'Adjusted'), pch=c(1,16))",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Data I</span>"
    ]
  },
  {
    "objectID": "03_01_IntermediateRegression.html#factor-variables",
    "href": "03_01_IntermediateRegression.html#factor-variables",
    "title": "15  Multivariate Data I",
    "section": "15.2 Factor Variables",
    "text": "15.2 Factor Variables\nSo far, we have discussed cardinal data where the difference between units always means the same thing: e.g., \\(4-3=2-1\\). There are also factor variables\n\nOrdered: refers to Ordinal data. The difference between units means something, but not always the same thing. For example, \\(4th - 3rd \\neq 2nd - 1st\\).\nUnordered: refers to Categorical data. The difference between units is meaningless. For example, \\(B-A=?\\)\n\nTo analyze either factor, we often convert them into indicator variables or dummies; \\(D_{c}=\\mathbf{1}( Factor = c)\\). One common case is if you have observations of individuals over time periods, then you may have two factor variables. An unordered factor that indicates who an individual is; for example \\(D_{i}=\\mathbf{1}( Individual = i)\\), and an order factor that indicates the time period; for example \\(D_{t}=\\mathbf{1}( Time \\in [month~ t, month~ t+1) )\\). There are many other cases you see factor variables, including spatial ID’s in purely cross sectional data.\nBe careful not to handle categorical data as if they were cardinal. E.g., generate city data with Leipzig=1, Lausanne=2, LosAngeles=3, … and then include city as if it were a cardinal number (that’s a big no-no). The same applied to ordinal data; PopulationLeipzig=2, PopulationLausanne=3, PopulationLosAngeles=1.\n\n\nCode\nN &lt;- 1000\nx &lt;- runif(N,3,8)\ne &lt;- rnorm(N,0,0.4)\nfo &lt;- factor(rbinom(N,4,.5), ordered=T)\nfu &lt;- factor(rep(c('A','B'),N/2), ordered=F)\ndA &lt;- 1*(fu=='A')\ny &lt;- (2^as.integer(fo)*dA )*sqrt(x)+ 2*as.integer(fo)*e\ndat_f &lt;- data.frame(y,x,fo,fu)\n\n\nWith factors, you can still include them in the design matrix of an OLS regression \\[\ny_{it} = x_{it} \\beta_{x} + d_{t}\\beta_{t}\n\\] When, as commonly done, the factors are modeled as being additively seperable, they are modeled “fixed effects”.1 Simply including the factors into the OLS regression yields a “dummy variable” fixed effects estimator. Hansen Econometrics, Theorem 17.1: The fixed effects estimator of \\(\\beta\\) algebraically equals the dummy variable estimator of \\(\\beta\\). The two estimators have the same residuals. \n\n\nCode\nlibrary(fixest)\nfe_reg1 &lt;- feols(y~x|fo+fu, dat_f)\ncoef(fe_reg1)\n##        x \n## 1.047302\nfixef(fe_reg1)[1:2]\n## $fo\n##         0         1         2         3         4 \n##  7.431407 10.233385 14.989790 24.427619 38.051168 \n## \n## $fu\n##         A         B \n##   0.00000 -23.11941\n\n# Compare Coefficients\nfe_reg0 &lt;- lm(y~-1+x+fo+fu, dat_f)\ncoef( fe_reg0 )\n##          x        fo0        fo1        fo2        fo3        fo4        fuB \n##   1.047302   7.431407  10.233385  14.989790  24.427619  38.051168 -23.119406\n\n\nWith fixed effects, we can also compute averages for each group and construct a between estimator: \\(\\bar{y}_i = \\alpha + \\bar{x}_i \\beta\\). Or we can subtract the average from each group to construct a within estimator: \\((y_{it} - \\bar{y}_i) = (x_{it}-\\bar{x}_i)\\beta\\).",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Data I</span>"
    ]
  },
  {
    "objectID": "03_01_IntermediateRegression.html#variability-estimates",
    "href": "03_01_IntermediateRegression.html#variability-estimates",
    "title": "15  Multivariate Data I",
    "section": "15.3 Variability Estimates",
    "text": "15.3 Variability Estimates\nTo estimate the variability of our estimates, we can use the same data-driven methods introduced in the last section. As before, we can conduct independent hypothesis tests using t-values.\nWe can also conduct joint tests that account for interdependancies in our estimates. For example, to test whether two coefficients both equal \\(0\\), we bootstrap the joint distribution of coefficients.\n\n\nCode\n# Bootstrap SE's\nboots &lt;- 1:399\nboot_regs &lt;- lapply(boots, function(b){\n    b_id &lt;- sample( nrow(USArrests), replace=T)\n    xy_b &lt;- USArrests[b_id,]\n    reg_b &lt;- lm(Murder~Assault+UrbanPop, dat=xy_b)\n})\nboot_coefs &lt;- sapply(boot_regs, coef)\n\n# Recenter at 0 to impose the null\n#boot_means &lt;- rowMeans(boot_coefs)\n#boot_coefs0 &lt;- sweep(boot_coefs, MARGIN=1, STATS=boot_means)\n\n\n\n\nCode\nboot_coef_df &lt;- as.data.frame(cbind(ID=boots, t(boot_coefs)))\nfig &lt;- plotly::plot_ly(boot_coef_df,\n    type = 'scatter', mode = 'markers',\n    x = ~UrbanPop, y = ~Assault,\n    text = ~paste('&lt;b&gt; bootstrap dataset: ', ID, '&lt;/b&gt;',\n            '&lt;br&gt;Coef. Urban  :', round(UrbanPop,3),\n            '&lt;br&gt;Coef. Murder :', round(Assault,3),\n            '&lt;br&gt;Coef. Intercept :', round(`(Intercept)`,3)),\n    hoverinfo='text',\n    showlegend=F,\n    marker=list( color='rgba(0, 0, 0, 0.5)'))\nfig &lt;- plotly::layout(fig,\n    showlegend=F,\n    title='Joint Distribution of Coefficients (under the null)',\n    xaxis = list(title='UrbanPop Coefficient'),\n    yaxis = list(title='Assualt Coefficient'))\nfig",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Data I</span>"
    ]
  },
  {
    "objectID": "03_01_IntermediateRegression.html#hypothesis-tests",
    "href": "03_01_IntermediateRegression.html#hypothesis-tests",
    "title": "15  Multivariate Data I",
    "section": "15.4 Hypothesis Tests",
    "text": "15.4 Hypothesis Tests\n\nF-statistic.\nWe can also use an \\(F\\) test for any \\(q\\) hypotheses. Specifically, when \\(q\\) hypotheses restrict a model, the degrees of freedom drop from \\(k_{u}\\) to \\(k_{r}\\) and the residual sum of squares \\(RSS=\\sum_{i}(y_{i}-\\widehat{y}_{i})^2\\) typically increases. We compute the statistic \\[\nF_{q} = \\frac{(RSS_{r}-RSS_{u})/(k_{u}-k_{r})}{RSS_{u}/(N-k_{u})}\n\\]\nIf you test whether all \\(K\\) variables are significant, the restricted model is a simple intercept and \\(RSS_{r}=TSS\\), and \\(F_{q}\\) can be written in terms of \\(R^2\\): \\(F_{K} = \\frac{R^2}{1-R^2} \\frac{N-K}{K-1}\\). The first fraction is the relative goodness of fit, and the second fraction is an adjustment for degrees of freedom (similar to how we adjusted the \\(R^2\\) term before).\nTo conduct a hypothesis test, first compute a null distribution by randomly reshuffling the outcomes and recompute the \\(F\\) statistic, and then compare how often random data give something as extreme as your initial statistic. For some intuition on this F test, examine how the adjusted \\(R^2\\) statistic varies with bootstrap samples.\n\n\nCode\n# Bootstrap under the null\nboots &lt;- 1:399\nboot_regs0 &lt;- lapply(boots, function(b){\n  # Generate bootstrap sample\n  xy_b &lt;- USArrests\n  b_id &lt;- sample( nrow(USArrests), replace=T)\n  # Impose the null\n  xy_b$Murder &lt;-  xy_b$Murder[b_id]\n  # Run regression\n  reg_b &lt;- lm(Murder~Assault+UrbanPop, dat=xy_b)\n})\n# Get null distribution for adjusted R2\nR2adj_sim0 &lt;- sapply(boot_regs0, function(reg_k){\n    summary(reg_k)$adj.r.squared })\nhist(R2adj_sim0, xlim=c(-.1,1), breaks=25, border=NA,\n    main='', xlab=expression('adj.'~R[b]^2))\n\n# Compare to initial statistic\nabline(v=summary(reg)$adj.r.squared, lwd=2, col=2)\n\n\n\n\n\n\n\n\n\nNote that hypothesis testing is not to be done routinely, as additional complications arise when testing multiple hypothesis sequentially.\nUnder some additional assumptions \\(F_{q}\\) follows an F-distribution. For more about F-testing, see https://online.stat.psu.edu/stat501/lesson/6/6.2 and https://www.econometrics.blog/post/understanding-the-f-statistic/\n\n\nANOVA\nUnder some additional parametric assumptions about the data generating process, the F-statistic follows an \\(F\\) distribution. This case is well-studied historically, often under the title Analysis of Variance (ANOVA).\nAn important case corresponds to the restricted model having no explanatory variables (i.e., our model is \\(Y_{i}=\\overline{Y}\\))\n\n\nCode\nreg_full &lt;- lm(Murder ~ Assault + UrbanPop + Rape, data = USArrests)\nreg_none &lt;- lm(Murder ~ 1, dat=USArrests)\nanova(reg_none, reg_full)\n## Analysis of Variance Table\n## \n## Model 1: Murder ~ 1\n## Model 2: Murder ~ Assault + UrbanPop + Rape\n##   Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n## 1     49 929.55                                  \n## 2     46 304.83  3    624.72 31.424 3.322e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Manual F-test\nrss0 &lt;- sum(resid(reg_none)^2) # restricted\nrss1 &lt;- sum(resid(reg_full)^2) # unrestricted\ndf0  &lt;- df.residual(reg_none)\ndf1  &lt;- df.residual(reg_full)\nF    &lt;- ((rss0 - rss1)/(df0-df1)) / (rss1/df1) # observed F stat\np    &lt;- 1-pf(F, df0-df1, df1) # where F falls in the F-distribution\ncbind(F, p)\n##             F            p\n## [1,] 31.42399 3.322431e-11\n\n\nWhether you take a parametric or nonparametric approach to hypothesis testing, you can easily test whether variables are additively separable with an \\(F\\) test.\n\n\nCode\n# Empirical Example\nreg1 &lt;- lm(Murder~Assault+UrbanPop, USArrests)\nreg2 &lt;- lm(Murder~Assault*UrbanPop, USArrests)\nanova(reg1, reg2)\n## Analysis of Variance Table\n## \n## Model 1: Murder ~ Assault + UrbanPop\n## Model 2: Murder ~ Assault * UrbanPop\n##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n## 1     47 312.87                           \n## 2     46 300.93  1    11.944 1.8258 0.1832\n\n\n\n\nCode\n## # Simulation Example\n## N &lt;- 1000\n## x &lt;- runif(N,3,8)\n## e &lt;- rnorm(N,0,0.4)\n## fo &lt;- factor(rbinom(N,4,.5), ordered=T)\n## fu &lt;- factor(rep(c('A','B'),N/2), ordered=F)\n## dA &lt;- 1*(fu=='A')\n## y &lt;- (2^as.integer(fo)*dA )*sqrt(x)+ 2*as.integer(fo)*e\n## dat_f &lt;- data.frame(y,x,fo,fu)\n## \n## reg0 &lt;- lm(y~-1+x+fo+fu, dat_f)\n## reg1 &lt;- lm(y~-1+x+fo*fu, dat_f)\n## reg2 &lt;- lm(y~-1+x*fo*fu, dat_f)\n## \n## anova(reg0, reg2)\n## anova(reg0, reg1, reg2)",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Data I</span>"
    ]
  },
  {
    "objectID": "03_01_IntermediateRegression.html#further-reading",
    "href": "03_01_IntermediateRegression.html#further-reading",
    "title": "15  Multivariate Data I",
    "section": "15.5 Further Reading",
    "text": "15.5 Further Reading\nFor OLS, see\n\nhttps://bookdown.org/josiesmith/qrmbook/linear-estimation-and-minimizing-error.html\nhttps://www.econometrics-with-r.org/4-lrwor.html\nhttps://www.econometrics-with-r.org/6-rmwmr.html\nhttps://www.econometrics-with-r.org/7-htaciimr.html\nhttps://bookdown.org/ripberjt/labbook/bivariate-linear-regression.html\nhttps://bookdown.org/ripberjt/labbook/multivariable-linear-regression.html\nhttps://online.stat.psu.edu/stat462/node/137/\nhttps://book.stat420.org/\nHill, Griffiths & Lim (2007), Principles of Econometrics, 3rd ed., Wiley, S. 86f.\nVerbeek (2004), A Guide to Modern Econometrics, 2nd ed., Wiley, S. 51ff.\nAsteriou & Hall (2011), Applied Econometrics, 2nd ed., Palgrave MacMillan, S. 177ff.\nhttps://online.stat.psu.edu/stat485/lesson/11/\n\nTo derive OLS coefficients in Matrix form, see\n\nhttps://jrnold.github.io/intro-methods-notes/ols-in-matrix-form.html\nhttps://www.fsb.miamioh.edu/lij14/411_note_matrix.pdf\nhttps://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf\n\nFor fixed effects, see\n\nhttps://www.econometrics-with-r.org/10-rwpd.html\nhttps://bookdown.org/josiesmith/qrmbook/topics-in-multiple-regression.html\nhttps://bookdown.org/ripberjt/labbook/multivariable-linear-regression.html\nhttps://www.princeton.edu/~otorres/Panel101.pdf\nhttps://www.stata.com/manuals13/xtxtreg.pdf",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Data I</span>"
    ]
  },
  {
    "objectID": "03_01_IntermediateRegression.html#footnotes",
    "href": "03_01_IntermediateRegression.html#footnotes",
    "title": "15  Multivariate Data I",
    "section": "",
    "text": "There are also random effects: the factor variable comes from a distribution that is uncorrelated with the regressors. This is rarely used in economics today, however, and are mostly included for historical reasons and special cases where fixed effects cannot be estimated due to data limitations.↩︎",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Data I</span>"
    ]
  },
  {
    "objectID": "03_02_InterprettingRegression.html",
    "href": "03_02_InterprettingRegression.html",
    "title": "16  Multivariate Data II",
    "section": "",
    "text": "16.1 Coefficient Interpretation\nNotice that we have gotten pretty far without actually trying to meaningfully interpret regression coefficients. That is because the above procedure will always give us number, regardless as to whether the true data generating process is linear or not. So, to be cautious, we have been interpreting the regression outputs while being agnostic as to how the data are generated. We now consider a special situation where we know the data are generated according to a linear process and are only uncertain about the parameter values.\nIf the data generating process is \\[\ny=X\\beta + \\epsilon\\\\\n\\mathbb{E}[\\epsilon | X]=0,\n\\] then we have a famous result that lets us attach a simple interpretation of OLS coefficients as unbiased estimates of the effect of X: \\[\n\\hat{\\beta} = (X'X)^{-1}X'y = (X'X)^{-1}X'(X\\beta + \\epsilon) = \\beta + (X'X)^{-1}X'\\epsilon\\\\\n\\mathbb{E}\\left[ \\hat{\\beta} \\right] = \\mathbb{E}\\left[ (X'X)^{-1}X'y \\right] = \\beta + (X'X)^{-1}\\mathbb{E}\\left[ X'\\epsilon \\right] = \\beta\n\\]\nGenerate a simulated dataset with 30 observations and two exogenous variables. Assume the following relationship: \\(y_{i} = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i\\) where the variables and the error term are realizations of the following data generating processes (DGP):\nCode\nN &lt;- 30\nB &lt;- c(10, 2, -1)\n\nx1 &lt;- runif(N, 0, 5)\nx2 &lt;- rbinom(N,1,.7)\nX &lt;- cbind(1,x1,x2)\ne &lt;- rnorm(N,0,3)\nY &lt;- X%*%B + e\ndat &lt;- data.frame(Y,X)\ncoef(lm(Y~x1+x2, data=dat))\n## (Intercept)          x1          x2 \n##   7.9996843   2.5558594  -0.4091467\nSimulate the distribution of coefficients under a correctly specified model. Interpret the average.\nCode\nN &lt;- 30\nB &lt;- c(10, 2, -1)\n\nCoefs &lt;- sapply(1:400, function(sim){\n    x1 &lt;- runif(N, 0, 5)\n    x2 &lt;- rbinom(N,1,.7)\n    X &lt;- cbind(1,x1,x2)\n    e &lt;- rnorm(N,0,3)\n    Y &lt;- X%*%B + e\n    dat &lt;- data.frame(Y,x1,x2)\n    coef(lm(Y~x1+x2, data=dat))\n})\n\npar(mfrow=c(1,2))\nfor(i in 2:3){\n    hist(Coefs[i,], xlab=bquote(beta[.(i)]), main='', border=NA)\n    abline(v=mean(Coefs[i,]), lwd=2)\n    abline(v=B[i], col=rgb(1,0,0))\n}\nMany economic phenomena are nonlinear, even when including potential transforms of \\(Y\\) and \\(X\\). Sometimes the linear model may still be a good or even great approximation. But sometimes not, and it is hard to know ex-ante. Examine the distribution of coefficients under this mispecified model and try to interpret the average.\nCode\nN &lt;- 30\n\nCoefs &lt;- sapply(1:600, function(sim){\n    x2 &lt;- runif(N, 0, 5)\n    x3 &lt;- rbinom(N,1,.7)\n    e &lt;- rnorm(N,0,3)\n    Y &lt;- 10*x3 + 2*log(x2)^x3 + e\n    dat &lt;- data.frame(Y,x2,x3)\n    coef(lm(Y~x2+x3, data=dat))\n})\n\npar(mfrow=c(1,2))\nfor(i in 2:3){\n    hist(Coefs[i,],  xlab=bquote(beta[.(i)]), main='', border=NA)\n    abline(v=mean(Coefs[i,]), col=1, lwd=2)\n}\nIn general, you can interpret your regression coefficients as “adjusted correlations”. There are (many) tests for whether the relationships in your dataset are actually additively separable and linear.",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Multivariate Data II</span>"
    ]
  },
  {
    "objectID": "03_02_InterprettingRegression.html#diagnostics",
    "href": "03_02_InterprettingRegression.html#diagnostics",
    "title": "16  Multivariate Data II",
    "section": "16.2 Diagnostics",
    "text": "16.2 Diagnostics\nThere’s little sense in getting great standard errors for a terrible model. Plotting your regression object a simple and easy step to help diagnose whether your model is in some way bad. We next go through what each of these figures show.\n\n\nCode\nreg &lt;- lm(Murder~Assault+UrbanPop, data=USArrests)\npar(mfrow=c(2,2))\nplot(reg, pch=16, col=grey(0,.5))\n\n\n\n\n\n\n\n\n\n\nOutliers.\nThe first diagnostic plot examines outliers in terms the outcome \\(y_i\\) being far from its prediction \\(\\hat{y}_i\\). You may be interested in such outliers because they can (but do not have to) unduly influence your estimates.\nThe third diagnostic plot examines another type of outlier, where an observation with the explanatory variable \\(x_i\\) is far from the center of mass of the other \\(x\\)’s. A point has high leverage if the estimates change dramatically when you estimate the model without that data point.\n\n\nCode\nN &lt;- 40\nx &lt;- c(25, runif(N-1,3,8))\ne &lt;- rnorm(N,0,0.4)\ny &lt;- 3 + 0.6*sqrt(x) + e\nplot(y~x, pch=16, col=grey(0,.5))\npoints(x[1],y[1], pch=16, col=rgb(1,0,0,.5))\n\nabline(lm(y~x), col=2, lty=2)\nabline(lm(y[-1]~x[-1]))\n\n\n\n\n\n\n\n\n\nSee AEJ-leverage and NBER-leverage for examples of leverage in economics.\nStandardized residuals are \\[\nr_i=\\frac{\\hat{\\epsilon}_i}{s_{[i]}\\sqrt{1-h_i}},\n\\] where \\(s_{[i]}\\) is the root mean squared error of a regression with the \\(i\\)th observation removed and \\(h_i\\) is the leverage of residual \\(\\hat{\\epsilon_i}\\).\n\n\nCode\nwhich.max(hatvalues(reg))\nwhich.max(rstandard(reg))\n\n\n(See https://www.r-bloggers.com/2016/06/leverage-and-influence-in-a-nutshell/ for a good interactive explanation, and https://online.stat.psu.edu/stat462/node/87/ for detail.)\nThe fourth plot further assesses outlier \\(X\\) using Cook’s Distance, which sums of all prediction changes when observation \\(i\\) is removed and scales proportionally to the mean square error $s^2 = . \\[\\begin{eqnarray}\nD_{i}\n= \\frac{\\sum_{j} \\left( \\hat{y_j} - \\hat{y_j}_{[i]} \\right)^2 }{ p s^2 }\n= \\frac{[e_{i}]^2}{p s^2 } \\frac{h_i}{(1-h_i)^2}\n\\end{eqnarray}\\]\n\n\nCode\nwhich.max(cooks.distance(reg))\ncar::influencePlot(reg)\n\n\n\n\nCollinearity.\nThis is when one explanatory variable in a multiple linear regression model can be linearly predicted from the others with a substantial degree of accuracy. Coefficient estimates may change erratically in response to small changes in the model or the data. (In the extreme case where there are more variables than observations \\(K&gt;N\\), the inverse of \\(X'X\\) has an infinite number of solutions.) To diagnose collinearity, we can use the Variance Inflation Factor \\[\nVIF_{k}=\\frac{1}{1-R^2_k},\n\\] where \\(R^2_k\\) is the \\(R^2\\) for the regression of \\(X_k\\) on the other covariates \\(X_{-k}\\) (a regression that does not involve the response variable Y)\n\n\nCode\ncar::vif(reg) \nsqrt(car::vif(reg)) &gt; 2 # problem?\n\n\n\n\nNormality.\nThe second plot examines whether the residuals are normally distributed. Your OLS coefficient estimates do not depend on the normality of the residuals. (Good thing, because there’s no reason the residuals of economic phenomena should be so well behaved.) Many hypothesis tests are, however, affected by the distribution of the residuals. For these reasons, you may be interested in assessing normality\n\n\nCode\npar(mfrow=c(1,2))\nhist(resid(reg), main='Histogram of Residuals',\n    font.main=1, border=NA)\n\nqqnorm(resid(reg), main=\"Normal Q-Q Plot of Residuals\",\n    font.main=1, col=grey(0,.5), pch=16)\nqqline(resid(reg), col=1, lty=2)\n\n#shapiro.test(resid(reg))\n\n\nHeterskedasticity may also matters for variability estimates. This is not shown in the plot, but you can conduct a simple test\n\n\nCode\nlibrary(lmtest)\nlmtest::bptest(reg)",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Multivariate Data II</span>"
    ]
  },
  {
    "objectID": "03_02_InterprettingRegression.html#transformations",
    "href": "03_02_InterprettingRegression.html#transformations",
    "title": "16  Multivariate Data II",
    "section": "16.3 Transformations",
    "text": "16.3 Transformations\nTransforming variables can often improve your model fit while still allowing it estimated via OLS. This is because OLS only requires the model to be linear in the parameters. Under the assumptions of the model is correctly specified, the following table is how we can interpret the coefficients of the transformed data. (Note for small changes, \\(\\Delta ln(x) \\approx \\Delta x / x = \\Delta x \\% \\cdot 100\\).)\n\n\n\n\n\n\n\n\n\n\nSpecification\nRegressand\nRegressor\nDerivative\nInterpretation (If True)\n\n\n\n\nlinear–linear\n\\(y\\)\n\\(x\\)\n\\(\\Delta y = \\beta_1\\cdot\\Delta x\\)\nChange \\(x\\) by one unit \\(\\rightarrow\\) change \\(y\\) by \\(\\beta_1\\) units.\n\n\nlog–linear\n\\(ln(y)\\)\n\\(x\\)\n\\(\\Delta y \\% \\cdot 100 \\approx \\beta_1 \\cdot \\Delta x\\)\nChange \\(x\\) by one unit \\(\\rightarrow\\) change \\(y\\) by \\(100 \\cdot \\beta_1\\) percent.\n\n\nlinear–log\n\\(y\\)\n\\(ln(x)\\)\n\\(\\Delta y \\approx  \\frac{\\beta_1}{100}\\cdot \\Delta x \\%\\)\nChange \\(x\\) by one percent \\(\\rightarrow\\) change \\(y\\) by \\(\\frac{\\beta_1}{100}\\) units\n\n\nlog–log\n\\(ln(y)\\)\n\\(ln(x)\\)\n\\(\\Delta y \\% \\approx \\beta_1\\cdot \\Delta x \\%\\)\nChange \\(x\\) by one percent \\(\\rightarrow\\) change \\(y\\) by \\(\\beta_1\\) percent\n\n\n\nNow recall from micro theory that an additively seperable and linear production function is referred to as ``perfect substitutes’‘. With a linear model and untranformed data, you have implicitly modelled the different regressors \\(X\\) as perfect substitutes. Further recall that the’‘perfect substitutes’’ model is a special case of the constant elasticity of substitution production function. Here, we will build on http://dx.doi.org/10.2139/ssrn.3917397, and consider box-cox transforming both \\(X\\) and \\(y\\). Specifically, apply the box-cox transform of \\(y\\) using parameter \\(\\lambda\\) and apply another box-cox transform to each \\(x\\) using the same parameter \\(\\rho\\) so that \\[\\begin{eqnarray}\ny^{(\\lambda)}_{i} &=& \\sum_{k=1}^{K}\\beta_{k} x^{(\\rho)}_{ik} + \\epsilon_{i}\\\\\ny^{(\\lambda)}_{i} &=&\n\\begin{cases}\n\\lambda^{-1}[ (y_i+1)^{\\lambda}- 1] & \\lambda \\neq 0 \\\\\nlog(y_i+1) &  \\lambda=0\n\\end{cases}.\\\\\nx^{(\\rho)}_{i} =\n\\begin{cases}\n\\rho^{-1}[ (x_i)^{\\rho}- 1] & \\rho \\neq 0 \\\\\nlog(x_{i}+1) &  \\rho=0\n\\end{cases}.\n\\end{eqnarray}\\]\nNotice that this nests:\n\nlinear-linear \\((\\rho=\\lambda=1)\\).\nlinear-log \\((\\rho=1, \\lambda=0)\\).\nlog-linear \\((\\rho=0, \\lambda=1)\\).\nlog-log \\((\\rho=\\lambda=0)\\).\n\nIf \\(\\rho=\\lambda\\), we get the CES production function. This nests the ‘’perfect substitutes’’ linear-linear model (\\(\\rho=\\lambda=1\\)) , the ‘’cobb-douglas’’ log-log model (\\(\\rho=\\lambda=0\\)), and many others. We can define \\(\\lambda=\\rho/\\lambda'\\) to be clear that this is indeed a CES-type transformation where\n\n\\(\\rho \\in (-\\infty,1]\\) controls the “substitutability” of explanatory variables. E.g., \\(\\rho &lt;0\\) is ‘’complementary’’.\n\\(\\lambda\\) determines ‘’returns to scale’‘. E.g., \\(\\lambda&lt;1\\) is’‘decreasing returns’’.\n\nWe compute the mean squared error in the original scale by inverting the predictions; \\[\n\\widehat{y}_{i} =\n\\begin{cases}\n[ \\widehat{y}_{i}^{(\\lambda)} \\cdot \\lambda ]^{1/\\lambda} -1 & \\lambda  \\neq 0 \\\\\nexp( \\widehat{y}_{i}^{(\\lambda)}) -1 &  \\lambda=0\n\\end{cases}.\n\\]\nIt is easiest to optimize parameters in a 2-step procedure called `concentrated optimization’. We first solve for \\(\\widehat{\\beta}(\\rho,\\lambda)\\) and compute the mean squared error \\(MSE(\\rho,\\lambda)\\). We then find the \\((\\rho,\\lambda)\\) which minimizes \\(MSE\\).\n\n\nCode\n# Box-Cox Transformation Function\nbxcx &lt;- function( xy, rho){\n    if (rho == 0L) {\n      log(xy+1)\n    } else if(rho == 1L){\n      xy\n    } else {\n      ((xy+1)^rho - 1)/rho\n    }\n}\nbxcx_inv &lt;- function( xy, rho){\n    if (rho == 0L) {\n      exp(xy) - 1\n    } else if(rho == 1L){\n      xy\n    } else {\n     (xy * rho + 1)^(1/rho) - 1\n    }\n}\n\n# Which Variables\nreg &lt;- lm(Murder~Assault+UrbanPop, data=USArrests)\nX &lt;- USArrests[,c('Assault','UrbanPop')]\nY &lt;- USArrests[,'Murder']\n\n# Simple Grid Search over potential (Rho,Lambda) \nrl_df &lt;- expand.grid(rho=seq(-2,2,by=.5),lambda=seq(-2,2,by=.5))\n\n# Compute Mean Squared Error\n# from OLS on Transformed Data\nerrors &lt;- apply(rl_df,1,function(rl){\n    Xr &lt;- bxcx(X,rl[[1]])\n    Yr &lt;- bxcx(Y,rl[[2]])\n    Datr &lt;- cbind(Murder=Yr,Xr)\n    Regr &lt;- lm(Murder~Assault+UrbanPop, data=Datr)\n    Predr &lt;- bxcx_inv(predict(Regr),rl[[2]])\n    Resr  &lt;- (Y - Predr)\n    return(Resr)\n})\nrl_df$mse &lt;- colMeans(errors^2)\n\n# Want Small MSE and Interpretable\nlayout(matrix(1:2,ncol=2), width=c(3,1), height=c(1,1))\npar(mar=c(4,4,2,0))\nplot(lambda~rho,rl_df, cex=8, pch=15,\n    xlab=expression(rho),\n    ylab=expression(lambda),\n    col=hcl.colors(25)[cut(1/rl_df$mse,25)])\n# Which min\nrl0 &lt;- rl_df[which.min(rl_df$mse),c('rho','lambda')]\npoints(rl0$rho, rl0$lambda, pch=0, col=1, cex=8, lwd=2)\n# Legend\nplot(c(0,2),c(0,1), type='n', axes=F,\n    xlab='',ylab='', cex.main=.8,\n    main=expression(frac(1,'Mean Square Error')))\nrasterImage(as.raster(matrix(hcl.colors(25), ncol=1)), 0, 0, 1,1)\ntext(x=1.5, y=seq(1,0,l=10), cex=.5,\n    labels=levels(cut(1/rl_df$mse,10)))\n\n\n\n\n\n\n\n\n\nThe parameters \\(-1,0,1,2\\) are easy to interpret and might be selected instead if there is only a small loss in fit. (In the above example, we might choose \\(\\lambda=0\\) instead of the \\(\\lambda\\) which minimized the mean square error). You can also plot the specific predictions to better understand the effect of data transformation beyond mean squared error.\n\n\nCode\n# Plot for Specific Comparisons\nXr &lt;- bxcx(X,rl0[[1]])\nYr &lt;- bxcx(Y,rl0[[2]])\nDatr &lt;- cbind(Murder=Yr,Xr)\nRegr &lt;- lm(Murder~Assault+UrbanPop, data=Datr)\nPredr &lt;- bxcx_inv(predict(Regr),rl0[[2]])\n\ncols &lt;- c(rgb(1,0,0,.5), col=rgb(0,0,1,.5))\nplot(Y, Predr, pch=16, col=cols[1], ylab='Prediction', \n    ylim=range(Y,Predr))\npoints(Y, predict(reg), pch=16, col=cols[2])\nlegend('topleft', pch=c(16), col=cols,\n    title=expression(rho~', '~lambda),\n    legend=c( paste0(rl0, collapse=', '),'1, 1') )\nabline(a=0,b=1, lty=2)\n\n\n\n\n\n\n\n\n\nWhen explicitly transforming data according to \\(\\lambda\\) and \\(\\rho\\), these parameters increase the degrees of freedom by two. The default hypothesis testing procedures do not account for you trying out different transformations, and should be adjusted by the increased degrees of freedom. Specification searches deflate standard errors and are a major source for false discoveries.\nNote that if you are ultimately interested in the outcome \\(Y\\), then transforming/untransforming \\(Y\\) can introduce a bias. To understand when you might be better off sticking with an untransformed outcome variable, see the literature on “smearing”.\n\nBreak Points.\nIncorporating Kinks and Discontinuities in \\(X\\) are a type of transformation that can be modeled using factor variables. As such, \\(F\\)-tests can be used to examine whether a breaks is statistically significant.\n\n\nCode\nlibrary(AER); data(CASchools)\nCASchools$score &lt;- (CASchools$read + CASchools$math) / 2\nreg &lt;- lm(score~income, data=CASchools)\n\n# F Test for Break\nreg2 &lt;- lm(score ~ income*I(income&gt;15), data=CASchools)\nanova(reg, reg2)\n\n# Chow Test for Break\ndata_splits &lt;- split(CASchools, CASchools$income &lt;= 15)\nresids &lt;- sapply(data_splits, function(dat){\n    reg &lt;- lm(score ~ income, data=dat)\n    sum( resid(reg)^2)\n})\nNs &lt;-  sapply(data_splits, function(dat){ nrow(dat)})\nRt &lt;- (sum(resid(reg)^2) - sum(resids))/sum(resids)\nRb &lt;- (sum(Ns)-2*reg$rank)/reg$rank\nFt &lt;- Rt*Rb\npf(Ft,reg$rank, sum(Ns)-2*reg$rank,lower.tail=F)\n\n# See also\n# strucchange::sctest(y~x, data=xy, type=\"Chow\", point=.5)\n# strucchange::Fstats(y~x, data=xy)\n\n# To Find Changes\n# segmented::segmented(reg)",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Multivariate Data II</span>"
    ]
  },
  {
    "objectID": "03_02_InterprettingRegression.html#regressograms",
    "href": "03_02_InterprettingRegression.html#regressograms",
    "title": "16  Multivariate Data II",
    "section": "16.4 Regressograms",
    "text": "16.4 Regressograms\nYou can estimate a nonparametric model with multiple \\(X\\) variables with a multivariate regressogram. Here, we cut the data into exclusive bins along each dimension (called dummy variables), and then run a regression on all dummy variables.\n\n\nCode\n## Simulate Data\nN &lt;- 10000\ne &lt;- rnorm(N)\nx1 &lt;- seq(.1,20,length.out=N)\nx2 &lt;- runif(N, 0,1)\ny  &lt;- 3*exp(-2*x2 + 1.5*x1 - .1*x1^2)*x1 + e\ndat &lt;- data.frame(x1, x2, y)\n\n## Create color palette (reused in later examples)\ncol_scale &lt;- seq(min(y)*1.1, max(y)*1.1, length.out=401)\nycol_pal &lt;- hcl.colors(length(col_scale),alpha=.5)\nnames(ycol_pal) &lt;- sort(col_scale)\n\n## Add legend (reused in later examples)\nadd_legend &lt;- function(col_scale,\n    yl=11,\n    colfun=function(x){ hcl.colors(x,alpha=.5) },\n    ...) {\n  opar &lt;- par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), \n              mar=c(0, 0, 0, 0), new=TRUE)\n  on.exit(par(opar))\n  h &lt;- hist(col_scale, plot=F, breaks=yl-1)$mids\n  plot(0, 0, type='n', bty='n', xaxt='n', yaxt='n')\n  legend(...,\n    legend=h,\n    fill=colfun(length(h)),\n    border=NA,\n    bty='n')\n}\n\n\n## Plot Data\npar(oma=c(0,0,0,2))\nplot(x1~x2, dat,\n    col=ycol_pal[cut(y,col_scale)],\n    pch=16, cex=.5, \n    main='Raw Data')\nadd_legend(x='topright', col_scale=col_scale,\n    yl=6, inset=c(0,.05), title='y')\n\n\n\n\n\n\n\n\n\n\n\nCode\n## OLS \nreg &lt;- lm(y~x1*x2, data=dat) #(with simple interaction)\nreg &lt;- lm(y~x1+x2, data=dat) #(without interaction)\n\n## Grid Points for Prediction\n# X1 bins\nl1 &lt;- 11\nbks1 &lt;- seq(0,20, length.out=l1)\nh1 &lt;- diff(bks1)[1]/2\nmids1 &lt;- bks1[-1]-h1\n# X2 bins\nl2 &lt;- 11\nbks2 &lt;- seq(0,1, length.out=l2)\nh2 &lt;- diff(bks2)[1]/2\nmids2 &lt;- bks2[-1]-h2\n# Grid\npred_x &lt;- expand.grid(x1=mids1, x2=mids2)\n\n## OLS Predictions\npred_ols &lt;- predict(reg, newdata=pred_x)\npred_df_ols  &lt;- cbind(pred_ols, pred_x)\n\n## Plot Predictions\npar(oma=c(0,0,0,2))\nplot(x1~x2, pred_df_ols,\n    col=ycol_pal[cut(pred_ols,col_scale)],\n    pch=15, cex=2, main='OLS Predictions')\nadd_legend(x='topright', col_scale=col_scale,\n    yl=6, inset=c(0,.05),title='y')\n\n\n\n\n\n\n\n\n\n\n\nCode\n##################\n# Multivariate Regressogram\n##################\n\n## Regressogram Bins\ndat$x1c &lt;- cut(dat$x1, bks1)\n#head(dat$x1c,3)\ndat$x2c &lt;- cut(dat$x2, bks2)\n\n## Regressogram\nreg &lt;- lm(y~x1c*x2c, data=dat) #nonlinear w/ complex interactions\n\n## Predicted Values\n## For Points in Middle of Each Bin\npred_df_rgrm &lt;- expand.grid(\n    x1c=levels(dat$x1c),\n    x2c=levels(dat$x2c))\npred_df_rgrm$yhat &lt;- predict(reg, newdata=pred_df_rgrm)\npred_df_rgrm &lt;- cbind(pred_df_rgrm, pred_x)\n\n## Plot Predictions\npar(oma=c(0,0,0,2))\nplot(x1~x2, pred_df_rgrm,\n    col=ycol_pal[cut(pred_df_rgrm$yhat,col_scale)],\n    pch=15, cex=2, main='Regressogram Predictions')\nadd_legend(x='topright', col_scale=col_scale,\n    yl=6, inset=c(0,.05),title='y')\n\n\n\n\n\n\n\n\n\nJust like with bivariate data, you can also use split-sample (or peicewise) regressions for multivariate data.\nAs such, there are two main ways to summarize gradients: how \\(Y\\) changes with \\(X\\).\n\nFor regressograms, you can approximate gradients with small finite differences. For some small \\(h_{p}\\), we can manually compute \\[\\begin{eqnarray}\n\\widehat{\\beta_{p}}(\\mathbf{x}) &=& \\frac{ \\widehat{Y}(x_{1},...,x_{p}+h_{p}...,x_{P})-\\widehat{Y}(x_{1},...,x_{p}-h_{p}...,x_{P})}{2h_{p}},\n\\end{eqnarray}\\]\nWhen using split-sample regressions, you can get all estimated coefficients that provides gradient estimates in each direction. \\[\\begin{eqnarray}\n\\widehat{\\beta}(\\mathbf{x}) &=& [\\mathbf{X}'\\mathbf{K}(\\mathbf{x})\\mathbf{X}]^{-1} \\mathbf{X}'\\mathbf{K}(\\mathbf{x})Y \\\\\n\\mathbf{K}(\\mathbf{x}) &=& \\begin{pmatrix}\nK\\left(\\frac{\\mathbf{X}_{1}-\\mathbf{x}}{h}\\right) & ... & 0\\\\\n\\vdots & & \\\\\n0 & ... & K\\left(\\frac{\\mathbf{X}_{P}-\\mathbf{x}}{h}\\right)\n\\end{pmatrix},\n\\end{eqnarray}\\]\n\nAfter computing gradients, you can summarize them in various plots\n\nHistograms, Scatterplots\nPlot of gradients and CI’s, \n\nYou may also be interested in a particular gradient or a single summary statistic. For example, a bivariate regressogram can estimate the marginal effect of \\(X_{1}\\) at the means; \\(\\widehat{\\beta_{1}}(\\overline{\\mathbf{x}}=[\\overline{x_{1}}, \\overline{x_{2}}])\\). You may also be interested in the mean of the marginal effects (sometimes said simply as “average effect”), which averages the marginal effect over all datapoints in the dataset: \\(1/N \\sum_{i}^{N} \\widehat{\\beta_{1}}(\\mathbf{X}_{i})\\), or the median marginal effect. Such statistics are single numbers that can be presented similar to an OLS regression table where each row corresponds a variable and each cell has two elements: “mean gradient (sd gradient)”.",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Multivariate Data II</span>"
    ]
  },
  {
    "objectID": "03_02_InterprettingRegression.html#more-literature",
    "href": "03_02_InterprettingRegression.html#more-literature",
    "title": "16  Multivariate Data II",
    "section": "16.5 More Literature",
    "text": "16.5 More Literature\nDiagnostics\n\nhttps://book.stat420.org/model-diagnostics.html#leverage\nhttps://socialsciences.mcmaster.ca/jfox/Books/RegressionDiagnostics/index.html\nhttps://bookdown.org/ripberjt/labbook/diagnosing-and-addressing-problems-in-linear-regression.html\nBelsley, D. A., Kuh, E., and Welsch, R. E. (1980). Regression Diagnostics: Identifying influential data and sources of collinearity. Wiley. https://doi.org/10.1002/0471725153\nFox, J. D. (2020). Regression diagnostics: An introduction (2nd ed.). SAGE. https://dx.doi.org/10.4135/9781071878651",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Multivariate Data II</span>"
    ]
  },
  {
    "objectID": "03_03_ObservationalData.html",
    "href": "03_03_ObservationalData.html",
    "title": "17  Observational Data",
    "section": "",
    "text": "17.1 Temporal Interdependence\nMany observational datasets have temporal dependence, meaning that values at one point in time are related to past values. This violates the standard assumption of independence used in many statistical methods.\nStock prices are classic examples of temporally dependent processes. If Apple’s stock was high yesterday, it is more likely (but not guaranteed) to be high today.\nCode\n# highest price each day\nlibrary(plotly)\nstock &lt;- read.csv('https://raw.githubusercontent.com/plotly/datasets/master/finance-charts-apple.csv')\nfig &lt;- plot_ly(stock, type = 'scatter', mode = 'lines')%&gt;%\n  add_trace(x = ~Date, y = ~AAPL.High) %&gt;%\n  layout(showlegend = F)\nfig\nA random walk is the simplest mathematical model of temporal dependence. Each new value is just the previous value plus a random shock (white noise).\nCode\n# Generate Random Walk\ntN &lt;- 200\ny &lt;- numeric(tN)\ny[1] &lt;- stock$AAPL.High[1]\nfor (ti in 2:tN) {\n    y[ti] &lt;- y[ti-1] + runif(1, -10, 10)\n}\n#x &lt;- runif(tN, -1,1) White Noise\n\ny_dat &lt;- data.frame(Date=1:tN, RandomWalk=y)\nfig &lt;- plot_ly(y_dat, type = 'scatter', mode = 'lines') %&gt;%\n  add_trace(x=~Date, y=~RandomWalk) %&gt;%\n  layout(showlegend = F)\nfig\nIn both plots, we see that today’s value is not independent of past values. In contrast to cross-sectional data (e.g. individual incomes), time series often require special methods to account for memory and nonstationarity.\nIn any case, if often helps to see the marginal distribution too.\nCode\nx0 &lt;- rbinom(600, 1, 0.5)\n\n# Setup 2 Plots, side by side\nlayout(matrix(c(1, 2), nrow = 1), widths = c(4, 1))\n\n# Plot Cumulative Averages\nx0_t &lt;- seq_len(length(x0))\nx0_mt &lt;- cumsum(x0)/x0_t\npar(mar=c(4,4,1,4))\nplot(x0_t, x0_mt, type='l',\n    ylab='Cumulative Average',\n    xlab='Flip #', \n    ylim=c(0,1), \n    lwd=2)\npoints(x0_t, x0, col=grey(0,.5),\n    pch=16, cex=.2)\n\n# Plot Long run proportions\npar(mar=c(4,4,1,1))\nx_hist &lt;- hist(x0, breaks=50, plot=F)\nx_freq &lt;- x_hist$count/length(x0)\nbarplot(x_freq ,\n    axes=FALSE,\n    space=0, horiz=TRUE, border=NA)\naxis(1)\naxis(2)\nmtext('Observed Frequency', 2, line=2.5)",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Observational Data</span>"
    ]
  },
  {
    "objectID": "03_03_ObservationalData.html#temporal-interdependence",
    "href": "03_03_ObservationalData.html#temporal-interdependence",
    "title": "17  Observational Data",
    "section": "",
    "text": "Stationary.\nA stationary time series is one whose statistical properties — mean, variance, and autocovariance — do not change over time. Formally\n\nStationary Means: \\(E[y_{t}]=E[y_{t'}]\\) for all periods \\(t, t'\\)\nStationary Vars: \\(V[y_{t}]=V[y_{t'}]\\) for all periods \\(t, t'\\)\n\nE.g., ( y_t = t + u_t, u_t (0, + t) )\n\n\nCode\ntN &lt;- 200\nsimulate_series &lt;- function(beta, alpha, sigma=.2){\n    y &lt;- numeric(tN)\n    for (ti in 1:tN) {\n        mean_ti &lt;- beta*ti\n        sd_ti &lt;- (.2 + alpha*ti)\n        y[ti] &lt;- mean_ti + rnorm(1, sd=sd_ti)\n    }\n    return(y)\n}\n\n# Plotting Functions\nplot_setup &lt;- function(alpha, beta){\n    plot.new()\n    plot.window(xlim=c(1,tN), ylim=c(-5,20))\n    axis(1)\n    axis(2)\n    mtext(expression(y[t]),2, line=2.5)\n    mtext(\"Time (t)\", 1, line=2.5)\n}\nplot_title &lt;- function(alpha, beta){\n    beta_name &lt;- ifelse(beta==0, 'Mean Stationary', 'Mean Nonstationary')\n    alpha_name &lt;- ifelse(alpha==0, 'Var Stationary', 'Var Nonstationary')\n    title(paste0(beta_name,', ', alpha_name), font.main=1, adj=0)\n}\n\npar(mfrow = c(2, 2))\nfor(alpha in c(0,.015)){\nfor(beta in c(0,.05)){\n    plot_setup(alpha=alpha, beta=beta)\n    for( sim in c('red','blue')){\n        y_sim &lt;- simulate_series(beta=beta, alpha=alpha)\n        lines(y_sim, col=adjustcolor(sim ,alpha.f=0.5), lwd=2)\n    }\n    plot_title(alpha=alpha, beta=beta)\n}}\n\n\n\n\n\n\n\n\n\n\n\nMeasures of temporal association.\nTime series often exhibit serial dependence—values today are related to past values, and potentially to other processes evolving over time. We can visualize this using correlation-based diagnostics.\nThe Autocorrelation Function (AFC) measures correlation between a time series and its own lagged values:\n\\(ACF_{Y}(k) = \\frac{Cov(Y_{t},Y_{t-k})}{ \\sqrt{Var(Y_{t})Var(Y_{t-k})}}\\)\nThis helps detect temporal persistence (memory). For stationary processes, the ACF typically decays quickly, whereas for nonstationary processes, it typically decays slowly or persists.\n\n\nCode\npar(mfrow = c(2, 2))\nfor(alpha in c(0,.015)){\nfor(beta in c(0,.05)){\n    y_sim &lt;- simulate_series(beta=beta, alpha=alpha)\n    acf(y_sim, main='')\n    plot_title(alpha=alpha, beta=beta)\n}}\n\n\n\n\n\n\n\n\n\nThe Cross-Correlation Function (CCF) measures correlation between two time series at different lags:\n\\(CCF_{YX}(k) = \\frac{Cov(Y_{t},X_{t-k})}{ \\sqrt{Var(Y_t)Var(X_{t-k})}}\\)\nThis is useful for detecting lagged relationships between two series, such as leading indicators or external drivers. (If \\(X\\) is white noise, any visible structure in the CCF likely reflects nonstationarity in \\(Y\\).)\n\n\nCode\nx_sim &lt;- runif(tN, -1,1) # White Noise\npar(mfrow = c(2, 2))\nfor(alpha in c(0,.015)){\nfor(beta in c(0,.05)){\n    y_sim &lt;- simulate_series(beta=beta, alpha=alpha)\n    ccf(y_sim, x_sim, main='')\n    plot_title(alpha=alpha, beta=beta)\n}}",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Observational Data</span>"
    ]
  },
  {
    "objectID": "03_03_ObservationalData.html#spatial-interdependence",
    "href": "03_03_ObservationalData.html#spatial-interdependence",
    "title": "17  Observational Data",
    "section": "17.2 Spatial Interdependence",
    "text": "17.2 Spatial Interdependence\nMany observational datasets exhibit spatial dependence, meaning that values at one location tend to be related to values at nearby locations. This violates the standard assumption of independent observations used in many classical statistical methods.\nFor example, elevation is spatially dependent: if one location is at high elevation, nearby locations are also likely (though not guaranteed) to be high. Similarly, socioeconomic outcomes like disease rates or income often cluster geographically due to shared environmental or social factors.\nJust as stock prices today depend on yesterday, spatial variables often depend on neighboring regions, creating a need for specialized statistical methods that account for spatial autocorrelation.\n\nRaster vs. Vector Data.\nSpatial data typically comes in two formats, each suited to different types of information:\n\nVector data uses geometric shapes (points, lines, polygons) to store data. E.g., a census tract map that stores data on population demographics.\nRaster data uses grid cells (typically squares, but sometimes hexagons) to store data. E.g., an image that stores data on elevation above seawater.\n\n\n\nCode\n# Vector Data\nlibrary(sf)\nnorthcarolina_vector &lt;- st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\n## Reading layer `nc' from data source `/home/Jadamso/R-Libs/sf/shape/nc.shp' using driver `ESRI Shapefile'\n## Simple feature collection with 100 features and 14 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\n## Geodetic CRS:  NAD27\nplot(northcarolina_vector['BIR74'], main='Number of Live Births in 1974')\n\n\n\n\n\n\n\n\n\nCode\n# https://r-spatial.github.io/spdep/articles/sids.html\n\n\n\n\nCode\n# Raster Data\nlibrary(terra)\nluxembourg_elevation_raster &lt;- rast(system.file(\"ex/elev.tif\", package=\"terra\"))\nplot(luxembourg_elevation_raster)\n\n\n\n\n\n\n\n\n\n\n\nStationary.\nJust as with temporal data, stationarity in spatial data means that the statistical properties (like mean, variance, or spatial correlation) are roughly the same across space.\n\nStationary Means: \\(E[y(s)]=E[y(s')]\\) for all locations \\(s,s'\\)\nStationary Vars: \\(V[y(s)]=V[y(s')]\\) for all locations \\(s,s'\\)\n\n\n\nCode\n# Simulated 2D spatial fields\nset.seed(1)\nn &lt;- 20\nx &lt;- y &lt;- seq(0, 1, length.out = n)\ngrid &lt;- expand.grid(x = x, y = y)\n\n# 1. Stationary: Gaussian with constant mean and var\nz_stationary &lt;- matrix(rnorm(n^2, 0, 1), n, n)\n\n# 2. Nonstationary: Mean increases with x and y\nz_nonstationary &lt;- outer(x, y, function(x, y) 3*x*y) + rnorm(n^2, 0, 1)\n\npar(mfrow = c(1, 2))\n# Stationary field\nimage(x, y, z_stationary,\n      main = \"Stationary Field\",\n      col = terrain.colors(100),\n      xlab = \"x\", ylab = \"y\")\n# Nonstationary field\nimage(x, y, z_nonstationary,\n      main = \"Nonstationary Field\",\n      col = terrain.colors(100),\n      xlab = \"x\", ylab = \"y\")\n\n\n\n\n\n\n\n\n\n\n\nMeasures of spatial association.\nJust like temporal data may exhibit autocorrelation, spatial data may show spatial autocorrelation or spatial cross-correlation—meaning that observations located near each other are more (or less) similar than we would expect under spatial independence.\nAutocorrelation. We can measure spatial autocorrelation using Moran’s I, a standard index of spatial dependence. Global Moran’s I summarizes overall spatial association (just like the ACF)\n\n\nCode\n# Raster Data Example\nautocor(luxembourg_elevation_raster, method='moran', global=T)\n## elevation \n## 0.8917057\n\n\nCross-Correlation. We can also assesses the relationship between two variables at varying distances.\n\n\nCode\n# Vector Data Example\ndat &lt;- as.data.frame(northcarolina_vector)[, c('BIR74', 'SID74')]\nmu &lt;- colMeans(dat)\n\n# Format Distances\ndmat &lt;- st_distance( st_centroid(northcarolina_vector) )\ndmat &lt;- units::set_units(dmat, 'km')\n\n# At Which Distances to Compute CCF\n# summary(dmat[,1])\nrdists &lt;- c(-1,seq(0,100,by=25)) # includes 0\nrdists &lt;- units::set_units(rdists , 'km')\n\n# Compute Cross-Covariances\nvarXY &lt;- prod( apply(dat, 2, sd) )\nCCF &lt;- lapply( seq(2, length(rdists)), function(ri){\n    # Which Observations are within (rmin, rmax] distance\n    dmat_r &lt;- dmat\n    d_id &lt;- (dmat_r &gt; rdists[ri-1] & dmat_r &lt;= rdists[ri]) \n    dmat_r[!d_id]  &lt;- NA\n    # Compute All Covariances (Stationary)\n    covs_r &lt;- lapply(1:nrow(dmat_r), function(i){\n        pairsi &lt;- which(!is.na(dmat_r[i,]))        \n        covXiYj &lt;- sapply(pairsi, function(j) {\n            dXi &lt;- dat[i,1] - mu[1]\n            dYj &lt;- dat[j,2] - mu[2]\n            return(dXi*dYj)\n        })\n        return(covXiYj)\n    })\n    corXY &lt;- unlist(covs_r)/varXY\n    return(corXY)\n} )\n\n\n\n\nCode\n# Plot Cross-Covariance Function\nx &lt;- as.numeric(rdists[-1])\n\npar(mfrow=c(1,2))\n\n# Distributional Summary\nboxplot(CCF,\n    outline=F, whisklty=0, staplelty=0,\n    ylim=c(-1,1), #quantile(unlist(CCF), probs=c(.05,.95)),\n    names=x, \n    main='',\n    font.main=1,\n    xlab='Distance [km]',\n    ylab='Cross-Correlation of BIR74 and SID74')\ntitle('Binned Medians and IQRs', font.main=1, adj=0)\nabline(h=0, lty=2)\n\n# Inferential Summary\nCCF_means &lt;- sapply(CCF, mean)\nplot(x, CCF_means,\n    ylim=c(-1,1),\n    type='o', pch=16,\n    main='',\n    xlab='Distance [km]',\n    ylab='Cross-Correlation of BIR74 and SID74')\ntitle('Binned Means + 95% Confidence Band', font.main=1, adj=0)\nabline(h=0, lty=2)    \n# Quick and Dirty Subsampling CI\nCCF_meanCI &lt;- sapply(CCF, function(corXY){\n    ss_size &lt;- floor(length(corXY)*3/4)\n    corXY_boot &lt;- sapply(1:200, function(b){\n        corXY_b &lt;- sample(corXY, ss_size, replace=F)\n        mean(corXY_b, na.rm=T)\n    })\n    quantile(corXY_boot,  probs=c(.025,.975), na.rm=T)\n})\npolygon( c(x, rev(x)), \n    c(CCF_meanCI[1,], rev(CCF_meanCI[2,])), \n    col=grey(0,.25), border=NA)",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Observational Data</span>"
    ]
  },
  {
    "objectID": "03_03_ObservationalData.html#economic-interdependence",
    "href": "03_03_ObservationalData.html#economic-interdependence",
    "title": "17  Observational Data",
    "section": "17.3 Economic Interdependence",
    "text": "17.3 Economic Interdependence\nIn addition to spatial and temporal dependence, many observational datasets exhibit interdependence between variables for economic reasons. In the minds of economists, many variables are endogenous: meaning that they are an economic outcome determined (or caused: \\(\\to\\)) by some other variable.\n\nIf \\(Y \\to X\\), then we have reverse causality\nIf \\(Y \\to X\\) and \\(X \\to Y\\), then we have simultaneity\nIf \\(Z\\to Y\\) and either \\(Z\\to X\\) or \\(X \\to Z\\), then we have omitted a potentially important variable\n\nThese endogeneity issues imply \\(X\\) and \\(\\epsilon\\) are correlated, which is a barrier to interpreting OLS estimates causally. (\\(X\\) and \\(\\epsilon\\) may be correlated for other reasons too, such as when \\(X\\) is measured with error.)\n\n\nCode\n# Simulate data with an endogeneity issue\nn &lt;- 300\nz &lt;- rbinom(n,1,.5)\nxy &lt;- sapply(z, function(zi){\n    y &lt;- rnorm(1,zi,1)\n    x &lt;- rnorm(1,zi*2,1)\n    c(x,y)\n})\nxy &lt;- data.frame(x=xy[1,],y=xy[2,])\nplot(y~x, data=xy, pch=16, col=grey(0,.5))\nabline(lm(y~x,data=xy))\n\n\n\n\n\n\n\n\n\nWith multiple linear regression, endogeneity biases are not just a problem for your main variable of interest. Suppose your interested in how \\(x_{1}\\) affects \\(y\\), conditional on \\(x_{2}\\). Letting \\(X=[x_{1}, x_{2}]\\), you estimate \\[\\begin{eqnarray}\n\\hat{\\beta}_{OLS} = [X'X]^{-1}X'y\n\\end{eqnarray}\\] You paid special attention in your research design to find a case where \\(x_{1}\\) is truly exogenous. Unfortunately, \\(x_{2}\\) is correlated with the error term. (How unfair, I know, especially after all that work). Nonetheless, \\[\\begin{eqnarray}\n\\mathbb{E}[X'\\epsilon] =\n\\begin{bmatrix}\n0 \\\\ \\rho\n\\end{bmatrix}\\\\\n\\mathbb{E}[ \\hat{\\beta}_{OLS} - \\beta] = [X'X]^{-1} \\begin{bmatrix}\n0 \\\\ \\rho\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\rho_{1} \\\\ \\rho_{2}\n\\end{bmatrix}\n\\end{eqnarray}\\] The magnitude of the bias for \\(x_{1}\\) thus depends on the correlations between \\(x_{1}\\) and \\(x_{2}\\) as well as \\(x_{2}\\) and \\(\\epsilon\\).\nI will focus on the seminal economic example to provide some intuition.\n\nCompetitive Market Equilibrium.\nThis model has three structural relationships: (1) market supply is the sum of quantities supplied by individual firms at a given price, (2) market demand is the sum of quantities demanded by individual people at a given price, and (3) market supply equals market demand in equilibrium. Assuming market supply and demand are linear, we can write these three relationships as \\[\\begin{eqnarray}\n\\label{eqn:market_supply}\nQ_{S}(P) &=& A_{S} + B_{S} P + E_{S},\\\\\n\\label{eqn:market_demand}\nQ_{D}(P) &=& A_{D} - B_{D} P + E_{D},\\\\\n\\label{eqn:market_eq}\nQ_{D} &=& Q_{S} = Q.\n%%  $Q_{D}(P) = \\sum_{i} q_{D}_{i}(P)$,\n\\end{eqnarray}\\] This last equation implies a simultaneous “reduced form” relationship where both the price and the quantity are outcomes. With a linear parametric structure to these equations, we can use algebra to solve for the equilibrium price and quantity analytically as \\[\\begin{eqnarray}\nP^{*} &=& \\frac{A_{D}-A_{S}}{B_{D}+B_{S}} + \\frac{E_{D} - E_{S}}{B_{D}+B_{S}}, \\\\\nQ^{*} &=& \\frac{A_{S}B_{D}+ A_{D}B_{S}}{B_{D}+B_{S}} + \\frac{E_{S}B_{D}+ E_{D}B_{S}}{B_{D}+B_{S}}.\n\\end{eqnarray}\\]\n\n\nCode\n# Demand Curve Simulator\nqd_fun &lt;- function(p, Ad=8, Bd=-.8, Ed_sigma=.25){\n    Qd &lt;- Ad + Bd*p + rnorm(1,0,Ed_sigma)\n    return(Qd)\n}\n\n# Supply Curve Simulator\nqs_fun &lt;- function(p, As=-8, Bs=1, Es_sigma=.25){\n    Qs &lt;- As + Bs*p + rnorm(1,0,Es_sigma)\n    return(Qs)\n}\n\n# Quantity Supplied and Demanded at 3 Prices\ncbind(P=8:10, D=qd_fun(8:10), S=qs_fun(8:10))\n##       P          D          S\n## [1,]  8  1.1925652 0.01120111\n## [2,]  9  0.3925652 1.01120111\n## [3,] 10 -0.4074348 2.01120111\n\n# Market Equilibrium Finder\neq_fun &lt;- function(demand, supply, P){\n    # Compute EQ (what we observe)\n    eq_id &lt;- which.min( abs(demand-supply) )\n    eq &lt;- c(P=P[eq_id], Q=demand[eq_id]) \n    return(eq)\n}\n\n\n\n\nCode\n# Simulations Parameters\nN &lt;- 300 # Number of Market Interactions\nP &lt;- seq(5,10,by=.01) # Price Range to Consider\n\n# Generate Data from Competitive Market  \n# Plot Underlying Process\nplot.new()\nplot.window(xlim=c(0,2), ylim=range(P))\nEQ1 &lt;- sapply(1:N, function(n){\n    # Market Data Generating Process\n    demand &lt;- qd_fun(P)\n    supply &lt;- qs_fun(P)\n    eq &lt;- eq_fun(demand, supply, P)    \n    # Plot Theoretical Supply and Demand\n    lines(demand, P, col=grey(0,.01))\n    lines(supply, P, col=grey(0,.01))\n    points(eq[2], eq[1], col=grey(0,.05), pch=16)\n    # Save Data\n    return(eq)\n})\naxis(1)\naxis(2)\nmtext('Quantity',1, line=2)\nmtext('Price',2, line=2)\n\n\n\n\n\n\n\n\n\nSuppose we ask “what is the effect of price on quantity?” You can simply run a regression of quantity (“Y”) on price (“X”): \\(\\widehat{\\beta}_{OLS} = Cov(Q^{*}, P^{*}) / Var(P^{*})\\). You get a number back, but it is hard to interpret meaningfully.\n\n\nCode\n# Analyze Market Data\ndat1 &lt;- data.frame(t(EQ1), cost='1', T=1:N)\nreg1 &lt;- lm(Q~P, data=dat1)\nsummary(reg1)\n## \n## Call:\n## lm(formula = Q ~ P, data = dat1)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.57279 -0.11977 -0.00272  0.11959  0.45525 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)  \n## (Intercept) -0.21323    0.43212  -0.493   0.6221  \n## P            0.12355    0.04864   2.540   0.0116 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1674 on 298 degrees of freedom\n## Multiple R-squared:  0.02119,    Adjusted R-squared:  0.0179 \n## F-statistic: 6.451 on 1 and 298 DF,  p-value: 0.0116\n\n\nThis simple derivation has a profound insight: price-quantity data does not generally tell you how price affects quantity (or vice-versa). The reason is simultaneity: price and quantity mutually cause one another in markets.1\nMoreover, this example also clarifies that our initial question “what is the effect of price on quantity?” is misguided. We could more sensibly ask “what is the effect of price on quantity supplied?” or “what is the effect of price on quantity demanded?”",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Observational Data</span>"
    ]
  },
  {
    "objectID": "03_03_ObservationalData.html#further-reading",
    "href": "03_03_ObservationalData.html#further-reading",
    "title": "17  Observational Data",
    "section": "17.4 Further Reading",
    "text": "17.4 Further Reading",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Observational Data</span>"
    ]
  },
  {
    "objectID": "03_03_ObservationalData.html#footnotes",
    "href": "03_03_ObservationalData.html#footnotes",
    "title": "17  Observational Data",
    "section": "",
    "text": "Although there are many ways this simultaneity can happen, economic theorists have made great strides in analyzing the simultaneity problem as it arises from equilibrium market relationships. In fact, 2SLS arose to understand agricultural markets.↩︎",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Observational Data</span>"
    ]
  },
  {
    "objectID": "03_04_ExperimentalData.html",
    "href": "03_04_ExperimentalData.html",
    "title": "18  Experimental Data",
    "section": "",
    "text": "18.1 Design Basics\nWe will consider with our example from the last chaper\nCode\n# Demand Curve Simulator\nqd_fun &lt;- function(p, Ad=8, Bd=-.8, Ed_sigma=.25){\n    Qd &lt;- Ad + Bd*p + rnorm(1,0,Ed_sigma)\n    return(Qd)\n}\n\n# Supply Curve Simulator\nqs_fun &lt;- function(p, As=-8, Bs=1, Es_sigma=.25){\n    Qs &lt;- As + Bs*p + rnorm(1,0,Es_sigma)\n    return(Qs)\n}\n\n# Quantity Supplied and Demanded at 3 Prices\ncbind(P=8:10, D=qd_fun(8:10), S=qs_fun(8:10))\n##       P          D          S\n## [1,]  8 1.65909899 -0.1005547\n## [2,]  9 0.85909899  0.8994453\n## [3,] 10 0.05909899  1.8994453\n\n# Market Equilibrium Finder\neq_fun &lt;- function(demand, supply, P){\n    # Compute EQ (what we observe)\n    eq_id &lt;- which.min( abs(demand-supply) )\n    eq &lt;- c(P=P[eq_id], Q=demand[eq_id]) \n    return(eq)\n}\nCode\nN &lt;- 300 # Number of Market Interactions\nP &lt;- seq(5,10,by=.01) # Price Range to Consider\nEQ1 &lt;- sapply(1:N, function(n){\n    # Market Data Generating Process\n    demand &lt;- qd_fun(P)\n    supply &lt;- qs_fun(P)\n    eq &lt;- eq_fun(demand, supply, P)    \n    return(eq)\n})\ndat1 &lt;- data.frame(t(EQ1), cost='1', T=1:N)",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Experimental Data</span>"
    ]
  },
  {
    "objectID": "03_04_ExperimentalData.html#design-basics",
    "href": "03_04_ExperimentalData.html#design-basics",
    "title": "18  Experimental Data",
    "section": "",
    "text": "Control and Randomize.\nBlocking and Clustering\n\n\nCompetitive Equilibrium Example.\nIf you have exogenous variation on one side of the market, you can get information on the other. For example, lower costs shift out supply (more is produced at given price), allowing you to trace out part of a demand curve.\nTo see this, consider an experiment where student subjects are recruited to a classroom and randomly assigned to be either buyers or sellers in a market for little red balls. In this case, the classroom environment allows the experimenter to control for various factors (e.g., the temperature of the room is constant for all subjects) and the explicit randomization of subjects means that there are not typically systematic differences in different groups of students.\nIn the experiment, sellers are given linear “cost functions” that theoretically yield individual supplies like \\(\\eqref{eqn:market_supply}\\) and are paid “price - cost”. Buyers are given linear “benefit functions” that theoretically yield individual demands like \\(\\eqref{eqn:market_demand}\\), and are paid “benefit - price”. The theoretical predictions are theorefore given in \\(\\eqref{eqn:market_supply}\\). Moreover, experimental manipulation of \\(A_{S}\\) leads to \\[\\begin{eqnarray}\n\\label{eqn:comp_market_statics}\n\\frac{d P^{*}}{d A_{S}} = \\frac{-1}{B_{D}+B_{S}}, \\\\\n\\frac{d Q^{*}}{d A_{S}} = \\frac{B_{D}}{B_{D}+B_{S}}.\n\\end{eqnarray}\\] In this case, the supply shock has identified the demand slope: \\(-B_{D}=d Q^{*}/d P^{*}\\).\n\n\nCode\n# New Observations After Cost Change\nEQ2 &lt;- sapply(1:N, function(n){\n    demand &lt;- qd_fun(P)\n    supply2 &lt;- qs_fun(P, As=-6.5) # More Supplied at Given Price\n    eq &lt;- eq_fun(demand, supply2, P)\n    return(eq)\n    # lines(supply2, P, col=rgb(0,0,1,.01))\n    #points(eq[2], eq[1], col=rgb(0,0,1,.05), pch=16)\n})\ndat2 &lt;- data.frame(t(EQ2), cost='2', T=(1:N) + N)\ndat2 &lt;- rbind(dat1, dat2)\n\n# Plot Simulated Market Data\ncols &lt;- ifelse(as.numeric(dat2$cost)==2, rgb(0,0,1,.2), rgb(0,0,0,.2))\nplot.new()\nplot.window(xlim=c(0,2), ylim=range(P))\npoints(dat2$Q, dat2$P, col=cols, pch=16)\naxis(1)\naxis(2)\nmtext('Quantity',1, line=2)\nmtext('Price',2, line=2)\n\n\n\n\n\n\n\n\n\nIf the function forms for supply and demand are different from what we predicted, we can still measure how much the experimental manipulation of production costs affects the equilibrium quantity sold (and compare that to what was predicted).1",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Experimental Data</span>"
    ]
  },
  {
    "objectID": "03_04_ExperimentalData.html#comparisons-over-time",
    "href": "03_04_ExperimentalData.html#comparisons-over-time",
    "title": "18  Experimental Data",
    "section": "18.2 Comparisons Over Time",
    "text": "18.2 Comparisons Over Time\n\nRegression Discontinuities/Kinks.\nThe basic idea of RDD/RKD is to examine how a variable changes just before and just after a treatment. RDD estimates the difference in the levels of an outcome variable, whereas RKD estimates the difference in the slope. Turning to our canonical competitive market example, the RDD estimate is the difference between the lines at \\(T=300\\).\n\n\nCode\n# Locally Linear Regression \n# (Compare means near break)\n\ncols &lt;- ifelse(as.numeric(dat2$cost)==2, rgb(0,0,1,.5), rgb(0,0,0,.5))\nplot(P~T, dat2, main='Effect of Cost Shock on Price', \n    font.main=1, pch=16, col=cols)\nregP1 &lt;- loess(P~T, dat2[dat2$cost==1,]) \nx1 &lt;- regP1$x\n#lm(): x1 &lt;- regP1$model$T \nlines(x1, predict(regP1), col=rgb(0,0,0), lwd=2)\nregP2 &lt;- loess(P~T, dat2[dat2$cost==2,])\nx2 &lt;- regP2$x #regP1$model$T\nlines(x2, predict(regP2), col=rgb(0,0,1), lwd=2)\n\n\n\n\n\n\n\n\n\nCode\n\nplot(Q~T, dat2, main='Effect of Cost Shock on Quantity',\n    font.main=1, pch=16, col=cols)\nregQ1 &lt;- loess(Q~T, dat2[dat2$cost==1,]) \nlines(x1, predict(regQ1), col=rgb(0,0,0), lwd=2)\nregQ2 &lt;- loess(Q~T, dat2[dat2$cost==2,])\nx2 &lt;- regP2$x #regP1$model$T\nlines(x2, predict(regQ2), col=rgb(0,0,1), lwd=2)\n\n\n\n\n\n\n\n\n\n\nCode\n# Linear Regression Alternative\nsub_id &lt;- (dat2$cost==1 & dat2$T &gt; 250) | (dat2$cost==2 & dat2$T &lt; 300)\ndat2W &lt;- dat2[sub_id,  ]\nregP &lt;- lm(P~T*cost, dat2)\nregQ &lt;- lm(Q~T*cost, dat2)\nstargazer::stargazer(regP, regQ, \n    type='html',\n    title='Recipe RDD',\n    header=F)\n\n\n\nRecipe RDD\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nP\n\n\nQ\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nT\n\n\n0.00002\n\n\n0.0001\n\n\n\n\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n\n\n\n\n\n\n\n\n\n\ncost2\n\n\n-0.905***\n\n\n0.716***\n\n\n\n\n\n\n(0.061)\n\n\n(0.057)\n\n\n\n\n\n\n\n\n\n\n\n\nT:cost2\n\n\n0.0001\n\n\n-0.0001\n\n\n\n\n\n\n(0.0002)\n\n\n(0.0002)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n8.883***\n\n\n0.863***\n\n\n\n\n\n\n(0.022)\n\n\n(0.020)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n600\n\n\n600\n\n\n\n\nR2\n\n\n0.837\n\n\n0.787\n\n\n\n\nAdjusted R2\n\n\n0.836\n\n\n0.786\n\n\n\n\nResidual Std. Error (df = 596)\n\n\n0.186\n\n\n0.175\n\n\n\n\nF Statistic (df = 3; 596)\n\n\n1,022.455***\n\n\n735.855***\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\nRemember that this is effect is local: different magnitudes of the cost shock or different demand curves generally yield different estimates.\nMoreover, note that more than just costs have changed over time: subjects in the later periods have history experience behind them while they do not in earlier periods. So hidden variables like “beliefs” are implicitly treated as well. This is one concrete reason to have an explicit control group.\n\n\nDifference in Differences.\nThe basic idea of DID is to examine how a variable changes in response to an exogenous shock, compared to a control group.\n\n\nCode\nEQ3 &lt;- sapply(1:(2*N), function(n){\n\n    # Market Mechanisms\n    demand &lt;- qd_fun(P)\n    supply &lt;- qs_fun(P)\n\n    # Compute EQ (what we observe)\n    eq_id &lt;- which.min( abs(demand-supply) )\n    eq &lt;- c(P=P[eq_id], Q=demand[eq_id]) \n\n    # Return Equilibrium Observations\n    return(eq)\n})\ndat3 &lt;- data.frame(t(EQ3), cost='1', T=1:ncol(EQ3))\ndat3_pre  &lt;- dat3[dat3$T &lt;= N ,]\ndat3_post &lt;- dat3[dat3$T &gt; N ,]\n\n# Plot Price Data\npar(mfrow=c(1,2))\nplot(P~T, dat2, main='Effect of Cost Shock on Price', \n    font.main=1, pch=16, col=cols, cex=.5)\nlines(x1, predict(regP1), col=rgb(0,0,0), lwd=2)\nlines(x2, predict(regP2), col=rgb(0,0,1), lwd=2)\n# W/ Control group\npoints(P~T, dat3, pch=16, col=rgb(1,0,0,.5), cex=.5)\nregP3a &lt;- loess(P~T, dat3_pre)\nx3a &lt;- regP3a$x\nlines(x3a, predict(regP3a), col=rgb(1,0,0), lwd=2)\nregP3b &lt;- loess(P~T, dat3_post)\nx3b &lt;- regP3b$x\nlines(x3b, predict(regP3b), col=rgb(1,0,0), lwd=2)\n\n\n# Plot Quantity Data\nplot(Q~T, dat2, main='Effect of Cost Shock on Quantity',\n    font.main=1, pch=17, col=cols, cex=.5)\nlines(x1, predict(regQ1), col=rgb(0,0,0), lwd=2)\nlines(x2, predict(regQ2), col=rgb(0,0,1), lwd=2)\n# W/ Control group\npoints(Q~T, dat3, pch=16, col=rgb(1,0,0,.5), cex=.5)\nregQ3a &lt;- loess(Q~T, dat3_pre) \nlines(x3a, predict(regQ3a), col=rgb(1,0,0), lwd=2)\nregQ3b &lt;- loess(Q~T, dat3_post) \nlines(x3b, predict(regQ3b), col=rgb(1,0,0), lwd=2)\n\n\n\n\n\n\n\n\n\nLinear Regression Estimates\n\nCode\n# Pool Data\ndat_pooled &lt;- rbind(\n    cbind(dat2, EverTreated=1, PostPeriod=(dat2$T &gt; N)),\n    cbind(dat3, EverTreated=0, PostPeriod=(dat3$T &gt; N)))\ndat_pooled$EverTreated &lt;- as.factor(dat_pooled$EverTreated)\ndat_pooled$PostPeriod &lt;- as.factor(dat_pooled$PostPeriod)\n\n# Estimate Level Shift for Different Groups after T=300\nregP &lt;- lm(P~PostPeriod*EverTreated, dat_pooled)\nregQ &lt;- lm(Q~PostPeriod*EverTreated, dat_pooled)\nstargazer::stargazer(regP, regQ, \n    type='html',\n    title='Recipe DiD',\n    header=F)\n\n\n\nRecipe DiD\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nP\n\n\nQ\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nPostPeriod\n\n\n-0.002\n\n\n0.027*\n\n\n\n\n\n\n(0.016)\n\n\n(0.014)\n\n\n\n\n\n\n\n\n\n\n\n\nEverTreated1\n\n\n0.013\n\n\n-0.001\n\n\n\n\n\n\n(0.016)\n\n\n(0.014)\n\n\n\n\n\n\n\n\n\n\n\n\nPostPeriodTRUE:EverTreated1\n\n\n-0.839***\n\n\n0.642***\n\n\n\n\n\n\n(0.022)\n\n\n(0.020)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n8.873***\n\n\n0.874***\n\n\n\n\n\n\n(0.011)\n\n\n(0.010)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n1,200\n\n\n1,200\n\n\n\n\nR2\n\n\n0.778\n\n\n0.725\n\n\n\n\nAdjusted R2\n\n\n0.777\n\n\n0.725\n\n\n\n\nResidual Std. Error (df = 1196)\n\n\n0.193\n\n\n0.176\n\n\n\n\nF Statistic (df = 3; 1196)\n\n\n1,396.986***\n\n\n1,052.117***\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Experimental Data</span>"
    ]
  },
  {
    "objectID": "03_04_ExperimentalData.html#natural-experiments",
    "href": "03_04_ExperimentalData.html#natural-experiments",
    "title": "18  Experimental Data",
    "section": "18.3 “Natural” Experiments",
    "text": "18.3 “Natural” Experiments\nNatural experiments are historical case studies that remedy the endogeneity issues in observational data. They assume that a historical events is quasi (or psuedo) random. In addition to “RDD” and “DID” methods discussed above, instrumental variables are used in historical event studies. The elementary versions use linear regression, so I can cover them here using our competitive equilibrium example from before.\n\nTwo Stage Least Squares (2SLS).\nConsider the market equilibrium example, which contains a cost shock. We can simply run another regression, but there will still be a problem.\n\n\nCode\n# Not exactly right, but at least right sign\nreg2 &lt;- lm(Q~P, data=dat2)\nsummary(reg2)\n## \n## Call:\n## lm(formula = Q ~ P, data = dat2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.70249 -0.14517  0.00104  0.14749  0.74487 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  6.72597    0.17305   38.87   &lt;2e-16 ***\n## P           -0.65177    0.02041  -31.93   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2298 on 598 degrees of freedom\n## Multiple R-squared:  0.6304, Adjusted R-squared:  0.6297 \n## F-statistic:  1020 on 1 and 598 DF,  p-value: &lt; 2.2e-16\n\n\nIt turns out that rhe noisiness of the process within each group affects our OLS estimate: \\(\\widehat{\\beta}_{OLS}=Cov(Q^{*}, P^{*}) / Var(P^{*})\\). For details, see\n\n\nWithin Group Variance\n\nYou can experiment with the effect of different variances on both OLS and IV in the code below. And note that if we had multiple supply shifts and recorded their magnitudes, then we could recover more information about demand, perhaps tracing it out entirely.\n\n\nCode\n# Examine\nEgrid &lt;- expand.grid(Ed_sigma=c(.001, .25, 1), Es_sigma=c(.001, .25, 1))\n\nEgrid_regs &lt;- lapply(1:nrow(Egrid), function(i){\n    Ed_sigma &lt;- Egrid[i,1]\n    Es_sigma &lt;- Egrid[i,2]    \n    EQ1 &lt;- sapply(1:N, function(n){\n        demand &lt;- qd_fun(P, Ed_sigma=Ed_sigma)\n        supply &lt;- qs_fun(P, Es_sigma=Es_sigma)\n        return(eq_fun(demand, supply, P))\n    })\n    EQ2 &lt;- sapply(1:N, function(n){\n        demand &lt;- qd_fun(P,Ed_sigma=Ed_sigma)\n        supply2 &lt;- qs_fun(P, As=-6.5,Es_sigma=Es_sigma)\n        return(eq_fun(demand, supply2, P))\n    })\n    dat &lt;- rbind(\n        data.frame(t(EQ1), cost='1'),\n        data.frame(t(EQ2), cost='2'))\n    return(dat)\n})\nEgrid_OLS &lt;- sapply(Egrid_regs, function(dat) coef( lm(Q~P, data=dat)))\nEgrid_IV &lt;- sapply(Egrid_regs, function(dat) coef( feols(Q~1|P~cost, data=dat)))\n\n#cbind(Egrid, coef_OLS=t(Egrid_OLS)[,2], coef_IV=t(Egrid_IV)[,2])\nlapply( list(Egrid_OLS, Egrid_IV), function(ei){\n    Emat &lt;- matrix(ei[2,],3,3)\n    rownames(Emat) &lt;- paste0('Ed_sigma.',c(.001, .25, 1))\n    colnames(Emat) &lt;- paste0('Es_sigma.',c(.001, .25, 1))\n    return( round(Emat,2))\n})\n\n\n\nTo overcome this issue, we can compute the change in the expected values \\(d \\mathbb{E}[Q^{*}] / d \\mathbb{E}[P^{*}] =-B_{D}\\). Empirically, this is estimated via the change in average value.\n\n\nCode\n# Wald (1940) Estimate\ndat_mean &lt;- rbind(\n    colMeans(dat2[dat2$cost==1,1:2]),\n    colMeans(dat2[dat2$cost==2,1:2]))\ndat_mean\n##             P         Q\n## [1,] 8.886667 0.8732294\n## [2,] 8.045767 1.5426921\nB_est &lt;- diff(dat_mean[,2])/diff(dat_mean[,1])\nround(B_est, 2)\n## [1] -0.8\n\n\nWe can also separately recover \\(d \\mathbb{E}[Q^{*}] / d \\mathbb{E}[A_{S}]\\) and \\(d \\mathbb{E}[P^{*}] / d \\mathbb{E}[A_{S}]\\) from separate regressions.2\n\n\nCode\n# Heckman (2000, p.58) Estimate\nols_1 &lt;- lm(P~cost, data=dat2)\nols_2 &lt;- lm(Q~cost, data=dat2)\nB_est2 &lt;- coef(ols_2)/coef(ols_1)\nround(B_est2[[2]],2)\n## [1] -0.8\n\n\nAlternatively, we can recover the same estimate using an 2SLS regression with two equations: \\[\\begin{eqnarray}\nP &=& \\alpha_{1} + A_{S} \\beta_{1} + \\epsilon_{1} \\\\\nQ &=& \\alpha_{2} + \\hat{P} \\beta_{2} + \\epsilon_{2}.\n\\end{eqnarray}\\] In the first regression, we estimate the average effect of the cost shock on prices. In the second equation, we estimate how the average effect of prices which are exogenous to demand affect quantity demanded. To see this, first substitute the equilibrium condition into the supply equation: \\(Q_{D}=Q_{S}=A_{S}+B_{S} P + E_{S}\\), lets us rewrite \\(P\\) as a function of \\(Q_{D}\\). This yields two theoretical equations \\[\\begin{eqnarray}\n\\label{eqn:linear_supply_iv}\nP &=& -\\frac{A_{S}}{{B_{S}}} + \\frac{Q_{D}}{B_{S}} - \\frac{E_{S}}{B_{S}} \\\\\n\\label{eqn:linear_demand_iv}\nQ_{D} &=&  A_{D} + B_{D} P  + E_{D}.\n\\end{eqnarray}\\]\n\n\nCode\n# Two Stage Least Squares Estimate\nols_1 &lt;- lm(P~cost, data=dat2)\ndat2_new  &lt;- cbind(dat2, Phat=predict(ols_1))\nreg_2sls &lt;- lm(Q~Phat, data=dat2_new)\nsummary(reg_2sls)\n## \n## Call:\n## lm(formula = Q ~ Phat, data = dat2_new)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.46168 -0.11623 -0.00432  0.11137  0.55247 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  7.94814    0.14352   55.38   &lt;2e-16 ***\n## Phat        -0.79613    0.01693  -47.02   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1744 on 598 degrees of freedom\n## Multiple R-squared:  0.7871, Adjusted R-squared:  0.7868 \n## F-statistic:  2211 on 1 and 598 DF,  p-value: &lt; 2.2e-16\n\n# One Stage Instrumental Variables Estimate\nlibrary(fixest)\nreg2_iv &lt;- feols(Q~1|P~cost, data=dat2)\nsummary(reg2_iv)\n## TSLS estimation - Dep. Var.: Q\n##                   Endo.    : P\n##                   Instr.   : cost\n## Second stage: Dep. Var.: Q\n## Observations: 600\n## Standard-errors: IID \n##              Estimate Std. Error  t value  Pr(&gt;|t|)    \n## (Intercept)  7.948139   0.196867  40.3732 &lt; 2.2e-16 ***\n## fit_P       -0.796126   0.023225 -34.2795 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## RMSE: 0.238788   Adj. R2: 0.78676\n## F-test (1st stage), P: stat = 3,068.43, p &lt; 2.2e-16, on 1 and 598 DoF.\n##            Wu-Hausman: stat =   449.01, p &lt; 2.2e-16, on 1 and 597 DoF.\n\n\n\n\nCaveats.\n2SLS regression analysis can be very insightful, but I also want to stress some caveats about their practical application.\nWe always get coefficients back when running feols, and sometimes the computed p-values are very small. The interpretation of those numbers rests on many assumptions:\n\nInstrument exogeneity (Exclusion Restriction): The instrument must affect outcomes only through the treatment variable (e.g., only supply is affected directly, not demand).\nInstrument relevance: The instrument must be strongly correlated with the endogenous regressor, implying the shock creates meaningful variation.\nFunctional form correctness: Supply and demand are assumed linear and additively separable.\nMultiple hypothesis testing risks: We were not repeatedly testing different instruments, which can artificially produce significant findings by chance.\n\nWe are rarely sure that all of these assumptions hold, and this is one reason why researchers often also report their OLS results. But that is insufficient, as spatial and temporal dependence also complicate inference:\n\nExclusion restriction violations: Spatial or temporal spillovers may cause instruments to affect the outcome through unintended channels, undermining instrument exogeneity.\nWeak instruments: Spatial clustering, serial correlation, or network interdependencies can reduce instrument variation, causing weak instruments.\nInference and standard errors: Spatial or temporal interdependence reduces the effective sample size, making conventional standard errors misleadingly small.",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Experimental Data</span>"
    ]
  },
  {
    "objectID": "03_04_ExperimentalData.html#further-reading",
    "href": "03_04_ExperimentalData.html#further-reading",
    "title": "18  Experimental Data",
    "section": "18.4 Further Reading",
    "text": "18.4 Further Reading\nYou are directed to the following resources which discusses endogeneity in more detail and how it applies generally.\n\nCausal Inference for Statistics, Social, and Biomedical Sciences: An Introduction\nhttps://www.mostlyharmlesseconometrics.com/\nhttps://www.econometrics-with-r.org\nhttps://bookdown.org/paul/applied-causal-analysis/\nhttps://mixtape.scunning.com/\nhttps://theeffectbook.net/\nhttps://www.r-causal.org/\nhttps://matheusfacure.github.io/python-causality-handbook/landing-page.html\n\nFor RDD and DID methods in natural experiments, see\n\nhttps://bookdown.org/paul/applied-causal-analysis/rdd-regression-discontinuity-design.html\nhttps://mixtape.scunning.com/06-regression_discontinuity\nhttps://theeffectbook.net/ch-RegressionDiscontinuity.html\nhttps://mixtape.scunning.com/09-difference_in_differences\nhttps://theeffectbook.net/ch-DifferenceinDifference.html\nhttp://www.urfie.net/read/index.html#page/226\n\nFor IV methods in natural experiments, see\n\nhttps://cameron.econ.ucdavis.edu/e240a/ch04iv.pdf\nhttps://mru.org/courses/mastering-econometrics/introduction-instrumental-variables-part-one\nhttps://www.econometrics-with-r.org/12-ivr.html\nhttps://bookdown.org/paul/applied-causal-analysis/estimation-2.html\nhttps://mixtape.scunning.com/07-instrumental_variables\nhttps://theeffectbook.net/ch-InstrumentalVariables.html\nhttp://www.urfie.net/read/index.html#page/247",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Experimental Data</span>"
    ]
  },
  {
    "objectID": "03_04_ExperimentalData.html#footnotes",
    "href": "03_04_ExperimentalData.html#footnotes",
    "title": "18  Experimental Data",
    "section": "",
    "text": "Notice that even in this linear model, however, all effects are conditional: The effect of a cost change on quantity or price depends on the demand curve. A change in costs affects quantity supplied but not quantity demanded (which then affects equilibrium price) but the demand side of the market still matters! The change in price from a change in costs depends on the elasticity of demand.↩︎\nMathematically, we can also do this in a single step by exploiting linear algebra: \\(\\frac{\\frac{ Cov(Q^{*},A_{S})}{ V(A_{S}) } }{\\frac{ Cov(P^{*},A_{S})}{ V(A_{S}) }} = \\frac{Cov(Q^{*},A_{S} )}{ Cov(P^{*},A_{S})}.\\)↩︎",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Experimental Data</span>"
    ]
  },
  {
    "objectID": "03_05_DataScientism.html",
    "href": "03_05_DataScientism.html",
    "title": "19  Data Scientism",
    "section": "",
    "text": "19.1 False Positives\nIn practice, it is hard to find a good natural experiment. For example, suppose we asked “what is the effect of wages on police demanded?” and examined a policy which lowered the educational requirements from 4 years to 2 to become an officer. This increases the labour supply, but it also affects the demand curve through “general equilibrium”: as some of the new officers were potentially criminals and, with fewer criminals, the demand for police shifts down.\nIn practice, it is also easy to find a bad instrument. Paradoxically, natural experiments are something you are supposed to find but never search for. As you search for good instruments, for example, sometimes random noise will appear like a good instrument (spurious instruments). In this age of big data, we are getting increasingly more data and, perhaps surprisingly, this makes it easier to make false discoveries.\nWe will consider three classical ways for false discoveries to arise. After that, there are examples with the latest and greatest empirical recipes—we don’t have so many theoretical results yet but I think you can understand the issue with the numerical example. Although it is difficult to express numerically, you must also know that if you search for a good natural experiment for too long, you can also be led astray from important questions. There are good reasons to be excited about empirical social science, but we would be wise to recall some earlier wisdom from economists on the matter.",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Scientism</span>"
    ]
  },
  {
    "objectID": "03_05_DataScientism.html#false-positives",
    "href": "03_05_DataScientism.html#false-positives",
    "title": "19  Data Scientism",
    "section": "",
    "text": "Data Errors.\nA huge amount of data normally means a huge amount of data cleaning/merging/aggregating. This avoids many copy-paste errors, which are a recipe for disaster, but may also introduce other types of errors. Some spurious results are driven by honest errors in data cleaning. According to one estimate, this is responsible for around one fifth of all medical science retractions (there is even a whole book about this!). Although there are not similar meta-analysis in economics, there are some high-profile examples. This includes papers that are highly influential, like Lott, Levitt and Reinhart and Rogoff as well as others the top economics journals, like the RESTUD and AER. There are some reasons to think such errors are more widespread across the social sciences; e.g., in Census data and Aid data. So be careful!\nNote: one reason to plot your data is to help spot such errors.\n\n\nP-Hacking.\nAnother class of errors pertains to P-hacking (and it’s various synonyms: data drudging, star mining,….). While there are cases of fraudulent data manipulation (which can be considered as a dishonest data error), P-hacking need not even be intentional. You can simply be trying different variable transformations to uncover patterns in the data, for example, without accounting for how easy it is to find patterns when transforming completely random data. P-hacking is pernicious and widespread.\n\n\nCode\n# P-hacking OSLS with different explanatory vars\nset.seed(123)\nn &lt;- 50\nX1 &lt;- runif(n)\n\n# Regression Machine:\n# repeatedly finds covariate, runs regression\n# stops when statistically significant at .1%\np &lt;- 1\ni &lt;- 0\nwhile(p &gt;= .001){ \n    # Get Random Covariate\n    X2 &lt;-  runif(n)\n    # Merge and `Analyze'\n    dat_i &lt;- data.frame(X1,X2)\n    reg_i &lt;- lm(X1~X2, data=dat_i)\n    # update results in global environment\n    p &lt;- summary(reg_i)$coefficients[2,4]\n    i &lt;- i+1\n}\n#summary(reg_i)\n\nplot(X1~X2, data=dat_i,\n    pch=16, col=grey(0,.5), font.main=1,\n    main=paste0('Random Dataset ', i,\":   p=\",\n        formatC(p,digits=2, format='fg')))\nabline(reg_i)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# P-hacking 2SLS with different explanatory vars\n# and different instrumental vars\nlibrary(fixest)\np &lt;- 1\nii &lt;- 0\nset.seed(123)\nwhile(p &gt;= .05){\n    # Get Random Covariates\n    X2 &lt;-  runif(n)    \n    X3 &lt;-  runif(n)\n    # Create Treatment Variable based on Cutoff\n    cutoffs &lt;- seq(0,1,length.out=11)[-c(1,11)]\n    for(tau in cutoffs){\n        T3 &lt;- 1*(X3 &gt; tau)\n        # Merge and `Analyze'\n        dat_i &lt;- data.frame(X1,X2,T3)\n        ivreg_i &lt;- feols(X1~1|X2~T3, data=dat_i)\n        # Update results in global environment\n        ptab &lt;- summary(ivreg_i)$coeftable\n        if( nrow(ptab)==2){\n            p &lt;- ptab[2,4]\n            ii &lt;- ii+1\n        }\n    }\n}\nsummary(ivreg_i)\n## TSLS estimation - Dep. Var.: X1\n##                   Endo.    : X2\n##                   Instr.   : T3\n## Second stage: Dep. Var.: X1\n## Observations: 50\n## Standard-errors: IID \n##              Estimate Std. Error   t value  Pr(&gt;|t|)    \n## (Intercept) -9.95e-14      1e-06 -9.95e-08         1    \n## fit_X2       1.00e+00      1e-06  1.00e+06 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## RMSE: 5.81e-14   Adj. R2: -0.006886\n## F-test (1st stage), X2: stat = 0.66488, p = 0.418869, on 1 and 48 DoF.\n##             Wu-Hausman: stat = 0.23218, p = 0.632145, on 1 and 47 DoF.",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Scientism</span>"
    ]
  },
  {
    "objectID": "03_05_DataScientism.html#spurious-regression",
    "href": "03_05_DataScientism.html#spurious-regression",
    "title": "19  Data Scientism",
    "section": "19.2 Spurious Regression",
    "text": "19.2 Spurious Regression\nEven without any coding errors or p-hacking, you can sometimes make a false discovery. We begin with a motivating empirical example of “US Gov’t Spending on Science”.\nFirst, get and inspect some data from https://tylervigen.com/spurious-correlations\n\n\nCode\n# Your data is not made up in the computer (hopefully!)\nvigen_csv &lt;- read.csv( paste0(\n'https://raw.githubusercontent.com/the-mad-statter/',\n'whysospurious/master/data-raw/tylervigen.csv') ) \nclass(vigen_csv)\n## [1] \"data.frame\"\nnames(vigen_csv)\n##  [1] \"year\"                         \"science_spending\"            \n##  [3] \"hanging_suicides\"             \"pool_fall_drownings\"         \n##  [5] \"cage_films\"                   \"cheese_percap\"               \n##  [7] \"bed_deaths\"                   \"maine_divorce_rate\"          \n##  [9] \"margarine_percap\"             \"miss_usa_age\"                \n## [11] \"steam_murders\"                \"arcade_revenue\"              \n## [13] \"computer_science_doctorates\"  \"noncom_space_launches\"       \n## [15] \"sociology_doctorates\"         \"mozzarella_percap\"           \n## [17] \"civil_engineering_doctorates\" \"fishing_drownings\"           \n## [19] \"kentucky_marriage_rate\"       \"oil_imports_norway\"          \n## [21] \"chicken_percap\"               \"train_collision_deaths\"      \n## [23] \"oil_imports_total\"            \"pool_drownings\"              \n## [25] \"nuclear_power\"                \"japanese_cars_sold\"          \n## [27] \"motor_vehicle_suicides\"       \"spelling_bee_word_length\"    \n## [29] \"spider_deaths\"                \"math_doctorates\"             \n## [31] \"uranium\"\nvigen_csv[1:5,1:5]\n##   year science_spending hanging_suicides pool_fall_drownings cage_films\n## 1 1996               NA               NA                  NA         NA\n## 2 1997               NA               NA                  NA         NA\n## 3 1998               NA               NA                  NA         NA\n## 4 1999            18079             5427                 109          2\n## 5 2000            18594             5688                 102          2\n\n\nExamine some data\n\n\nCode\npar(mfrow=c(1,2), mar=c(2,2,2,1))\nplot.new()\nplot.window(xlim=c(1999, 2009), ylim=c(5,9)*1000)\nlines(science_spending/3~year, data=vigen_csv, lty=1, col=2, pch=16)\ntext(2003, 8200, 'US spending on science, space, technology (USD/3)', col=2, cex=.6, srt=30)\nlines(hanging_suicides~year, data=vigen_csv, lty=1, col=4, pch=16)\ntext(2004, 6500, 'US Suicides by hanging, strangulation, suffocation (Deaths)', col=4, cex=.6, srt=30)\naxis(1)\naxis(2)\n\n\nplot.new()\nplot.window(xlim=c(2002, 2009), ylim=c(0,5))\nlines(cage_films~year, data=vigen_csv[vigen_csv$year&gt;=2002,], lty=1, col=2, pch=16)\ntext(2006, 0.5, 'Number of films with Nicolas Cage (Films)', col=2, cex=.6, srt=0)\nlines(pool_fall_drownings/25~year, data=vigen_csv[vigen_csv$year&gt;=2002,], lty=1, col=4, pch=16)\ntext(2006, 4.5, 'Number of drownings by falling into pool (US Deaths/25)', col=4, cex=.6, srt=0)\naxis(1)\naxis(2)\n\n\n\n\n\n\n\n\n\n\nCode\n# Include an intercept to regression 1\n#reg2 &lt;-  lm(cage_films ~ science_spending, data=vigen_csv)\n#suppressMessages(library(stargazer))\n#stargazer(reg1, reg2, type='html')\n\n\nAnother Example.\nThe US government spending on science is ruining cinema (p&lt;.001)!?\n\n\nCode\n# Drop Data before 1999\nvigen_csv &lt;- vigen_csv[vigen_csv$year &gt;= 1999,] \n\n# Run OLS Regression\nreg1 &lt;-  lm(cage_films ~ -1 + science_spending, data=vigen_csv)\nsummary(reg1)\n## \n## Call:\n## lm(formula = cage_films ~ -1 + science_spending, data = vigen_csv)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.7670 -0.7165  0.1447  0.7890  1.4531 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(&gt;|t|)    \n## science_spending 9.978e-05  1.350e-05    7.39 2.34e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.033 on 10 degrees of freedom\n##   (1 observation deleted due to missingness)\n## Multiple R-squared:  0.8452, Adjusted R-squared:  0.8297 \n## F-statistic: 54.61 on 1 and 10 DF,  p-value: 2.343e-05\n\n\nIt’s not all bad, because people in Maine stay married longer?\n\n\nCode\nplot.new()\nplot.window(xlim=c(1999, 2009), ylim=c(7,9))\nlines(log(maine_divorce_rate*1000)~year, data=vigen_csv)\nlines(log(science_spending/10)~year, data=vigen_csv, lty=2)\naxis(1)\naxis(2)\nlegend('topright', lty=c(1,2), legend=c(\n    'log(maine_divorce_rate*1000)',\n    'log(science_spending/10)'))\n\n\n\n\n\n\n\n\n\nFor more intuition on spurious correlations, try http://shiny.calpoly.sh/Corr_Reg_Game/ The same principles apply to more sophisticated methods.",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Scientism</span>"
    ]
  },
  {
    "objectID": "03_05_DataScientism.html#spurious-causal-impacts",
    "href": "03_05_DataScientism.html#spurious-causal-impacts",
    "title": "19  Data Scientism",
    "section": "19.3 Spurious Causal Impacts",
    "text": "19.3 Spurious Causal Impacts\nIn practice, it is hard to find “good” natural experiments. For example, suppose we asked “what is the effect of wages on police demanded?” and examined a policy which lowered the educational requirements from 4 years to 2 to become an officer. This increases the labour supply, but it also affects the demand curve through “general equilibrium”: as some of the new officers were potentially criminals. With fewer criminals, the demand for likely police shifts down.\nIn practice, it is also surprisingly easy to find “bad” natural experiments. Paradoxically, natural experiments are something you are supposed to find but never search for. As you search for good instruments, for example, sometimes random noise will appear like a good instrument (Spurious instruments). Worse, if you search for a good instrument for too long, you can also be led astray from important questions.\n\nExample: Vigen IV’s.\nWe now run IV regressions for different variable combinations in the dataset of spurious relationships\n\n\nCode\nknames &lt;- names(vigen_csv)[2:11] # First 10 Variables\n#knames &lt;- names(vigen_csv)[-1] # Try All Variables\np &lt;- 1\nii &lt;- 1\nivreg_list &lt;- vector(\"list\", factorial(length(knames))/factorial(length(knames)-3))\n\n# Choose 3 variable\nfor( k1 in knames){\nfor( k2 in setdiff(knames,k1)){\nfor( k3 in setdiff(knames,c(k1,k2)) ){   \n    X1 &lt;- vigen_csv[,k1]\n    X2 &lt;- vigen_csv[,k2]\n    X3 &lt;- vigen_csv[,k3]\n    # Merge and `Analyze'        \n    dat_i &lt;- na.omit(data.frame(X1,X2,X3))\n    ivreg_i &lt;- feols(X1~1|X2~X3, data=dat_i)\n    ivreg_list[[ii]] &lt;- list(ivreg_i, c(k1,k2,k3))\n    ii &lt;- ii+1\n}}}\npvals &lt;- sapply(ivreg_list, function(ivreg_i){ivreg_i[[1]]$coeftable[2,4]})\n\nplot(ecdf(pvals), xlab='p-value', ylab='CDF', font.main=1,\n    main='Frequency IV is Statistically Significant')\nabline(v=c(.01,.05), col=c(2,4))\n\n\n\n\n\n\n\n\n\nCode\n\n# Most Significant Spurious Combinations\npvars &lt;- sapply(ivreg_list, function(ivreg_i){ivreg_i[[2]]})\npdat &lt;- data.frame(t(pvars), pvals)\npdat &lt;- pdat[order(pdat$pvals),]\nhead(pdat)\n##                     X1                 X2            X3        pvals\n## 4     science_spending   hanging_suicides    bed_deaths 3.049883e-08\n## 76    hanging_suicides   science_spending    bed_deaths 3.049883e-08\n## 3     science_spending   hanging_suicides cheese_percap 3.344890e-08\n## 75    hanging_suicides   science_spending cheese_percap 3.344890e-08\n## 485 maine_divorce_rate   margarine_percap cheese_percap 3.997738e-08\n## 557   margarine_percap maine_divorce_rate cheese_percap 3.997738e-08\n\n\n\n\nSimulation Study.\nWe apply the three major credible methods (IV, RDD, DID) to random walks. Each time, we find a result that fits mold and add various extensions that make it appear robust. One could tell a story about how \\(X_{2}\\) affects \\(X_{1}\\) but \\(X_{1}\\) might also affect \\(X_{2}\\), and how they discovered an instrument \\(X_{3}\\) to provide the first causal estimate of \\(X_{2}\\) on \\(X_{1}\\). The analysis looks scientific and the story sounds plausible, so you could probably be convinced if it were not just random noise.\n\n\nCode\nn &lt;- 1000\nn_index &lt;- seq(n)\n\nset.seed(1)\nrandom_walk1 &lt;- cumsum(runif(n,-1,1))\n\nset.seed(2)\nrandom_walk2 &lt;- cumsum(runif(n,-1,1))\n\npar(mfrow=c(1,2))\nplot(random_walk1, pch=16, col=rgb(1,0,0,.25),\n    xlab='Time', ylab='Random Value')\nplot(random_walk2, pch=16, col=rgb(0,0,1,.25),\n    xlab='Time', ylab='Random Value')\n\n\n\n\n\n\n\n\n\nIV. First, find an instrument that satisfy various statistical criterion to provide a causal estimate of \\(X_{2}\\) on \\(X_{1}\\).\n\n\nCode\n# \"Find\" \"valid\" ingredients\nlibrary(fixest)\nrandom_walk3 &lt;- cumsum(runif(n,-1,1))\ndat_i &lt;- data.frame(\n    X1=random_walk1,\n    X2=random_walk2,\n    X3=random_walk3)\nivreg_i &lt;- feols(X1~1|X2~X3, data=dat_i)\nsummary(ivreg_i)\n## TSLS estimation - Dep. Var.: X1\n##                   Endo.    : X2\n##                   Instr.   : X3\n## Second stage: Dep. Var.: X1\n## Observations: 1,000\n## Standard-errors: IID \n##             Estimate Std. Error t value   Pr(&gt;|t|)    \n## (Intercept)  8.53309   1.644285 5.18954 2.5533e-07 ***\n## fit_X2       1.79901   0.472285 3.80916 1.4796e-04 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## RMSE: 6.25733   Adj. R2: 0.032314\n## F-test (1st stage), X2: stat = 10.804, p = 0.001048, on 1 and 998 DoF.\n##             Wu-Hausman: stat = 23.407, p = 1.518e-6, on 1 and 997 DoF.\n\n# After experimenting with different instruments\n# you can find even stronger results!\n\n\nRDD. Second, find a large discrete change in the data that you can associate with a policy. You can use this as an instrument too, also providing a causal estimate of \\(X_{2}\\) on \\(X_{1}\\).\n\n\nCode\n# Let the data take shape\n# (around the large differences before and after)\nn1 &lt;- 290\nwind1 &lt;- c(n1-300,n1+300)\ndat1 &lt;- data.frame(t=n_index, y=random_walk1, d=1*(n_index &gt; n1))\ndat1_sub &lt;- dat1[ n_index&gt;wind1[1] & n_index &lt; wind1[2],]\n\n# Then find your big break\nreg0 &lt;- lm(y~t, data=dat1_sub[dat1_sub$d==0,])\nreg1 &lt;- lm(y~t, data=dat1_sub[dat1_sub$d==1,])\n\n# The evidence should show openly (it's just science)\nplot(random_walk1, pch=16, col=rgb(0,0,1,.25),\n    xlim=wind1, xlab='Time', ylab='Random Value')\nabline(v=n1, lty=2)\nlines(reg0$model$t, reg0$fitted.values, col=1)\nlines(reg1$model$t, reg1$fitted.values, col=1)\n\n\n\n\n\n\n\n\n\n\nCode\n# Dress with some statistics for added credibility\nrdd_sub &lt;- lm(y~d+t+d*t, data=dat1_sub)\nrdd_full &lt;- lm(y~d+t+d*t, data=dat1)\nstargazer::stargazer(rdd_sub, rdd_full, \n    type='html',\n    title='Recipe RDD',\n    header=F,\n    omit=c('Constant'),\n    notes=c('First column uses a dataset around the discontinuity.',\n    'Smaller windows are more causal, and where the effect is bigger.'))\n\n\n\nRecipe RDD\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ny\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nd\n\n\n-13.169***\n\n\n-9.639***\n\n\n\n\n\n\n(0.569)\n\n\n(0.527)\n\n\n\n\n\n\n\n\n\n\n\n\nt\n\n\n0.011***\n\n\n0.011***\n\n\n\n\n\n\n(0.001)\n\n\n(0.002)\n\n\n\n\n\n\n\n\n\n\n\n\nd:t\n\n\n0.009***\n\n\n0.004*\n\n\n\n\n\n\n(0.002)\n\n\n(0.002)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n589\n\n\n1,000\n\n\n\n\nR2\n\n\n0.771\n\n\n0.447\n\n\n\n\nAdjusted R2\n\n\n0.770\n\n\n0.446\n\n\n\n\nResidual Std. Error\n\n\n1.764 (df = 585)\n\n\n3.081 (df = 996)\n\n\n\n\nF Statistic\n\n\n658.281*** (df = 3; 585)\n\n\n268.763*** (df = 3; 996)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\n\nFirst column uses a dataset around the discontinuity.\n\n\n\n\n\n\nSmaller windows are more causal, and where the effect is bigger.\n\n\n\nDID. Third, find a change in the data that you can associate with a policy where the control group has parallel trends. This also provides a causal estimate of \\(X_{2}\\) on \\(X_{1}\\).\n\n\nCode\n# Find a reversal of fortune\n# (A good story always goes well with a nice pre-trend)\nn2 &lt;- 318\nwind2 &lt;- c(n2-20,n2+20)\nplot(random_walk2, pch=16, col=rgb(0,0,1,.5),\n    xlim=wind2, ylim=c(-15,15), xlab='Time', ylab='Random Value')\npoints(random_walk1, pch=16, col=rgb(1,0,0,.5))\nabline(v=n2, lty=2)\n\n\n\n\n\n\n\n\n\n\nCode\n# Knead out any effects that are non-causal (aka correlation)\ndat2A &lt;- data.frame(t=n_index, y=random_walk1, d=1*(n_index &gt; n2), RWid=1)\ndat2B &lt;- data.frame(t=n_index, y=random_walk2, d=0, RWid=2)\ndat2  &lt;- rbind(dat2A, dat2B)\ndat2$RWid &lt;- as.factor(dat2$RWid)\ndat2$tid &lt;- as.factor(dat2$t)\ndat2_sub &lt;- dat2[ dat2$t&gt;wind2[1] & dat2$t &lt; wind2[2],]\n\n# Report the stars for all to enjoy\n# (what about the intercept?)\n# (stable coefficients are the good ones?)\ndid_fe1 &lt;- lm(y~d+tid, data=dat2_sub)\ndid_fe2 &lt;- lm(y~d+RWid, data=dat2_sub)\ndid_fe3 &lt;- lm(y~d*RWid+tid, data=dat2_sub)\nstargazer::stargazer(did_fe1, did_fe2, did_fe3,\n    type='html',\n    title='Recipe DID',\n    header=F,\n    omit=c('tid','RWid', 'Constant'),\n    notes=c(\n     'Fixed effects for time in column 1, for id in column 2, and both in column 3.',\n     'Fixed effects control for most of your concerns.',\n     'Anything else creates a bias in the opposite direction.'))\n\n\n\nRecipe DID\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ny\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n\n\n\n\n\n\nd\n\n\n1.804*\n\n\n1.847***\n\n\n5.851***\n\n\n\n\n\n\n(0.892)\n\n\n(0.652)\n\n\n(0.828)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n78\n\n\n78\n\n\n78\n\n\n\n\nR2\n\n\n0.227\n\n\n0.164\n\n\n0.668\n\n\n\n\nAdjusted R2\n\n\n-0.566\n\n\n0.142\n\n\n0.309\n\n\n\n\nResidual Std. Error\n\n\n2.750 (df = 38)\n\n\n2.035 (df = 75)\n\n\n1.827 (df = 37)\n\n\n\n\nF Statistic\n\n\n0.287 (df = 39; 38)\n\n\n7.379*** (df = 2; 75)\n\n\n1.860** (df = 40; 37)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\n\nFixed effects for time in column 1, for id in column 2, and both in column 3.\n\n\n\n\n\n\nFixed effects control for most of your concerns.\n\n\n\n\n\n\nAnything else creates a bias in the opposite direction.",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Scientism</span>"
    ]
  },
  {
    "objectID": "03_10_MiscTopics.html",
    "href": "03_10_MiscTopics.html",
    "title": "20  Misc. Multivariate Topics",
    "section": "",
    "text": "Filtering.\nIn some cases, we may want to smooth time series data instead of predict into the future. We can distinguish two types of smoothing, incorporating future observations or not. When only weighting two other observations, the differences can be expressed as trying to estimate the average with different available data:\n\nFiltering: \\(\\mathbb{E}[Y_{t} | X_{t-1}, X_{t-2}]\\)\nSmoothing: \\(\\mathbb{E}[Y_{t} | X_{t-1}, X_{t+1}]\\)\n\nOne example of filtering is Exponential Filtering (sometimes confusingly referred to as “Exponential Smoothing”) which weights only previous observations using an exponential kernel.\n\n\nCode\n##################\n# Time series data\n##################\n\nset.seed(1)\n## Underlying Trend\nx0 &lt;- cumsum(rnorm(500,0,1))\n## Observed Datapoints\nx &lt;- x0 + runif(length(x0),-10,10)\ndat &lt;- data.frame(t=seq(x), x0=x0, x=x)\n\n## Asymmetric Kernel\n#bw &lt;- c(2/3,1/3)\n#s1 &lt;- filter(x, bw/sum(bw), sides=1)\n\n## Symmetric Kernel\n#bw &lt;- c(1/6,2/3,1/6)\n#s2 &lt;- filter(x,  bw/sum(bw), sides=2)\n\n\nThere are several cross-validation procedures for filtering time series data . One is called time series cross-validation (TSCV), which is useful for temporally dependent data .\n\n\nCode\n## Plot Simulated Data\nx &lt;- dat$x\nx0 &lt;- dat$x0\npar(fig = c(0,1,0,1), new=F)\nplot(x, pch=16, col=grey(0,.25))\nlines(x0, col=1, lty=1, lwd=2)\n\n## Work with differenced data?\n#n       &lt;- length(Yt)\n#plot(Yt[1:(n-1)], Yt[2:n],\n#    xlab='d Y (t-1)', ylab='d Y (t)', \n#    col=grey(0,.5), pch=16)\n#Yt &lt;- diff(Yt)\n\n## TSCV One Sided Moving Average\nfilter_bws &lt;- seq(1,20,by=1)\nfilter_mape_bws &lt;- sapply(filter_bws, function(h){\n    bw &lt;- c(0,rep(1/h,h)) ## Leave current one out\n    s2 &lt;- filter(x, bw, sides=1)\n    pe &lt;- s2 - x\n    mape &lt;- mean( abs(pe)^2, na.rm=T)\n})\nfilter_mape_star &lt;- filter_mape_bws[which.min(filter_mape_bws)]\nfilter_h_star &lt;- filter_bws[which.min(filter_mape_bws)]\nfilter_tscv &lt;- filter(x,  c(rep(1/filter_h_star,filter_h_star)), sides=1)\n# Plot Optimization Results\n#par(fig = c(0.07, 0.35, 0.07, 0.35), new=T) \n#plot(filter_bws, filter_mape_bws, type='o', ylab='mape', pch=16)\n#points(filter_h_star, filter_mape_star, pch=19, col=2, cex=1.5)\n\n## TSCV for LLLS\nlibrary(np)\nllls_bws &lt;- seq(8,28,by=1)\nllls_burnin &lt;- 10\nllls_mape_bws &lt;- sapply(llls_bws, function(h){ # cat(h,'\\n')\n    pe &lt;- sapply(llls_burnin:nrow(dat), function(t_end){\n        dat_t &lt;- dat[dat$t&lt;t_end, ]\n        reg &lt;- npreg(x~t, data=dat_t, bws=h,\n            ckertype='epanechnikov',\n            bandwidth.compute=F, regtype='ll')\n        edat &lt;- dat[dat$t==t_end,]\n        pred &lt;- predict(reg, newdata=edat)\n        pe &lt;- edat$x - pred\n        return(pe)    \n    })\n    mape &lt;- mean( abs(pe)^2, na.rm=T)\n})\nllls_mape_star &lt;- llls_mape_bws[which.min(llls_mape_bws)]\nllls_h_star &lt;- llls_bws[which.min(llls_mape_bws)]\n#llls_tscv &lt;- predict( npreg(x~t, data=dat, bws=llls_h_star,\n#    bandwidth.compute=F, regtype='ll', ckertype='epanechnikov'))\nllls_tscv &lt;- sapply(llls_burnin:nrow(dat), function(t_end, h=llls_h_star){\n    dat_t &lt;- dat[dat$t&lt;t_end, ]\n    reg &lt;- npreg(x~t, data=dat_t, bws=h,\n        ckertype='epanechnikov',\n        bandwidth.compute=F, regtype='ll')\n    edat &lt;- dat[dat$t==t_end,]\n    pred &lt;- predict(reg, newdata=edat)\n    return(pred)    \n})\n\n## Compare Fits Qualitatively\nlines(filter_tscv, col=2, lty=1, lwd=1)\nlines(llls_burnin:nrow(dat), llls_tscv, col=4, lty=1, lwd=1)\nlegend('topleft', lty=1, col=c(1,2,4), bty='n',\n    c('Underlying Trend', 'MA-1sided + TSCV', 'LLLS-1sided + TSCV'))\n\n## Compare Fits Quantitatively\ncbind(\n    bandwidth=c(LLLS=llls_h_star, MA=filter_h_star),\n    mape=round(c(LLLS=llls_mape_star, MA=filter_mape_star),2) )\n\n## See also https://cran.r-project.org/web/packages/smoots/smoots.pdf\n#https://otexts.com/fpp3/tscv.html\n#https://robjhyndman.com/hyndsight/tscvexample/",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Misc. Multivariate Topics</span>"
    ]
  }
]