[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introductory Economic Statistics: A Data-Driven Approach in R",
    "section": "",
    "text": "Preface\nThis Rbook introduces students to econometrics without parametric assumptions and formulas. In many ways, it is a modern version of “Introductory Econometrics: Using Monte Carlo Simulation with Microsoft Excel” by Barreto and Howland, updated to adhere to modern statistics teaching guidelines and give econometrics students the best tools for their labor market. Altogether, students learn to produce statistical analyses of economic data relevant to both the private and public sector, as well as an intuitive foundation for more advanced courses on nonparametric statistics or structural econometrics. This Rbook is organized into two parts.\nPart I: Introduction to Data Analysis introduces students to the basics of programming and statistical analysis of economic data using R. There are many practical examples, including on how to analyze data interactively and communicate results. I aimed to replace mathematical proofs with simulations whenever possible. We also cover statistical reporting using R + markdown, which research suggests is a good combination 1 2.\nPart II: Introduction to Linear Regression refines material from several introductory econometrics textbooks and covers linear models only from a “minimum distance” perspective. (We operate under the maxim “All models are wrong” and do not prove unbiasedness.) Also included is a novel chapter on “Data scientism” that more clearly illustrates the ways that simplistic approaches can mislead rather than illuminate. (I stress “gun safety” instead of “pull to shoot”, which I feel is missing from many textbooks.) Overall, there is a more humble view towards what we can infer from linear regressions that opens the door towards more advanced courses in model development and interpretation.\n\nAlthough any interested reader may find it useful, this Rbook is primarily developed for my students.\nIf you use this Rbook, please cite\n@book{Adamson2025_Rbook,\n  title={Introductory Economic Statistics: A Data-Driven Approach using R},\n  author={Adamson, Jordan},\n  year={2025},\n  publisher={Bookdown},\n  url={https://jadamso.github.io/Rbooks/}\n}\nPlease also report any errors or issues at https://github.com/Jadamso/Rbooks/issues.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-00-BasicStats.html",
    "href": "01-00-BasicStats.html",
    "title": "Introduction to Data Analysis",
    "section": "",
    "text": "This section introduces basic statistics from a computational rather than mathematical approach. We will use some basic probability theory, which you can review at the high-school level if this is not familiar to you. We will also use R statistical software, which you will be introduced to in the first chapter.",
    "crumbs": [
      "Introduction to Data Analysis"
    ]
  },
  {
    "objectID": "01_01_FirstSteps.html",
    "href": "01_01_FirstSteps.html",
    "title": "1  First Steps",
    "section": "",
    "text": "1.1 Why Program in R?\nYou should program your statistical analysis, and we will cover some of the basics of how to do this in R. You also want your work to be replicable\nYou can read more about the distinction in many places, including\nWe focus on R because it is good for complex stats, concise figures, and coherent organization. It is built and developed by applied statisticians for statistics, and used by many in academia and industry. For students, think about labor demand and what may be good for getting a job. Do some of your own research to best understand how much to invest.\nMy main sell to you is that being reproducible is in your own self-interest.",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First Steps</span>"
    ]
  },
  {
    "objectID": "01_01_FirstSteps.html#why-program-in-r",
    "href": "01_01_FirstSteps.html#why-program-in-r",
    "title": "1  First Steps",
    "section": "",
    "text": "Replicable: someone collecting new data comes to the same results.\nReproducibile: someone reusing your data comes to the same results.\n\n\n\nhttps://www.annualreviews.org/doi/10.1146/annurev-psych-020821-114157\nhttps://nceas.github.io/sasap-training/materials/reproducible_research_in_r_fairbanks/\n\n\n\n\nAn example workflow.\nFirst Steps…\nStep 1: Some ideas and data about how variable \\(X_{1}\\) affects variable \\(Y_{1}\\), which we denote as \\(X_{1}\\to Y_{1}\\)\n\nYou copy some data into a spreadsheet, manually aggregate\ndo some calculations and tables the same spreadsheet\nsome other analysis from here and there, using this software and that.\n\nStep 2: Pursuing the lead for a week or two\n\nyou extend your dataset with more observations\ncopy in a spreadsheet data, manually aggregate\ndo some more calculations and tables, same as before\n\nA Little Way Down the Road …\n1 month later: someone asks about another factor: \\(X_{2}\\)\n\nyou download some other type of data\nYou repeat Step 2 with some data on \\(X_{2}\\).\nThe details from your “point and click” method are a bit fuzzy.\nIt takes a little time, but you successfully redo the analysis.\n\n4 months later: someone asks about another factor \\(X_{3}\\to Y_{1}\\)\n\nYou again repeat Step 2 with some data on \\(X_{3}\\).\nYou’re pretty sure none of tables your tried messed up the order of the rows or columns.\nIt takes more time and effort. The data processing was not transparent, but you eventually redo the analysis.\n\n6 months later: you want to explore another outcome \\(X_{2} \\to Y_{2}\\).\n\nYou found out Excel had some bugs in it’s statistical calculations (see e.g., https://biostat.app.vumc.org/wiki/pub/Main/TheresaScott/StatsInExcel.TAScot.handout.pdf). You now use a new version of the spreadsheet\nYou’re not sure you merged everything correctly. After much time and effort, most (but not all) of the numbers match exactly.\n\n2 years later: your boss wants you to replicate \\(\\{ X_{1}, X_{2}, X_{3} \\} \\to Y_{1}\\)\n\nA rival has proposed something new. Their idea doesn’t actually make any sense, but their figures and statistics look better.\nYou don’t even use that computer anymore and a collaborator who handled the data on \\(X_{2}\\) has moved on.\n\n\n\nAn alternative workflow.\nSuppose you decided to code what you did beginning with Step 2.\nIt does not take much time to update or replicate your results.\n\nYour computer runs for 2 hours and reproduces the figures and tables.\nYou also rewrote your big calculations to use multiple cores, this took two hours to do but saved 6 hours each time you rerun your code.\nYou add some more data. It adds almost no time to see whether much has changed.\n\nYour results are transparent and easier to build on.\n\nYou see the exact steps you took and found an error\n\nGoogle “worst Excel errors” and note the frequency they arise from copy/paste via the “point-and-click” approach. E.g., Fidelity’s $2.6 Billion Dividend Error.\n\nGlad you found a problem before sending your research out! See https://retractionwatch.com/ and https://econjwatch.org/. Future economists should also read https://core.ac.uk/download/pdf/300464894.pdf.\n\n\nYou try out a new plot you found in The Visual Display of Quantitative Information, by Edward Tufte.\n\nIt’s not a standard plot, but google answers most of your questions.\nTutorials help avoid bad practices, such as plotting 2D data as a 3D object (see e.g., https://clauswilke.com/dataviz/no-3d.html).\n\nYou try out an obscure statistical approach that’s hot in your field.\n\nit doesn’t make the report, but you have some confidence that candidate issue isn’t a big problem",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First Steps</span>"
    ]
  },
  {
    "objectID": "01_01_FirstSteps.html#first-steps",
    "href": "01_01_FirstSteps.html#first-steps",
    "title": "1  First Steps",
    "section": "1.2 First Steps",
    "text": "1.2 First Steps\n\nInstall R.\nFirst Install R. Then Install Rstudio.\nFor help setting up, see any of the following links\n\nhttps://learnr-examples.shinyapps.io/ex-setup-r/\nhttps://rstudio-education.github.io/hopr/starting.html\nhttps://a-little-book-of-r-for-bioinformatics.readthedocs.io/en/latest/src/installr.html\nhttps://cran.r-project.org/doc/manuals/R-admin.html\nhttps://courses.edx.org/courses/UTAustinX/UT.7.01x/3T2014/56c5437b88fa43cf828bff5371c6a924/\nhttps://owi.usgs.gov/R/training-curriculum/installr/\nhttps://www.earthdatascience.org/courses/earth-analytics/document-your-science/setup-r-rstudio/\n\nFor Fedora users, note that you need to first enable the repo and then install\n\n\nCode\nsudo dnf install 'dnf-command(copr)'\nsudo dnf copr enable iucar/rstudio\nsudo dnf install rstudio-desktop\n\n\nMake sure you have the latest version of R and Rstudio for class. If not, then reinstall.\n\n\nInterfacing with R.\nRstudio is perhaps the easiest to get going with. (There are other GUI’s.)\nIn Rstudio, there are 4 panes. (If you do not see 4, click “file &gt; new file &gt; R script” on the top left of the toolbar.)\n\n\n\n\n\n\n\n\n\nThe top left pane is where you write your code. For example, type\n\n1+1\n\nThe pane below is where your code is executed. Keep you mouse on the same line as your code, and then click “Run”. You should see\n&gt; 1+1\n[1] 2\nIf you click “Run” again, you should see that same output printed again.\nAs we proceed, you can see both my source code and output like this:\n\n\nCode\n1+1\n## [1] 2\n\n\nYou should add comments to your codes, and you do this with hashtags. For example\n\n\nCode\n## This is my first comment!\n1+1 # The simplest calculation I could think of\n## [1] 2\n\n\nYou can execute each line one-at-a-time. Or you can highlight them both, to take advantage of how R executes commands line-by-line.\n\n\nAssignment.\nYou can create “variables” that store values. For example,\n\n\nCode\nx &lt;- 1 # Make your first variable\nx + 1 # The simplest calculation I could think of\n## [1] 2\n\n\n\n\nCode\nx &lt;- 23 #Another example\nx + 1\n## [1] 24\n\n\n\n\nCode\ny &lt;- x + 1 #Another example\ny\n## [1] 24\n\n\nYour variables must be defined in order to use them. Otherwise you get an error. For example,\n\n\nCode\nX +   1 # notice that R is sensitive to capitalization but not spacing\n## Error: object 'X' not found\n\n\nYour variable names do not matter technically, but they should be informative\n\n\nCode\none &lt;- 1 # good variable name\none\n## [1] 1\n\none &lt;- 43 # bad variable name\none\n## [1] 43\n\n\nGood names avoid confusion later\n\n\nCode\none_plus_2 &lt;- one + 2  #bad names propogate\n\nx &lt;- 43\nx_plus_two &lt;- x + 2 # better\n\n\n\n\nScripting.\n\nCreate a folder on your computer to save your scripts\nSave your R Script file as My_First_Script.R in your folder\nClose Rstudio\nOpen your script and re-run it\n\nAs you work through the material, make sure to both execute and save your scripts. Add lots of commentary to your scripts. Name your scripts systematically.\nThere are often many ways to accomplish the same goal. You first scripts will be very basic and rough, but you can edit them later based on what you learn. And you can always ask R for help\n\n\nCode\nsum(x, 2) # x + 2\n?sum",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First Steps</span>"
    ]
  },
  {
    "objectID": "01_01_FirstSteps.html#further-reading",
    "href": "01_01_FirstSteps.html#further-reading",
    "title": "1  First Steps",
    "section": "1.3 Further Reading",
    "text": "1.3 Further Reading\nThere are many good and free programming materials online.\nThe most common tasks can be found https://github.com/rstudio/cheatsheets/blob/main/rstudio-ide.pdf\nSome of my programming examples originally come from https://r4ds.had.co.nz/ and I recommend https://intro2r.com.\nI have also used online material from many places over the years, as there are many good yet free-online tutorials and courses specifically on R programming. See e.g.,\n\nhttps://cran.r-project.org/doc/manuals/R-intro.html\nR Graphics Cookbook, 2nd edition. Winston Chang. 2021. https://r-graphics.org/\nR for Data Science. H. Wickham and G. Grolemund. 2017. https://r4ds.had.co.nz/index.html\nAn Introduction to R. W. N. Venables, D. M. Smith, R Core Team. 2017. https://colinfay.me/intro-to-r/\nIntroduction to R for Econometrics. Kieran Marray. https://bookdown.org/kieranmarray/intro_to_r_for_econometrics/\nWollschläger, D. (2020). Grundlagen der Datenanalyse mit R: eine anwendungsorientierte Einführung. http://www.dwoll.de/rexrepos/\nSpatial Data Science with R: Introduction to R. Robert J. Hijmans. 2021. https://rspatial.org/intr/index.html\nhttps://www.econometrics-with-r.org/1.2-a-very-short-introduction-to-r-and-rstudio.html\nhttps://rafalab.github.io/dsbook/\nhttps://moderndive.com/foreword.html\nhttps://rstudio.cloud/learn/primers/1.2\nhttps://cran.r-project.org/manuals.html\nhttps://stats.idre.ucla.edu/stat/data/intro_r/intro_r_interactive_flat.html\nhttps://cswr.nrhstat.org/app-r\n\nFor more on why to program in R, see\n\nhttp://www.r-bloggers.com/the-reproducibility-crisis-in-science-and-prospects-for-r/\nhttp://fmwww.bc.edu/GStat/docs/pointclick.html\nhttps://github.com/qinwf/awesome-R\\#reproducible-research\nA Guide to Reproducible Code in Ecology and Evolution\nhttps://biostat.app.vumc.org/wiki/pub/Main/TheresaScott/ReproducibleResearch.TAScott.handout.pdf",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First Steps</span>"
    ]
  },
  {
    "objectID": "01_02_Mathematics.html",
    "href": "01_02_Mathematics.html",
    "title": "2  Mathematics",
    "section": "",
    "text": "2.1 Objects\nIn R: scalars, vectors, and matrices are different kinds of “objects”.\nThese objects are used extensively in data analysis\nVectors are probably your most common object in R, but we will start with scalars.",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mathematics</span>"
    ]
  },
  {
    "objectID": "01_02_Mathematics.html#objects",
    "href": "01_02_Mathematics.html#objects",
    "title": "2  Mathematics",
    "section": "",
    "text": "scalars: summary statistics (average household income).\nvectors: single variables in data sets (the household income of each family in Vancouver).\nmatrices: two variables in data sets (the age and education level of every person in class).\n\n\n\nScalars.\nMake your first scalar\n\n\nCode\nxs &lt;- 2 # Make your first scalar\nxs  # Print the scalar\n## [1] 2\n\n\nPerform simple calculations and see how R is doing the math for you\n\n\nCode\nxs + 2\n## [1] 4\nxs*2 # Perform and print a simple calculation\n## [1] 4\n(xs+1)^2 # Perform and print a simple calculation\n## [1] 9\nxs + NA # often used for missing values\n## [1] NA\n\n\nNow change xs, predict what will happen, then re-run the code.\n\n\nVectors.\nMake Your First Vector\n\n\nCode\nx &lt;- c(0,1,3,10,6) # Your First Vector\nx # Print the vector\n## [1]  0  1  3 10  6\nx[2] # Print the 2nd Element; 1\n## [1] 1\nx+2 # Print simple calculation; 2,3,5,8,12\n## [1]  2  3  5 12  8\nx*2\n## [1]  0  2  6 20 12\nx^2\n## [1]   0   1   9 100  36\n\n\nApply mathematical calculations elementwise\n\n\nCode\nx+x\n## [1]  0  2  6 20 12\nx*x\n## [1]   0   1   9 100  36\nx^x\n## [1] 1.0000e+00 1.0000e+00 2.7000e+01 1.0000e+10 4.6656e+04\n\n\nIn R, scalars are treated as a vector with one element.\n\n\nCode\nc(1)\n## [1] 1\n\n\nSometimes, we will use vectors that are entirely ordered.\n\n\nCode\n1:7\n## [1] 1 2 3 4 5 6 7\nseq(0,1,by=.1)\n##  [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\n# Ordering data\nsort(x)\n## [1]  0  1  3  6 10\nx[order(x)]\n## [1]  0  1  3  6 10\n\n\n\n\nMatrices.\nMatrices are also common objects\n\n\nCode\nx1 &lt;- c(1,4,9)\nx2 &lt;- c(3,0,2)\nx_mat &lt;- rbind(x1, x2)\n\nx_mat       # Print full matrix\n##    [,1] [,2] [,3]\n## x1    1    4    9\n## x2    3    0    2\nx_mat[2,]   # Print Second Row\n## [1] 3 0 2\nx_mat[,2]   # Print Second Column\n## x1 x2 \n##  4  0\nx_mat[2,2]  # Print Element in Second Column and Second Row\n## x2 \n##  0\n\n\nThere are elementwise calculations\n\n\nCode\nx_mat+2\n##    [,1] [,2] [,3]\n## x1    3    6   11\n## x2    5    2    4\nx_mat*2\n##    [,1] [,2] [,3]\n## x1    2    8   18\n## x2    6    0    4\nx_mat^2\n##    [,1] [,2] [,3]\n## x1    1   16   81\n## x2    9    0    4\n\nx_mat + x_mat\n##    [,1] [,2] [,3]\n## x1    2    8   18\n## x2    6    0    4\nx_mat*x_mat #NOT classical matrix multiplication\n##    [,1] [,2] [,3]\n## x1    1   16   81\n## x2    9    0    4\nx_mat^x_mat\n##    [,1] [,2]      [,3]\n## x1    1  256 387420489\n## x2   27    1         4\n\n\nAnd you can also use matrix algebra\n\n\nCode\nx_mat1 &lt;- matrix(2:7,2,3)\nx_mat1\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n\nx_mat2 &lt;- matrix(4:-1,2,3)\nx_mat2\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\ntcrossprod(x_mat1, x_mat2) #x_mat1 %*% t(x_mat2)\n##      [,1] [,2]\n## [1,]   16    4\n## [2,]   22    7\n\ncrossprod(x_mat1, x_mat2)\n##      [,1] [,2] [,3]\n## [1,]   17    7   -3\n## [2,]   31   13   -5\n## [3,]   45   19   -7",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mathematics</span>"
    ]
  },
  {
    "objectID": "01_02_Mathematics.html#functions",
    "href": "01_02_Mathematics.html#functions",
    "title": "2  Mathematics",
    "section": "2.2 Functions",
    "text": "2.2 Functions\nFunctions are applied to objects\n\n\nCode\n# Define a function that adds two to any vector\nadd_two &lt;- function(input_vector) { #input_vector is a placeholder\n    output_vector &lt;- input_vector + 2 # new object defined locally \n    return(output_vector) # return new object \n}\n# Apply that function to a vector\nx &lt;- c(0,1,3,10,6)\nadd_two(input_vector=x) #same as add_two(x)\n## [1]  2  3  5 12  8\n\n\nCommon mistakes:\n\n\nCode\nprint(output_vector)\n# This is not available globally\n\n# Seeing \"+ add_2(x)\" in the bottom console\n# means you forgot to close the function with \"}\" \n# press \"Escape\" and try again\n\n# Double check your spelling\n\n\nThere are many many generalizations\n\n\nCode\nadd_vec &lt;- function(input_vector1, input_vector2) {\n    output_vector &lt;- input_vector1 + input_vector2\n    return(output_vector)\n}\nadd_vec(x,3)\n## [1]  3  4  6 13  9\nadd_vec(x,x)\n## [1]  0  2  6 20 12\n\nsum_squared &lt;- function(x1, x2) {\n    y &lt;- (x1 + x2)^2\n    return(y)\n}\n\nsum_squared(1, 3)\n## [1] 16\nsum_squared(x, 2)\n## [1]   4   9  25 144  64\nsum_squared(x, NA) \n## [1] NA NA NA NA NA\nsum_squared(x, x)\n## [1]   0   4  36 400 144\nsum_squared(x, 2*x)\n## [1]   0   9  81 900 324\n\n\nFunctions can take functions as arguments. Note that a statistic is defined as a function of data.\n\n\nCode\nstatistic &lt;- function(x,f){\n    y &lt;- f(x)\n    return(y)\n}\nstatistic(x, mean)\n## [1] 4\n\n\nYou can apply functions to matrices\n\n\nCode\nsum_squared(x_mat, x_mat)\n##    [,1] [,2] [,3]\n## x1    4   64  324\n## x2   36    0   16\n\n# Apply function to each matrix row\ny &lt;- apply(x_mat, 1, sum)^2 \n# ?apply  #checks the function details\ny - sum_squared(x, x) # tests if there are any differences\n## [1]  196   21  160 -375   52\n\n\nThere are many possible functions you can apply\n\n\nCode\n# Return Y-value with minimum absolute difference from 3\nabs_diff_y &lt;- abs( y - 3 ) \nabs_diff_y # is this the luckiest number?\n##  x1  x2 \n## 193  22\n\n#min(abs_diff_y)\n#which.min(abs_diff_y)\ny[ which.min(abs_diff_y) ]\n## x2 \n## 25\n\n\n\n\nCode\nfun_of_seq &lt;- function(f){\n    x1 &lt;- seq(1,3, length.out=12)\n    x2 &lt;- x1+2\n    x &lt;- cbind(x1,x2)\n    y &lt;- f(x)\n    return(y)\n}\nfun_of_seq(mean)\n## [1] 3\nfun_of_seq(sd)\n## [1] 1.206045\n\n\nThere are also some useful built in functions\n\n\nCode\nm &lt;- matrix(c(1:3,2*(1:3)),byrow=TRUE,ncol=3)\nm\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    2    4    6\n\n# normalize rows\nm/rowSums(m)\n##           [,1]      [,2] [,3]\n## [1,] 0.1666667 0.3333333  0.5\n## [2,] 0.1666667 0.3333333  0.5\n\n# normalize columns\nt(t(m)/colSums(m))\n##           [,1]      [,2]      [,3]\n## [1,] 0.3333333 0.3333333 0.3333333\n## [2,] 0.6666667 0.6666667 0.6666667\n\n# de-mean rows\nsweep(m,1,rowMeans(m), '-')\n##      [,1] [,2] [,3]\n## [1,]   -1    0    1\n## [2,]   -2    0    2\n\n# de-mean columns\nsweep(m,2,colMeans(m), '-')\n##      [,1] [,2] [,3]\n## [1,] -0.5   -1 -1.5\n## [2,]  0.5    1  1.5\n\n\n\nLoops.\nApplying the same function over and over again\n\n\nCode\n#Create empty vector\nexp_vector &lt;- vector(length=3)\n#Fill empty vector\nfor(i in 1:3){\n    exp_vector[i] &lt;- exp(i)\n}\n\n# Compare\nexp_vector\n## [1]  2.718282  7.389056 20.085537\nc( exp(1), exp(2), exp(3))\n## [1]  2.718282  7.389056 20.085537\n\n\nA more complicated example\n\n\nCode\ncomplicated_fun &lt;- function(i, j=0){\n    x &lt;- i^(i-1)\n    y &lt;- x + mean( j:i )\n    z &lt;- log(y)/i\n    return(z)\n}\ncomplicated_vector &lt;- vector(length=10)\nfor(i in 1:10){\n    complicated_vector[i] &lt;- complicated_fun(i)\n}\n\n\nA recursive example\n\n\nCode\nx &lt;- vector(length=4)\nx[1] &lt;- 1\nfor(i in 2:4){\n    x[i] &lt;- (x[i-1]+1)^2\n}\nx\n## [1]   1   4  25 676\n\n\n\n\n\nLogic.\nBasic Logic\n\n\nCode\nx &lt;- c(1,2,3,NA)\nx &gt; 2\n## [1] FALSE FALSE  TRUE    NA\nx==2\n## [1] FALSE  TRUE FALSE    NA\n\nany(x==2)\n## [1] TRUE\nall(x==2)\n## [1] FALSE\n2 %in% x\n## [1] TRUE\n\nis.numeric(x)\n## [1] TRUE\nis.na(x)\n## [1] FALSE FALSE FALSE  TRUE\n\n\nThe “&” and “|” commands are logical calculations that compare vectors to the left and right.\n\n\nCode\nx &lt;- 1:3\nis.numeric(x) & (x &lt; 2)\n## [1]  TRUE FALSE FALSE\nis.numeric(x) | (x &lt; 2)\n## [1] TRUE TRUE TRUE\n\nif(length(x) &gt;= 5 & x[5] &gt; 12) print(\"ok\")\n\n\nAdvanced Logic.\n\n\nCode\nx &lt;- 1:10\ncut(x, 4)\n##  [1] (0.991,3.25] (0.991,3.25] (0.991,3.25] (3.25,5.5]   (3.25,5.5]  \n##  [6] (5.5,7.75]   (5.5,7.75]   (7.75,10]    (7.75,10]    (7.75,10]   \n## Levels: (0.991,3.25] (3.25,5.5] (5.5,7.75] (7.75,10]\nsplit(x, cut(x, 4))\n## $`(0.991,3.25]`\n## [1] 1 2 3\n## \n## $`(3.25,5.5]`\n## [1] 4 5\n## \n## $`(5.5,7.75]`\n## [1] 6 7\n## \n## $`(7.75,10]`\n## [1]  8  9 10\n\n\n\n\nCode\nxs &lt;- split(x, cut(x, 4))\nsapply(xs, mean)\n## (0.991,3.25]   (3.25,5.5]   (5.5,7.75]    (7.75,10] \n##          2.0          4.5          6.5          9.0\n\n# shortcut\naggregate(x, list(cut(x,4)), mean)\n##        Group.1   x\n## 1 (0.991,3.25] 2.0\n## 2   (3.25,5.5] 4.5\n## 3   (5.5,7.75] 6.5\n## 4    (7.75,10] 9.0\n\n\nSee https://bookdown.org/rwnahhas/IntroToR/logical.html",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mathematics</span>"
    ]
  },
  {
    "objectID": "01_02_Mathematics.html#datasets",
    "href": "01_02_Mathematics.html#datasets",
    "title": "2  Mathematics",
    "section": "2.3 Datasets",
    "text": "2.3 Datasets\nDatasets can be stored in a variety of formats on your computer. But they can be analyzed in R in three basic ways.\n\nLists.\nLists are probably the most basic type\n\n\nCode\nx &lt;- 1:10\ny &lt;- 2*x\nlist(x, y)  # list of vectors\n## [[1]]\n##  [1]  1  2  3  4  5  6  7  8  9 10\n## \n## [[2]]\n##  [1]  2  4  6  8 10 12 14 16 18 20\n\nlist(x_mat1, x_mat2)  # list of matrices\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\n\nLists are useful for storing unstructured data\n\n\nCode\nlist(list(x_mat1), list(x_mat2))  # list of lists\n## [[1]]\n## [[1]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## \n## [[2]]\n## [[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\nlist(x_mat1, list(x_mat1, x_mat2)) # list of different objects\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]]\n## [[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]][[2]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\n# ...inception...\nlist(x_mat1,\n    list(x_mat1, x_mat2), \n    list(x_mat1, list(x_mat2)\n    )) \n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]]\n## [[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]][[2]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n## \n## \n## [[3]]\n## [[3]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[3]][[2]]\n## [[3]][[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\n\n\n\nData.frames.\nA data.frame looks like a matrix but each column is actually a list rather than a vector. This allows you to combine different data types into a single object for analysis, which is why it might be your most common object.\n\n\nCode\n# data.frames: \n    # matrix of different data-types\n    # well-ordered lists\ndata.frame(x, y)  # list of vectors\n##     x  y\n## 1   1  2\n## 2   2  4\n## 3   3  6\n## 4   4  8\n## 5   5 10\n## 6   6 12\n## 7   7 14\n## 8   8 16\n## 9   9 18\n## 10 10 20\n\n\n\n\nArrays.\nArrays are generalization of matrices to multiple dimensions. They are a very efficient way to store well-formatted numeric data, and are often used in spatial econometrics and time series (often in the form of “data cubes”).\n\n\nCode\n# data square (matrix)\narray(data = 1:24, dim = c(3,8))\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n## [1,]    1    4    7   10   13   16   19   22\n## [2,]    2    5    8   11   14   17   20   23\n## [3,]    3    6    9   12   15   18   21   24\n\n# data cube\na &lt;- array(data = 1:24, dim = c(3, 2, 4))\na\n## , , 1\n## \n##      [,1] [,2]\n## [1,]    1    4\n## [2,]    2    5\n## [3,]    3    6\n## \n## , , 2\n## \n##      [,1] [,2]\n## [1,]    7   10\n## [2,]    8   11\n## [3,]    9   12\n## \n## , , 3\n## \n##      [,1] [,2]\n## [1,]   13   16\n## [2,]   14   17\n## [3,]   15   18\n## \n## , , 4\n## \n##      [,1] [,2]\n## [1,]   19   22\n## [2,]   20   23\n## [3,]   21   24\n\n\n\n\nCode\na[1, , , drop = FALSE]  # Row 1\n#a[, 1, , drop = FALSE]  # Column 1\n#a[, , 1, drop = FALSE]  # Layer 1\n\na[ 1, 1,  ]  # Row 1, column 1\n#a[ 1,  , 1]  # Row 1, \"layer\" 1\n#a[  , 1, 1]  # Column 1, \"layer\" 1\na[1 , 1, 1]  # Row 1, column 1, \"layer\" 1\n\n\nApply extends to arrays\n\n\nCode\napply(a, 1, mean)    # Row means\n## [1] 11.5 12.5 13.5\napply(a, 2, mean)    # Column means\n## [1] 11 14\napply(a, 3, mean)    # \"Layer\" means\n## [1]  3.5  9.5 15.5 21.5\napply(a, 1:2, mean)  # Row/Column combination \n##      [,1] [,2]\n## [1,]   10   13\n## [2,]   11   14\n## [3,]   12   15\n\n\nOuter products yield arrays\n\n\nCode\nx &lt;- c(1,2,3)\nx_mat1 &lt;- outer(x, x) # x %o% x\nx_mat1\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    2    4    6\n## [3,]    3    6    9\nis.array(x_mat) # Matrices are arrays\n## [1] TRUE\n\nx_mat2 &lt;- matrix(6:1,2,3)\nouter(x_mat2, x)\n## , , 1\n## \n##      [,1] [,2] [,3]\n## [1,]    6    4    2\n## [2,]    5    3    1\n## \n## , , 2\n## \n##      [,1] [,2] [,3]\n## [1,]   12    8    4\n## [2,]   10    6    2\n## \n## , , 3\n## \n##      [,1] [,2] [,3]\n## [1,]   18   12    6\n## [2,]   15    9    3\n# outer(x_mat2, matrix(x))\n# outer(x_mat2, t(x))\n# outer(x_mat1, x_mat2)",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mathematics</span>"
    ]
  },
  {
    "objectID": "01_03_Data.html",
    "href": "01_03_Data.html",
    "title": "3  Data",
    "section": "",
    "text": "3.1 Types",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "01_03_Data.html#types",
    "href": "01_03_Data.html#types",
    "title": "3  Data",
    "section": "",
    "text": "Basic Types.\nThe two basic types of data are cardinal (aka numeric) data and factor data. We can further distinguish between whether cardinal data are discrete or continuous. We can also further distinguish between whether factor data are ordered or not\n\nCardinal (Numeric): the difference between elements always means the same thing.\n\nDiscrete: E.g., \\(2-1=3-2\\).\nContinuous: E.g., \\(2.9-1.4348=3.9-2.4348\\)\n\nFactor: the difference between elements does not always mean the same thing.\n\nOrdered: E.g., First place - Second place ?? Second place - Third place.\nUnordered (categorical): E.g., A - B ????\n\n\nHere are some examples\n\n\nCode\ndat_card1 &lt;- 1:3 # Cardinal data (Discrete)\ndat_card1\n## [1] 1 2 3\n\ndat_card2 &lt;- c(1.1, 2/3, 3) # Cardinal data (Continuous)\ndat_card2\n## [1] 1.1000000 0.6666667 3.0000000\n\ndat_fact1 &lt;- factor( c('A','B','C'), ordered=T) # Factor data (Ordinal)\ndat_fact1\n## [1] A B C\n## Levels: A &lt; B &lt; C\n\ndat_fact2 &lt;- factor( c('Leipzig','Los Angeles','Logan'), ordered=F) # Factor data (Categorical)\ndat_fact2\n## [1] Leipzig     Los Angeles Logan      \n## Levels: Leipzig Logan Los Angeles\n\ndat_fact3 &lt;- factor( c(T,F), ordered=F) # Factor data (Categorical)\ndat_fact3\n## [1] TRUE  FALSE\n## Levels: FALSE TRUE\n\n# Explicitly check the data types:\n#class(dat_card1)\n#class(dat_card2)\n\n\nNote that for theoretical analysis, the types are sometimes grouped differently as\n\ncontinuous (continuous cardinal data)\ndiscrete (discrete cardinal, ordered factor, and unordered factor data)\n\nIn any case, these data are often analyzed in data.frame objects\n\n\nCode\n# data.frames: your most common data type\n    # matrix of different data-types\n    # well-ordered lists\nd0 &lt;- data.frame(x=dat_fact2, y=dat_card2)\nd0\n##             x         y\n## 1     Leipzig 1.1000000\n## 2 Los Angeles 0.6666667\n## 3       Logan 3.0000000\n\nd0[,'y'] #d0$y\n## [1] 1.1000000 0.6666667 3.0000000\n\n\n\n\nStrings.\nNote that R allows for unstructured plain text, called character strings, which we can then format as factors\n\n\nCode\nc('A','B','C')  # character strings\n## [1] \"A\" \"B\" \"C\"\nc('Leipzig','Los Angeles','Logan')  # character strings\n## [1] \"Leipzig\"     \"Los Angeles\" \"Logan\"\n\n\nAlso note that strings are encounter in a variety of settings, and you often have to format them after reading them into R.1\n\n\nCode\n# Strings\npaste( 'hi', 'mom')\n## [1] \"hi mom\"\npaste( c('hi', 'mom'), collapse='--')\n## [1] \"hi--mom\"\n\nkingText &lt;- \"The king infringes the law on playing curling.\"\ngsub(pattern=\"ing\", replacement=\"\", kingText)\n## [1] \"The k infres the law on play curl.\"\n# advanced usage\n#gsub(\"[aeiouy]\", \"_\", kingText)\n#gsub(\"([[:alpha:]]{3})ing\\\\b\", \"\\\\1\", kingText) \n\n\nSee\n\nhttps://meek-parfait-60672c.netlify.app/docs/M1_R-intro_03_text.html\nhttps://raw.githubusercontent.com/rstudio/cheatsheets/main/regex.pdf",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "01_03_Data.html#densities-and-distributions",
    "href": "01_03_Data.html#densities-and-distributions",
    "title": "3  Data",
    "section": "3.2 Densities and Distributions",
    "text": "3.2 Densities and Distributions\n\nInitial Data Inspection.\nRegardless of the data types you have, you typically begin by inspecting your data by examining the first few observations.\nConsider, for example, historical data on crime in the US.\n\n\nCode\nhead(USArrests)\n##            Murder Assault UrbanPop Rape\n## Alabama      13.2     236       58 21.2\n## Alaska       10.0     263       48 44.5\n## Arizona       8.1     294       80 31.0\n## Arkansas      8.8     190       50 19.5\n## California    9.0     276       91 40.6\n## Colorado      7.9     204       78 38.7\n\n# Check NA values\nX &lt;- c(1,NA,2,3)\nsum(is.na(X))\n## [1] 1\n\n\nTo further examine a particular variable, we look at its distribution. In what follows, we will often work with data as vector \\(X=(X_{1}, X_{2}, ....X_{N})\\), where there are \\(N\\) observations and \\(X_{i}\\) is the value of the \\(i\\)th one.\n\n\nHistogram Density Estimate.\nThe histogram divides the range of the data into \\(L\\) exclusive bins of equal-width \\(h\\), and count the number of observations within each bin. We often rescale the counts so that the total area of all bins sums to one, which allows us to interpret the numbers as a density measuring a proportion of the data in each bin. Mathematically, for an exclusive bin \\(\\left[x-\\frac{h}{2}, x+\\frac{h}{2} \\right)\\) defined by their midpoint \\(x\\) and width \\(h\\), we compute \\[\\begin{eqnarray}\n\\widehat{f}_{HIST}(x) &=& \\frac{  \\sum_{i=1}^{N} \\mathbf{1}\\left( X_{i} \\in \\left(x-\\frac{h}{2}, x+\\frac{h}{2} \\right] \\right) }{N h}.\n\\end{eqnarray}\\] Note that \\(\\mathbf{1}()\\) is an indicator function, which equals \\(1\\) if the expression inside is TRUE and \\(0\\) otherwise. I.e., if \\(x-\\frac{h}{2} &lt; X_{i} \\leq x+\\frac{h}{2}\\) then \\(\\mathbf{1}\\left( X_{i} \\in \\left(x-\\frac{h}{2}, x+\\frac{h}{2} \\right] \\right) =1\\). Also note that we compute \\(\\widehat{f}_{HIST}(x)\\) for each bin midpoint \\(x\\).2\nFor example, let \\(X=(3,3.1,0.02)\\) and use bins \\((0,1], (1,2], (2,3], (3,4]\\). In this case, the midpoints are \\(x=(0.5,1.5,2.5,3.5)\\) and \\(h=1\\). Then the counts at each midpoints are \\((1,0,0,2)\\). Since \\(\\frac{1}{Nh}=1/3\\), we also have \\(\\widehat{f}(x)=(1/3,0,1/3,1/3)\\). Now intuitively work through an example with three bins instead of four.\n\n\nCode\n# Intuitive Examples\nX &lt;- c(3,3.1,0.02)\nhist(X, breaks=c(0,1,2,3,4), plot=F)\n\nhist(X, breaks=c(0,4/3,8/3,4), plot=F)\n\n# as a default, R uses bins (,] instead of [,)\n# but you can change that \nhist(X, breaks=c(0,4/3,8/3,4), plot=F, right=F)\n\n\n\n\nCode\n# Practical Example\nhist(USArrests[,'Murder'], freq=F, breaks=20,\n    border=NA, \n    main='',\n    xlab='Murder Arrests',\n    ylab='Proportion of States in each bin')\n# Raw Observations\nrug(USArrests[,'Murder'], col=grey(0,.5))\n\n\n\n\n\n\n\n\n\nNote that if you your data are factor data, or discrete cardinal data, you can directly plot the counts or proportions: for each unique outcome \\(k\\) we compute \\(\\widehat{p}_{k}=\\sum_{i=1}^{N}\\mathbf{1}\\left(X_{i}=k\\right)/N\\).\n\n\nCode\n# Discretized data\nxr &lt;- floor(USArrests[,'Murder']) #rounded down\n#table(xr)\nproportions &lt;- table(xr)/length(xr)\nplot(proportions, col=grey(0,.5),\n    xlab='Murder Rate (Discretized)',\n    ylab='Proportion of States with each value')\n\n\n\n\n\n\n\n\n\n\n\nEmpirical Cumulative Distribution Function.\nThe ECDF counts the proportion of observations whose values are less than or equal to \\(x\\); \\[\\begin{eqnarray}\n\\widehat{F}_{ECDF}(x) = \\frac{1}{N} \\sum_{i}^{N} \\mathbf{1}(X_{i} \\leq x).\n\\end{eqnarray}\\] Typically, we compute this for each unique value of \\(x\\) in the dataset, but sometimes other values of \\(x\\) too.\nFor example, let \\(X=(3,3.1,0.02)\\) and consider the points \\(x=(0.5,1.5,2.5,3.5)\\). Then the counts are \\((1,1,1,3)\\). Since \\(N=3\\), \\(\\widehat{F}(x)=(1/3,1/3,1/3,1)\\).\n\n\nCode\nF_murder &lt;- ecdf(USArrests[,'Murder'])\n# proportion of murders &lt;= 10\nF_murder(10)\n## [1] 0.7\n# proportion of murders &lt;= x, for all x\nplot(F_murder, main='', xlab='Murder Arrests',\n    pch=16, col=grey(0,.5))\nrug(USArrests[,'Murder'])\n\n\n\n\n\n\n\n\n\n\n\nBoxplots.\nBoxplots summarize the distribution of data using quantiles: the \\(q\\)th quantile is the value where \\(q\\) percent of the data are below and (\\(1-q\\)) percent are above.\n\nThe median is the point where half of the data has lower values and the other half has higher values.\nThe lower quartile is the point where \\(25%\\) of the data has lower values and the other \\(75%\\) has higher values.\nThe min is the smallest value (or the most negative value if there are any), where \\(0%\\) of the data has lower values.\n\nFor example, if \\(X=(0,0,0.02,3,5)\\) then the median is \\(0.02\\), the lower quartile is \\(0\\), and the upper quartile is \\(3\\). (The number \\(0\\) is also special: the most frequent observation is called the mode.) Now work through an intuitive example with \\(N=24\\) data points (hint: split the ordered observations into groups of six).\n\n\nCode\nX &lt;-  c(3.1, 3, 0.02)\nquantile(X, probs=c(0,.5,1))\n##   0%  50% 100% \n## 0.02 3.00 3.10\n\n# quantiles\nX &lt;- USArrests[,'Murder']\nquantile(X)\n##     0%    25%    50%    75%   100% \n##  0.800  4.075  7.250 11.250 17.400\n\n# deciles are quantiles\nquantile(X, probs=seq(0,1, by=.1))\n##    0%   10%   20%   30%   40%   50%   60%   70%   80%   90%  100% \n##  0.80  2.56  3.38  4.75  6.00  7.25  8.62 10.12 12.12 13.32 17.40\n\n\nTo actually calculate quantiles, we sort the observations from smallest to largest as \\(X_{(1)}, X_{(2)},... X_{(N)}\\), and then compute quantiles as \\(X_{ (q*N) }\\). Note that \\((q*N)\\) is rounded and there are different ways to break ties.\n\n\nCode\nX &lt;- USArrests[,'Murder']\nXo &lt;- sort(X)\nXo\n##  [1]  0.8  2.1  2.1  2.2  2.2  2.6  2.6  2.7  3.2  3.3  3.4  3.8  4.0  4.3  4.4\n## [16]  4.9  5.3  5.7  5.9  6.0  6.0  6.3  6.6  6.8  7.2  7.3  7.4  7.9  8.1  8.5\n## [31]  8.8  9.0  9.0  9.7 10.0 10.4 11.1 11.3 11.4 12.1 12.2 12.7 13.0 13.2 13.2\n## [46] 14.4 15.4 15.4 16.1 17.4\n\n# median\nXo[length(Xo)*.5]\n## [1] 7.2\nquantile(X, probs=.5, type=4)\n## 50% \n## 7.2\n\n# min\nXo[1]\n## [1] 0.8\nmin(Xo)\n## [1] 0.8\nquantile(Xo, probs=0)\n##  0% \n## 0.8\n\n\nThe boxplot shows the median (solid black line) and interquartile range (\\(IQR=\\) upper quartile \\(-\\) lower quartile; filled box).3 As a default, whiskers are shown as \\(1.5\\times IQR\\) and values beyond that are highlighted as outliers—so whiskers do not typically show the data range. You can alternatively show all the raw data points instead of whisker+outliers.\n\n\nCode\nboxplot(USArrests[,'Murder'],\n    main='', ylab='Murder Arrests',\n    whisklty=0, staplelty=0, outline=F)\n# Raw Observations\nstripchart(USArrests[,'Murder'],\n    pch='-', col=grey(0,.5), cex=2,\n    vert=T, add=T)",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "01_03_Data.html#footnotes",
    "href": "01_03_Data.html#footnotes",
    "title": "3  Data",
    "section": "",
    "text": "We will not cover the statistical analysis of text in this course, but strings are amenable to statistical analysis.↩︎\nIf the bins exactly span the range, then \\(h=[\\text{max}(X_{i}) - \\text{min}(X_{i})]/L\\) and \\(x\\in \\left\\{ \\frac{\\ell h}{2} + \\text{min}(X_{i}) \\right\\}_{\\ell=1}^{L}\\).↩︎\nTechnically, the upper and lower hinges use two different versions of the first and third quartile. See https://stackoverflow.com/questions/40634693/lower-and-upper-quartiles-in-boxplot-in-r.↩︎",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "01_04_RandomVariables.html",
    "href": "01_04_RandomVariables.html",
    "title": "4  Random Variables",
    "section": "",
    "text": "4.1 Discrete\nIn the last section we computed a distribution given the data, whereas now we generate data given the distribution.\nRandom variables are vectors that are generated from a known Cumulative Distribution Function or Probability Distribution Function, which describes the long run frequencies of all possible outcomes. Random variables have a\nThere are only two basic types of sample spaces: discrete (encompassing cardinal-discrete, factor-ordered, and factor-unordered data) and continuous, which lead to two types of random variables. In any case, probabilities must sum up to 1.\nHowever, each type of random variable has many different probability distributions. The most common ones are easily accessible and can be described using the Cumulative Distribution Function \\[\\begin{eqnarray}\nF(x) &=& Prob(X_{i} \\leq x).\n\\end{eqnarray}\\] Note that this is just like the ECDF, \\(\\widehat{F}(x)\\), except that it is now theoretically known instead of estimated.1\nThe random variable can take one of several values in a set. E.g., any number in \\(\\{1,2,3,...\\}\\) or any letter in \\(\\{A,B,C,...\\}\\).",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "01_04_RandomVariables.html#discrete",
    "href": "01_04_RandomVariables.html#discrete",
    "title": "4  Random Variables",
    "section": "",
    "text": "Bernoulli.\nThink of a Coin Flip: Heads=1 or Tails=0, with equal probability. In general, the probability of heads can vary. \\[\\begin{eqnarray}\nX_{i} &\\in& \\{0,1\\} \\\\\nProb(X_{i} =0) &=& 1-p \\\\\nProb(X_{i} =1) &=& p.\n\\end{eqnarray}\\]\nHere is an example\n\n\nCode\nrbinom(1, 1, 0.25) # 1 Flip\n## [1] 1\nrbinom(4, 1, 0.25) # 4 Flips\n## [1] 0 0 0 0\nX0 &lt;- rbinom(400, 1, 0.25)\n\n# Plot Cumulative Proportion\nX0_t &lt;- seq_len(length(X0)) #head(X0_t)\nX0_mt &lt;- cumsum(X0)/X0_t #head(X0_mt)\npar(mar=c(4,4,1,4))\nplot(X0_t, X0_mt, type='l',\n    ylab='Cumulative Proportion (p)',\n    xlab='Flip #', \n    ylim=c(0,1), \n    lwd=2)\n# Add individual flip outcomes\npoints(X0_t, X0, col=grey(0,.5),\n    pch='|', cex=.3)\n\n\n\n\n\n\n\n\n\nCode\n\n# Plot Long run proportions\nproportions &lt;- table(X0)/length(X0)\nplot(proportions, col=grey(0,.5),\n    xlab='Flip Outcome', ylab='Count', main=NA)\npoints(c(0,1), c(.75, .25), pch=16, col='blue') # Theoretical values\n\n\n\n\n\n\n\n\n\n\n\nDiscrete Uniform.\nDiscrete numbers with equal probability \\[\\begin{eqnarray}\nX_{i} &\\in& \\{1,...N\\} \\\\\nProb(X_{i} =1) &=& Prob(X_{i} =2) = ... = 1/N.\n\\end{eqnarray}\\]\nHere is an example with \\(N=4\\).\nThe probability of a value smaller than or equal to \\(3\\) is \\(Prob(X_{i} \\leq 3)=1/4 + 1/4 + 1/4 = 3/4\\).\nThe probability of a value larger than \\(3\\) is \\(Prob(X_{i} &gt; 3) = 1-Prob(X_{i} \\leq 3)=1/4\\).\nThe probability of a value of a value \\(&gt;\\) 1 and \\(\\leq 3\\) is \\(Prob(1 &lt; X_{i} \\leq 3) = Prob(X_{i} \\leq 3) - \\left[ 1- Prob(X_{i} \\leq 1) \\right] = 3/4 - 1/4 = 2/4\\).2\n\n\nCode\nx &lt;- 1:4\nx_probs &lt;- rep(1/4,4)\n# sample(x, 1, replace=T, prob=x_probs) # sample of 1\nX1 &lt;- sample(x, 2000, replace=T, prob=rep(1/4,4))\n\n# Plot Long run proportions\nproportions &lt;- table(X1)/length(X1)\nplot(proportions, col=grey(0,.5),\n    xlab='Outcome', ylab='Proportion', main=NA)\npoints(x, x_probs, pch=16, col='blue') # Theoretical values\n\n\n\n\n\n\n\n\n\nCode\n\n# Hist w/ Theoretical Counts\n# hist(X1, breaks=50, border=NA, main=NA, ylab='Count')\n# points(x, x_probs*length(X1), pch='-') \n\n# Alternative Plot\nplot( ecdf(X1), pch=16, col=grey(0,.5), main=NA)\n\n\n\n\n\n\n\n\n\nCode\n\n# Alternative Plot 2\n#props &lt;- table(X1)\n#barplot(props, ylim = c(0, 0.35), ylab = \"Proportion\", xlab = \"Value\")\n#abline(h = 1/4, lty = 2)\n\n\n\n\nMultinoulli (aka Categorical).\nNumbers 1,…N (or letters A,…) with unequal probabilities. \\[\\begin{eqnarray}\nX_{i} &\\in& \\{1,...N\\} \\\\\nProb(X_{i} =1) &=& p_{1} \\\\\nProb(X_{i} =2) &=& p_{2} \\\\\n        &\\vdots& \\\\\np_{1} + p_{2} + ... &=& 1\n\\end{eqnarray}\\]\nHere is an empirical example with three outcomes\n\n\nCode\nx &lt;- c('A', 'B', 'C')\nx_probs &lt;- c(3,6,1)/10\nsum(x_probs)\n## [1] 1\nX1 &lt;- sample(x, 2000, replace=T, prob=x_probs) # sample of 2000,\n\n# Plot Long run proportions\nproportions &lt;- table(X1)/length(X1)\nplot(proportions, col=grey(0,.5),\n    xlab='Outcome', ylab='Proportion', main=NA)\npoints(x_probs, pch=16, col='blue') # Theoretical values\n\n\n\n\n\n\n\n\n\nCode\n\n# Histogram version\n# X1_alt &lt;- X1\n# X1_alt[X1_alt=='A'] &lt;- 1\n#X1_alt[X1_alt=='B'] &lt;- 2\n#X1_alt[X1_alt=='C'] &lt;- 3\n#X1_alt &lt;- as.numeric(X1_alt)\n#hist(X1_alt, breaks=50, border=NA, \n#    main=NA, ylab='Count')\n#points(x, x_probs*length(X1_alt), pch=16) ## Theoretical Counts\n\n# Alternative Plot\n# plot( ecdf(X1), pch=16, col=grey(0,.5), main=NA)",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "01_04_RandomVariables.html#continuous",
    "href": "01_04_RandomVariables.html#continuous",
    "title": "4  Random Variables",
    "section": "4.2 Continuous",
    "text": "4.2 Continuous\nThe random variable can take one value out of an uncountably infinite number. We describe these variables with the cumulative distribution function \\(F\\), or the probability density function \\(f\\).\nWith a continuous random variable, the probability of any individual point is zero. The probability of \\(X_{i}\\) falling into a range \\([a,b]\\) is \\(Prob(a \\leq X_{i} \\leq b) = F(b) - F(a)\\).\n\nContinuous Uniform.\nAny number on a unit interval allowing for any number of decimal points, with every number having the same probability. \\[\\begin{eqnarray}\nX_{i} &\\in& [0,1] \\\\\nF(x) &=& \\begin{cases}\n0 & x &lt; 0 \\\\\nx & x \\in [0,1] \\\\\n1 & x &gt; 1\n\\end{cases}\\\\\nf(x) &=& \\begin{cases}\n1 & x \\in [0,1] \\\\\n0 & \\text{Otherwise}.\n\\end{cases}\n\\end{eqnarray}\\]\nThe probability of a value being exactly \\(0.25\\) is \\(Prob(X_{i} =0.25)=0\\).\nThe probability of a value smaller that \\(0.25\\) is \\(F(0.25)=0.25\\).\nThe probability of a value larger than \\(0.25\\) is \\(1-F(0.25)=0.75\\).\nThe probability of a value in \\((0.25,0.75]\\) is \\(Prob(0.25 &lt; X_{i} \\leq 0.75) = Prob(X_{i} \\leq 0.75) - \\left[ 1- Prob(X_{i} \\leq 0.25) \\right] = 0.75 - 0.25 = 0.5\\).\n\n\nCode\nrunif(3) # 3 draws\n## [1] 0.7614374 0.6044773 0.3525145\n\n# Empirical Density \nX2 &lt;- runif(2000)\nhist(X2, breaks=20, border=NA, main=NA, freq=F)\n# Theoretical Density\nx &lt;- seq(0,1,by=.01)\nfx &lt;- dunif(x)\nlines(x, fx, col='blue')\n\n\n\n\n\n\n\n\n\nCode\n\n# CDF examples\npunif(0.25)\n## [1] 0.25\n1-punif(0.25)\n## [1] 0.75\npunif(0.75) - punif(0.25)\n## [1] 0.5\n\n\nNote that the continuous uniform distribution generalizes to an arbitrary interval, \\(X_{i} \\in [a,b]\\). What is the probability of a value larger than \\(0.25\\) when \\(a=-b=2\\)? First use the computer to suggest an answer and then show the answer mathematically.\n\n\nBeta.\nAny number on the unit interval, \\(X_{i} \\in [0,1]\\), but with unequal probabilities.\n\n\nCode\nX3 &lt;- rbeta(2000,2,2) ## two shape parameters\nhist(X3, breaks=20, border=NA, main=NA, freq=F)\n\n#See the underlying probabilities\n#f_25 &lt;- dbeta(.25, 2, 2)\n\nx &lt;- seq(0,1,by=.01)\nfx &lt;- dbeta(x, 2, 2)\nlines(x, fx, col='blue')\n\n\n\n\n\n\n\n\n\nThe beta distribution is mathematically complicated to write, and so we omit it. But it is often used, as the density function has two parameters that allow it to take many different shapes.\n\n\nCode\nop &lt;- par(no.readonly = TRUE); on.exit(par(op), add = TRUE)\nx &lt;- seq(0,1,by=.01)\npars &lt;- expand.grid( c(.5,1,2), c(.5,1,2) )\npar(mfrow=c(3,3))\napply(pars, 1, function(p){\n    fx &lt;- dbeta( x,p[1], p[2])\n    plot(x, fx, type='l', xlim=c(0,1), ylim=c(0,4), lwd=2, col='blue')\n    #hist(rbeta(2000, p[1], p[2]), breaks=50, border=NA, main=NA, freq=F)\n})\ntitle('Beta densities', outer=T, line=-1)\n\n\n\n\n\n\n\n\n\n\n\nExponential.\nAny positive number, \\(X_{i} \\in [0,\\infty)\\).3 The exponential distribution has a single parameter, \\(\\lambda&gt;0\\), that governs its shape \\[\\begin{eqnarray}\nf(x) = \\lambda exp\\left\\{ -\\lambda x \\right\\}\n\\end{eqnarray}\\]\n\n\nCode\nrexp(3) # 3 draws\n## [1] 1.576255 1.541077 1.584803\n\nX3 &lt;- rexp(2000)\nhist(X3, breaks=20,\n    border=NA, main=NA,\n    freq=F, ylim=c(0,1), xlim=c(0,10))\n    \nx &lt;- seq(0,10,by=.1)\nfx &lt;- dexp(x)\nlines(x, fx, col='blue')\n\n\n\n\n\n\n\n\n\n\n\nNormal (Gaussian).\nAny number on the real line, \\(X_{i} \\in (\\infty,\\infty)\\), with bell shaped probabilities. The distribution is mathematically complex, but we will encounter it again and again and again. The density function \\(f\\) has two parameters \\(\\mu \\in (\\infty,\\infty)\\) and \\(\\sigma &gt; 0\\) \\[\\begin{eqnarray}\nf(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} exp\\left\\{ \\frac{-(x-\\mu)^2}{2\\sigma^2} \\right\\}\n\\end{eqnarray}\\]\n\n\nCode\nrnorm(3) # 3 draws\n## [1] 0.03567944 0.34690792 1.81718121\n\nX4 &lt;- rnorm(2000)\nhist(X4, breaks=20,\n    border=NA, main=NA,\n    freq=F, ylim=c(0,.4), xlim=c(-4,4))\n\nx &lt;- seq(-10,10,by=.025)\nfx &lt;- dnorm(x)\nlines(x, fx, col='blue')\n\n\n\n\n\n\n\n\n\nEven thought the distribution function is complex, we can compute CDF values using the computer\n\n\nCode\npnorm( c(-1.645, 1.645) )\n## [1] 0.04998491 0.95001509\npnorm( c(-2.326, 2.326) )\n## [1] 0.01000928 0.98999072",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "01_04_RandomVariables.html#drawing-samples",
    "href": "01_04_RandomVariables.html#drawing-samples",
    "title": "4  Random Variables",
    "section": "4.3 Drawing Samples",
    "text": "4.3 Drawing Samples\nTo generate a random variable from known distributions, you can use some type of physical machine. E.g., you can roll a fair die to generate Discrete Uniform data or you can roll weighted die to generate Categorical data.\nThere are also several ways to computationally generate random variables from a probability distribution. Perhaps the most common one is “inverse sampling”.\nRandom variables have an associated quantile function, which is the inverse of the CDF: the \\(x\\) value where \\(p\\) percent of the data fall below it. \\[\\begin{eqnarray}\nQ(p) = F^{-1}(p), \\quad p\\in [0,1]\n\\end{eqnarray}\\] (Recall that the median is the value \\(x\\) where \\(50\\%\\) of the data fall below \\(x\\), for example.) To generate a random variable using inverse sampling, first sample \\(p\\) from a uniform distribution and then find the associated quantile.4\n\nUsing Data.\nYou can generate a random variable from a known empirical distribution. Inverse sampling randomly selects observations from the dataset with equal probabilities. To implement this, we\n\norder the data and associate each observation with an ECDF value\ndraw an ECDF probability \\(p\\) as a random variable with equal probabilities\nfinding the associated data point\n\n\n\nCode\n# Empirical Distribution\nX &lt;- USArrests$Murder\nFX_hat &lt;- ecdf(X)\nplot(FX_hat, lwd=2, xlim=c(0,20),\n    pch=16, col=grey(0,.5), main='')\n\n# Two Examples of generating a random variable\np &lt;- c(.25, .9) # pretended to be random\ncols &lt;- c(2,4)\nQX_hat &lt;- quantile(X, p, type=1)\nsegments(QX_hat, p, -10, p, col=cols)\nsegments(QX_hat, p, QX_hat, 0, col=cols)\nmtext( round(QX_hat,2), 1, at=QX_hat, col=cols)\n\n\n\n\n\n\n\n\n\nCode\n\n# Multiple Draws\np &lt;- runif(3000)\nQX_hat &lt;- quantile(x, p,type=1)\nQX_hat[1:5]\n## 39.80391000% 43.39977696%  6.04136374% 96.69610378% 54.72484343% \n##       -2.050       -1.325       -8.800        9.350        0.950\n\n\n\n\nUsing Math.\nIf you know the distribution function that generates the data, then you can derive the quantile function and do inverse sampling. Here is an in-depth example of the Dagum distribution. The distribution function is \\(F(x)=(1+(x/b)^{-a})^{-c}\\). For a given \\(p=F(x)\\), we can then solve for the quantile \\(Q(p)=\\frac{ b p^{\\frac{1}{ac}} }{(1-p^{1/c})^{1/a}}\\). Afterwhich, we sample \\(p\\) from a uniform distribution and then find the associated quantile.\n\n\nCode\n# Theoretical Quantile Function (from VGAM::qdagum)\nqdagum &lt;- function(p, scale.b=1, shape1.a, shape2.c) {\n  # Quantile function (theoretically derived from the CDF)\n  ans &lt;- scale.b * (expm1(-log(p) / shape2.c))^(-1 / shape1.a)\n  # Special known cases\n  ans[p == 0] &lt;- 0\n  ans[p == 1] &lt;- Inf\n  # Safety Checks\n  ans[p &lt; 0] &lt;- NaN\n  ans[p &gt; 1] &lt;- NaN\n  if(scale.b &lt;= 0 | shape1.a &lt;= 0 | shape2.c &lt;= 0){ ans &lt;- ans*NaN }\n  # Return\n  return(ans)\n}\n\n# Generate Random Variables (VGAM::rdagum)\nrdagum &lt;-function(n, scale.b=1, shape1.a, shape2.c){\n    p &lt;- runif(n) # generate random probabilities\n    x &lt;- qdagum(p, scale.b=scale.b, shape1.a=shape1.a, shape2.c=shape2.c) #find the inverses\n    return(x)\n}\n\n# Example\nset.seed(123)\nX &lt;- rdagum(3000,1,3,1)\nX[1:5]\n## [1] 0.7390476 1.5499868 0.8845006 1.9616251 2.5091656",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "01_04_RandomVariables.html#footnotes",
    "href": "01_04_RandomVariables.html#footnotes",
    "title": "4  Random Variables",
    "section": "",
    "text": "Alternatively, you can think of \\(F(x)\\) as the ECDF for a dataset with an infinite number of observations.↩︎\nThis is the general formula using CDFs, and you can verify it works in this instance by directly adding the probability of each 2 or 3 event: \\(Prob(X_{i} = 2) +  Prob(X_{i} = 3) = 1/4 + 1/4 = 2/4\\).↩︎\nIn other classes, you may further distinguish types of random variables based on whether their maximum value is theoretically finite or infinite.↩︎\nDrawing random uniform samples with computers is actually quite complex and beyond the scope of this course.↩︎",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html",
    "href": "01_05_Statistics.html",
    "title": "5  Statistics",
    "section": "",
    "text": "5.1 Mean and Variance\nWe often summarize distributions with statistics: functions of data. The most basic way to do this is with summary, whose values can all be calculated individually. (E.g., the “mean” computes the [sum of all values] divided by [number of values].) There are many other statistics.\nThe most basic statistics summarize the center of a distribution and how far apart the values are spread.",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html#mean-and-variance",
    "href": "01_05_Statistics.html#mean-and-variance",
    "title": "5  Statistics",
    "section": "",
    "text": "Mean.\nPerhaps the most common statistic is the mean; \\[\\overline{X}=\\frac{\\sum_{i=1}^{N}X_{i}}{N},\\] where \\(X_{i}\\) denotes the value of the \\(i\\)th observation.\n\n\nCode\n# compute the mean of a random sample\nX &lt;- runif(100)\nhist(X, border=NA, main=NA)\nx_bar &lt;- mean(X)  #sum(x)/length(x)\nabline(v=x_bar, col=2, lwd=2)\ntitle(paste0('mean= ', round(x_bar,2)), font.main=1)\n\n\n\n\n\n\n\n\n\n\n\nVariance.\nPerhaps the second most common statistic is the variance: the average squared deviation from the mean \\[V_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\overline{X}]^2}{N}.\\] The standard deviation is simply \\(s_{X} = \\sqrt{V_{X}}\\).\n\n\nCode\ns &lt;- sd(X) # sqrt(var(X))\nhist(X, border=NA, main=NA, freq=F)\ns_lh &lt;- c(x_bar - s,  x_bar + s)\nabline(v=s_lh, col=4)\ntext(s_lh, -.02,\n    c( expression(bar(X)-s[X]), expression(bar(X)+s[X])),\n    col=4, adj=0)\ntitle(paste0('sd= ', round(s,2)), font.main=1)\n\n\n\n\n\n\n\n\n\nNote that a “corrected version” is used by R and many statisticians: \\(V_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\overline{X}]^2}{N-1}\\).\n\n\nCode\nvar(X)\n## [1] 0.08416629\nx_bar &lt;- mean(X)\nmean( (X - x_bar)^2 )\n## [1] 0.08332462\n\n\nTogether, these statistics summarize the central tendency and dispersion of a distribution. In some special cases, such as with the normal distribution, they completely describe the distribution. Other distributions are easier to describe with other statistics.",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html#other-centerspread-statistics",
    "href": "01_05_Statistics.html#other-centerspread-statistics",
    "title": "5  Statistics",
    "section": "5.2 Other Center/Spread Statistics",
    "text": "5.2 Other Center/Spread Statistics\n\nAbsolute Deviations.\nWe can use the Median as a “robust alternative” to means. Recall that the \\(q\\)th quantile is the value where \\(q\\) percent of the data are below and (\\(1-q\\)) percent are above. The median (\\(q=.5\\)) is the point where half of the data is lower values and the other half is higher. This means that median is not sensitive to extreme values (whereas the mean is).\n\n\nCode\nX &lt;- rgeom(50, .4)\nX\n##  [1] 0 2 4 0 0 5 0 1 1 0 0 0 0 2 4 0 2 1 0 0 0 7 1 1 1 1 7 1 4 0 7 1 2 2 2 1 0 0\n## [39] 1 0 1 5 1 3 1 1 0 1 0 0\n\nproportions &lt;- table(X)/length(X)\nplot(proportions, ylab='proportion')\n\n\n\n\n\n\n\n\n\nCode\n\n# measures of central tendency\nmean(X)\n## [1] 1.48\nmedian(X)\n## [1] 1\n\n# robustness to an extreme value\nX_extreme &lt;- c(X, 1000)\nmean( X_extreme )\n## [1] 21.05882\nmedian( X_extreme )\n## [1] 1\n\n\nWe can also use the Interquartile Range or Median Absolute Deviation as an alternative to variance. The first and third quartiles (\\(q=.25\\) and \\(q=.75\\)) together measure is the middle 50 percent of the data. The size of that range (interquartile range: the difference between the quartiles) represents “spread” or “dispersion” of the data. The median absolute deviation also measures spread \\[\\begin{eqnarray}\n\\tilde{X} &=& Med(X_{i}) \\\\\nMAD_{X} &=& Med\\left( | X_{i} - \\tilde{X} | \\right).\n\\end{eqnarray}\\]\n\n\nCode\n#sd(X)\nmad(X, constant=1) # median( abs(X - median(X)) )\n## [1] 1\n\n#sd(X) # another alternative\n#IQR(X) # diff( quantile(x, probs=c(.25,.75)))\n\n\nNote that there other absolute deviations:\n\n\nCode\nmean( abs(X - mean(X)) )\nmean( abs(X - median(X)) )\nmedian( abs(X - mean(X)) )\n\n\n\n\nMode and Share Concentration.\nSometimes, none of the above work well. With categorical data, for example, distributions are easier to describe with other statistics. The mode is the most common observation: the value with the highest observed frequency. We can also measure the spread of the frequencies or concentration at the mode vs elsewhere.\n\n\nCode\n# Draw 3 Random Letters\nK &lt;- length(LETTERS)\nX_id &lt;- rmultinom(3, 1, prob=rep(1/K,K))\nX_id\n##       [,1] [,2] [,3]\n##  [1,]    0    0    0\n##  [2,]    0    0    0\n##  [3,]    0    0    0\n##  [4,]    0    0    0\n##  [5,]    0    0    0\n##  [6,]    0    0    0\n##  [7,]    0    0    1\n##  [8,]    0    0    0\n##  [9,]    0    0    0\n## [10,]    0    0    0\n## [11,]    0    0    0\n## [12,]    0    0    0\n## [13,]    1    0    0\n## [14,]    0    0    0\n## [15,]    0    0    0\n## [16,]    0    0    0\n## [17,]    0    0    0\n## [18,]    0    0    0\n## [19,]    0    0    0\n## [20,]    0    1    0\n## [21,]    0    0    0\n## [22,]    0    0    0\n## [23,]    0    0    0\n## [24,]    0    0    0\n## [25,]    0    0    0\n## [26,]    0    0    0\n\n# Draw Random Letters 100 Times\nX_id &lt;- rowSums(rmultinom(100, 1, prob=rep(1/K,K)))\nX &lt;- lapply(1:K, function(k){\n    rep(LETTERS[k], X_id[k])\n})\nX &lt;- factor(unlist(X), levels=LETTERS)\n\nproportions &lt;- table(X)/length(X)\nplot(proportions)\n\n\n\n\n\n\n\n\n\nCode\n\n# mode(s)\nnames(proportions)[proportions==max(proportions)]\n## [1] \"E\"\n\n# freq. spreads\nsd(proportions)\n## [1] 0.02166884\nsum(proportions^2)\n## [1] 0.0502\n# freq. concentration at mode\nmax(proportions)/mean(proportions)\n## [1] 2.6",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html#shape-statistics",
    "href": "01_05_Statistics.html#shape-statistics",
    "title": "5  Statistics",
    "section": "5.3 Shape Statistics",
    "text": "5.3 Shape Statistics\nCentral tendency and dispersion are often insufficient to describe a distribution. To further describe shape, we can compute the “standard moments” skew and kurtosis, as well as other statistics.\n\nSkewness.\nThis captures how symmetric the distribution is. \\[W_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\overline{X}]^3 / N}{ [s_{X}]^3 }\\]\n\n\nCode\nX &lt;- rweibull(1000, shape=1)\nhist(X, border=NA, main=NA, freq=F, breaks=20)\n\n\n\n\n\n\n\n\n\nCode\n\nskewness &lt;-  function(X){\n X_bar &lt;- mean(X)\n m3 &lt;- mean((X - X_bar)^3)\n s3 &lt;- sd(X)^3\n skew &lt;- m3/s3\n return(skew)\n}\n\nskewness( rweibull(1000, shape=1))\n## [1] 2.06796\nskewness( rweibull(1000, shape=10) )\n## [1] -0.5995069\n\n\n\n\nKurtosis.\nThis captures how many “outliers” there are. \\[K_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\overline{X}]^4 / N}{ [s_{X}]^4 }.\\]\n\n\nCode\nX &lt;- rweibull(1000, shape=1)\nboxplot(X, main=NA)\n\n\n\n\n\n\n\n\n\nCode\n\nkurtosis &lt;- function(X){  \n X_bar &lt;- mean(X)\n m4 &lt;- mean((X - X_bar)^4) \n s4 &lt;- sd(X)^4\n kurt &lt;- m4/s4 - 3\n return(kurt)\n}\n\nkurtosis( rweibull(1000, shape=1))\n## [1] 10.15072\nkurtosis( rweibull(1000, shape=10) )\n## [1] 0.4116621\n\n\n\n\nClusters/Gaps.\nYou can also describe distributions in terms of how clustered the values are. Remember: a picture is worth a thousand words.\n\n\nCode\n# Number of Modes\nX &lt;- rbeta(1000, .6, .6)\nhist(X, border=NA, main=NA, freq=F, breaks=20)\n\n\n\n\n\n\n\n\n\nCode\n\n# Random Number Generator \nr_ugly1 &lt;- function(n, theta1=c(-8,-1), theta2=c(-2,2), rho=.25){\n    omega   &lt;- rbinom(n, size=1, rho)\n    epsilon &lt;- omega * runif(n, theta1[1], theta2[1]) +\n        (1-omega) * rnorm(n, theta1[2], theta2[2])\n    return(epsilon)\n}\n# Large Sample\npar(mfrow=c(1,1))\nx &lt;- seq(-12,6,by=.001)\nrX &lt;- r_ugly1(1000000)\nhist(rX, breaks=1000,  freq=F, border=NA,\n    xlab=\"x\", main='')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Show True Density\nd_ugly1 &lt;- function(x, theta1=c(-8,-1), theta2=c(-2,2), rho=.25){\n    rho     * dunif(x, theta1[1], theta2[1]) +\n    (1-rho) * dnorm(x, theta1[2], theta2[2]) }\ndx &lt;- d_ugly1(x)\nlines(x, dx, col=1)",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html#probability-theory",
    "href": "01_05_Statistics.html#probability-theory",
    "title": "5  Statistics",
    "section": "5.4 Probability Theory",
    "text": "5.4 Probability Theory\nYou were already introduced to this with https://jadamso.github.io/Rbooks/random-variables.html and probability distributions. In this section, we will dig a little deeper theoretically into the statistics we are most likely to use in practice.\nThe mean and variance are probably the two most basic statistics we might compute, and are often used. To understand them theoretically, we separately analyze how they are computed for discrete and continuous random variables.\n\nDiscrete Random Variables.\nIf the sample space is discrete, we can compute the theoretical mean (or expected value) as \\[\n\\mathbb{E}[X_{i}] = \\sum_{x} x Prob(X_{i}=x),\n\\] where \\(Prob(X_{i}=x)\\) is the probability the random variable \\(X_{i}\\) takes the particular value \\(x\\). Similarly, we can compute the theoretical variance as \\[\n\\mathbb{V}[X_{i}] = \\sum_{x} \\left(x - \\mathbb{E}[X_{i}] \\right)^2 Prob(X_{i}=x),\n\\]\nFor example, consider an unfair coin with a \\(.75\\) probability of heads (\\(x=1\\)) and a \\(.25\\) probability of tails (\\(x=0\\)) has a theoretical mean of \\[\n\\mathbb{E}[X_{i}] = 1\\times.75 + 0 \\times .25 = .75\n\\] and a theoretical variance of \\[\n\\mathbb{V}[X_{i}] = [1 - .75]^2 \\times.75 + [0 - .75]^2 \\times.25 = 0.1875\n\\]\n\n\nCode\nX &lt;- rbinom(10000, size=1, prob=.75)\n\nround( mean(X), 4)\n## [1] 0.741\n\nround( var(X), 4)\n## [1] 0.1919\n\n\n\n\nWeighted Data Examples.\nSometimes, you may have a dataset of values and probability weights. Othertimes, you can calculate them yourself. In either case, you can explicitly do the computations for discrete data\n\n\nCode\n# Compute probability weights for unique values\nh  &lt;- table(X) #table of counts\nwt &lt;- c(h)/length(X) #probabilities (must sum to 1)\nxt &lt;- as.numeric(names(h)) #values\n# Weighted Mean\nX_mean &lt;- sum(wt*xt)\nX_mean\n## [1] 0.741\n\n\nTry computing the mean both ways for another random sample\n\n\nCode\nX  &lt;-  sample(c(0,1,2), 1000, replace=T)\n\n#Try also computing a weighted variance\n# X_var &lt;- sum(wt * (X - x_mean)^2)/sum(wt)\n\n\nYou can again explicitly do the computations with weighted data, but here we have an additional approximation error\n\n\nCode\n# values and probabilities\nh  &lt;- hist(X, plot=F)\nwt &lt;- h[['counts']]/length(x) \nxt &lt;- h[['mids']]\n# Weighted mean\nX_mean &lt;- sum(wt*xt)\nX_mean\n## [1] 0.05599689\n\n# Compare to \"mean(x)\"\n\n\n\n\nContinuous Random Variables (Advanced).\nIf the sample space is continuous, we can compute the theoretical mean (or expected value) as \\[\n\\mathbb{E}[X] = \\int x f(x) d x,\n\\] where \\(f(x)\\) is the probability the random variable takes the particular value \\(x\\). Similarly, we can compute the theoretical variance as \\[\n\\mathbb{V}[X_{i}]= \\int \\left(x - \\mathbb{E}[X_{i}] \\right)^2 f(x) d x,\n\\]\nFor example, consider a random variable with a continuous uniform distribution over [-1, 1]. In this case, \\(f(x)=1/[1 - (-1)]=1/2\\) for each \\(x\\) in [-1, 1] and \\[\n\\mathbb{E}[X_{i}] = \\int_{-1}^{1} \\frac{x}{2} d x = \\int_{-1}^{0} \\frac{x}{2} d x + \\int_{0}^{1} \\frac{x}{2} d x = 0\n\\] and \\[\n\\mathbb{V}[X_{i}]= \\int_{-1}^{1} x^2 \\frac{1}{2} d x = \\frac{1}{2} \\frac{x^3}{3}|_{-1}^{1} = \\frac{1}{6}[1 - (-1)] = 2/6 =1/3\n\\]\n\n\nCode\nX &lt;- runif(10000, -1,1)\nround( mean(X), 4)\n## [1] 0.003\nround( var(X), 4)\n## [1] 0.3348",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html#further-reading",
    "href": "01_05_Statistics.html#further-reading",
    "title": "5  Statistics",
    "section": "5.5 Further Reading",
    "text": "5.5 Further Reading\nProbability Theory\n\n[Refresher] https://www.khanacademy.org/math/statistics-probability/probability-library/basic-theoretical-probability/a/probability-the-basics\nhttps://book.stat420.org/probability-and-statistics-in-r.html\nhttps://bookdown.org/speegled/foundations-of-statistics/\nhttps://math.dartmouth.edu/~prob/prob/prob.pdf\nhttps://bookdown.org/probability/beta/discrete-random-variables.html\nhttps://www.econometrics-with-r.org/2.1-random-variables-and-probability-distributions.html\nhttps://probability4datascience.com/ch02.html\nhttps://statsthinking21.github.io/statsthinking21-R-site/probability-in-r-with-lucy-king.html\nhttps://bookdown.org/probability/statistics/\nhttps://www.atmos.albany.edu/facstaff/timm/ATM315spring14/R/IPSUR.pdf\nhttps://rc2e.com/probability\nhttps://bookdown.org/probability/beta/\nhttps://bookdown.org/a_shaker/STM1001_Topic_3/\nhttps://bookdown.org/fsancier/bookdown-demo/\nhttps://bookdown.org/kevin_davisross/probsim-book/\nhttps://bookdown.org/machar1991/ITER/2-pt.html\nhttps://www.atmos.albany.edu/facstaff/timm/ATM315spring14/R/IPSUR.pdf\nhttps://math.dartmouth.edu/~prob/prob/prob.pdf\n\nFor weighted statistics, see\n\nhttps://seismo.berkeley.edu/~kirchner/Toolkits/Toolkit_12.pdf\nhttps://www.bookdown.org/rwnahhas/RMPH/survey-desc.html\n\nNote that many random variables are related to each other\n\nhttps://en.wikipedia.org/wiki/Relationships_among_probability_distributions\nhttps://www.math.wm.edu/~leemis/chart/UDR/UDR.html\nhttps://qiangbo-workspace.oss-cn-shanghai.aliyuncs.com/2018-11-11-common-probability-distributions/distab.pdf\n\nAlso note that numbers randomly generated on your computer cannot be truly random, they are “Pseudorandom”.",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html",
    "href": "01_06_Sampling.html",
    "title": "6  (Re)Sampling",
    "section": "",
    "text": "6.1 Sample Distributions\nThe sampling distribution of a statistic shows us how much a statistic varies from sample to sample.\nFor example, see how the mean varies from sample to sample to sample.\nCode\n# Three Sample Example\npar(mfrow=c(1,3))\nfor(i in 1:3){\n    x &lt;- runif(100) \n    m &lt;-  mean(x)\n    hist(x,\n        breaks=seq(0,1,by=.1), #for comparability\n        main=NA, border=NA)\n    abline(v=m, col=2, lwd=2)\n    title(paste0('mean= ', round(m,2)),  font.main=1)\n}\nExamine the sampling distribution of the mean\nCode\nsample_means &lt;- sapply(1:1000, function(i){\n    m &lt;- mean(runif(100))\n    return(m)\n})\nhist(sample_means, breaks=50, border=NA,\n    col=2, font.main=1,\n    main='Sampling Distribution of the mean')\nIn this figure, you see two the most profound results known in statistics",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html#sample-distributions",
    "href": "01_06_Sampling.html#sample-distributions",
    "title": "6  (Re)Sampling",
    "section": "",
    "text": "Law of Large Numbers: the sample mean is centered around the true mean.\nCentral Limit Theorem: the sampling distribution of the mean is approximately standard normal.\n\n\nLaw of Large Numbers.\nThe Law of Large Numbers follows from Linearity of Expectations: the expected value of a sum of random variables is the sum of their individual expected values.\n\\[\\begin{eqnarray}\n\\mathbb{E}[X_{1}+X_{2}] &=&\\mathbb{E}[X_{1}]+\\mathbb{E}[X_{2}]\\\\\n\\mathbb{E}\\left[\\bar{X}\\right] = \\mathbb{E}\\left[ \\sum_{i=1}^{N} X_{i}/N \\right] &=& \\sum_{i=1}^{N} \\mathbb{E}[X_{i}]/N.\n\\end{eqnarray}\\]\nAssuming each data point has identical means; \\(\\mathbb{E}[X_{i}]=\\mu\\), the expected value of the sample average is the mean; \\(\\mathbb{E}\\left[\\bar{X}\\right] = \\sum_{i=1}^{N} \\mu/N = \\mu\\).\nSee https://dlsun.github.io/probability/linearity.html\n\n\nCentral Limit Theorem.\nThere are actually many different variants of the central limit theorem, as it applies more generally: the sampling distribution of many statistics are standard normal. For example, examine the sampling distribution of the standard deviation.\n\n\nCode\nthree_sds &lt;- c(  sd(runif(100)),  sd(runif(100)),  sd(runif(100))  )\nthree_sds\n## [1] 0.3200469 0.2775849 0.3002484\n\nsample_sds &lt;- sapply(1:1000, function(i){\n    s &lt;- sd(runif(100))\n    return(s)\n})\nhist(sample_sds, breaks=50, border=NA,\n    col=4, font.main=1,\n    main='Sampling Distribution of the sd')\n\n\n\n\n\n\n\n\n\nIt is beyond this class to prove this result mathematically, but you should know that not all sampling distributions are standard normal. For example, examine which sampling distribution of the three main “order statistics” looks normal.\n\n\nCode\n# Create 300 samples, each with 1000 random uniform variables\nx &lt;- sapply(1:300, function(i) runif(1000) )\n# Each row is a new sample\nlength(x[1,])\n## [1] 300\n\n# Median looks normal, Maximum and Minumum do not!\nxmin &lt;- apply(x,1,quantile, probs=0)\nxmed &lt;- apply(x,1,quantile, probs=.5)\nxmax &lt;- apply(x,1,quantile, probs=1)\npar(mfrow=c(1,3))\nhist(xmin, breaks=100, border=NA, main='Min', font.main=1)\nhist(xmed, breaks=100, border=NA, main='Med', font.main=1)\nhist(xmax, breaks=100, border=NA, main='Max', font.main=1)\ntitle('Sampling Distributions', outer=T, line=-1)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# To explore, try any function!\nfun_of_rv &lt;- function(f, n=100){\n  x &lt;- runif(n)\n  y &lt;- f(x)\n  return(y)\n}\n\nfun_of_rv( f=mean )\n## [1] 0.5040264\n\nfun_of_rv( f=function(i){ diff(range(exp(i))) } )\n## [1] 1.667346",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html#resampling",
    "href": "01_06_Sampling.html#resampling",
    "title": "6  (Re)Sampling",
    "section": "6.2 Resampling",
    "text": "6.2 Resampling\nOften, we only have one sample. How then can we estimate the sampling distribution of a statistic?\n\n\nCode\nsample_dat &lt;- USArrests[,'Murder']\nmean(sample_dat)\n## [1] 7.788\n\n\nWe can “resample” our data. Hesterberg (2015) provides a nice illustration of the idea. The two most basic versions are the jackknife and the bootstrap, which are discussed below.\n\n\n\n\n\n\n\n\n\nNote that we do not use the mean of the resampled statistics as a replacement for the original estimate. This is because the resampled distributions are centered at the observed statistic, not the population parameter. (The bootstrapped mean is centered at the sample mean, for example, not the population mean.) This means that we cannot use resampling to improve on \\(\\overline{x}\\). We use resampling to estimate sampling variability.\n\nJackknife Distribution.\nHere, we compute all “leave-one-out” estimates. Specifically, for a dataset with \\(n\\) observations, the jackknife uses \\(n-1\\) observations other than \\(i\\) for each unique subsample. Taking the mean, for example, we have\n\n\nCode\nsample_dat &lt;- USArrests[,'Murder']\nsample_mean &lt;- mean(sample_dat)\n\n# Jackknife Estimates\nn &lt;- length(sample_dat)\nJmeans &lt;- sapply(1:n, function(i){\n    dati &lt;- sample_dat[-i]\n    mean(dati)\n})\nhist(Jmeans, breaks=25, border=NA,\n    main='', xlab=expression(bar(X)[-i]))\nabline(v=sample_mean, col='red', lty=2)\n\n\n\n\n\n\n\n\n\n\n\nBootstrap Distribution.\nHere, we draw \\(n\\) observations with replacement from the original data to create a bootstrap sample and calculate a statistic. Each bootstrap sample \\(b=1...B\\) uses a random set of observations (denoted \\(N_{b}\\)) to compute a statistic. We repeat that many times, say \\(B=9999\\), to estimate the sampling distribution. Consider the sample mean as an example;\n\n\nCode\n# Bootstrap estimates\nset.seed(2)\nBmeans &lt;- sapply(1:10^4, function(i) {\n    dat_b &lt;- sample(sample_dat, replace=T) # c.f. jackknife\n    mean(dat_b)\n})\n\nhist(Bmeans, breaks=25, border=NA,\n    main='', xlab=expression(bar(X)[b]))\nabline(v=sample_mean, col='red', lty=2)\n\n\n\n\n\n\n\n\n\nNote that both resampling methods provide imperfect estimates, and can give different numbers. Percentiles of jackknife resamples are systematically less variable than they should be. Until you know more, a conservative approach is to take the larger estimate.\n\n\nCode\n# Boot CI\nboot_ci &lt;- quantile(Bmeans, probs=c(.025, .975))\nboot_ci\n##    2.5%   97.5% \n## 6.58200 8.97005\n\n# Jack CI\njack_ci &lt;- quantile(Jmeans, probs=c(.025, .975))\njack_ci\n##     2.5%    97.5% \n## 7.621582 7.904082\n\n# more conservative estimate\nci_est &lt;- boot_ci",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html#intervals",
    "href": "01_06_Sampling.html#intervals",
    "title": "6  (Re)Sampling",
    "section": "6.3 Intervals",
    "text": "6.3 Intervals\nUsing either the bootstrap or jackknife distribution for subsamples, or across multiple samples if we can get them, we can calculate\n\nConfidence Interval: range your statistic varies across different samples.\nStandard Error: variance of your statistic across different samples (square rooted).\n\n\n\nCode\nsample_means &lt;- apply(x,1,mean)\n# standard error\nsd(sample_means)\n## [1] 0.0162792\n\n\nNote that in some cases, you can estimate the standard error to get a confidence interval.\n\n\nCode\nx00 &lt;- x[1,]\n# standard error\ns00 &lt;- sd(x00)/sqrt(length(x00))\nci &lt;- mean(x00) + c(1.96, -1.96)*s00\n\n\n\nConfidence Interval.\nCompute the upper and lower quantiles of the sampling distribution.\nFor example, consider the sample mean. We simulate the sampling distribution of the sample mean and construct a 90% confidence interval by taking the 5th and 95th percentiles of the simulated means. This gives an empirical estimate of the interval within which the true mean is expected to lie with 90% confidence, assuming repeated sampling.\n\n\nCode\n# Middle 90%\nmq &lt;- quantile(sample_means, probs=c(.05,.95))\npaste0('we are 90% confident that the mean is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\n## [1] \"we are 90% confident that the mean is between 0.47 and 0.53\"\n\nbks &lt;- seq(.4,.6,by=.001)\nhist(sample_means, breaks=bks, border=NA,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='Confidence Interval for the mean')\nabline(v=mq)\n\n\n\n\n\n\n\n\n\nFor another example, consider an extreme sample percentile. We now repeat the above process to estimate the 99th percentile for each sample, instead of the mean for each sample. We then construct a 95% confidence interval for the 99th percentile estimator, using the 2.5th and 97.5th quantiles of these estimates.\n\n\nCode\n## Upper Percentile\nsample_quants &lt;- apply(x,1,quantile, probs=.99)\n\n# Middle 95% of estimates\nmq &lt;- quantile(sample_quants, probs=c(.025,.975))\npaste0('we are 95% confident that the upper percentile is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\n## [1] \"we are 95% confident that the upper percentile is between 0.97 and 1\"\n\nbks &lt;- seq(.92,1,by=.001)\nhist(sample_quants, breaks=bks, border=NA,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='95% Confidence Interval for the 99% percentile')\nabline(v=mq)\n\n\n\n\n\n\n\n\n\nNote that \\(X%\\) confidence intervals do not generally cover \\(X%\\) of the data. Those intervals are a type of prediction interval that is covered later. See also https://online.stat.psu.edu/stat200/lesson/4/4.4/4.4.2.\n\n\nAdvanced Intervals.\nIn many cases, we want a X% interval to mean that X% of the intervals we generate will contain the true mean. E.g., in repeated sampling, 50% of constructed confidence intervals are expected to contain the true population mean.\n\n\nCode\n# Theoretically: [-1 sd, +1 sd] has 2/3 coverage\n\n# Confidence Interval for each sample\nxq &lt;- apply(x,1, function(r){ #theoretical se's \n    mean(r) + c(-1,1)*sd(r)/sqrt(length(r))\n})\n# First 4 interval estimates\nxq[,1:4]\n##          [,1]      [,2]      [,3]      [,4]\n## [1,] 0.484375 0.4978603 0.5036879 0.4947738\n## [2,] 0.517120 0.5306726 0.5370112 0.5282508\n\n# Explicit calculation\nmu_true &lt;- 0.5\n# Logical vector: whether the true mean is in each CI\ncovered &lt;- mu_true &gt;= xq[1, ] & mu_true &lt;= xq[2, ]\n# Empirical coverage rate\ncoverage_rate &lt;- mean(covered)\ncat(sprintf(\"Estimated coverage probability: %.2f%%\\n\", 100 * coverage_rate))\n## Estimated coverage probability: 69.60%\n\n\n\n\nCode\n# Visualize first N confidence intervals\nN &lt;- 100\nplot.new()\nplot.window(xlim = range(xq), ylim = c(0, N))\nfor (i in 1:N) {\n  col_i &lt;- if (covered[i]) rgb(0, 0, 0, 0.3) else rgb(1, 0, 0, 0.5)\n  segments(xq[1, i], i, xq[2, i], i, col = col_i, lwd = 2)\n}\nabline(v = mu_true, col = \"blue\", lwd = 2)\naxis(1)\ntitle(\"Visualizing CI Coverage (Red = Missed)\")\n\n\n\n\n\n\n\n\n\nThis differs from a pointwise inclusion frequency interval\n\n\nCode\n# Frequency each point was in an interval\nbks &lt;- seq(0,1,by=.01)\nxcovr &lt;- sapply(bks, function(b){\n    bl &lt;- b &gt;= xq[1,]\n    bu &lt;- b &lt;= xq[2,]\n    mean( bl & bu )\n})\n# 50\\% Coverage\nc_ul &lt;- range(bks[xcovr&gt;=.5])\nc_ul # 50% confidence interval\n## [1] 0.49 0.51\n\nplot.new()\nplot.window(xlim=c(0,1), ylim=c(0,1))\npolygon( c(bks, rev(bks)), c(xcovr, xcovr*0), col=grey(.5,.5), border=NA)\nmtext('Frequency each value was in an interval',2, line=2.5)\naxis(1)\naxis(2)\nabline(h=.5, lwd=2)\nsegments(c_ul,0,c_ul,.5, lty=2)",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html#standard-errors",
    "href": "01_06_Sampling.html#standard-errors",
    "title": "6  (Re)Sampling",
    "section": "6.4 Standard Errors",
    "text": "6.4 Standard Errors\nNote that the standard deviation refers to variance within a single sample, and is hence different from the standard error. Nonetheless, they can both be used to estimate the variability of a statistic.\n\n\nCode\nboot_se &lt;- sd(Bmeans)\n\nsample_sd &lt;- sd(sample_dat)\ntheory_se &lt;- sample_sd/sqrt(n)\n\nc(boot_se, theory_se)\n## [1] 0.6056902 0.6159621\n\n\nA famous theoretical result in statistics is that if we have independent and identical data (i.e., that each new \\(X_{i}\\) has the same mean \\(\\mu\\) and same variance \\(\\sigma^2\\) and is drawn without any dependence on the previous draws), then the standard error of the sample mean is “root N” proportional to the theoretical standard error. \\[\\begin{eqnarray}\n\\mathbb{V}\\left( \\bar{X} \\right)\n&=& \\mathbb{V}\\left( \\frac{\\sum_{i}^{n} X_{i}}{n} \\right)\n= \\sum_{i}^{n} \\mathbb{V}\\left(\\frac{X_{i}}{n}\\right)\n= \\sum_{i}^{n} \\frac{\\sigma^2}{n^2}\n= \\sigma^2/n\\\\\n\\mathbb{s}\\left(\\bar{X}\\right) &=& \\sqrt{\\sigma^2/n} = \\sigma/\\sqrt{n}.\n\\end{eqnarray}\\]\nNotice that each additional data point you have provides more information, which ultimately decreases the standard error of your estimates. This is why statisticians will often recommend that you to get more data. However, the improvement in the standard error increases at a diminishing rate. In economics, this is known as diminishing returns and why economists may recommend you do not get more data.\n\n\nCode\nNseq &lt;- seq(1,100, by=1) # Sample sizes\nB &lt;- 1000 # Number of draws per sample\n\nSE &lt;- sapply(Nseq, function(n){\n    sample_statistics &lt;- sapply(1:B, function(b){\n        x &lt;- rnorm(n) # Sample of size N\n        quantile(x,probs=.4) # Statistic\n    })\n    sd(sample_statistics)\n})\n\npar(mfrow=c(1,2))\nplot(Nseq, SE, pch=16, col=grey(0,.5),\n    main='Absolute Gain', font.main=1,\n    ylab='standard error', xlab='sample size')\nplot(Nseq[-1], abs(diff(SE)), pch=16, col=grey(0,.5),\n    main='Marginal Gain', font.main=1,\n    ylab='decrease in standard error', xlab='sample size')",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html#further-reading",
    "href": "01_06_Sampling.html#further-reading",
    "title": "6  (Re)Sampling",
    "section": "6.5 Further Reading",
    "text": "6.5 Further Reading\nSee\n\nhttps://www.r-bloggers.com/2025/02/bootstrap-vs-standard-error-confidence-intervals/",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_07_HypothesisTests.html",
    "href": "01_07_HypothesisTests.html",
    "title": "7  Hypothesis Tests",
    "section": "",
    "text": "7.1 Basic Ideas\nIn this section, we test hypotheses using data-driven methods that assume much less about the data generating process. There are two main ways to conduct a hypothesis test to do so: inverting a confidence interval and imposing the null.",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "01_07_HypothesisTests.html#basic-ideas",
    "href": "01_07_HypothesisTests.html#basic-ideas",
    "title": "7  Hypothesis Tests",
    "section": "",
    "text": "Invert a CI.\nOne main way to conduct hypothesis tests is to examine whether a confidence interval contains a hypothesized value. We then use this decision rule\n\nreject the null if value falls outside of the interval\nfail to reject the null if value falls inside of the interval\n\n\n\nCode\nsample_dat &lt;- USArrests[,'Murder']\nsample_mean &lt;- mean(sample_dat)\n\nn &lt;- length(sample_dat)\nJmeans &lt;- sapply(1:n, function(i){\n    dati &lt;- sample_dat[-i]\n    mean(dati)\n})\nhist(Jmeans, breaks=25,\n    border=NA, xlim=c(7.5,8.1),\n    main='', xlab=expression( bar(X)[-i]))\n# CI\nci_95 &lt;- quantile(Jmeans, probs=c(.025, .975))\nabline(v=ci_95, lwd=2)\n# H0: mean=8\nabline(v=8, col=2, lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nImpose the Null.\nWe can also compute a null distribution: the sampling distribution of the statistic under the null hypothesis (assuming your null hypothesis was true). We use the bootstrap to loop through a large number of “resamples”. In each iteration of the loop, we impose the null hypothesis and re-estimate the statistic of interest. We then calculate the range of the statistic across all resamples and compare how extreme the original value we observed is. We use a 95% confidence interval of the null distribution to create a rejection region.\n\n\nCode\nsample_dat &lt;- USArrests[,'Murder']\nsample_mean &lt;- mean(sample_dat)\n\n# Bootstrap NULL: mean=8\nset.seed(1)\nBmeans0 &lt;- sapply(1:10^4, function(i) {\n    dat_b &lt;- sample(sample_dat, replace=T) \n    mean_b &lt;- mean(dat_b) + (8 - sample_mean) # impose the null by recentering\n    return(mean_b)\n})\nhist(Bmeans0, breaks=25, border=NA,\n    main='', xlab=expression( bar(X)[b]) )\nci_95 &lt;- quantile(Bmeans0, probs=c(.025, .975)) # critical region\nabline(v=ci_95, lwd=2)\nabline(v=sample_mean, lwd=2, col=2)",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "01_07_HypothesisTests.html#default-statistics",
    "href": "01_07_HypothesisTests.html#default-statistics",
    "title": "7  Hypothesis Tests",
    "section": "7.2 Default Statistics",
    "text": "7.2 Default Statistics\n\np-values.\nThis is the frequency you would see something as extreme as your statistic when sampling from the null distribution.\nThere are three associated tests: the two-sided test (observed statistic is extremely high or low) or one of the one-sided tests (observed statistic is extremely low, observed statistic is extremely high). E.g.\n\n\\(HA​: \\bar{X} &gt; 8\\) implies a right tail test\n\\(HA: \\bar{X} &lt; 8\\) implies a left tail test\n\\(HA​: \\bar{X} \\neq 8\\) implies a two tail test\n\nIn any case, typically “p&lt;.05: statistically significant” and “p&gt;.05: not statistically significant”.\nOne sided example\n\n\nCode\n# One-Sided Test, ALTERNATIVE: mean &gt; 8\n# Prob( boot0_means &gt; sample_mean) \nFhat0 &lt;- ecdf(Bmeans0) # Right tail\nplot(Fhat0,\n    xlab=expression( beta[b] ),\n    main='Null Bootstrap Distribution for means', font.main=1)\nabline(v=sample_mean, col='red')\n\n\n\n\n\n\n\n\n\n\n\nCode\np &lt;- 1- Fhat0(sample_mean) #Right Tail\nif(p &gt;.05){\n    message('fail to reject the null that sample_mean=8, at the 5% level')\n} else {\n    message('reject the null that sample_mean=8 in favor of &gt;8, at the 5% level')\n}\n\n\nTwo sided example\n\n\nCode\n# Two-Sided Test, ALTERNATIVE: mean &lt; 8 or mean &gt;8\n# Prob(boot0_means &gt; sample_mean or -boot0_means &lt; sample_mean)\n\nFhat0 &lt;- ecdf(Bmeans0)\np_left &lt;- Fhat0(sample_mean) #Left Tail\np_right &lt;- 1 - Fhat0(sample_mean) #Right Tail\np &lt;- 2*min(p_left, p_right)\n\nif(p &gt;.05){\n    message('fail to reject the null that sample_mean=8 at the 5% level')\n} else {\n    message('reject the null that sample_mean=8 in favor of either &lt;8 or &gt;8 at the 5% level')\n}\n\n\n\n\nt-values.\nA t-value standardizes the statistic you are using for hypothesis testing: \\[ t = (\\hat{\\mu} - \\mu_{0}) / \\hat{s_{\\mu}} \\]\n\n\nCode\njack_se &lt;- sd(Jmeans)\nmean0 &lt;- 8\njack_t &lt;- (sample_mean - mean0)/jack_se\n\n# Note that you can also use a corrected se\n# jack_se &lt;- sqrt((n-1)/n) * sd(Jmeans)\n\n\nThere are several benefits to this:\n\nmakes the statistic comparable across different studies\nmakes the null distribution not depend on theoretical parameters (\\(\\sigma\\))\nmakes the null distribution theoretically known asymptotically (approximately)\n\nThe last point implies we are dealing with a symmetric distributions: \\(Prob( t_{boot} &gt; t ~\\text{or}~ t_{boot} &lt; -t) = Prob( |t| &lt; |t_{boot}| )\\).1\n\n\nCode\nset.seed(1)\nboot_t0 &lt;- sapply(1:10^3, function(i) {\n    dat_b &lt;- sample(sample_dat, replace=T) \n    mean_b &lt;- mean(dat_b) + (8 - sample_mean) # impose the null by recentering\n    # jack ses\n    jack_se_b &lt;- sd( sapply(1:length(dat_b), function(i){\n        mean(dat_b[-i])\n    }) )\n    jack_t &lt;- (mean_b - mean0)/jack_se_b\n})\n\n# Two Sided Test\nFhat0 &lt;- ecdf(abs(boot_t0))\nplot(Fhat0, xlim=range(boot_t0, jack_t),\n    xlab=expression( abs(hat(t)[b]) ),\n    main='Null Bootstrap Distribution for t', font.main=1)\nabline(v=abs(jack_t), col='red')\n\n\n\n\n\n\n\n\n\nCode\np &lt;- 1 - Fhat0( abs(jack_t) ) \np\n## [1] 0.727\n\nif(p &gt;.05){\n    message('fail to reject the null that sample_mean=8, at the 5% level')\n} else {\n    message('reject the null that sample_mean=8 in favor of either &lt;8 or &gt;8, at the 5% level')\n}",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "01_07_HypothesisTests.html#two-sample-differences",
    "href": "01_07_HypothesisTests.html#two-sample-differences",
    "title": "7  Hypothesis Tests",
    "section": "7.3 Two-Sample Differences",
    "text": "7.3 Two-Sample Differences\nSuppose we have 2 samples of data.\nEach \\(X_{is}\\) is an individual observation \\(i\\) from the sample \\(s=1,2\\). (For example, the wages for men and women in Canada. For another example, homicide rates in two different American states.)\n\n\nCode\nlibrary(wooldridge)\nx1 &lt;- wage1[wage1$educ == 15, 'wage']\nx2 &lt;- wage1[wage1$educ == 16, 'wage']\n\n\nFor simplicity, we will assume that each observation is an independent observation. We will further assume the data from each group are normally distributed, but the mean and variance can be different across groups.\n\n\nCode\n# Sample 1 (e.g., males)\nn1 &lt;- 100\nx1 &lt;- rnorm(n1, 0, 2)\n# Sample 2 (e.g., females)\nn2 &lt;- 80\nx2 &lt;- rnorm(n2, 1, 1)\n\npar(mfrow=c(1,2))\nbks &lt;- seq(-7,7, by=.5)\nhist(x1, border=NA, breaks=bks,\n    main='Sample 1', font.main=1)\n\nhist(x2, border=NA, breaks=bks, \n    main='Sample 2', font.main=1)\n\n\n\n\n\n\n\n\n\nThere may be several differences between these samples. Often, the first summary statistic we investigate is the difference in means.\n\nEqual Means.\nWe often want to know if the means of different sample are different in . To test this hypothesis, we compute the sample mean \\(\\overline{X}_{s}\\) over all observations in each sample and then examine the differences term \\[\\begin{eqnarray}\nD = \\overline{X}_{1} - \\overline{X}_{2},\n\\end{eqnarray}\\] with a null hypothesis of \\(D=0\\).\n\n\nCode\n# Differences between means\nm1 &lt;- mean(x1)\nm2 &lt;- mean(x2)\nd &lt;- m1-m2\n    \n# Bootstrap Distribution\nboot_d &lt;- sapply(1:10^4, function(b){\n    x1_b &lt;- sample(x1, replace=T)\n    x2_b &lt;- sample(x2, replace=T)\n    m1_b &lt;- mean(x1_b)\n    m2_b &lt;- mean(x2_b)\n    d_b &lt;- m1_b - m2_b\n    return(d_b)\n})\nhist(boot_d, border=NA, font.main=1,\n    main='Difference in Means')\n\n# 2-Sided Test\nboot_ci &lt;- quantile(boot_d, probs=c(.025, .975))\nabline(v=boot_ci, lwd=2)\nabline(v=0, lwd=2, col=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# p-value\n1 - ecdf(boot_d)(0)\n## [1] 0\n\n\nJust as with one sample tests, we can compute a standardized differences, where \\(D\\) is converted into a \\(t\\) statistic. Note, however, that we have to compute the standard error for the difference statistic, which is a bit more complicated. However, this allows us to easily conduct one or two sided hypothesis tests using a standard normal approximation.\n\n\nCode\nse_hat &lt;- sqrt(var(x1)/n1 + var(x2)/n2);\nt_obs &lt;- d/se_hat\n\n\n\n\nOther Differences.\nThe above procedure generalized from differences in “means” to other statistics like “quantiles”.\n\n\nCode\n# Bootstrap Distribution Function\nboot_fun &lt;- function( fun, B=10^4, ...){\n    boot_d &lt;- sapply(1:B, function(b){\n        x1_b &lt;- sample(x1, replace=T)\n        x2_b &lt;- sample(x2, replace=T)\n        f1_b &lt;- fun(x1_b, ...)\n        f2_b &lt;- fun(x2_b, ...)\n        d_b &lt;- f1_b - f2_b\n        return(d_b)\n    })\n    return(boot_d)\n}\n\n# 2-Sided Test for Median Differences\n# d &lt;- median(x2) - median(x1)\nboot_d &lt;- boot_fun(median)\nhist(boot_d, border=NA, font.main=1,\n    main='Difference in Medians')\nabline(v=quantile(boot_d, probs=c(.025, .975)), lwd=2)\nabline(v=0, lwd=2, col=2)\n\n\n\n\n\n\n\n\n\nCode\n1 - ecdf(boot_d)(0)\n## [1] 0\n\n\nNote that these estimates suffer from a finite-sample bias, which we can correct for. Also note that bootstrap tests can perform poorly with highly unequal variances or skewed data.\n\n\nCode\n# 2-Sided Test for SD Differences\n#d &lt;- sd(x2) - sd(x1)\nboot_d &lt;- boot_fun(sd)\nhist(boot_d, border=NA, font.main=1,\n    main='Difference in Standard Deviations')\nabline(v=quantile(boot_d, probs=c(.025, .975)), lwd=2)\nabline(v=0, lwd=2, col=2)\n1 - ecdf(boot_d)(0)\n\n\n# Try any function!\n# boot_fun( function(xs) { IQR(xs)/median(xs) } )",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "01_07_HypothesisTests.html#further-reading",
    "href": "01_07_HypothesisTests.html#further-reading",
    "title": "7  Hypothesis Tests",
    "section": "7.4 Further Reading",
    "text": "7.4 Further Reading\n\nhttps://learningstatisticswithr.com/book/hypothesistesting.html\nhttps://okanbulut.github.io/rbook/part5.html",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "01_07_HypothesisTests.html#footnotes",
    "href": "01_07_HypothesisTests.html#footnotes",
    "title": "7  Hypothesis Tests",
    "section": "",
    "text": "In another statistics class, you will learn the math behind the null t-distribution. In this class, we skip this because we can simply bootstrap the t-statistic too.↩︎",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "01_08_DataAnalysis.html",
    "href": "01_08_DataAnalysis.html",
    "title": "8  Data Analysis",
    "section": "",
    "text": "8.1 Beyond Basics\nUse expansion “packages” for less common procedures and more functionality",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "01_08_DataAnalysis.html#beyond-basics",
    "href": "01_08_DataAnalysis.html#beyond-basics",
    "title": "8  Data Analysis",
    "section": "",
    "text": "CRAN.\nMost packages can be found on CRAN and can be easily installed\n\n\nCode\n# commonly used packages\ninstall.packages(\"stargazer\")\ninstall.packages(\"data.table\")\ninstall.packages(\"plotly\")\n# other statistical packages\ninstall.packages(\"extraDistr\")\ninstall.packages(\"twosamples\")\n# install.packages(\"purrr\")\n# install.packages(\"reshape2\")\n\n\nThe most common tasks also have cheatsheets you can use.\nFor example, to generate ‘exotic’ probability distributions\n\n\nCode\nlibrary(extraDistr)\n\npar(mfrow=c(1,2))\nfor(p in c(-.5,0)){\n    x &lt;- rgev(2000, mu=0, sigma=1, xi=p)\n    hist(x, breaks=50, border=NA, main=NA, freq=F)\n}\ntitle('GEV densities', outer=T, line=-1)\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(extraDistr)\n\npar(mfrow=c(1,3))\nfor(p in c(-1, 0,2)){\n    x &lt;- rtlambda(2000, p)\n    hist(x, breaks=100, border=NA, main=NA, freq=F)\n}\ntitle('Tukey-Lambda densities', outer=T, line=-1)\n\n\n\n\n\n\n\n\n\n\n\nGithub.\nSometimes you will want to install a package from GitHub. For this, you can use devtools or its light-weight version remotes\n\n\nCode\ninstall.packages(\"devtools\")\ninstall.packages(\"remotes\")\n\n\nNote that to install devtools, you also need to have developer tools installed on your computer.\n\nWindows: Rtools\nMac: Xcode\n\nTo color terminal output on Linux systems, you can use the colorout package\n\n\nCode\nlibrary(remotes)\n# Install https://github.com/jalvesaq/colorout\n# to .libPaths()[1]\ninstall_github('jalvesaq/colorout')\nlibrary(colorout)\n\n\n\n\nBase.\nWhile additional packages can make your code faster, they also create dependancies that can lead to problems. So learn base R well before becoming dependant on other packages\n\nhttps://bitsofanalytics.org/posts/base-vs-tidy/\nhttps://jtr13.github.io/cc21fall2/comparison-among-base-r-tidyverse-and-datatable.html\n\n\n\nUpdating.\nMake sure R and your packages are up to date. The current version of R and any packages used can be found (and recorded) with\n\n\nCode\nsessionInfo()\n\n\nTo update your R packages, use\n\n\nCode\nupdate.packages()\n\n\n\n\nRare Tricks.\nNote that after updating R, you can update all packages stored in all .libPaths() with the following command\n\n\nCode\nupdate.packages(checkBuilt=T, ask=F)\n# install.packages(old.packages(checkBuilt=T)[,\"Package\"])\n\n\nSometimes there is a problem. To find specific broken packages after an update\n\n\nCode\nlibrary(purrr)\n\nset_names(.libPaths()) %&gt;%\n  map(function(lib) {\n    .packages(all.available = TRUE, lib.loc = lib) %&gt;%\n        keep(function(pkg) {\n            f &lt;- system.file('Meta', 'package.rds', package = pkg, lib.loc = lib)\n            tryCatch({readRDS(f); FALSE}, error = function(e) TRUE)\n        })\n  })\n# https://stackoverflow.com/questions/31935516/installing-r-packages-error-in-readrdsfile-error-reading-from-connection/55997765\n\n\nTo remove packages duplicated in multiple libraries\n\n\nCode\n# Libraries\ni &lt;- installed.packages()\nlibs &lt;- .libPaths()\n# Find Duplicated Packages\ni1 &lt;- i[ i[,'LibPath']==libs[1], ]\ni2 &lt;- i[ i[,'LibPath']==libs[2], ]\ndups &lt;- i2[,'Package'] %in% i1[,'Package']\nall( dups )\n# Remove\nremove.packages(  i2[,'Package'], libs[2] )",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "01_08_DataAnalysis.html#inputs",
    "href": "01_08_DataAnalysis.html#inputs",
    "title": "8  Data Analysis",
    "section": "8.2 Inputs",
    "text": "8.2 Inputs\n\nReading Data.\nThe first step in data analysis is getting data into R. There are many ways to do this, depending on your data structure. Perhaps the most common case is reading in a csv file.\n\n\nCode\n# Read in csv (downloaded from online)\n# download source 'http://www.stern.nyu.edu/~wgreene/Text/Edition7/TableF19-3.csv'\n# download destination '~/TableF19-3.csv'\nread.csv('~/TableF19-3.csv')\n \n# Can read in csv (directly from online)\n# dat_csv &lt;- read.csv('http://www.stern.nyu.edu/~wgreene/Text/Edition7/TableF19-3.csv')\n\n\nReading in other types of data can require the use of “packages”. For example, the “wooldridge” package contains datasets on crime. To use this data, we must first install the package on our computer. Then, to access the data, we must first load the package.\n\n\nCode\n# Install R Data Package and Load in\ninstall.packages('wooldridge') # only once\nlibrary(wooldridge) # anytime you want to use the data\n\ndata('crime2') \ndata('crime4')\n\n\nWe can use packages to access many different types of data. To read in a Stata data file, for example, we can use the “haven” package.\n\n\nCode\n# Read in stata data file from online\n#library(haven)\n#dat_stata &lt;- read_dta('https://www.ssc.wisc.edu/~bhansen/econometrics/DS2004.dta')\n#dat_stata &lt;- as.data.frame(dat_stata)\n\n# For More Introductory Econometrics Data, see \n# https://www.ssc.wisc.edu/~bhansen/econometrics/Econometrics%20Data.zip\n# https://pages.stern.nyu.edu/~wgreene/Text/Edition7/tablelist8new.htm\n# R packages: wooldridge, causaldata, Ecdat, AER, ....\n\n\n\n\nCleaning Data.\nData transformation is often necessary before analysis, so remember to be careful and check your code is doing what you want. (If you have large datasets, you can always test out the code on a sample.)\n\n\nCode\n# Function to Create Sample Datasets\nmake_noisy_data &lt;- function(n, b=0){\n    # Simple Data Generating Process\n    x &lt;- seq(1,10, length.out=n) \n    e &lt;- rnorm(n, mean=0, sd=10)\n    y &lt;- b*x + e \n    # Obervations\n    xy_mat &lt;- data.frame(ID=seq(x), x=x, y=y)\n    return(xy_mat)\n}\n\n# Two simulated datasets\ndat1 &lt;- make_noisy_data(6)\ndat2 &lt;- make_noisy_data(6)\n\n# Merging data in long format\ndat_merged_long &lt;- rbind(\n    cbind(dat1,DF=1),\n    cbind(dat2,DF=2))\n\n\nNow suppose we want to transform into wide format\n\n\nCode\n# Merging data in wide format, First Attempt\ndat_merged_wide &lt;- cbind( dat1, dat2)\nnames(dat_merged_wide) &lt;- c(paste0(names(dat1),'.1'), paste0(names(dat2),'.2'))\n\n# Merging data in wide format, Second Attempt\n# higher performance\ndat_merged_wide2 &lt;- merge(dat1, dat2,\n    by='ID', suffixes=c('.1','.2'))\n## CHECK they are the same.\nidentical(dat_merged_wide, dat_merged_wide2)\n## [1] FALSE\n# Inspect any differences\n\n# Merging data in wide format, Third Attempt with dedicated package\n# (highest performance but with new type of object)\nlibrary(data.table)\ndat_merged_longDT &lt;- as.data.table(dat_merged_long)\ndat_melted &lt;- melt(dat_merged_longDT, id.vars=c('ID', 'DF'))\ndat_merged_wide3 &lt;- dcast(dat_melted, ID~DF+variable)\n\n## CHECK they are the same.\nidentical(dat_merged_wide, dat_merged_wide3)\n## [1] FALSE\n\n\nOften, however, we ultimately want data in long format\n\n\nCode\n# Merging data in long format, Second Attempt with dedicated package \ndat_melted2 &lt;- melt(dat_merged_wide3, measure=c(\"1_x\",\"1_y\",\"2_x\",\"2_y\"))\nmelt_vars &lt;- strsplit(as.character(dat_melted2[['variable']]),'_')\ndat_melted2[,'DF'] &lt;- sapply(melt_vars, `[[`,1)\ndat_melted2[,'variable'] &lt;- sapply(melt_vars, `[[`,2)\ndat_merged_long2 &lt;- dcast(dat_melted2, DF+ID~variable)\ndat_merged_long2 &lt;- as.data.frame(dat_merged_long2)\n\n## CHECK they are the same.\nidentical( dat_merged_long2, dat_merged_long)\n## [1] FALSE\n\n# Further Inspect\ndat_merged_long2 &lt;- dat_merged_long2[,c('ID','x','y','DF')]\nmapply( identical, dat_merged_long2, dat_merged_long)\n##    ID     x     y    DF \n##  TRUE  TRUE  TRUE FALSE\n\n\nFor more tips, see https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-import.pdf and https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reshape.html",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "01_08_DataAnalysis.html#outputs",
    "href": "01_08_DataAnalysis.html#outputs",
    "title": "8  Data Analysis",
    "section": "8.3 Outputs",
    "text": "8.3 Outputs\n\nPolishing.\nYour first figures are typically standard.\n\n\nCode\n# Random Data\nx &lt;- seq(1, 10, by=.0002)\ne &lt;- rnorm(length(x), mean=0, sd=1)\ny &lt;- .25*x + e \n\n# First Drafts\n# qqplot(x, y)\n# plot(x, y)\n\n\nEdit your plot to focus on the most useful information. For others to easily comprehend your work, you must also polish the plot.\n\n\nCode\n# Second Draft: Focus\n# (In this example: comparing shapes)\nxs &lt;- scale(x)\nys &lt;- scale(y)\n# qqplot(xs, ys)\n\n# Third Draft: Polish\nqqplot(ys, xs, \n    xlab=expression('['~X-bar(X)~'] /'~s[X]),\n    ylab=expression('['~Y-bar(Y)~'] /'~s[Y]),\n    pch=16, cex=.5, col=grey(0,.2))\nabline(a=0, b=1, lty=2)\n\n\n\n\n\n\n\n\n\nWhen polishing, you must do two things\n\nAdd details that are necessary to understand the figure\nRemove unnecessary details (see e.g., https://www.edwardtufte.com/notebook/chartjunk/ and https://www.biostat.wisc.edu/~kbroman/topten_worstgraphs/)\n\n\n\nCode\n# Another Example\nxy_dat &lt;- data.frame(x=x, y=y)\npar(fig=c(0,1,0,0.9), new=F)\nplot(y~x, xy_dat, pch=16, col=rgb(0,0,0,.05), cex=.5,\n    xlab='', ylab='') # Format Axis Labels Seperately\nmtext( 'y=0.25 x + e\\n e ~ standard-normal', 2, line=2.2)\nmtext( expression(x%in%~'[0,10]'), 1, line=2.2)\n#abline( lm(y~x, data=xy_dat), lty=2)\ntitle('Plot with good features, but too excessive in several ways',\n    adj=0, font.main=1)\n\n# Outer Legend (https://stackoverflow.com/questions/3932038/)\nouter_legend &lt;- function(...) {\n  opar &lt;- par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), \n    mar=c(0, 0, 0, 0), new=TRUE)\n  on.exit(par(opar))\n  plot(0, 0, type='n', bty='n', xaxt='n', yaxt='n')\n  legend(...)\n}\nouter_legend('topright', legend='single data point',\n    title='do you see the normal distribution?',\n    pch=16, col=rgb(0,0,0,.1), cex=1, bty='n')\n\n\n\n\n\n\n\n\n\nFor useful tips, see C. Wilke (2019) “Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures” https://clauswilke.com/dataviz/\n\n\nSaving.\nYou can export figures with specific dimensions\n\n\nCode\npdf( 'Figures/plot_example.pdf', height=5, width=5)\n# plot goes here\ndev.off()\n\n\nFor plotting math, see https://astrostatistics.psu.edu/su07/R/html/grDevices/html/plotmath.html and https://library.virginia.edu/data/articles/mathematical-annotation-in-r\nFor exporting options, see ?pdf. For saving other types of files, see png(\"*.png\"), tiff(\"*.tiff\"), and jpeg(\"*.jpg\")\nWhich features are most informative depends on what you want to show, and you can always mix and match. Learn to edit your figures:\n\nhttps://websites.umich.edu/~jpboyd/eng403_chap2_tuftegospel.pdf\nhttps://jtr13.github.io/cc19/tuftes-principles-of-data-ink.html\nhttps://github.com/cxli233/FriendsDontLetFriends\n\nand be aware that each type has benefits and costs. E.g., see\n\nhttps://www.data-to-viz.com/caveats.html\nhttps://x.com/EdwardTufte/status/1092717905156993024/photo/1\nhttps://towardsdatascience.com/why-a-box-plot-should-not-be-used-alone-and-some-plots-to-use-it-with-23381f7e3cb6/\n\nFor small datasets, you can plot individual data points with a strip chart. For datasets with spatial information, a map is also helpful. Sometime tables are better than graphs (see https://www.edwardtufte.com/notebook/boxplots-data-test)\n\n\nInteractive Figures.\nHistograms. See https://plotly.com/r/histograms/\n\n\nCode\npop_mean &lt;- mean(USArrests[,'UrbanPop'])\npop_cut &lt;- USArrests[,'UrbanPop'] &lt; pop_mean\nmurder_lowpop &lt;- USArrests[ pop_cut,'Murder']\nmurder_highpop &lt;- USArrests[ !pop_cut,'Murder']\n\nlibrary(plotly)\nfig &lt;- plot_ly(alpha=0.6, \n    hovertemplate=\"%{y}\")\nfig &lt;- fig %&gt;% add_histogram(murder_lowpop, name='Low Pop. (&lt; Mean)')\nfig &lt;- fig %&gt;% add_histogram(murder_highpop, name='High Pop (&gt;= Mean)')\nfig &lt;- fig %&gt;% layout(barmode=\"stack\") # barmode=\"overlay\"\nfig &lt;- fig %&gt;% layout(\n    title=\"Crime and Urbanization in America 1975\",\n    xaxis = list(title='Murders Arrests per 100,000 People'),\n    yaxis = list(title='Number of States'),\n    legend=list(title=list(text='&lt;b&gt; % Urban Pop. &lt;/b&gt;'))\n)\nfig\n\n\n\n\n\n\nBoxplots. See https://plotly.com/r/box-plots/\n\n\nCode\nUSArrests[,'ID'] &lt;- rownames(USArrests)\nfig &lt;- plot_ly(USArrests,\n    y=~Murder, color=~cut(UrbanPop,4),\n    alpha=0.6, type=\"box\",\n    pointpos=0, boxpoints = 'all',\n    hoverinfo='text',    \n    text = ~paste('&lt;b&gt;', ID, '&lt;/b&gt;',\n        \"&lt;br&gt;Urban  :\", UrbanPop,\n        \"&lt;br&gt;Assault:\", Assault,\n        \"&lt;br&gt;Murder :\", Murder))    \nfig &lt;- layout(fig,\n    showlegend=FALSE,\n    title='Crime and Urbanization in America 1975',\n    xaxis = list(title = 'Percent of People in an Urban Area'),\n    yaxis = list(title = 'Murders Arrests per 100,000 People'))\nfig\n\n\n\n\n\n\n\n\nTables.\nYou can also export tables in a variety of formats, for other software programs to easily read\n\nCode\nlibrary(stargazer)\n# summary statistics\nstargazer(USArrests,\n    type='html', \n    summary=T,\n    title='Summary Statistics for USArrests')\n\n\n\nSummary Statistics for USArrests\n\n\n\n\n\n\n\nStatistic\n\n\nN\n\n\nMean\n\n\nSt. Dev.\n\n\nMin\n\n\nMax\n\n\n\n\n\n\n\n\nMurder\n\n\n50\n\n\n7.788\n\n\n4.356\n\n\n0.800\n\n\n17.400\n\n\n\n\nAssault\n\n\n50\n\n\n170.760\n\n\n83.338\n\n\n45\n\n\n337\n\n\n\n\nUrbanPop\n\n\n50\n\n\n65.540\n\n\n14.475\n\n\n32\n\n\n91\n\n\n\n\nRape\n\n\n50\n\n\n21.232\n\n\n9.366\n\n\n7.300\n\n\n46.000\n\n\n\n\n\n\n\nYou can create a basic interactive table to explore raw data.\n\n\nCode\ndata(\"USArrests\")\nlibrary(reactable)\nreactable(USArrests, filterable=T, highlight=T)\n\n\n\n\n\n\nFor further data exploration, your plots can also be made interactive via https://plotly.com/r/. For more details, see examples and then applications.\n\n\nCode\n#install.packages(\"plotly\")\nlibrary(plotly)\n\n\n\n\nCustom Figures.\nMany of the best plots are custom made (see https://www.r-graph-gallery.com/). Here are some ones that I have made over the years.",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "01_08_DataAnalysis.html#r-markdown-reports",
    "href": "01_08_DataAnalysis.html#r-markdown-reports",
    "title": "8  Data Analysis",
    "section": "8.4 R-Markdown Reports",
    "text": "8.4 R-Markdown Reports\nWe will use R Markdown for communicating results to each other. Note that R and R Markdown are both languages. R studio interprets R code make statistical computations and interprets R Markdown code to produce pretty documents that contain both writing and statistics. Altogether, your project will use\n\nR: does statistical computations\nR Markdown: formats statistical computations for sharing\nRstudio: graphical user interface that allows you to easily use both R and R Markdown.\n\nHomework reports are probably the smallest document you can create. These little reports are almost entirely self-contained (showing both code and output). To make them, you will need to\nFirst install Pandoc on your computer.\nThen install any required packages\n\n\nCode\n# Packages for Rmarkdown\ninstall.packages(\"knitr\")\ninstall.packages(\"rmarkdown\")\n\n# Other packages frequently used\n#install.packages(\"plotly\") #for interactive plots\n#install.packages(\"sf\") #for spatial data\n\n\nWe will create simple reproducible reports via R Markdown.\n\nExample 1: Data Scientism.\n\nSee DataScientism.html and then create it by\n\nClicking the “Code” button in the top right and then “Download Rmd”\nOpen with Rstudio\nChange the name and title to your own, make other edits\nThen point-and-click “knit”\n\nAlternatively,\n\nDownload the source file from DataScientism.Rmd\nChange the name and title to your own, make other edits\nUse the console to run\n\n\n\nCode\nrmarkdown::render('DataScientism.Rmd')\n\n\n\n\nExample 2: Homework Assignment.\nBelow is a template of what homework questions (and answers) look like. Create a new .Rmd file from scratch and produce a .html file that looks similar to this:\nProblem: Simulate 100 random observations of the form \\(y=x\\beta+\\epsilon\\) and plot the relationship. Plot and explore the data interactively via plotly, https://plotly.com/r/line-and-scatter/. Then play around with different styles, https://www.r-graph-gallery.com/13-scatter-plot.html, to best express your point.\nSolution: I simulate \\(400\\) observations for \\(\\epsilon \\sim 2\\times N(0,1)\\) and \\(\\beta=4\\), as seen in this single chunk. Notice an upward trend.\n\n\nCode\n# Simulation\nn &lt;- 100\nE &lt;- rnorm(n)\nX &lt;- seq(n)\nY &lt;- 4*X + 2*E\n# Plot\nlibrary(plotly)\ndat &lt;- data.frame(X=X,Y=Y)\nplot_ly( data=dat, x=~X, y=~Y)\n\n\n\n\n\n\nCode\n\n# To Do:\n# 1. Fit a regression line\n# 2. Color points by their residual value",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "01_08_DataAnalysis.html#further-reading",
    "href": "01_08_DataAnalysis.html#further-reading",
    "title": "8  Data Analysis",
    "section": "8.5 Further Reading",
    "text": "8.5 Further Reading\nFor more guidance on how to create Rmarkdown documents, see\n\nhttps://github.com/rstudio/cheatsheets/blob/main/rmarkdown.pdf\nhttps://cran.r-project.org/web/packages/rmarkdown/vignettes/rmarkdown.html\nhttp://rmarkdown.rstudio.com\nhttps://bookdown.org/yihui/rmarkdown/\nhttps://bookdown.org/yihui/rmarkdown-cookbook/\nhttps://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/rmarkdown.html\nAn Introduction to the Advanced Theory and Practice of Nonparametric Econometrics. Raccine 2019. Appendices B & D.\nhttps://rmd4sci.njtierney.com/using-rmarkdown.html\nhttps://alexd106.github.io/intro2R/Rmarkdown_intro.html\n\nIf you are still lost, try one of the many online tutorials (such as these)\n\nhttps://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf\nhttps://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\nhttps://www.neonscience.org/resources/learning-hub/tutorials/rmd-code-intro\nhttps://m-clark.github.io/Introduction-to-Rmarkdown/\nhttps://www.stat.cmu.edu/~cshalizi/rmarkdown/\nhttp://math.wsu.edu/faculty/xchen/stat412/HwWriteUp.Rmd\nhttp://math.wsu.edu/faculty/xchen/stat412/HwWriteUp.html\nhttps://holtzy.github.io/Pimp-my-rmd/\nhttps://ntaback.github.io/UofT_STA130/Rmarkdownforclassreports.html\nhttps://crd150.github.io/hw_guidelines.html\nhttps://r4ds.had.co.nz/r-markdown.html\nhttp://www.stat.cmu.edu/~cshalizi/rmarkdown\nhttp://www.ssc.wisc.edu/sscc/pubs/RFR/RFR_RMarkdown.html\nhttp://kbroman.org/knitr_knutshell/pages/Rmarkdown.html",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "01_09_BivariateData.html",
    "href": "01_09_BivariateData.html",
    "title": "9  Bivariate Data",
    "section": "",
    "text": "9.1 Types of Distributions\nWe will now study them in more detail. Suppose we have two discrete variables \\(X_{i}\\) and \\(Y_{i}\\). The data for each observation data can be grouped together as a vector \\((X_{i}, Y_{i})\\).",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate Data</span>"
    ]
  },
  {
    "objectID": "01_09_BivariateData.html#types-of-distributions",
    "href": "01_09_BivariateData.html#types-of-distributions",
    "title": "9  Bivariate Data",
    "section": "",
    "text": "Joint Distributions.\nScatterplots are used frequently to summarize the joint relationship between two variables, multiple observations of \\((X_{i}, Y_{i})\\). They can be enhanced in several ways. As a default, use semi-transparent points so as not to hide any points (and perhaps see if your observations are concentrated anywhere). You can also add other features that help summarize the relationship, although I will defer this until later.\n\n\nCode\nplot(Murder~UrbanPop, USArrests, pch=16, col=grey(0.,.5))\n\n\n\n\n\n\n\n\n\nIf you have many points, you can also use a 2D histogram instead. https://plotly.com/r/2D-Histogram/.\n\n\nCode\nlibrary(plotly)\nfig &lt;- plot_ly(\n    USArrests, x = ~UrbanPop, y = ~Assault)\nfig &lt;- add_histogram2d(fig, nbinsx=25, nbinsy=25)\nfig\n\n\n\n\nMarginal Distributions.\nYou can also show the distributions of each variable along each axis.\n\n\nCode\n# Setup Plot\nlayout( matrix(c(2,0,1,3), ncol=2, byrow=TRUE),\n    widths=c(9/10,1/10), heights=c(1/10,9/10))\n\n# Scatterplot\npar(mar=c(4,4,1,1))\nplot(Murder~UrbanPop, USArrests, pch=16, col=rgb(0,0,0,.5))\n\n# Add Marginals\npar(mar=c(0,4,1,1))\nxhist &lt;- hist(USArrests[,'UrbanPop'], plot=FALSE)\nbarplot(xhist[['counts']], axes=FALSE, space=0, border=NA)\n\npar(mar=c(4,0,1,1))\nyhist &lt;- hist(USArrests[,'Murder'], plot=FALSE)\nbarplot(yhist[['counts']], axes=FALSE, space=0, horiz=TRUE, border=NA)\n\n\n\n\n\n\n\n\n\n\n\nConditional Distributions.\nWe can show how distributions and densities change according to a second (or even third) variable using data splits. E.g.,\n\n\nCode\n# Tailored Histogram \nylim &lt;- c(0,8)\nxbks &lt;-  seq(min(USArrests[,'Murder'])-1, max(USArrests[,'Murder'])+1, by=1)\n\n# Also show more information\n# Split Data by Urban Population above/below mean\npop_mean &lt;- mean(USArrests[,'UrbanPop'])\npop_cut &lt;- USArrests[,'UrbanPop']&lt; pop_mean\nmurder_lowpop &lt;- USArrests[pop_cut,'Murder']\nmurder_highpop &lt;- USArrests[!pop_cut,'Murder']\ncols &lt;- c(low=rgb(0,0,1,.75), high=rgb(1,0,0,.75))\n\npar(mfrow=c(1,2))\nhist(murder_lowpop,\n    breaks=xbks, col=cols[1],\n    main='Urban Pop &gt;= Mean', font.main=1,\n    xlab='Murder Arrests',\n    border=NA, ylim=ylim)\n\nhist(murder_highpop,\n    breaks=xbks, col=cols[2],\n    main='Urban Pop &lt; Mean', font.main=1,\n    xlab='Murder Arrests',\n    border=NA, ylim=ylim)\n\n\n\n\n\n\n\n\n\nIt is sometimes it is preferable to show the ECDF instead. And you can glue various combinations together to convey more information all at once\n\n\nCode\npar(mfrow=c(1,2))\n# Full Sample Density\nhist(USArrests[,'Murder'], \n    main='Density Function Estimate', font.main=1,\n    xlab='Murder Arrests',\n    breaks=xbks, freq=F, border=NA)\n\n# Split Sample Distribution Comparison\nF_lowpop &lt;- ecdf(murder_lowpop)\nplot(F_lowpop, col=cols[1],\n    pch=16, xlab='Murder Arrests',\n    main='Distribution Function Estimates',\n    font.main=1, bty='n')\nF_highpop &lt;- ecdf(murder_highpop)\nplot(F_highpop, add=T, col=cols[2], pch=16)\n\nlegend('bottomright', col=cols,\n    pch=16, bty='n', inset=c(0,.1),\n    title='% Urban Pop.',\n    legend=c('Low (&lt;= Mean)','High (&gt;= Mean)'))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Simple Interactive Scatter Plot\n# plot(Assault~UrbanPop, USArrests, col=grey(0,.5), pch=16,\n#    cex=USArrests[,'Murder']/diff(range(USArrests[,'Murder']))*2,\n#    main='US Murder arrests (per 100,000)')\n\n\nYou can also split data into grouped boxplots in the same way\n\n\nCode\nlayout( t(c(1,2,2)))\nboxplot(USArrests[,'Murder'], main='',\n    xlab='All Data', ylab='Murder Arrests')\n\n# K Groups with even spacing\nK &lt;- 3\nUSArrests[,'UrbanPop_Kcut'] &lt;- cut(USArrests[,'UrbanPop'],K)\nKcols &lt;- hcl.colors(K,alpha=.5)\nboxplot(Murder~UrbanPop_Kcut, USArrests,\n    main='', col=Kcols,\n    xlab='Urban Population', ylab='')\n\n\n\n\n\n\n\n\n\nCode\n\n# 4 Groups with equal numbers of observations\n#Qcuts &lt;- c(\n#    '0%'=min(USArrests[,'UrbanPop'])-10*.Machine[['double.eps']],\n#    quantile(USArrests[,'UrbanPop'], probs=c(.25,.5,.75,1)))\n#USArrests[,'UrbanPop']_cut &lt;- cut(USArrests[,'UrbanPop'], Qcuts)\n#boxplot(Murder~UrbanPop_cut, USArrests, col=hcl.colors(4,alpha=.5))",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate Data</span>"
    ]
  },
  {
    "objectID": "01_09_BivariateData.html#statistics-of-association",
    "href": "01_09_BivariateData.html#statistics-of-association",
    "title": "9  Bivariate Data",
    "section": "9.2 Statistics of Association",
    "text": "9.2 Statistics of Association\nThere are several ways to statistically describe the relationship between two variables. The major differences surround whether the data are cardinal or an ordered/unordered factor.\n\nCardinal.\nPearson (Linear) Correlation. Suppose \\(X\\) and \\(Y\\) are both cardinal data. As such, you can compute the most famous measure of association, the covariance: \\[\nC_{XY} =  \\sum_{i} [X_i - \\overline{X}] [Y_i - \\overline{Y}] / N\n\\]\n\n\nCode\n#plot(xy, pch=16, col=grey(0,.25))\ncov(xy)\n##             Murder   UrbanPop\n## Murder   18.970465   4.386204\n## UrbanPop  4.386204 209.518776\n\n\nNote that \\(C_{XX}=V_{X}\\). For ease of interpretation and comparison, we rescale this statistic to always lay between \\(-1\\) and \\(1\\) \\[\nr_{XY} = \\frac{ C_{XY} }{ \\sqrt{V_X} \\sqrt{V_Y}}\n\\]\n\n\nCode\ncor(xy)[2]\n## [1] 0.06957262\n\n\nFalk Codeviance. The Codeviance is a robust alternative to Covariance. Instead of relying on means (which can be sensitive to outliers), it uses medians (\\(\\tilde{X}\\)) to capture the central tendency.1 We can also scale the Codeviance by the median absolute deviation to compute the median correlation. \\[\\begin{eqnarray}\n\\text{CoDev}(X,Y) = \\text{Med}\\left\\{ |X_i - \\tilde{X}| |Y_i - \\tilde{Y}| \\right\\} \\\\\n\\tilde{r}_{XY} = \\frac{ \\text{CoDev}(X,Y) }{ \\text{MAD}(X) \\text{MAD}(Y) }.\n\\end{eqnarray}\\]\n\n\nCode\ncor_m &lt;- function(xy) {\n  # Compute medians for each column\n  med &lt;- apply(xy, 2, median)\n  # Subtract the medians from each column\n  xm &lt;- sweep(xy, 2, med, \"-\")\n  # Compute CoDev\n  CoDev &lt;- median(xm[, 1] * xm[, 2])\n  # Compute the medians of absolute deviation\n  MadProd &lt;- prod( apply(abs(xm), 2, median) )\n  # Return the robust correlation measure\n  return( CoDev / MadProd)\n}\ncor_m(xy)\n## [1] 0.005707763\n\n\n\n\nOrdered Factor.\nSuppose \\(X\\) and \\(Y\\) are both ordered variables. Kendall’s Tau measures the strength and direction of association by counting the number of concordant pairs (where the ranks agree) versus discordant pairs (where the ranks disagree). A value of \\(\\tau = 1\\) implies perfect agreement in rankings, \\(\\tau = -1\\) indicates perfect disagreement, and \\(\\tau = 0\\) suggests no association in the ordering. \\[\n\\tau = \\frac{2}{n(n-1)} \\sum_{i} \\sum_{j &gt; i} \\text{sgn} \\Bigl( (X_i - X_j)(Y_i - Y_j) \\Bigr),\n\\] where the sign function is: \\[\n\\text{sgn}(z) =\n\\begin{cases}\n+1 & \\text{if } z &gt; 0\\\\\n0  & \\text{if } z = 0 \\\\\n-1 & \\text{if} z &lt; 0\n\\end{cases}.\n\\]\n\n\nCode\nxy &lt;- USArrests[,c('Murder','UrbanPop')]\nxy[,1] &lt;- rank(xy[,1] )\nxy[,2] &lt;- rank(xy[,2] )\n# plot(xy, pch=16, col=grey(0,.25))\ntau &lt;- cor(xy[, 1], xy[, 2], method = \"kendall\")\nround(tau, 3)\n## [1] 0.074\n\n\n\n\nUnordered Factor.\nSuppose \\(X\\) and \\(Y\\) are both categorical variables; the value of \\(X\\) is one of \\(1...r\\) categories and the value of \\(Y\\) is one of \\(1...k\\) categories. Cramer’s V quantifies the strength of association by adjusting a “chi-squared” statistic to provide a measure that ranges from \\(0\\) to \\(1\\); \\(0\\) indicates no association while a value closer to \\(1\\) signifies a strong association.\nFirst, consider a contingency table for \\(X\\) and \\(Y\\) with \\(r\\) rows and \\(k\\) columns. The chi-square statistic is then defined as:\n\\[\n\\chi^2 = \\sum_{i=1}^{r} \\sum_{j=1}^{k} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}.\n\\]\nwhere\n\n\\(O_{ij}\\) denote the observed frequency in cell \\((i, j)\\),\n\\(E_{ij} = \\frac{R_i \\cdot C_j}{n}\\) is the expected frequency for each cell if \\(X\\) and \\(Y\\) are independent\n\\(R_i\\) denote the total frequency for row \\(i\\) (i.e., \\(R_i = \\sum_{j=1}^{k} O_{ij}\\)),\n\\(C_j\\) denote the total frequency for column \\(j\\) (i.e., \\(C_j = \\sum_{i=1}^{r} O_{ij}\\)),\n\\(n\\) be the grand total of observations, so that \\(n = \\sum_{i=1}^{r} \\sum_{j=1}^{k} O_{ij}\\).\n\nSecond, normalize the chi-square statistic with the sample size and the degrees of freedom to compute Cramer’s V.\n\\[\nV = \\sqrt{\\frac{\\chi^2 / n}{\\min(k - 1, \\, r - 1)}},\n\\]\nwhere:\n\n\\(n\\) is the total sample size,\n\\(k\\) is the number of categories for one variable,\n\\(r\\) is the number of categories for the other variable.\n\n\n\nCode\nxy &lt;- USArrests[,c('Murder','UrbanPop')]\nxy[,1] &lt;- cut(xy[,1],3)\nxy[,2] &lt;- cut(xy[,2],4)\ntable(xy)\n##               UrbanPop\n## Murder         (31.9,46.8] (46.8,61.5] (61.5,76.2] (76.2,91.1]\n##   (0.783,6.33]           4           5           8           5\n##   (6.33,11.9]            0           4           7           6\n##   (11.9,17.4]            2           4           2           3\n\ncor_v &lt;- function(xy){\n    # Create a contingency table from the categorical variables\n    tbl &lt;- table(xy)\n    # Compute the chi-square statistic (without Yates' continuity correction)\n    chi2 &lt;- chisq.test(tbl, correct=FALSE)[['statistic']]\n    # Total sample size\n    n &lt;- sum(tbl)\n    # Compute the minimum degrees of freedom (min(rows-1, columns-1))\n    df_min &lt;- min(nrow(tbl) - 1, ncol(tbl) - 1)\n    # Calculate Cramer's V\n    V &lt;- sqrt((chi2 / n) / df_min)\n    return(V)\n}\ncor_v(xy)\n## X-squared \n## 0.2307071\n\n# DescTools::CramerV( table(xy) )\n\n\n\n\nMixed.\nFor mixed data, where \\(Y_{i}\\) is a cardinal variable and \\(X_{i}\\) is an unordered factor variable, we analyze associations via group comparisons. You have already seen via two-sample difference, which corresponds to an \\(X_{i}\\) with two categories.",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate Data</span>"
    ]
  },
  {
    "objectID": "01_09_BivariateData.html#probability-theory",
    "href": "01_09_BivariateData.html#probability-theory",
    "title": "9  Bivariate Data",
    "section": "9.3 Probability Theory",
    "text": "9.3 Probability Theory\n\nDefinitions for Discrete Data.\nThe joint distribution is defined as \\[\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y)\n\\end{eqnarray}\\] Variables are statistically independent if \\(Prob(X_{i} = x, Y_{i} = y)= Prob(X_{i} = x) Prob(Y_{i} = y)\\) for all \\(x, y\\). Independance is sometimes assumed for mathematical simplicity, not because it generally fits data well.2\nThe conditional distributions are defined as \\[\\begin{eqnarray}\nProb(X_{i} = x | Y_{i} = y) = \\frac{ Prob(X_{i} = x, Y_{i} = y)}{ Prob( Y_{i} = y )}\\\\\nProb(Y_{i} = y | X_{i} = x) = \\frac{ Prob(X_{i} = x, Y_{i} = y)}{ Prob( X_{i} = x )}\n\\end{eqnarray}\\] The marginal distributions are then defined as \\[\\begin{eqnarray}\nProb(X_{i} = x) = \\sum_{y} Prob(X_{i} = x | Y_{i} = y) Prob( Y_{i} = y ) \\\\\nProb(Y_{i} = y) = \\sum_{x} Prob(Y_{i} = y | X_{i} = x) Prob( X_{i} = x ),\n\\end{eqnarray}\\] which is also known as the law of total probability.\n\n\nFair Coin Flips Example.\nFor one example, Consider flipping two coins. Denoted each coin as \\(k \\in \\{1, 2\\}\\), and mark whether “heads” is face up; \\(X_{ki}=1\\) if Heads and \\(=0\\) if Tails. Suppose both coins are “fair”: \\(Prob(X_{i}=1)= 1/2\\) and \\(Prob(Y_{i}=1|X_{i})=1/2\\), then the four potential outcomes have equal probabilities. The joint distribution is \\[\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y) &=& Prob(X_{i} = x) Prob(Y_{i} = y)\\\\\nProb(X_{i} = 0, Y_{i} = 0) &=& 1/2 \\times 1/2 = 1/4 \\\\\nProb(X_{i} = 0, Y_{i} = 1) &=& 1/4 \\\\\nProb(X_{i} = 1, Y_{i} = 0) &=& 1/4 \\\\\nProb(X_{i} = 1, Y_{i} = 1) &=& 1/4 .\n\\end{eqnarray}\\] The marginal distribution of the second coin is \\[\\begin{eqnarray}\nProb(Y_{i} = 0) &=& Prob(Y_{i} = 0 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 0 | X_{i} = 1) Prob(X_{i}=1)\\\\\n&=& 1/2 \\times 1/2 + 1/2 \\times 1/2 = 1/2\\\\\nProb(Y_{i} = 1) &=& Prob(Y_{i} = 1 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 1 | X_{i} = 1) Prob(X_{i}=1)\\\\\n&=& 1/2 \\times 1/2 + 1/2 \\times 1/2 = 1/2\n\\end{eqnarray}\\]\n\n\nCode\n# Create a 2x2 matrix for the joint distribution.\n# Rows correspond to X1 (coin 1), and columns correspond to X2 (coin 2).\nP_fair &lt;- matrix(1/4, nrow = 2, ncol = 2)\nrownames(P_fair) &lt;- c(\"X1=0\", \"X1=1\")\ncolnames(P_fair) &lt;- c(\"X2=0\", \"X2=1\")\nP_fair\n##      X2=0 X2=1\n## X1=0 0.25 0.25\n## X1=1 0.25 0.25\n\n# Compute the marginal distributions.\n# Marginal for X1: sum across columns.\nP_X1 &lt;- rowSums(P_fair)\nP_X1\n## X1=0 X1=1 \n##  0.5  0.5\n# Marginal for X2: sum across rows.\nP_X2 &lt;- colSums(P_fair)\nP_X2\n## X2=0 X2=1 \n##  0.5  0.5\n\n# Compute the conditional probabilities Prob(X2 | X1).\ncond_X2_given_X1 &lt;- matrix(0, nrow = 2, ncol = 2)\nfor (j in 1:2) {\n  cond_X2_given_X1[, j] &lt;- P_fair[, j] / P_X1[j]\n}\nrownames(cond_X2_given_X1) &lt;- c(\"X2=0\", \"X2=1\")\ncolnames(cond_X2_given_X1) &lt;- c(\"given X1=0\", \"given X1=1\")\ncond_X2_given_X1\n##      given X1=0 given X1=1\n## X2=0        0.5        0.5\n## X2=1        0.5        0.5\n\n\n\n\nUnFair Coin Flips Example.\nConsider a second example, where the second coin is “Completely Unfair”, so that it is always the same as the first. The outcomes generated with a Completely Unfair coin are the same as if we only flipped one coin. \\[\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y) &=& Prob(X_{i} = x) \\mathbf{1}( x=y )\\\\\nProb(X_{i} = 0, Y_{i} = 0) &=& 1/2 \\\\\nProb(X_{i} = 0, Y_{i} = 1) &=& 0 \\\\\nProb(X_{i} = 1, Y_{i} = 0) &=& 0 \\\\\nProb(X_{i} = 1, Y_{i} = 1) &=& 1/2 .\n\\end{eqnarray}\\] Note that \\(\\mathbf{1}(X_{i}=1)\\) means \\(X_{i}= 1\\) and \\(0\\) if \\(X_{i}\\neq0\\). The marginal distribution of the second coin is \\[\\begin{eqnarray}\nProb(Y_{i} = 0)\n&=& Prob(Y_{i} = 0 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 0 | X_{i} = 1) Prob(X_{i} = 1)\\\\\n&=& 1/2 \\times 1 + 0 \\times 1/2 = 1/2 .\\\\\nProb(Y_{i} = 1)\n&=& Prob(Y_{i} = 1 | X_{i} =0) Prob( X_{i} = 0) + Prob(Y_{i} = 1 | X_{i} = 1) Prob( X_{i} = 1)\\\\\n&=& 0\\times 1/2 + 1 \\times 1/2 = 1/2 .\n\\end{eqnarray}\\] which is the same as in the first example! Different joint distributions can have the same marginal distributions.\n\n\nCode\n# Create the joint distribution matrix for the unfair coin case.\nP_unfair &lt;- matrix(c(0.5, 0, 0, 0.5), nrow = 2, ncol = 2, byrow = TRUE)\nrownames(P_unfair) &lt;- c(\"X1=0\", \"X1=1\")\ncolnames(P_unfair) &lt;- c(\"X2=0\", \"X2=1\")\nP_unfair\n##      X2=0 X2=1\n## X1=0  0.5  0.0\n## X1=1  0.0  0.5\n\n# Compute the marginal distribution for X2 in the unfair case.\nP_X2_unfair &lt;- colSums(P_unfair)\nP_X1_unfair &lt;- rowSums(P_unfair)\n\n# Compute the conditional probabilities Prob(X1 | X2) for the unfair coin.\ncond_X2_given_X1_unfair &lt;- matrix(NA, nrow = 2, ncol = 2)\nfor (j in 1:2) {\n  if (P_X1_unfair[j] &gt; 0) {\n    cond_X2_given_X1_unfair[, j] &lt;- P_unfair[, j] / P_X1_unfair[j]\n  }\n}\nrownames(cond_X2_given_X1_unfair) &lt;- c(\"X2=0\", \"X2=1\")\ncolnames(cond_X2_given_X1_unfair) &lt;- c(\"given X1=0\", \"given X1=1\")\ncond_X2_given_X1_unfair\n##      given X1=0 given X1=1\n## X2=0          1          0\n## X2=1          0          1\n\n\n\n\nDefinitions for Continuous Data.\nThe joint distribution is defined as \\[\\begin{eqnarray}\nF(x, y) &=& Prob(X_{i} \\leq x, Y_{i} \\leq y)\n\\end{eqnarray}\\] The marginal distributions are then defined as \\[\\begin{eqnarray}\nF_{X}(x) &=& F(x, \\infty)\\\\\nF_{Y}(y) &=& F(\\infty, y).\n\\end{eqnarray}\\] which is also known as the law of total probability. Variables are statistically independent if \\(F(x, y) = F_{X}(x)F_{Y}(y)\\) for all \\(x, y\\).\nFor example, suppose \\((X_{i},Y_{i})\\) is bivariate normal with means \\((\\mu_{X}, \\mu_{Y})\\), variances \\((\\sigma_{X}, \\sigma_{Y})\\) and covariance \\(\\rho\\).\n\n\nCode\n# Simulate Bivariate Data\nN &lt;- 10000\nMu &lt;- c(2,2) ## Means\n\nSigma1 &lt;- matrix(c(2,-.8,-.8,1),2,2) ## CoVariance Matrix\nMVdat1 &lt;- mvtnorm::rmvnorm(N, Mu, Sigma1)\n\nSigma2 &lt;- matrix(c(2,.4,.4,1),2,2) ## CoVariance Matrix\nMVdat2 &lt;- mvtnorm::rmvnorm(N, Mu, Sigma2)\n\npar(mfrow=c(1,2))\n## Different diagonals\nplot(MVdat2, col=rgb(1,0,0,0.02), pch=16,\n    main='Joint', font.main=1,\n    ylim=c(-4,8), xlim=c(-4,8), xlab='X1', ylab='X2')\npoints(MVdat1,col=rgb(0,0,1,0.02),pch=16)\n## Same marginal distributions\nxbks &lt;- seq(-4,8,by=.2)\nhist(MVdat2[,2], col=rgb(1,0,0,0.5),\n    breaks=xbks, border=NA, xlab='X2',\n    main='Marginal', font.main=1)\nhist(MVdat1[,2], col=rgb(0,0,1,0.5),\n    add=T, breaks=xbks, border=NA)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# See that independent data are a special case\nn &lt;- 2e4\n## 2 Indepenant RV\nXYiid &lt;- cbind( rnorm(n),  rnorm(n))\n## As a single Joint Draw\nXYjoint &lt;- mvtnorm::rmvnorm(n, c(0,0))\n## Plot\npar(mfrow=c(1,2))\nplot(XYiid, xlab=\n    col=grey(0,.05), pch=16, xlim=c(-5,5), ylim=c(-5,5))\nplot(XYjoint,\n    col=grey(0,.05), pch=16, xlim=c(-5,5), ylim=c(-5,5))\n\n# Compare densities\n#d1 &lt;- dnorm(XYiid[,1],0)*dnorm(XYiid[,2],0)\n#d2 &lt;- mvtnorm::dmvnorm(XYiid, c(0,0))\n#head(cbind(d1,d2))\n\n\nThe multivariate normal is a workhorse for analytical work on multivariate random variables, but there are many more. See e.g., https://cran.r-project.org/web/packages/NonNorMvtDist/NonNorMvtDist.pdf\n\n\nImportant Applications.\nNote Simpson’s Paradox:\nAlso note Bayes’ Theorem: \\[\\begin{eqnarray}\nProb(X_{i} = x | Y_{i} = y)  Prob( Y_{i} = y)\n    &=& Prob(X_{i} = x, Y_{i} = y) = Prob(Y_{i} = y | X_{i} = x) Prob(X_{i} = x).\\\\\nProb(X_{i} = x | Y_{i} = y)\n    &=& \\frac{ Prob(Y_{i} = y | X_{i} = x) Prob(X_{i}=x) }{ Prob( Y_{i} = y) }.\n\\end{eqnarray}\\]\n\n\nCode\n# Verify Bayes' theorem for the unfair coin case:\n# Compute Prob(X1=1 | X2=1) using the formula:\n#   Prob(X1=1 | X2=1) = [Prob(X2=1 | X1=1) * Prob(X1=1)] / Prob(X2=1)\n\nP_X1_1 &lt;- 0.5\nP_X2_1_given_X1_1 &lt;- 1  # Since coin 2 copies coin 1.\nP_X2_1 &lt;- P_X2_unfair[\"X2=1\"]\n\nbayes_result &lt;- (P_X2_1_given_X1_1 * P_X1_1) / P_X2_1\nbayes_result\n## X2=1 \n##    1",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate Data</span>"
    ]
  },
  {
    "objectID": "01_09_BivariateData.html#further-reading",
    "href": "01_09_BivariateData.html#further-reading",
    "title": "9  Bivariate Data",
    "section": "9.4 Further Reading",
    "text": "9.4 Further Reading\nFor plotting histograms and marginal distributions, see\n\nhttps://www.r-bloggers.com/2011/06/example-8-41-scatterplot-with-marginal-histograms/\nhttps://r-graph-gallery.com/histogram.html\nhttps://r-graph-gallery.com/74-margin-and-oma-cheatsheet.html\nhttps://jtr13.github.io/cc21fall2/tutorial-for-scatter-plot-with-marginal-distribution.html\n\nMany introductory econometrics textbooks have a good appendix on probability and statistics. There are many useful statistical texts online too\nSee the Further reading about Probability Theory in the Statistics chapter.\n\nhttps://www.r-bloggers.com/2024/03/calculating-conditional-probability-in-r/\n\nOther Statistics\n\nhttps://cran.r-project.org/web/packages/qualvar/vignettes/wilcox1973.html",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate Data</span>"
    ]
  },
  {
    "objectID": "01_09_BivariateData.html#footnotes",
    "href": "01_09_BivariateData.html#footnotes",
    "title": "9  Bivariate Data",
    "section": "",
    "text": "See also Theil-Sen Estimator, which may be seen as a precursor.↩︎\nThe same can be said about assuming normally distributed errors, although at least that can be motivated by the Central Limit Theorems.↩︎",
    "crumbs": [
      "Introduction to Data Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate Data</span>"
    ]
  },
  {
    "objectID": "02-00-LinearRegression.html",
    "href": "02-00-LinearRegression.html",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "This section introduces linear regression models from the perspective that ``all models are wrong, but some are useful’’. All linear models are estimated via Ordinary Least Squares (OLS). For more in-depth introductions, which typically begin by assuming a linear data generating process, see https://jadamso.github.io/Rbooks/ordinary-least-squares.html#more-literature.",
    "crumbs": [
      "Introduction to Linear Regression"
    ]
  },
  {
    "objectID": "02_01_BasicRegression.html",
    "href": "02_01_BasicRegression.html",
    "title": "10  Simple Regression",
    "section": "",
    "text": "10.1 Simple Linear Regression\nSuppose we have some bivariate data. First, we inspect it as in Part I.\nNow we will assess the association between variables by fitting a line through the data points using a “regression”.\nThis refers to fitting a linear model to bivariate data. Specifically, our model is \\[\ny_i=\\beta_{0}+\\beta_{1} x_i+\\epsilon_{i}\n\\] and our objective function is \\[\nmin_{\\beta_{0}, \\beta_{1}} \\sum_{i=1}^{N} \\left( \\epsilon_{i} \\right)^2 =  min_{\\beta_{0}, \\beta_{1}} \\sum_{i=1} \\left( y_i - [\\beta_{0}+\\beta_{1} x_i] \\right).\n\\] Minimizing the sum of squared errors yields parameter estimates \\[\n\\hat{\\beta_{0}}=\\bar{Y}-\\hat{\\beta_{1}}\\bar{X} \\\\\n\\hat{\\beta_{1}}=\\frac{\\sum_{i}^{}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i}^{}(x_i-\\bar{x})^2} = \\frac{C_{XY}}{V_{X}}\n\\] and predictions \\[\n\\hat{y}_i=\\hat{\\beta_{0}}+\\hat{\\beta}x_i\\\\\n\\hat{\\epsilon}_i=y_i-\\hat{y}_i\n\\]\nCode\n# Run a Regression Coefficients\nreg &lt;- lm(y~x, dat=xy)\n# predict(reg)\n# resid(reg)\n# coef(reg)",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple Regression</span>"
    ]
  },
  {
    "objectID": "02_01_BasicRegression.html#simple-linear-regression",
    "href": "02_01_BasicRegression.html#simple-linear-regression",
    "title": "10  Simple Regression",
    "section": "",
    "text": "Goodness of Fit.\nFirst, we qualitatively analyze the ‘’Goodness of fit’’ of our model, we plot our predictions for a qualitative analysis\n\n\nCode\n# Plot Data and Predictions\nlibrary(plotly)\nxy$ID &lt;- rownames(USArrests)\nxy$pred &lt;- predict(reg)\nxy$resid &lt;- resid(reg)\nfig &lt;- plotly::plot_ly(\n  xy, x=~x, y=~y,\n  mode='markers',\n  type='scatter',\n  hoverinfo='text',\n  marker=list(color=grey(0,.25), size=10),\n  text=~paste('&lt;b&gt;', ID, '&lt;/b&gt;',\n              '&lt;br&gt;Urban  :', x,\n              '&lt;br&gt;Murder :', y,\n              '&lt;br&gt;Predicted Murder :', round(pred,2),\n              '&lt;br&gt;Residual :', round(resid,2)))              \n# Add Legend\nfig &lt;- plotly::layout(fig,\n          showlegend=F,\n          title='Crime and Urbanization in America 1975',\n          xaxis = list(title='Percent of People in an Urban Area'),\n          yaxis = list(title='Homicide Arrests per 100,000 People'))\n# Plot Model Predictions\nadd_trace(fig, x=~x, y=~pred,\n    inherit=F, hoverinfo='none',\n    mode='lines+markers', type='scatter',\n    color=I('black'),\n    line=list(width=1/2),\n    marker=list(symbol=134, size=5))\n\n\n\n\n\n\nFor a quantitative summary, we can also compute the linear correlation between the predictions and the data \\[\nR = Cor( \\hat{y}_i, y)\n\\] With linear models, we typically compute \\(R^2\\), known as the “coefficient of determination”, using the sums of squared errors (Total, Explained, and Residual) \\[\n\\underbrace{\\sum_{i}(y_i-\\bar{y})^2}_\\text{TSS}=\\underbrace{\\sum_{i}(\\hat{y}_i-\\bar{y})^2}_\\text{ESS}+\\underbrace{\\sum_{i}\\hat{\\epsilon_{i}}^2}_\\text{RSS}\\\\\nR^2 = \\frac{ESS}{TSS}=1-\\frac{RSS}{TSS}\n\\]\n\n\nCode\n# Manually Compute R2\nEhat &lt;- resid(reg)\nRSS  &lt;- sum(Ehat^2)\nY &lt;- xy$y\nTSS  &lt;- sum((Y-mean(Y))^2)\nR2 &lt;- 1 - RSS/TSS\nR2\n## [1] 0.00484035\n\n# Check R2\nsummary(reg)$r.squared\n## [1] 0.00484035\n\n# Double Check R2\nR &lt;- cor(xy$y, predict(reg))\nR^2\n## [1] 0.00484035",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple Regression</span>"
    ]
  },
  {
    "objectID": "02_01_BasicRegression.html#variability-estimates",
    "href": "02_01_BasicRegression.html#variability-estimates",
    "title": "10  Simple Regression",
    "section": "10.2 Variability Estimates",
    "text": "10.2 Variability Estimates\nA regression coefficient is a statistic. And, just like all statistics, we can calculate\n\nstandard deviation: variability within a single sample.\nstandard error: variability across different samples.\nconfidence interval: range your statistic varies across different samples.\n\nNote that values reported by your computer do not necessarily satisfy this definition. To calculate these statistics, we will estimate variability using data-driven methods. (For some theoretical background, see, e.g., https://www.sagepub.com/sites/default/files/upm-binaries/21122_Chapter_21.pdf.)\n\nJackknife.\nWe first consider the simplest, the jackknife. In this procedure, we loop through each row of the dataset. And, in each iteration of the loop, we drop that observation from the dataset and reestimate the statistic of interest. We then calculate the standard deviation of the statistic across all ``subsamples’’.\n\n\nCode\n# Jackknife Standard Errors for OLS Coefficient\njack_regs &lt;- lapply(1:nrow(xy), function(i){\n    xy_i &lt;- xy[-i,]\n    reg_i &lt;- lm(y~x, dat=xy_i)\n})\njack_coefs &lt;- sapply(jack_regs, coef)['x',]\njack_se &lt;- sd(jack_coefs)\n# classic_se &lt;- sqrt(diag(vcov(reg)))[['x']]\n\n\n# Jackknife Sampling Distribution\nhist(jack_coefs, breaks=25,\n    main=paste0('SE est. = ', round(jack_se,4)),\n    font.main=1, border=NA,\n    xlab=expression(beta[-i]))\n# Original Estimate\nabline(v=coef(reg)['x'], lwd=2)\n# Jackknife Confidence Intervals\njack_ci_percentile &lt;- quantile(jack_coefs, probs=c(.025,.975))\nabline(v=jack_ci_percentile, lty=2)\n\n\n\n\n\n\n\n\n\nCode\n\n\n# Plot Normal Approximation\n# jack_ci_normal &lt;- jack_mean+c(-1.96, +1.96)*jack_se\n# abline(v=jack_ci_normal, col=\"red\", lty=3)\n\n\n\n\nBootstrap.\nThere are several resampling techniques. The other main one is the bootstrap, which resamples with replacement for an arbitrary number of iterations. When bootstrapping a dataset with \\(n\\) observations, you randomly resample all \\(n\\) rows in your data set \\(B\\) times. Random subsampling is one of many hybrid approaches that tries to combine the best of both worlds.\n\n\n\n\nSample Size per Iteration\nNumber of Iterations\nResample\n\n\n\n\nBootstrap\n\\(n\\)\n\\(B\\)\nWith Replacement\n\n\nJackknife\n\\(n-1\\)\n\\(n\\)\nWithout Replacement\n\n\nRandom Subsample\n\\(m &lt; n\\)\n\\(B\\)\nWithout Replacement\n\n\n\n\n\nCode\n# Bootstrap\nboot_regs &lt;- lapply(1:399, function(b){\n    b_id &lt;- sample( nrow(xy), replace=T)\n    xy_b &lt;- xy[b_id,]\n    reg_b &lt;- lm(y~x, dat=xy_b)\n})\nboot_coefs &lt;- sapply(boot_regs, coef)['x',]\nboot_se &lt;- sd(boot_coefs)\n\nhist(boot_coefs, breaks=25,\n    main=paste0('SE est. = ', round(boot_se,4)),\n    font.main=1, border=NA,\n    xlab=expression(beta[b]))\nboot_ci_percentile &lt;- quantile(boot_coefs, probs=c(.025,.975))\nabline(v=boot_ci_percentile, lty=2)\nabline(v=coef(reg)['x'], lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Random Subsamples\nrs_regs &lt;- lapply(1:399, function(b){\n    b_id &lt;- sample( nrow(xy), nrow(xy)-10, replace=F)\n    xy_b &lt;- xy[b_id,]\n    reg_b &lt;- lm(y~x, dat=xy_b)\n})\nrs_coefs &lt;- sapply(rs_regs, coef)['x',]\nrs_se &lt;- sd(rs_coefs)\n\nhist(rs_coefs, breaks=25,\n    main=paste0('SE est. = ', round(rs_se,4)),\n    font.main=1, border=NA,\n    xlab=expression(beta[b]))\nabline(v=coef(reg)['x'], lwd=2)\nrs_ci_percentile &lt;- quantile(rs_coefs, probs=c(.025,.975))\nabline(v=rs_ci_percentile, lty=2)\n\n\n\n\n\n\n\n\n\nWe can also bootstrap other statistics, such as a t-statistic or \\(R^2\\). We do such things to test a null hypothesis, which is often ``no relationship’’. We are rarely interested in computing standard errors and conducting hypothesis tests for two variables. However, we work through the ideas in the two-variable case to better understand the multi-variable case.",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple Regression</span>"
    ]
  },
  {
    "objectID": "02_01_BasicRegression.html#hypothesis-tests",
    "href": "02_01_BasicRegression.html#hypothesis-tests",
    "title": "10  Simple Regression",
    "section": "10.3 Hypothesis Tests",
    "text": "10.3 Hypothesis Tests\n\nInvert a CI.\nOne main way to conduct hypothesis tests is to examine whether a confidence interval contains a hypothesized value. Does the slope coefficient equal \\(0\\)? For reasons we won’t go into in this class, we typically normalize the coefficient by its standard error: \\[ \\hat{t} = \\frac{\\hat{\\beta}}{\\hat{\\sigma}_{\\hat{\\beta}}} \\]\n\n\nCode\ntvalue &lt;- coef(reg)['x']/jack_se\n\njack_t &lt;- sapply(jack_regs, function(reg_b){\n    # Data\n    xy_b &lt;- reg_b$model\n    # Coefficient\n    beta_b &lt;- coef(reg_b)[['x']]\n    t_hat_b &lt;- beta_b/jack_se\n    return(t_hat_b)\n})\n\nhist(jack_t, breaks=25,\n    main='Jackknife t Density',\n    font.main=1, border=NA,\n    xlab=expression(hat(t)[b]), \n    xlim=range(c(0, jack_t)) )\nabline(v=quantile(jack_t, probs=c(.025,.975)), lty=2)\nabline(v=0, col=\"red\", lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nImpose the Null.\nWe can also compute a null distribution. We focus on the simplest: bootstrap simulations that each impose the null hypothesis and re-estimate the statistic of interest. Specifically, we compute the distribution of t-values on data with randomly reshuffled outcomes (imposing the null), and compare how extreme the observed value is.\n\n\nCode\n# Null Distribution for Beta\nboot_t0 &lt;- sapply( 1:399, function(b){\n    xy_b &lt;- xy\n    xy_b$y &lt;- sample( xy_b$y, replace=T)\n    reg_b &lt;- lm(y~x, dat=xy_b)\n    beta_b &lt;- coef(reg_b)[['x']]\n    t_hat_b &lt;- beta_b/jack_se\n    return(t_hat_b)\n})\n\n# Null Bootstrap Distribution\nboot_ci_percentile0 &lt;- quantile(boot_t0, probs=c(.025,.975))\nhist(boot_t0, breaks=25,\n    main='Null Bootstrap Density',\n    font.main=1, border=NA,\n    xlab=expression(hat(t)[b]),\n    xlim=range(boot_t0))\nabline(v=boot_ci_percentile0, lty=2)\nabline(v=tvalue, col=\"red\", lwd=2)\n\n\n\n\n\n\n\n\n\nAlternatively, you can impose the null by recentering the sampling distribution around the theoretical value; \\[\\hat{t} = \\frac{\\hat{\\beta} - \\beta_{0} }{\\hat{\\sigma}_{\\hat{\\beta}}}.\\] Under some assumptions, the null distribution follows a t-distribution. (For more on parametric t-testing based on statistical theory, see https://www.econometrics-with-r.org/4-lrwor.html.)\nIn any case, we can calculate a p-value: the probability you would see something as extreme as your statistic under the null (assuming your null hypothesis was true). We can always calculate a p-value from an explicit null distribution.\n\n\nCode\n# One Sided Test for P(t &gt; boot_t | Null) = 1 - P(t &lt; boot_t | Null)\nThat_NullDist1 &lt;- ecdf(boot_t0)\nPhat1  &lt;- 1-That_NullDist1(jack_t)\n\n# Two Sided Test for P(t &gt; jack_t or t &lt; -jack_t | Null)\nThat_NullDist2 &lt;- ecdf(abs(boot_t0))\nplot(That_NullDist2, xlim=range(boot_t0, jack_t),\n    xlab=expression( abs(hat(t)[b]) ),\n    main='Null Bootstrap Distribution', font.main=1)\nabline(v=tvalue, col='red')\n\n\n\n\n\n\n\n\n\nCode\n\nPhat2  &lt;-  1-That_NullDist2( abs(tvalue))\nPhat2\n## [1] 0.6441103",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple Regression</span>"
    ]
  },
  {
    "objectID": "02_01_BasicRegression.html#data-transformations",
    "href": "02_01_BasicRegression.html#data-transformations",
    "title": "10  Simple Regression",
    "section": "10.4 Data Transformations",
    "text": "10.4 Data Transformations\n\nJensen’s Inequality.\n\n\nQuantile Results.",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple Regression</span>"
    ]
  },
  {
    "objectID": "02_02_KernelIntro.html",
    "href": "02_02_KernelIntro.html",
    "title": "11  Local Regression",
    "section": "",
    "text": "11.1 Local Relationships",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Local Regression</span>"
    ]
  },
  {
    "objectID": "02_02_KernelIntro.html#local-relationships",
    "href": "02_02_KernelIntro.html#local-relationships",
    "title": "11  Local Regression",
    "section": "",
    "text": "The Effect.\nThe interpretation of regression coefficients as ``the effect’’ assumes the linear model is true. If you fit a line to a non-linear relationship then you will get back a number, but there is no singular the effect if the true relationship is non-linear! Consider a classic example, Anscombe’s Quartet, which shows four very different datasets that give the same linear regression coefficient. You understand the problem because we used scatterplots to visual the data.1\n\n\nCode\n##################\n# Anscombe\n##################\n\npar(mfrow=c(2,2))\nfor(i in 1:4){\n    xi &lt;- anscombe[,paste0('x',i)]\n    yi &lt;- anscombe[,paste0('y',i)]\n    plot(xi, yi, ylim=c(4,13), xlim=c(4,20),\n        pch=16, col=grey(0,.6))\n    reg &lt;- lm(yi~xi)\n    b &lt;- round(coef(reg)[2],2)\n    p &lt;- round(summary(reg)$coefficients[2,4],4)\n    abline(reg, col='orange')\n    title(paste0(\"Slope=\", b,', p=',p), font.main=1)\n}\n\n\n\n\n\n\n\n\n\nCode\n\n## For an even better example, see `Datasaurus Dozen'\n#browseURL(\n#'https://bookdown.org/paul/applied-data-visualization/\n#why-look-the-datasaurus-dozen.html')\n\n\nIt is true that “OLS is the best linear predictor of the nonlinear regression function if the mean-squared error is used as the loss function.” . But this is not a carte-blanche justification for OLS—the best of the bad predictors is still a bad predictor. For many economic applications, it is more helpful to think and speak of “dose response curves” instead of “the effect”.\nWhile adding interaction terms or squared terms allows one incorporate heterogeneity and non-linearity, they change several features of the model—most of which are not intended. Often, there are nonsensical predicted values. For example, if the most of your age data are between \\([23,65]\\), a quadratic term can imply silly things for people aged \\(10\\) or \\(90\\).\nNonetheless, OLS provides an important piece of quantitative information that is understood by many. All models are an approximation, and sometimes only unimportant nuances are missing from a vanilla linear model. Other times, that model can be seriously misleading. (This is especially true if your making policy recommondations based on a universal ``the effect’’.) As an exploratory tool, OLS is a good guess but one whose point estimates should not be taken too seriously (in which case, the standard errors are also much less important). Before trying to find a regression specification that makes sense for the entire dataset, explore local relationships.\n\n\nLocal Relationships.\nScatterplots are a great and simplest plot for bivariate data that simply plots each observation. There are many extensions and similar tools. The example below shows two ways of summarizing the information; in both cases helping you understand how the central tendency and dispersion change.\n\n\nCode\n##################\n# Application: Summarizing wages\n##################\nlibrary(wooldridge)\n\n## Plot 1\nplot(wage~educ, data=wage1, pch=16, col=grey(0,.1))\neduc_means &lt;- aggregate(wage1[,c(\"wage\",\"educ\")], list(wage1$educ), mean)\npoints(wage~educ, data=educ_means, pch=17, col='blue', type='b')\ntitle(\"Grouped Means and Scatterplot\", font.main=1)\n\n\n\n\n\n\n\n\n\nCode\n\n## Plot 2 (Less informative!)\n#barplot(wage~educ, data=educ_means)\n#title(\"Bar Plot of Grouped Means\")\n\n\n\n\nRegressograms.\nJust as we use histograms to describe the distribution of a random variable, we can use a regressogram for conditional relationships. Specifically, we can use dummies for exclusive intervals or bins to estimate how the average value of \\(Y\\) varies with \\(X\\).\nAfter dividing \\(X\\) into \\(1,...L\\) exclusive bins of width \\(h\\). Each bin has a midpoint, \\(x\\), and an associated dummy variable \\(D(X_{i},x,h)\\). Then conduct a dummy variable regression \\[\\begin{eqnarray}\nY_{i} &=& \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} \\alpha(x) D(X_{i},x,h)  + e_{i}.\n\\end{eqnarray}\\] Notice that each bin has $N(x,h) = {i}^{N}D(X{i},x,h) $ observations. We can split the dataset into parts associated with each bin; \\[\\begin{eqnarray}\n\\label{eqn:regressogram1}\n\\sum_{i}^{N}\\left[e_{i}\\right]^2\n&=& \\sum_{i}^{N}\\left[Y_{i}- \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} \\alpha(x,h) D(X_{i},x,h) \\right]^2 \\\\\n&=& \\sum_{i}^{N(x_{1},h)}\\left[Y_{i}- \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} \\alpha(x,h) D(X_{i},x,h) \\right]^2 + ...  \\nonumber  \\\\\n& & \\sum_{i}^{N(x_{L},h)}\\left[Y_{i}- \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} \\alpha(x,h) D(X_{i},x,h) \\right]^2 \\\\\n&=& \\sum_{i}^{N(x_{1},h)}\\left[Y_{i}- \\alpha\\left(x_1,h\\right) \\right]^2 + ... \\sum_{i}^{N(x_L,h)}\\left[Y_{i}-\\alpha\\left(x_L,h\\right) \\right]^2 % +~ (N-1)\\sum_{i}Y_{i}\n\\end{eqnarray}\\] Then notice that we can optimize each bin separately; \\[\\begin{eqnarray}\n\\label{eqn:regressogram2}\n\\text{argmin}_{ \\left\\{ \\alpha(x,h) \\right\\} } \\sum_{i}^{N}\\left[e_{i}\\right]^2\n&=& \\text{argmin}_{ \\left\\{ \\alpha(x,h) \\right\\} } \\sum_{i}^{N(x,h)}\\left[Y_{i}- \\alpha\\left(x,h\\right) \\right]^2,\n\\end{eqnarray}\\] since, in either case, minimizing yields \\[\\begin{eqnarray}\n0 &=& -2 \\sum_{i}^{N(x,h)}\\left[ Y_{i} - \\alpha(x,h)  \\right] \\\\\n\\widehat{\\alpha}(x,h) &=& \\frac{\\sum_{i}^{N(x,h)} Y_{i}}{ N(x,h) } .\n\\end{eqnarray}\\] As such, the OLS regression yields coefficients that are intepreted as the conditional mean of \\(Y\\). We can directly compute the same statistic directly by simply takes the average value of \\(Y\\) for all \\(i\\) observations in a particular bin.\nInterestingly, we can obtain the same statistic from weighted least squares regression. For some specific design point, \\(x\\), we can find \\(\\widehat{\\alpha}(x, h)\\) by minimizing \\[\\begin{eqnarray}\n& & \\sum_{i}^{N}\\left[ Y_{i}- \\alpha(x,h) \\right]^2  D(X_{i},x,h) \\\\\n&=& \\sum_{i}^{N(x_{1},h)}\\left[ Y_{i}- \\alpha(x,h) \\right]^2  D(X_{i},x,h) + ... \\sum_{i}^{N(x_{L},h)}\\left[ Y_{i}- \\alpha(x,h) \\right]^2  D(X_{i},x,h) \\\\\n&=& \\sum_{i}^{N(x,h)}\\left[Y_{i}- \\alpha\\left(x,h\\right) \\right]^2\n\\end{eqnarray}\\]\nFinally, predicted values are \\(\\widehat{Y}_{i} = \\sum_{x} \\widehat{\\alpha}(x,h) D(X_{i},x,h)\\).\nConsider this three-bin example of how age affects wage. \\[\\begin{eqnarray}\n\\text{Wage} &=& \\alpha_{1} \\mathbf{1}\\left(\\text{Age} &lt; 23\\right) + \\alpha_{2} \\mathbf{1}\\left(\\text{Age} \\in [23,65) \\right) +  \\alpha_{2} \\mathbf{1}\\left( \\text{Age} \\geq 65) \\right) + \\epsilon \\nonumber\n\\end{eqnarray}\\] I.e., the main effect on wages is whether your the right age to work (not in school or retired). But you could also look at yearly bins and see if that tri-part grouping emerges naturally or not (e.g., whether we shouldn’t group all working-age people together). For example,\n\n\nCode\n##################\n# Regressogram\n##################\n\n## Ages\nXmx &lt;- 70\nXmn &lt;- 15\n\n##Generate N Observations\ndat_sim &lt;- function(n=1000){\n    n  &lt;- 1000\n    X &lt;- seq(Xmn,Xmx,length.out=n)\n    ## Random Productivity\n    e &lt;- runif(n, 0, 1E6)\n    beta &lt;-  1E-10*exp(1.4*X -.015*X^2)\n    Y    &lt;-  (beta*X + e)/10\n    return(data.frame(Y,X))\n}\n\n\ndat &lt;- dat_sim(1000)\nX &lt;- dat$X\n## Plot\nplot(Y~X, data=dat, pch=16, col=grey(0,.1),\n    ylab='Yearly Productivity ($)', xlab='Age' )\n\n## Regression Estimates\nreg1  &lt;- lm(Y~X, data=dat) ## OLS\npred1 &lt;- cbind( Y=predict(reg1), X)[order(X),]\n\ndat$xcc   &lt;- cut(X, seq(Xmn-1,Xmx,length.out=6)) ## Course Age Bins\nreg2  &lt;- lm(Y~xcc, data=dat)\npred2 &lt;- cbind( Y=predict(reg2), X)[order(X),]\n\ndat$xcf   &lt;- cut(X, seq(Xmn-1, Xmx, length.out=31)) ## Fine Age Bins\nreg3  &lt;- lm(Y~xcf, data=dat)\npred3 &lt;- cbind( Y=predict(reg3), X)[order(X),]\n\n## Compare Models\nlines(Y~X, data=pred1, lwd=2, col=2)\nlines(Y~X, data=pred2, lwd=2, col=3)\nlines(Y~X, data=pred3, lwd=2, col=4)\nlegend('topleft',\n    legend=c('Linear Regression','Regressogram (5)','Regressogram (30)'),\n    lty=1, col=2:4, cex=.8)\n\n\n\n\n\n\n\n\n\n\n\nPiecewise Regression.\nThe regressogram depicts locally constant relationships. We can also included slope terms within each bin to allow for locally linear relationships. This is often called segmented/piecewise regression, which runs a separate regression for different subsets of the data.\n\\[\\begin{eqnarray}\nY_{i} &=& \\sum_{x} \\left[\\alpha(x) + X_{i}\\beta(x) \\right] D(X_{i},x,h) + \\epsilon_{i}.\n\\end{eqnarray}\\]\n\n\nCode\n##################\n# Regressogram w/ Slopes\n##################\n\n## Plot\ndat &lt;- dat_sim(1000)\nX &lt;- dat$X\nplot(Y~X, data=dat, pch=16, col=grey(0,.1),\n    ylab='Yearly Productivity ($)', xlab='Age' )\n\n## Course Age Bins\n#### Single Regression\ndat$xcc   &lt;- cut(X, seq(Xmn-1,Xmx,length.out=6)) ## Course Age Bins\npred4 &lt;- cbind( Y=predict( lm(Y~xcc*X, data=dat) ), X)[order(X),]\n#### Split Sample Regressions\ndat4 &lt;- split( dat, dat$xcc)\npred4_B &lt;- lapply(dat4, function(d){\n    pred_d &lt;- cbind( Y=predict( lm(Y~X, d)), X=d$X)\n})\npred4_B &lt;- as.data.frame(do.call('rbind', pred4_B))\npred4_B &lt;- pred4_B[order(pred4_B$X),]\n\nlines(Y~X, data=pred4, lwd=2, col=5, lty=1)\nlines(Y~X, data=pred4_B, lwd=4, col=5, lty=3)\n\n\n## Fine Age Bins\n#### Single Regression\ndat$xcf  &lt;- cut(X, seq(Xmn-1,Xmx,length.out=31)) ## Course Age Bins\npred5 &lt;- cbind( Y=predict(lm(Y~xcf*X,data=dat)), X)[order(X),]\n#### Split Sample Regressions\ndat5 &lt;- split(dat, dat$xcf)\npred5_B &lt;- lapply(dat5, function(d){\n    pred_d &lt;- cbind( Y=predict(lm(Y~X, d)), X=d$X)\n})\npred5_B &lt;- as.data.frame(do.call('rbind', pred5_B))\npred5_B &lt;- pred5_B[order(pred5_B$X),]\n\n## Compare Models\nlines(Y~X, data=pred5, lwd=2, col=6, lty=1)\nlines(Y~X, data=pred5_B, lwd=4, col=6, lty=3)\nlegend('topleft', \n    legend=c('5 bins','30 bins'),\n    lty=1, col=5:6, cex=.8)\n\n\n\n\n\n\n\n\n\nHere is another example with a real dataset\n\n\nCode\nxy &lt;- USArrests[,c('Murder','UrbanPop')]\ncolnames(xy) &lt;- c('y','x')\n\n# Globally Linear\nreg &lt;- lm(y~x, data=xy)\n\n# Diagnose Fit\n#plot( fitted(reg), resid(reg), pch=16, col=grey(0,.5))\n#plot( xy$x, resid(reg), pch=16, col=grey(0,.5))\n\n# Linear in 2 Pieces (subsets)\nxcut2 &lt;- cut(xy$x,2)\nxy_list2 &lt;- split(xy, xcut2)\nregs2 &lt;- lapply(xy_list2, function(xy_s){\n    lm(y~x, data=xy_s)\n})\nsapply(regs2, coef)\n##             (31.9,61.5] (61.5,91.1]\n## (Intercept)  -0.2836303  4.15337509\n## x             0.1628157  0.04760783\n\n# Linear in 3 Pieces (subsets or bins)\nxcut3 &lt;- cut(xy$x, seq(32,92,by=20)) # Finer Bins\nxy_list3 &lt;- split(xy, xcut3)\nregs3 &lt;- lapply(xy_list3, function(xy_s){\n    lm(y~x, data=xy_s)\n})\nsapply(regs3, coef)\n##                (32,52]    (52,72]      (72,92]\n## (Intercept) 4.60313390 2.36291848  8.653829140\n## x           0.08233618 0.08132841 -0.007174454\n\n\nCompare Predictions\n\n\nCode\npred1 &lt;- data.frame(yhat=predict(reg), x=reg$model$x)\npred1 &lt;- pred1[order(pred1$x),]\n\npred2 &lt;- lapply(regs2, function(reg){\n    data.frame(yhat=predict(reg), x=reg$model$x)\n})\npred2 &lt;- do.call(rbind,pred2)\npred2 &lt;- pred2[order(pred2$x),]\n\npred3 &lt;- lapply(regs3, function(reg){\n    data.frame(yhat=predict(reg), x=reg$model$x)\n})\npred3 &lt;- do.call(rbind,pred3)\npred3 &lt;- pred3[order(pred3$x),]\n\n# Compare Predictions\nplot(y ~ x, pch=16, col=grey(0,.5), dat=xy)\nlines(yhat~x, pred1, lwd=2, col=2)\nlines(yhat~x, pred2, lwd=2, col=4)\nlines(yhat~x, pred3, lwd=2, col=3)\nlegend('topleft',\n    legend=c('Globally Linear', 'Peicewise Linear (2)','Peicewise Linear (3)'),\n    lty=1, col=c(2,4,3), cex=.8)\n\n\n\n\n\n\n\n\n\nFor many things, this pseudo-smoothing is “good enough” or even “great”. It really depends on the question. (It is also super computationally efficient.) But sometimes we want (a) smooth estimates or (b) to make predictions or estimate derivatives at the data. To cover more advanced regression methods, we will need to first learn about kernel density estimation.",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Local Regression</span>"
    ]
  },
  {
    "objectID": "02_02_KernelIntro.html#kernel-density-estimation",
    "href": "02_02_KernelIntro.html#kernel-density-estimation",
    "title": "11  Local Regression",
    "section": "11.2 Kernel Density Estimation",
    "text": "11.2 Kernel Density Estimation\nA kernel density is generally a “smooth” version of a histogram. We estimate the density at many points (e.g., all unique values \\(x\\) in the dataset), not just the midpoints of exclusive bins. The uniform kernel and density estimator is \\[\\begin{eqnarray}\n\\label{eqn:uniform}\n\\widehat{f}_{unif}(x) &=& \\frac{1}{N} \\sum_{i}^{N} \\frac{k_{U}(X, x_{i}, h) }{2h} \\\\\nk_{U}\\left( X_{i}, x, h \\right) &=& \\mathbf{1}\\left(\\frac{|X_{i}-x|}{h}&lt;1\\right)\n= \\mathbf{1}\\left( X_{i} \\in \\left( x-h, x + h\\right) \\right).\n\\end{eqnarray}\\] Comparing equations \\(\\ref{eqn:uniform}\\) to \\(\\ref{eqn:indicator}\\), we can see the uniform kernel is essentially the histogram but without the restriction that \\(x\\) must be a midpoint of exclusive bins. Typically, the points \\(x\\) are chosen to be either the unique observations or some equidistant set of “design points”.\nWe can also replace the uniform kernel with a more general kernel function \\(k\\left( X_{i}, x, h \\right)= K\\left( \\frac{|X_{i}-x|}{h} \\right)\\). (We normalize \\(k\\), which is easier to program, so that the kernel function \\(K\\) take a single argument that is easier to read.) The general idea behind kernel density is to use windows around each \\(x\\) that potentially overlap, rather than partitioning the range of \\(X\\) into exclusive bins.\nWe define a general kernel function as a non-negative real-valued function \\(K\\) that integrates to unity: \\[\\begin{eqnarray}\n\\int_{-\\infty}^{\\infty} K(v) dv &=& 1\n\\end{eqnarray}\\] We also only examine symmetric kernels, as some texts also include symmetric in the definition of a kernel; \\(K(v) = K(-v)\\).\nFor examples of some common kernels, see https://en.wikipedia.org/wiki/Kernel_(statistics)#In_non-parametric_statistics. In my view, these are the most intuitive and common.\n\n\nCode\n##################\n# Kernel Density Functions\n##################\n\nX &lt;- seq(-2,2, length.out=1001)\n\nplot.new()\nplot.window(xlim=c(-1,1), ylim=c(0,1))\n\nh &lt;- 1\nlines( dunif(X,-h,h)~X, col=1, lty=1)\n\nh &lt;- 1/2\nlines( dnorm(X,0,h)~X, col=2, lty=1)\n\ndtricub &lt;- function(X, x=0, h){\n    u &lt;- abs(X-x)/h\n    fu &lt;- 70/81*(1-u^3)^3/h*(u &lt;= 1)\n    return(fu)\n}\nh &lt;- 1\nlines( dtricub(X,0,h)~X, col=3, lty=1)\n\nh &lt;- 1/2\nlines(density(x=0, bw=h, kernel=\"epanechnikov\"), col=4, lty=1)\n## Note that \"density\" defines h slightly differently\n\nsegments(0,1,0,0, col=grey(0,1), lwd=2, lend=2)\naxis(1)\naxis(2)\n\nlegend('topright', lty=1, col=1:4,\n    legend=c('uniform(1)', 'gaussian(1/2)', 'tricubic(1)', 'epanechnikov(1)'))\n\n\n\n\n\n\n\n\n\nCode\n\n\n## Try others:\n## lines(density(x=0, bw=1/2, kernel=\"triangular\"),col=4, lty=1)\n\n\nOnce we have picked a kernel (which particular one is not particularly important) we can use it to compute density estimates.\n\n\nCode\n##################\n# Kernel Density Estimation\n##################\n\nN &lt;- 1000\ne &lt;- rweibull(N,100,100)\nebins &lt;- seq(floor(min(e)), ceiling(max(e)), length.out=12)\n\n## Histogram Estimates at 12 points\nxbks &lt;- c(ebins[1]-diff(ebins)[1]/2, ebins+diff(ebins)[1]/2)\nhist(e, freq=F, main='', breaks=xbks, ylim=c(0,.4), border=NA)\nrug(e, lwd=.07, col=grey(0,.5))  ## Sample\n\n\n## Manually Compute Uniform Estimate at X=100 with h=2\n# w100 &lt;- (e &lt; 101)*(e &gt; 99)\n# sum(w100)/(N*2)\n\n## Gaussian Estimates at same points as histogram\nF_hat &lt;- sapply(ebins, function(x,h=.5){\n    kx &lt;- dnorm( abs(e-x)/h )\n    fx &lt;- sum(kx,na.rm=T)/(h*N)\n    fx\n})\nlines(ebins, F_hat, col=1, lty=1, type='o')\n## Verify the same\nfhat1 &lt;- density(e, n=12, from=min(ebins), to=max(ebins), bw=.5)\npoints(fhat1$x, fhat1$y, pch=16, col=rgb(0,0,1,.5), cex=1.5)\n\n## Gaussian Estimates at all sample points\nfhat2 &lt;- density(e, n=1000, from=min(ebins), to=max(ebins), bw=.5)\npoints(fhat2$x, fhat2$y, pch=16, col=rgb(1,0,0,.25), cex=.5)\n\nlegend('topleft', pch=c(15,16,16),\n    col=c(grey(0,.5),rgb(0,0,1,.5), rgb(1,0,0,.25)),\n    title='Type (# Design Points)', bty='n',\n    legend=c('Histogram (12)',\n    'Gaussian-Kernel (12)',\n    'Gaussian-Kernel (1000)'))",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Local Regression</span>"
    ]
  },
  {
    "objectID": "02_02_KernelIntro.html#local-linear-regression",
    "href": "02_02_KernelIntro.html#local-linear-regression",
    "title": "11  Local Regression",
    "section": "11.3 Local Linear Regression",
    "text": "11.3 Local Linear Regression\nIt is safer to assume that you could be analyzing data with nonlinear relationships. A general nonparametric model is written as \\[\\begin{eqnarray}\nY_{i} = m(X_{i}) + \\epsilon_{i}\n\\end{eqnarray}\\] where \\(m\\) is an unknown continuous function and \\(\\epsilon\\) is white noise. (As such, the linear model is a special case.) You can estimate the model (the mean of \\(Y\\) conditional on \\(X=x\\)) with a regressogram or a variety of other least-squares procedures.\n\nLocally Constant.\nConsider a point \\(x\\) and suppose \\(Y_{i} = \\alpha(x) + e_{i}\\) locally. Then notice a weighted OLS estimator with uniform kernel weights yields \\[\\begin{eqnarray} \\label{eqn:lcls}\n& & \\min_{\\alpha(x)}~ \\sum_{i}^{N}\\left[e_{i} \\right]^2 k_{U}\\left( X_{i}, x, h \\right) \\\\\n\\Rightarrow & & -2 \\sum_{i}^{N}\\left[Y_{i}- \\alpha(x) \\right] k_{U}\\left(X_{i}, x, h\\right) = 0\\\\\n\\label{eqn:lcls1}\n\\Rightarrow & & \\widehat{\\alpha_{U}}(x)\n= \\frac{\\sum_{i} Y_{i} k_{U} \\left( X_{i}, x, h \\right) }{ \\sum_{i} k_{U}\\left( X_{i}, x, h \\right) }\n= \\sum_{i} Y_{i} \\left[ \\frac{ k_{U} \\left( X_{i}, x, h \\right) }{ \\sum_{i} k_{U}\\left( X_{i}, x, h \\right)} \\right] =  \\sum_{i} Y_{i} w_{i},\n\\end{eqnarray}\\] where weight \\(w_{i} = \\mathbf{1}\\left( |X_{i} - x| &lt; h \\right)/N\\). The last equality is derived analogously to equation \\(\\ref{eqn:sum}\\); where $ k_{U} ( X_{i}, x, h )$ is either one or zero, and \\(\\sum_{i} k_{U} \\left( X_{i}, x, h \\right) = N(x)\\).\nWhen \\(N\\) is small, \\(\\widehat{\\alpha_U}(x)\\) is typically estimated for each observed value: \\(x \\in \\{ x_{1},...x_{N} \\}\\). For large datasets, you can select a subset or evenly spaced values of \\(x\\) for which to estimate \\(\\widehat{\\alpha_{U}}(x)\\). If we use exclusive bins, then equation \\(\\ref{eqn:regressogram1}\\) equals \\(\\ref{eqn:lcls1}\\), which shows the regressogram is a kernel regression weights that recovers the conditional mean. This regressogram is more crude but can be estimated with OLS.\n\n\nCode\n##################\n# LCLS\n##################\n## Generate Sample Data\nx &lt;- 1:5\ny &lt;- runif(length(x))\n## plot(x,y)\n\n## Manually Compute Estimate at X=3\nw3 &lt;- dunif(x-3,-1,1) #(x &lt; 4)*(x &gt; 2)\nyhat_3 &lt;- sum(w3*y)\nyhat_3\n## [1] 0.4885941\n\n\nThe basic idea also generalizes other kernels. As such, a kernel regression using uniform weights is often called a ``naive kernel regression’’. Typically, kernel regressions use kernels that weight nearby observations more heavily. We can also add a slope term to improve the fit.\n\n\nLocally Linear.\nA less simple case is a local linear regression which conducts a linear regression for each data point using a subsample of data around it. Consider a point \\(x\\) and suppose \\(Y_{i} = \\alpha(x) + \\beta(x) X_{i} + e_{i}\\) for data near \\(x\\). The weighted OLS estimator with kernel weights is \\[\\begin{eqnarray}\n& & \\min_{\\alpha(x),\\beta(x)}~ \\sum_{i}^{N}\\left[Y_{i}- \\alpha(x) - \\beta(x) X_{i} \\right]^2 K\\left(\\frac{|X_{i}-x|}{h}\\right)\n\\end{eqnarray}\\] Deriving the optimal values \\(\\widehat{\\alpha}(x)\\) and \\(\\widehat{\\beta}(x)\\) for \\(k_{U}\\) is left as a homework exercise.2\n\n\nCode\n# ``Naive\" Smoother\npred_fun &lt;- function(x0, h, xy){\n    # Assign equal weight to observations within h distance to x0\n    # 0 weight for all other observations\n    ki   &lt;- dunif(xy$x, x0-h, x0+h) \n    llls &lt;- lm(y~x, data=xy, weights=ki)\n    yhat_i &lt;- predict(llls, newdata=data.frame(x=x0))\n}\n\nX0 &lt;- sort(unique(xy$x))\npred_lo1 &lt;- sapply(X0, pred_fun, h=2, xy=xy)\npred_lo2 &lt;- sapply(X0, pred_fun, h=20, xy=xy)\n\nplot(y~x, pch=16, data=xy, col=grey(0,.5),\n    ylab='Murder Rate', xlab='Population Density')\ncols &lt;- c(rgb(.8,0,0,.5), rgb(0,0,.8,.5))\nlines(X0, pred_lo1, col=cols[1], lwd=1, type='o')\nlines(X0, pred_lo2, col=cols[2], lwd=1, type='o')\nlegend('topleft', title='Locally Linear',\n    legend=c('h=2 ', 'h=20'),\n    lty=1, col=cols, cex=.8)\n\n\n\n\n\n\n\n\n\nNote that there are more complex versions of local linear regressions (see https://shinyserv.es/shiny/kreg/ for a nice illustration.) An even more complex (and more powerful) version is loess, which uses adaptive bandwidths in order to have a similar number of data points in each subsample (especially useful when \\(X\\) is not uniform.)\n\n\nCode\n# Adaptive-width subsamples with non-uniform weights\nxy0 &lt;- xy[order(xy$x),]\nplot(y~x, pch=16, col=grey(0,.5), dat=xy0)\n\nreg_lo4 &lt;- loess(y~x, data=xy0, span=.4)\nreg_lo8 &lt;- loess(y~x, data=xy0, span=.8)\n\ncols &lt;- hcl.colors(3,alpha=.75)[-3]\nlines(xy0$x, predict(reg_lo4),\n    col=cols[1], type='o', pch=2)\nlines(xy0$x, predict(reg_lo8),\n    col=cols[2], type='o', pch=2)\n\nlegend('topleft', title='Loess',\n    legend=c('span=.4 ', 'span=.8'),\n    lty=1, col=cols, cex=.8)\n\n\n\n\n\n\n\n\n\n\n\nConfidence Bands.\nThe smoothed predicted values estimate the local means. So we can also construct confidence bands\n\n\nCode\n# Loess\nxy0 &lt;- xy[order(xy$x),]\nX0 &lt;- unique(xy0$x)\nreg_lo &lt;- loess(y~x, data=xy0, span=.8)\n\n# Jackknife CI\njack_lo &lt;- sapply(1:nrow(xy), function(i){\n    xy_i &lt;- xy[-i,]\n    reg_i &lt;- loess(y~x, dat=xy_i, span=.8)\n    predict(reg_i, newdata=data.frame(x=X0))\n})\njack_cb &lt;- apply(jack_lo,1, quantile,\n    probs=c(.025,.975), na.rm=T)\n\n# Plot\nplot(y~x, pch=16, col=grey(0,.5), dat=xy0)\npreds_lo &lt;- predict(reg_lo, newdata=data.frame(x=X0))\nlines(X0, preds_lo,\n    col=hcl.colors(3,alpha=.75)[2],\n    type='o', pch=2)\n# Plot CI\npolygon(\n    c(X0, rev(X0)),\n    c(jack_cb[1,], rev(jack_cb[2,])),\n    col=hcl.colors(3,alpha=.25)[2],\n    border=NA)",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Local Regression</span>"
    ]
  },
  {
    "objectID": "02_02_KernelIntro.html#footnotes",
    "href": "02_02_KernelIntro.html#footnotes",
    "title": "11  Local Regression",
    "section": "",
    "text": "The same principles holds when comparing two groups: http://www.stat.columbia.edu/~gelman/research/published/causal_quartet_second_revision.pdf↩︎\nNote that one general benefit of LLLS is with edge effects (see homework). Another is that it is theoretically motivated: assuming that \\(Y_{i}=m(X_{i}) + \\epsilon_{i}\\), we can then take a Taylor approximation: \\(m(X_{i}) + \\epsilon_{i} \\approx m(x) + m'(x)[X_{i}-x] + \\epsilon_{i} = [m(x) - m'(x)x ] + m'(x)X_{i} + \\epsilon_{i} = \\alpha(x) + \\beta(x) X_{i}\\). As such, a third benefit is that the estimated coefficient \\(\\widehat{\\beta}\\) can be interpreted as a gradient estimate at \\(x\\).↩︎",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Local Regression</span>"
    ]
  },
  {
    "objectID": "02_03_IntermediateRegression.html",
    "href": "02_03_IntermediateRegression.html",
    "title": "12  Multivariate Data",
    "section": "",
    "text": "12.1 Multiple Linear Regression\nFirst, note that you can summarize a dataset with multiple variables using the previous tools.\nYou can also use size, color, and shape to distinguish conditional relationships.\nSee also https://plotly.com/r/bubble-charts/\nWith \\(K\\) variables, the linear model is \\[\ny_i=\\beta_0+\\beta_1 x_{i1}+\\beta_2 x_{i2}+\\ldots+\\beta_K x_{iK}+\\epsilon_i = [1~~  x_{i1} ~~...~~ x_{iK}] \\beta + \\epsilon_i\n\\] and our objective is \\[\nmin_{\\beta} \\sum_{i=1}^{N} (\\epsilon_i)^2.\n\\]\nDenoting \\[\ny= \\begin{pmatrix}\ny_{1} \\\\ \\vdots \\\\ y_{N}\n\\end{pmatrix} \\quad\n\\textbf{X} = \\begin{pmatrix}\n1 & x_{11} & ... & x_{1K} \\\\\n& \\vdots & & \\\\\n1 & x_{N1} & ... & x_{NK}\n\\end{pmatrix},\n\\] we can also write the model and objective in matrix form \\[\ny=\\textbf{X}\\beta+\\epsilon\\\\\nmin_{\\beta} (\\epsilon' \\epsilon)\n\\]\nMinimizing the squared errors yields coefficient estimates \\[\n\\hat{\\beta}=(\\textbf{X}'\\textbf{X})^{-1}\\textbf{X}'y\n\\] and predictions \\[\n\\hat{y}=\\textbf{X} \\hat{\\beta} \\\\\n\\hat{\\epsilon}=y - \\hat{y} \\\\\n\\]\nCode\n# Manually Compute\nY &lt;- USArrests[,'Murder']\nX &lt;- USArrests[,c('Assault','UrbanPop')]\nX &lt;- as.matrix(cbind(1,X))\n\nXtXi &lt;- solve(t(X)%*%X)\nBhat &lt;- XtXi %*% (t(X)%*%Y)\nc(Bhat)\n## [1]  3.20715340  0.04390995 -0.04451047\n\n# Check\nreg &lt;- lm(Murder~Assault+UrbanPop, data=USArrests)\ncoef(reg)\n## (Intercept)     Assault    UrbanPop \n##  3.20715340  0.04390995 -0.04451047\nTo measure the ``Goodness of fit’’ of the model, we can again plot our predictions.\nCode\nplot(USArrests$Murder, predict(reg), pch=16, col=grey(0,.5))\nabline(a=0,b=1, lty=2)\nWe can also again compute sums of squared errors. Adding random data may sometimes improve the fit, however, so we adjust the \\(R^2\\) by the number of covariates \\(K\\). \\[\nR^2 = \\frac{ESS}{TSS}=1-\\frac{RSS}{TSS}\\\\\nR^2_{\\text{adj.}} = 1-\\frac{N-1}{N-K}(1-R^2)\n\\]\nCode\nksims &lt;- 1:30\nfor(k in ksims){ \n    USArrests[,paste0('R',k)] &lt;- runif(nrow(USArrests),0,20)\n}\nreg_sim &lt;- lapply(ksims, function(k){\n    rvars &lt;- c('Assault','UrbanPop', paste0('R',1:k))\n    rvars2 &lt;- paste0(rvars, collapse='+')\n    reg_k &lt;- lm( paste0('Murder~',rvars2), data=USArrests)\n})\nR2_sim &lt;- sapply(reg_sim, function(reg_k){  summary(reg_k)$r.squared })\nR2adj_sim &lt;- sapply(reg_sim, function(reg_k){  summary(reg_k)$adj.r.squared })\n\nplot.new()\nplot.window(xlim=c(0,30), ylim=c(0,1))\npoints(ksims, R2_sim)\npoints(ksims, R2adj_sim, pch=16)\naxis(1)\naxis(2)\nmtext(expression(R^2),2, line=3)\nmtext('Additional Random Covariates', 1, line=3)\nlegend('topleft', horiz=T,\n    legend=c('Undjusted', 'Adjusted'), pch=c(1,16))",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multivariate Data</span>"
    ]
  },
  {
    "objectID": "02_03_IntermediateRegression.html#factor-variables",
    "href": "02_03_IntermediateRegression.html#factor-variables",
    "title": "12  Multivariate Data",
    "section": "12.2 Factor Variables",
    "text": "12.2 Factor Variables\nSo far, we have discussed cardinal data where the difference between units always means the same thing: e.g., \\(4-3=2-1\\). There are also factor variables\n\nOrdered: refers to Ordinal data. The difference between units means something, but not always the same thing. For example, \\(4th - 3rd \\neq 2nd - 1st\\).\nUnordered: refers to Categorical data. The difference between units is meaningless. For example, \\(B-A=?\\)\n\nTo analyze either factor, we often convert them into indicator variables or dummies; \\(D_{c}=\\mathbf{1}( Factor = c)\\). One common case is if you have observations of individuals over time periods, then you may have two factor variables. An unordered factor that indicates who an individual is; for example \\(D_{i}=\\mathbf{1}( Individual = i)\\), and an order factor that indicates the time period; for example \\(D_{t}=\\mathbf{1}( Time \\in [month~ t, month~ t+1) )\\). There are many other cases you see factor variables, including spatial ID’s in purely cross sectional data.\nBe careful not to handle categorical data as if they were cardinal. E.g., generate city data with Leipzig=1, Lausanne=2, LosAngeles=3, … and then include city as if it were a cardinal number (that’s a big no-no). The same applied to ordinal data; PopulationLeipzig=2, PopulationLausanne=3, PopulationLosAngeles=1.\n\n\nCode\nN &lt;- 1000\nx &lt;- runif(N,3,8)\ne &lt;- rnorm(N,0,0.4)\nfo &lt;- factor(rbinom(N,4,.5), ordered=T)\nfu &lt;- factor(rep(c('A','B'),N/2), ordered=F)\ndA &lt;- 1*(fu=='A')\ny &lt;- (2^as.integer(fo)*dA )*sqrt(x)+ 2*as.integer(fo)*e\ndat_f &lt;- data.frame(y,x,fo,fu)\n\n\nWith factors, you can still include them in the design matrix of an OLS regression \\[\ny_{it} = x_{it} \\beta_{x} + d_{t}\\beta_{t}\n\\] When, as commonly done, the factors are modeled as being additively seperable, they are modeled “fixed effects”.1 Simply including the factors into the OLS regression yields a “dummy variable” fixed effects estimator. Hansen Econometrics, Theorem 17.1: The fixed effects estimator of \\(\\beta\\) algebraically equals the dummy variable estimator of \\(\\beta\\). The two estimators have the same residuals. \n\n\nCode\nlibrary(fixest)\nfe_reg1 &lt;- feols(y~x|fo+fu, dat_f)\ncoef(fe_reg1)\n##        x \n## 1.217143\nfixef(fe_reg1)[1:2]\n## $fo\n##         0         1         2         3         4 \n##  9.173843 10.736186 14.734972 24.545451 41.283593 \n## \n## $fu\n##         A         B \n##   0.00000 -24.37372\n\n# Compare Coefficients\nfe_reg0 &lt;- lm(y~-1+x+fo+fu, dat_f)\ncoef( fe_reg0 )\n##          x        fo0        fo1        fo2        fo3        fo4        fuB \n##   1.217143   9.173843  10.736186  14.734972  24.545451  41.283593 -24.373716\n\n\nWith fixed effects, we can also compute averages for each group and construct a between estimator: \\(\\bar{y}_i = \\alpha + \\bar{x}_i \\beta\\). Or we can subtract the average from each group to construct a within estimator: \\((y_{it} - \\bar{y}_i) = (x_{it}-\\bar{x}_i)\\beta\\).\nBut note that many factors are not additively separable. This is easy to check with an F-test;\n\n\nCode\nreg0 &lt;- lm(y~-1+x+fo+fu, dat_f)\nreg1 &lt;- lm(y~-1+x+fo*fu, dat_f)\nreg2 &lt;- lm(y~-1+x*fo*fu, dat_f)\n\nanova(reg0, reg2)\n## Analysis of Variance Table\n## \n## Model 1: y ~ -1 + x + fo + fu\n## Model 2: y ~ -1 + x * fo * fu\n##   Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    \n## 1    993 86550                                  \n## 2    980  6374 13     80177 948.26 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(reg0, reg1, reg2)\n## Analysis of Variance Table\n## \n## Model 1: y ~ -1 + x + fo + fu\n## Model 2: y ~ -1 + x + fo * fu\n## Model 3: y ~ -1 + x * fo * fu\n##   Res.Df   RSS Df Sum of Sq        F    Pr(&gt;F)    \n## 1    993 86550                                    \n## 2    989 12052  4     74499 2863.601 &lt; 2.2e-16 ***\n## 3    980  6374  9      5678   97.003 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multivariate Data</span>"
    ]
  },
  {
    "objectID": "02_03_IntermediateRegression.html#variability-estimates",
    "href": "02_03_IntermediateRegression.html#variability-estimates",
    "title": "12  Multivariate Data",
    "section": "12.3 Variability Estimates",
    "text": "12.3 Variability Estimates\nTo estimate the variability of our estimates, we can use the same data-driven methods introduced in the last section. As before, we can conduct independent hypothesis tests using t-values.\nWe can also conduct joint tests that account for interdependancies in our estimates. For example, to test whether two coefficients both equal \\(0\\), we bootstrap the joint distribution of coefficients.\n\n\nCode\n# Bootstrap SE's\nboots &lt;- 1:399\nboot_regs &lt;- lapply(boots, function(b){\n    b_id &lt;- sample( nrow(USArrests), replace=T)\n    xy_b &lt;- USArrests[b_id,]\n    reg_b &lt;- lm(Murder~Assault+UrbanPop, dat=xy_b)\n})\nboot_coefs &lt;- sapply(boot_regs, coef)\n\n# Recenter at 0 to impose the null\n#boot_means &lt;- rowMeans(boot_coefs)\n#boot_coefs0 &lt;- sweep(boot_coefs, MARGIN=1, STATS=boot_means)\n\n\n\n\nCode\nboot_coef_df &lt;- as.data.frame(cbind(ID=boots, t(boot_coefs)))\nfig &lt;- plotly::plot_ly(boot_coef_df,\n    type = 'scatter', mode = 'markers',\n    x = ~UrbanPop, y = ~Assault,\n    text = ~paste('&lt;b&gt; bootstrap dataset: ', ID, '&lt;/b&gt;',\n            '&lt;br&gt;Coef. Urban  :', round(UrbanPop,3),\n            '&lt;br&gt;Coef. Murder :', round(Assault,3),\n            '&lt;br&gt;Coef. Intercept :', round(`(Intercept)`,3)),\n    hoverinfo='text',\n    showlegend=F,\n    marker=list( color='rgba(0, 0, 0, 0.5)'))\nfig &lt;- plotly::layout(fig,\n    showlegend=F,\n    title='Joint Distribution of Coefficients (under the null)',\n    xaxis = list(title='UrbanPop Coefficient'),\n    yaxis = list(title='Assualt Coefficient'))\nfig",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multivariate Data</span>"
    ]
  },
  {
    "objectID": "02_03_IntermediateRegression.html#hypothesis-tests",
    "href": "02_03_IntermediateRegression.html#hypothesis-tests",
    "title": "12  Multivariate Data",
    "section": "12.4 Hypothesis Tests",
    "text": "12.4 Hypothesis Tests\n\nF-statistic.\nWe can also use an \\(F\\) test for any \\(q\\) hypotheses. Specifically, when \\(q\\) hypotheses restrict a model, the degrees of freedom drop from \\(k_{u}\\) to \\(k_{r}\\) and the residual sum of squares \\(RSS=\\sum_{i}(y_{i}-\\widehat{y}_{i})^2\\) typically increases. We compute the statistic \\[\nF_{q} = \\frac{(RSS_{r}-RSS_{u})/(k_{u}-k_{r})}{RSS_{u}/(N-k_{u})}\n\\]\nIf you test whether all \\(K\\) variables are significant, the restricted model is a simple intercept and \\(RSS_{r}=TSS\\), and \\(F_{q}\\) can be written in terms of \\(R^2\\): \\(F_{K} = \\frac{R^2}{1-R^2} \\frac{N-K}{K-1}\\). The first fraction is the relative goodness of fit, and the second fraction is an adjustment for degrees of freedom (similar to how we adjusted the \\(R^2\\) term before).\nTo conduct a hypothesis test, first compute a null distribution by randomly reshuffling the outcomes and recompute the \\(F\\) statistic, and then compare how often random data give something as extreme as your initial statistic. For some intuition on this F test, examine how the adjusted \\(R^2\\) statistic varies with bootstrap samples.\n\n\nCode\n# Bootstrap under the null\nboots &lt;- 1:399\nboot_regs0 &lt;- lapply(boots, function(b){\n  # Generate bootstrap sample\n  xy_b &lt;- USArrests\n  b_id &lt;- sample( nrow(USArrests), replace=T)\n  # Impose the null\n  xy_b$Murder &lt;-  xy_b$Murder[b_id]\n  # Run regression\n  reg_b &lt;- lm(Murder~Assault+UrbanPop, dat=xy_b)\n})\n# Get null distribution for adjusted R2\nR2adj_sim0 &lt;- sapply(boot_regs0, function(reg_k){\n    summary(reg_k)$adj.r.squared })\nhist(R2adj_sim0, xlim=c(-.1,1), breaks=25, border=NA,\n    main='', xlab=expression('adj.'~R[b]^2))\n\n# Compare to initial statistic\nabline(v=summary(reg)$adj.r.squared, lwd=2, col=2)\n\n\n\n\n\n\n\n\n\nNote that hypothesis testing is not to be done routinely, as additional complications arise when testing multiple hypothesis sequentially.\nUnder some additional assumptions \\(F_{q}\\) follows an F-distribution. For more about F-testing, see https://online.stat.psu.edu/stat501/lesson/6/6.2 and https://www.econometrics.blog/post/understanding-the-f-statistic/\n\n\nANOVA",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multivariate Data</span>"
    ]
  },
  {
    "objectID": "02_03_IntermediateRegression.html#further-reading",
    "href": "02_03_IntermediateRegression.html#further-reading",
    "title": "12  Multivariate Data",
    "section": "12.5 Further Reading",
    "text": "12.5 Further Reading\nFor OLS, see\n\nhttps://bookdown.org/josiesmith/qrmbook/linear-estimation-and-minimizing-error.html\nhttps://www.econometrics-with-r.org/4-lrwor.html\nhttps://www.econometrics-with-r.org/6-rmwmr.html\nhttps://www.econometrics-with-r.org/7-htaciimr.html\nhttps://bookdown.org/ripberjt/labbook/bivariate-linear-regression.html\nhttps://bookdown.org/ripberjt/labbook/multivariable-linear-regression.html\nhttps://online.stat.psu.edu/stat462/node/137/\nhttps://book.stat420.org/\nHill, Griffiths & Lim (2007), Principles of Econometrics, 3rd ed., Wiley, S. 86f.\nVerbeek (2004), A Guide to Modern Econometrics, 2nd ed., Wiley, S. 51ff.\nAsteriou & Hall (2011), Applied Econometrics, 2nd ed., Palgrave MacMillan, S. 177ff.\nhttps://online.stat.psu.edu/stat485/lesson/11/\n\nTo derive OLS coefficients in Matrix form, see\n\nhttps://jrnold.github.io/intro-methods-notes/ols-in-matrix-form.html\nhttps://www.fsb.miamioh.edu/lij14/411_note_matrix.pdf\nhttps://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf\n\nFor fixed effects, see\n\nhttps://www.econometrics-with-r.org/10-rwpd.html\nhttps://bookdown.org/josiesmith/qrmbook/topics-in-multiple-regression.html\nhttps://bookdown.org/ripberjt/labbook/multivariable-linear-regression.html\nhttps://www.princeton.edu/~otorres/Panel101.pdf\nhttps://www.stata.com/manuals13/xtxtreg.pdf",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multivariate Data</span>"
    ]
  },
  {
    "objectID": "02_03_IntermediateRegression.html#footnotes",
    "href": "02_03_IntermediateRegression.html#footnotes",
    "title": "12  Multivariate Data",
    "section": "",
    "text": "There are also random effects: the factor variable comes from a distribution that is uncorrelated with the regressors. This is rarely used in economics today, however, and are mostly included for historical reasons and special cases where fixed effects cannot be estimated due to data limitations.↩︎",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multivariate Data</span>"
    ]
  },
  {
    "objectID": "02_04_InterprettingRegression.html",
    "href": "02_04_InterprettingRegression.html",
    "title": "13  Multivariate Data II",
    "section": "",
    "text": "13.1 Coefficient Interpretation\nNotice that we have gotten pretty far without actually trying to meaningfully interpret regression coefficients. That is because the above procedure will always give us number, regardless as to whether the true data generating process is linear or not. So, to be cautious, we have been interpreting the regression outputs while being agnostic as to how the data are generated. We now consider a special situation where we know the data are generated according to a linear process and are only uncertain about the parameter values.\nIf the data generating process is \\[\ny=X\\beta + \\epsilon\\\\\n\\mathbb{E}[\\epsilon | X]=0,\n\\] then we have a famous result that lets us attach a simple interpretation of OLS coefficients as unbiased estimates of the effect of X: \\[\n\\hat{\\beta} = (X'X)^{-1}X'y = (X'X)^{-1}X'(X\\beta + \\epsilon) = \\beta + (X'X)^{-1}X'\\epsilon\\\\\n\\mathbb{E}\\left[ \\hat{\\beta} \\right] = \\mathbb{E}\\left[ (X'X)^{-1}X'y \\right] = \\beta + (X'X)^{-1}\\mathbb{E}\\left[ X'\\epsilon \\right] = \\beta\n\\]\nGenerate a simulated dataset with 30 observations and two exogenous variables. Assume the following relationship: \\(y_{i} = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i\\) where the variables and the error term are realizations of the following data generating processes (DGP):\nCode\nN &lt;- 30\nB &lt;- c(10, 2, -1)\n\nx1 &lt;- runif(N, 0, 5)\nx2 &lt;- rbinom(N,1,.7)\nX &lt;- cbind(1,x1,x2)\ne &lt;- rnorm(N,0,3)\nY &lt;- X%*%B + e\ndat &lt;- data.frame(Y,X)\ncoef(lm(Y~x1+x2, data=dat))\n## (Intercept)          x1          x2 \n##  10.5138356   1.9870829  -0.1645539\nSimulate the distribution of coefficients under a correctly specified model. Interpret the average.\nCode\nN &lt;- 30\nB &lt;- c(10, 2, -1)\n\nCoefs &lt;- sapply(1:400, function(sim){\n    x1 &lt;- runif(N, 0, 5)\n    x2 &lt;- rbinom(N,1,.7)\n    X &lt;- cbind(1,x1,x2)\n    e &lt;- rnorm(N,0,3)\n    Y &lt;- X%*%B + e\n    dat &lt;- data.frame(Y,x1,x2)\n    coef(lm(Y~x1+x2, data=dat))\n})\n\npar(mfrow=c(1,2))\nfor(i in 2:3){\n    hist(Coefs[i,], xlab=bquote(beta[.(i)]), main='', border=NA)\n    abline(v=mean(Coefs[i,]), lwd=2)\n    abline(v=B[i], col=rgb(1,0,0))\n}\nMany economic phenomena are nonlinear, even when including potential transforms of \\(Y\\) and \\(X\\). Sometimes the linear model may still be a good or even great approximation. But sometimes not, and it is hard to know ex-ante. Examine the distribution of coefficients under this mispecified model and try to interpret the average.\nCode\nN &lt;- 30\n\nCoefs &lt;- sapply(1:600, function(sim){\n    x2 &lt;- runif(N, 0, 5)\n    x3 &lt;- rbinom(N,1,.7)\n    e &lt;- rnorm(N,0,3)\n    Y &lt;- 10*x3 + 2*log(x2)^x3 + e\n    dat &lt;- data.frame(Y,x2,x3)\n    coef(lm(Y~x2+x3, data=dat))\n})\n\npar(mfrow=c(1,2))\nfor(i in 2:3){\n    hist(Coefs[i,],  xlab=bquote(beta[.(i)]), main='', border=NA)\n    abline(v=mean(Coefs[i,]), col=1, lwd=2)\n}\nIn general, you can interpret your regression coefficients as “adjusted correlations”. There are (many) tests for whether the relationships in your dataset are actually additively separable and linear.",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multivariate Data II</span>"
    ]
  },
  {
    "objectID": "02_04_InterprettingRegression.html#diagnostics",
    "href": "02_04_InterprettingRegression.html#diagnostics",
    "title": "13  Multivariate Data II",
    "section": "13.2 Diagnostics",
    "text": "13.2 Diagnostics\nThere’s little sense in getting great standard errors for a terrible model. Plotting your regression object a simple and easy step to help diagnose whether your model is in some way bad. We next go through what each of these figures show.\n\n\nCode\nreg &lt;- lm(Murder~Assault+UrbanPop, data=USArrests)\npar(mfrow=c(2,2))\nplot(reg, pch=16, col=grey(0,.5))\n\n\n\n\n\n\n\n\n\n\nOutliers.\nThe first diagnostic plot examines outliers in terms the outcome \\(y_i\\) being far from its prediction \\(\\hat{y}_i\\). You may be interested in such outliers because they can (but do not have to) unduly influence your estimates.\nThe third diagnostic plot examines another type of outlier, where an observation with the explanatory variable \\(x_i\\) is far from the center of mass of the other \\(x\\)’s. A point has high leverage if the estimates change dramatically when you estimate the model without that data point.\n\n\nCode\nN &lt;- 40\nx &lt;- c(25, runif(N-1,3,8))\ne &lt;- rnorm(N,0,0.4)\ny &lt;- 3 + 0.6*sqrt(x) + e\nplot(y~x, pch=16, col=grey(0,.5))\npoints(x[1],y[1], pch=16, col=rgb(1,0,0,.5))\n\nabline(lm(y~x), col=2, lty=2)\nabline(lm(y[-1]~x[-1]))\n\n\n\n\n\n\n\n\n\nSee AEJ-leverage and NBER-leverage for examples of leverage in economics.\nStandardized residuals are \\[\nr_i=\\frac{\\hat{\\epsilon}_i}{s_{[i]}\\sqrt{1-h_i}},\n\\] where \\(s_{[i]}\\) is the root mean squared error of a regression with the \\(i\\)th observation removed and \\(h_i\\) is the leverage of residual \\(\\hat{\\epsilon_i}\\).\n\n\nCode\nwhich.max(hatvalues(reg))\nwhich.max(rstandard(reg))\n\n\n(See https://www.r-bloggers.com/2016/06/leverage-and-influence-in-a-nutshell/ for a good interactive explanation, and https://online.stat.psu.edu/stat462/node/87/ for detail.)\nThe fourth plot further assesses outlier \\(X\\) using Cook’s Distance, which sums of all prediction changes when observation \\(i\\) is removed and scales proportionally to the mean square error $s^2 = . \\[\\begin{eqnarray}\nD_{i}\n= \\frac{\\sum_{j} \\left( \\hat{y_j} - \\hat{y_j}_{[i]} \\right)^2 }{ p s^2 }\n= \\frac{[e_{i}]^2}{p s^2 } \\frac{h_i}{(1-h_i)^2}\n\\end{eqnarray}\\]\n\n\nCode\nwhich.max(cooks.distance(reg))\ncar::influencePlot(reg)\n\n\n\n\nCollinearity.\nThis is when one explanatory variable in a multiple linear regression model can be linearly predicted from the others with a substantial degree of accuracy. Coefficient estimates may change erratically in response to small changes in the model or the data. (In the extreme case where there are more variables than observations \\(K&gt;N\\), the inverse of \\(X'X\\) has an infinite number of solutions.) To diagnose collinearity, we can use the Variance Inflation Factor \\[\nVIF_{k}=\\frac{1}{1-R^2_k},\n\\] where \\(R^2_k\\) is the \\(R^2\\) for the regression of \\(X_k\\) on the other covariates \\(X_{-k}\\) (a regression that does not involve the response variable Y)\n\n\nCode\ncar::vif(reg) \nsqrt(car::vif(reg)) &gt; 2 # problem?\n\n\n\n\nNormality.\nThe second plot examines whether the residuals are normally distributed. Your OLS coefficient estimates do not depend on the normality of the residuals. (Good thing, because there’s no reason the residuals of economic phenomena should be so well behaved.) Many hypothesis tests are, however, affected by the distribution of the residuals. For these reasons, you may be interested in assessing normality\n\n\nCode\npar(mfrow=c(1,2))\nhist(resid(reg), main='Histogram of Residuals',\n    font.main=1, border=NA)\n\nqqnorm(resid(reg), main=\"Normal Q-Q Plot of Residuals\",\n    font.main=1, col=grey(0,.5), pch=16)\nqqline(resid(reg), col=1, lty=2)\n\n#shapiro.test(resid(reg))\n\n\nHeterskedasticity may also matters for variability estimates. This is not shown in the plot, but you can conduct a simple test\n\n\nCode\nlibrary(lmtest)\nlmtest::bptest(reg)",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multivariate Data II</span>"
    ]
  },
  {
    "objectID": "02_04_InterprettingRegression.html#transformations",
    "href": "02_04_InterprettingRegression.html#transformations",
    "title": "13  Multivariate Data II",
    "section": "13.3 Transformations",
    "text": "13.3 Transformations\nTransforming variables can often improve your model fit while still allowing it estimated via OLS. This is because OLS only requires the model to be linear in the parameters. Under the assumptions of the model is correctly specified, the following table is how we can interpret the coefficients of the transformed data. (Note for small changes, \\(\\Delta ln(x) \\approx \\Delta x / x = \\Delta x \\% \\cdot 100\\).)\n\n\n\n\n\n\n\n\n\n\nSpecification\nRegressand\nRegressor\nDerivative\nInterpretation (If True)\n\n\n\n\nlinear–linear\n\\(y\\)\n\\(x\\)\n\\(\\Delta y = \\beta_1\\cdot\\Delta x\\)\nChange \\(x\\) by one unit \\(\\rightarrow\\) change \\(y\\) by \\(\\beta_1\\) units.\n\n\nlog–linear\n\\(ln(y)\\)\n\\(x\\)\n\\(\\Delta y \\% \\cdot 100 \\approx \\beta_1 \\cdot \\Delta x\\)\nChange \\(x\\) by one unit \\(\\rightarrow\\) change \\(y\\) by \\(100 \\cdot \\beta_1\\) percent.\n\n\nlinear–log\n\\(y\\)\n\\(ln(x)\\)\n\\(\\Delta y \\approx  \\frac{\\beta_1}{100}\\cdot \\Delta x \\%\\)\nChange \\(x\\) by one percent \\(\\rightarrow\\) change \\(y\\) by \\(\\frac{\\beta_1}{100}\\) units\n\n\nlog–log\n\\(ln(y)\\)\n\\(ln(x)\\)\n\\(\\Delta y \\% \\approx \\beta_1\\cdot \\Delta x \\%\\)\nChange \\(x\\) by one percent \\(\\rightarrow\\) change \\(y\\) by \\(\\beta_1\\) percent\n\n\n\nNow recall from micro theory that an additively seperable and linear production function is referred to as ``perfect substitutes’‘. With a linear model and untranformed data, you have implicitly modelled the different regressors \\(X\\) as perfect substitutes. Further recall that the’‘perfect substitutes’’ model is a special case of the constant elasticity of substitution production function. Here, we will build on http://dx.doi.org/10.2139/ssrn.3917397, and consider box-cox transforming both \\(X\\) and \\(y\\). Specifically, apply the box-cox transform of \\(y\\) using parameter \\(\\lambda\\) and apply another box-cox transform to each \\(x\\) using the same parameter \\(\\rho\\) so that \\[\\begin{eqnarray}\ny^{(\\lambda)}_{i} &=& \\sum_{k=1}^{K}\\beta_{k} x^{(\\rho)}_{ik} + \\epsilon_{i}\\\\\ny^{(\\lambda)}_{i} &=&\n\\begin{cases}\n\\lambda^{-1}[ (y_i+1)^{\\lambda}- 1] & \\lambda \\neq 0 \\\\\nlog(y_i+1) &  \\lambda=0\n\\end{cases}.\\\\\nx^{(\\rho)}_{i} =\n\\begin{cases}\n\\rho^{-1}[ (x_i)^{\\rho}- 1] & \\rho \\neq 0 \\\\\nlog(x_{i}+1) &  \\rho=0\n\\end{cases}.\n\\end{eqnarray}\\]\nNotice that this nests:\n\nlinear-linear \\((\\rho=\\lambda=1)\\).\nlinear-log \\((\\rho=1, \\lambda=0)\\).\nlog-linear \\((\\rho=0, \\lambda=1)\\).\nlog-log \\((\\rho=\\lambda=0)\\).\n\nIf \\(\\rho=\\lambda\\), we get the CES production function. This nests the ‘’perfect substitutes’’ linear-linear model (\\(\\rho=\\lambda=1\\)) , the ‘’cobb-douglas’’ log-log model (\\(\\rho=\\lambda=0\\)), and many others. We can define \\(\\lambda=\\rho/\\lambda'\\) to be clear that this is indeed a CES-type transformation where\n\n\\(\\rho \\in (-\\infty,1]\\) controls the “substitutability” of explanatory variables. E.g., \\(\\rho &lt;0\\) is ‘’complementary’’.\n\\(\\lambda\\) determines ‘’returns to scale’‘. E.g., \\(\\lambda&lt;1\\) is’‘decreasing returns’’.\n\nWe compute the mean squared error in the original scale by inverting the predictions; \\[\n\\widehat{y}_{i} =\n\\begin{cases}\n[ \\widehat{y}_{i}^{(\\lambda)} \\cdot \\lambda ]^{1/\\lambda} -1 & \\lambda  \\neq 0 \\\\\nexp( \\widehat{y}_{i}^{(\\lambda)}) -1 &  \\lambda=0\n\\end{cases}.\n\\]\nIt is easiest to optimize parameters in a 2-step procedure called `concentrated optimization’. We first solve for \\(\\widehat{\\beta}(\\rho,\\lambda)\\) and compute the mean squared error \\(MSE(\\rho,\\lambda)\\). We then find the \\((\\rho,\\lambda)\\) which minimizes \\(MSE\\).\n\n\nCode\n# Box-Cox Transformation Function\nbxcx &lt;- function( xy, rho){\n    if (rho == 0L) {\n      log(xy+1)\n    } else if(rho == 1L){\n      xy\n    } else {\n      ((xy+1)^rho - 1)/rho\n    }\n}\nbxcx_inv &lt;- function( xy, rho){\n    if (rho == 0L) {\n      exp(xy) - 1\n    } else if(rho == 1L){\n      xy\n    } else {\n     (xy * rho + 1)^(1/rho) - 1\n    }\n}\n\n# Which Variables\nreg &lt;- lm(Murder~Assault+UrbanPop, data=USArrests)\nX &lt;- USArrests[,c('Assault','UrbanPop')]\nY &lt;- USArrests[,'Murder']\n\n# Simple Grid Search over potential (Rho,Lambda) \nrl_df &lt;- expand.grid(rho=seq(-2,2,by=.5),lambda=seq(-2,2,by=.5))\n\n# Compute Mean Squared Error\n# from OLS on Transformed Data\nerrors &lt;- apply(rl_df,1,function(rl){\n    Xr &lt;- bxcx(X,rl[[1]])\n    Yr &lt;- bxcx(Y,rl[[2]])\n    Datr &lt;- cbind(Murder=Yr,Xr)\n    Regr &lt;- lm(Murder~Assault+UrbanPop, data=Datr)\n    Predr &lt;- bxcx_inv(predict(Regr),rl[[2]])\n    Resr  &lt;- (Y - Predr)\n    return(Resr)\n})\nrl_df$mse &lt;- colMeans(errors^2)\n\n# Want Small MSE and Interpretable\nlayout(matrix(1:2,ncol=2), width=c(3,1), height=c(1,1))\npar(mar=c(4,4,2,0))\nplot(lambda~rho,rl_df, cex=8, pch=15,\n    xlab=expression(rho),\n    ylab=expression(lambda),\n    col=hcl.colors(25)[cut(1/rl_df$mse,25)])\n# Which min\nrl0 &lt;- rl_df[which.min(rl_df$mse),c('rho','lambda')]\npoints(rl0$rho, rl0$lambda, pch=0, col=1, cex=8, lwd=2)\n# Legend\nplot(c(0,2),c(0,1), type='n', axes=F,\n    xlab='',ylab='', cex.main=.8,\n    main=expression(frac(1,'Mean Square Error')))\nrasterImage(as.raster(matrix(hcl.colors(25), ncol=1)), 0, 0, 1,1)\ntext(x=1.5, y=seq(1,0,l=10), cex=.5,\n    labels=levels(cut(1/rl_df$mse,10)))\n\n\n\n\n\n\n\n\n\nThe parameters \\(-1,0,1,2\\) are easy to interpret and might be selected instead if there is only a small loss in fit. (In the above example, we might choose \\(\\lambda=0\\) instead of the \\(\\lambda\\) which minimized the mean square error). You can also plot the specific predictions to better understand the effect of data transformation beyond mean squared error.\n\n\nCode\n# Plot for Specific Comparisons\nXr &lt;- bxcx(X,rl0[[1]])\nYr &lt;- bxcx(Y,rl0[[2]])\nDatr &lt;- cbind(Murder=Yr,Xr)\nRegr &lt;- lm(Murder~Assault+UrbanPop, data=Datr)\nPredr &lt;- bxcx_inv(predict(Regr),rl0[[2]])\n\ncols &lt;- c(rgb(1,0,0,.5), col=rgb(0,0,1,.5))\nplot(Y, Predr, pch=16, col=cols[1], ylab='Prediction', \n    ylim=range(Y,Predr))\npoints(Y, predict(reg), pch=16, col=cols[2])\nlegend('topleft', pch=c(16), col=cols,\n    title=expression(rho~', '~lambda),\n    legend=c( paste0(rl0, collapse=', '),'1, 1') )\nabline(a=0,b=1, lty=2)\n\n\n\n\n\n\n\n\n\nWhen explicitly transforming data according to \\(\\lambda\\) and \\(\\rho\\), these parameters increase the degrees of freedom by two. The default hypothesis testing procedures do not account for you trying out different transformations, and should be adjusted by the increased degrees of freedom. Specification searches deflate standard errors and are a major source for false discoveries.\nNote that if you are ultimately interested in the outcome \\(Y\\), then transforming/untransforming \\(Y\\) can introduce a bias. To understand when you might be better off sticking with an untransformed outcome variable, see the literature on “smearing”.\n\nBreak Points.\nIncorporating Kinks and Discontinuities in \\(X\\) are a type of transformation that can be modeled using factor variables. As such, \\(F\\)-tests can be used to examine whether a breaks is statistically significant.\n\n\nCode\nlibrary(AER); data(CASchools)\nCASchools$score &lt;- (CASchools$read + CASchools$math) / 2\nreg &lt;- lm(score~income, data=CASchools)\n\n# F Test for Break\nreg2 &lt;- lm(score ~ income*I(income&gt;15), data=CASchools)\nanova(reg, reg2)\n\n# Chow Test for Break\ndata_splits &lt;- split(CASchools, CASchools$income &lt;= 15)\nresids &lt;- sapply(data_splits, function(dat){\n    reg &lt;- lm(score ~ income, data=dat)\n    sum( resid(reg)^2)\n})\nNs &lt;-  sapply(data_splits, function(dat){ nrow(dat)})\nRt &lt;- (sum(resid(reg)^2) - sum(resids))/sum(resids)\nRb &lt;- (sum(Ns)-2*reg$rank)/reg$rank\nFt &lt;- Rt*Rb\npf(Ft,reg$rank, sum(Ns)-2*reg$rank,lower.tail=F)\n\n# See also\n# strucchange::sctest(y~x, data=xy, type=\"Chow\", point=.5)\n# strucchange::Fstats(y~x, data=xy)\n\n# To Find Changes\n# segmented::segmented(reg)",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multivariate Data II</span>"
    ]
  },
  {
    "objectID": "02_04_InterprettingRegression.html#regressograms",
    "href": "02_04_InterprettingRegression.html#regressograms",
    "title": "13  Multivariate Data II",
    "section": "13.4 Regressograms",
    "text": "13.4 Regressograms\nYou can estimate a nonparametric model with multiple \\(X\\) variables with a multivariate regressogram. Here, we cut the data into exclusive bins along each dimension (called dummy variables), and then run a regression on all dummy variables.\n\n\nCode\n## Simulate Data\nN &lt;- 10000\ne &lt;- rnorm(N)\nx1 &lt;- seq(.1,20,length.out=N)\nx2 &lt;- runif(N, 0,1)\ny  &lt;- 3*exp(-2*x2 + 1.5*x1 - .1*x1^2)*x1 + e\ndat &lt;- data.frame(x1, x2, y)\n\n## Create color palette (reused in later examples)\ncol_scale &lt;- seq(min(y)*1.1, max(y)*1.1, length.out=401)\nycol_pal &lt;- hcl.colors(length(col_scale),alpha=.5)\nnames(ycol_pal) &lt;- sort(col_scale)\n\n## Add legend (reused in later examples)\nadd_legend &lt;- function(col_scale,\n    yl=11,\n    colfun=function(x){ hcl.colors(x,alpha=.5) },\n    ...) {\n  opar &lt;- par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), \n              mar=c(0, 0, 0, 0), new=TRUE)\n  on.exit(par(opar))\n  h &lt;- hist(col_scale, plot=F, breaks=yl-1)$mids\n  plot(0, 0, type='n', bty='n', xaxt='n', yaxt='n')\n  legend(...,\n    legend=h,\n    fill=colfun(length(h)),\n    border=NA,\n    bty='n')\n}\n\n\n## Plot Data\npar(oma=c(0,0,0,2))\nplot(x1~x2, dat,\n    col=ycol_pal[cut(y,col_scale)],\n    pch=16, cex=.5, \n    main='Raw Data')\nadd_legend(x='topright', col_scale=col_scale,\n    yl=6, inset=c(0,.05), title='y')\n\n\n\n\n\n\n\n\n\n\n\nCode\n## OLS \nreg &lt;- lm(y~x1*x2, data=dat) #(with simple interaction)\nreg &lt;- lm(y~x1+x2, data=dat) #(without interaction)\n\n## Grid Points for Prediction\n# X1 bins\nl1 &lt;- 11\nbks1 &lt;- seq(0,20, length.out=l1)\nh1 &lt;- diff(bks1)[1]/2\nmids1 &lt;- bks1[-1]-h1\n# X2 bins\nl2 &lt;- 11\nbks2 &lt;- seq(0,1, length.out=l2)\nh2 &lt;- diff(bks2)[1]/2\nmids2 &lt;- bks2[-1]-h2\n# Grid\npred_x &lt;- expand.grid(x1=mids1, x2=mids2)\n\n## OLS Predictions\npred_ols &lt;- predict(reg, newdata=pred_x)\npred_df_ols  &lt;- cbind(pred_ols, pred_x)\n\n## Plot Predictions\npar(oma=c(0,0,0,2))\nplot(x1~x2, pred_df_ols,\n    col=ycol_pal[cut(pred_ols,col_scale)],\n    pch=15, cex=2, main='OLS Predictions')\nadd_legend(x='topright', col_scale=col_scale,\n    yl=6, inset=c(0,.05),title='y')\n\n\n\n\n\n\n\n\n\n\n\nCode\n##################\n# Multivariate Regressogram\n##################\n\n## Regressogram Bins\ndat$x1c &lt;- cut(dat$x1, bks1)\n#head(dat$x1c,3)\ndat$x2c &lt;- cut(dat$x2, bks2)\n\n## Regressogram\nreg &lt;- lm(y~x1c*x2c, data=dat) #nonlinear w/ complex interactions\n\n## Predicted Values\n## For Points in Middle of Each Bin\npred_df_rgrm &lt;- expand.grid(\n    x1c=levels(dat$x1c),\n    x2c=levels(dat$x2c))\npred_df_rgrm$yhat &lt;- predict(reg, newdata=pred_df_rgrm)\npred_df_rgrm &lt;- cbind(pred_df_rgrm, pred_x)\n\n## Plot Predictions\npar(oma=c(0,0,0,2))\nplot(x1~x2, pred_df_rgrm,\n    col=ycol_pal[cut(pred_df_rgrm$yhat,col_scale)],\n    pch=15, cex=2, main='Regressogram Predictions')\nadd_legend(x='topright', col_scale=col_scale,\n    yl=6, inset=c(0,.05),title='y')\n\n\n\n\n\n\n\n\n\nJust like with bivariate data, you can also use split-sample (or peicewise) regressions for multivariate data.\nAs such, there are two main ways to summarize gradients: how \\(Y\\) changes with \\(X\\).\n\nFor regressograms, you can approximate gradients with small finite differences. For some small \\(h_{p}\\), we can manually compute \\[\\begin{eqnarray}\n\\widehat{\\beta_{p}}(\\mathbf{x}) &=& \\frac{ \\widehat{Y}(x_{1},...,x_{p}+h_{p}...,x_{P})-\\widehat{Y}(x_{1},...,x_{p}-h_{p}...,x_{P})}{2h_{p}},\n\\end{eqnarray}\\]\nWhen using split-sample regressions, you can get all estimated coefficients that provides gradient estimates in each direction. \\[\\begin{eqnarray}\n\\widehat{\\beta}(\\mathbf{x}) &=& [\\mathbf{X}'\\mathbf{K}(\\mathbf{x})\\mathbf{X}]^{-1} \\mathbf{X}'\\mathbf{K}(\\mathbf{x})Y \\\\\n\\mathbf{K}(\\mathbf{x}) &=& \\begin{pmatrix}\nK\\left(\\frac{\\mathbf{X}_{1}-\\mathbf{x}}{h}\\right) & ... & 0\\\\\n\\vdots & & \\\\\n0 & ... & K\\left(\\frac{\\mathbf{X}_{P}-\\mathbf{x}}{h}\\right)\n\\end{pmatrix},\n\\end{eqnarray}\\]\n\nAfter computing gradients, you can summarize them in various plots\n\nHistograms, Scatterplots\nPlot of gradients and CI’s, \n\nYou may also be interested in a particular gradient or a single summary statistic. For example, a bivariate regressogram can estimate the marginal effect of \\(X_{1}\\) at the means; \\(\\widehat{\\beta_{1}}(\\overline{\\mathbf{x}}=[\\overline{x_{1}}, \\overline{x_{2}}])\\). You may also be interested in the mean of the marginal effects (sometimes said simply as “average effect”), which averages the marginal effect over all datapoints in the dataset: \\(1/N \\sum_{i}^{N} \\widehat{\\beta_{1}}(\\mathbf{X}_{i})\\), or the median marginal effect. Such statistics are single numbers that can be presented similar to an OLS regression table where each row corresponds a variable and each cell has two elements: “mean gradient (sd gradient)”.",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multivariate Data II</span>"
    ]
  },
  {
    "objectID": "02_04_InterprettingRegression.html#more-literature",
    "href": "02_04_InterprettingRegression.html#more-literature",
    "title": "13  Multivariate Data II",
    "section": "13.5 More Literature",
    "text": "13.5 More Literature\nDiagnostics\n\nhttps://book.stat420.org/model-diagnostics.html#leverage\nhttps://socialsciences.mcmaster.ca/jfox/Books/RegressionDiagnostics/index.html\nhttps://bookdown.org/ripberjt/labbook/diagnosing-and-addressing-problems-in-linear-regression.html\nBelsley, D. A., Kuh, E., and Welsch, R. E. (1980). Regression Diagnostics: Identifying influential data and sources of collinearity. Wiley. https://doi.org/10.1002/0471725153\nFox, J. D. (2020). Regression diagnostics: An introduction (2nd ed.). SAGE. https://dx.doi.org/10.4135/9781071878651",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multivariate Data II</span>"
    ]
  },
  {
    "objectID": "02_05_ObservationalData.html",
    "href": "02_05_ObservationalData.html",
    "title": "14  Observational Data",
    "section": "",
    "text": "14.1 Temporal Interdependence\nMany observational datasets have temporal dependence, meaning that values at one point in time are related to past values. This violates the standard assumption of independence used in many statistical methods.\nStock prices are classic examples of temporally dependent processes. If Apple’s stock was high yesterday, it is more likely (but not guaranteed) to be high today.\nCode\n# highest price each day\nlibrary(plotly)\nstock &lt;- read.csv('https://raw.githubusercontent.com/plotly/datasets/master/finance-charts-apple.csv')\nfig &lt;- plot_ly(stock, type = 'scatter', mode = 'lines')%&gt;%\n  add_trace(x = ~Date, y = ~AAPL.High) %&gt;%\n  layout(showlegend = F)\nfig\nA random walk is the simplest mathematical model of temporal dependence. Each new value is just the previous value plus a random shock (white noise).\nCode\n# Generate Random Walk\ntN &lt;- 200\ny &lt;- numeric(tN)\ny[1] &lt;- stock$AAPL.High[1]\nfor (ti in 2:tN) {\n    y[ti] &lt;- y[ti-1] + runif(1, -10, 10)\n}\n#x &lt;- runif(tN, -1,1) White Noise\n\ny_dat &lt;- data.frame(Date=1:tN, RandomWalk=y)\nfig &lt;- plot_ly(y_dat, type = 'scatter', mode = 'lines') %&gt;%\n  add_trace(x=~Date, y=~RandomWalk) %&gt;%\n  layout(showlegend = F)\nfig\nIn both plots, we see that today’s value is not independent of past values. In contrast to cross-sectional data (e.g. individual incomes), time series often require special methods to account for memory and nonstationarity.\nIn any case, if often helps to see the marginal distribution too.\nCode\nx0 &lt;- rbinom(600, 1, 0.5)\n\n# Setup 2 Plots, side by side\nlayout(matrix(c(1, 2), nrow = 1), widths = c(4, 1))\n\n# Plot Cumulative Averages\nx0_t &lt;- seq_len(length(x0))\nx0_mt &lt;- cumsum(x0)/x0_t\npar(mar=c(4,4,1,4))\nplot(x0_t, x0_mt, type='l',\n    ylab='Cumulative Average',\n    xlab='Flip #', \n    ylim=c(0,1), \n    lwd=2)\npoints(x0_t, x0, col=grey(0,.5),\n    pch=16, cex=.2)\n\n# Plot Long run proportions\npar(mar=c(4,4,1,1))\nx_hist &lt;- hist(x0, breaks=50, plot=F)\nx_freq &lt;- x_hist$count/length(x0)\nbarplot(x_freq ,\n    axes=FALSE,\n    space=0, horiz=TRUE, border=NA)\naxis(1)\naxis(2)\nmtext('Observed Frequency', 2, line=2.5)",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Observational Data</span>"
    ]
  },
  {
    "objectID": "02_05_ObservationalData.html#temporal-interdependence",
    "href": "02_05_ObservationalData.html#temporal-interdependence",
    "title": "14  Observational Data",
    "section": "",
    "text": "Stationary.\nA stationary time series is one whose statistical properties — mean, variance, and autocovariance — do not change over time. Formally\n\nStationary Means: \\(E[y_{t}]=E[y_{t'}]\\) for all periods \\(t, t'\\)\nStationary Vars: \\(V[y_{t}]=V[y_{t'}]\\) for all periods \\(t, t'\\)\n\nE.g., ( y_t = t + u_t, u_t (0, + t) )\n\n\nCode\ntN &lt;- 200\nsimulate_series &lt;- function(beta, alpha, sigma=.2){\n    y &lt;- numeric(tN)\n    for (ti in 1:tN) {\n        mean_ti &lt;- beta*ti\n        sd_ti &lt;- (.2 + alpha*ti)\n        y[ti] &lt;- mean_ti + rnorm(1, sd=sd_ti)\n    }\n    return(y)\n}\n\n# Plotting Functions\nplot_setup &lt;- function(alpha, beta){\n    plot.new()\n    plot.window(xlim=c(1,tN), ylim=c(-5,20))\n    axis(1)\n    axis(2)\n    mtext(expression(y[t]),2, line=2.5)\n    mtext(\"Time (t)\", 1, line=2.5)\n}\nplot_title &lt;- function(alpha, beta){\n    beta_name &lt;- ifelse(beta==0, 'Mean Stationary', 'Mean Nonstationary')\n    alpha_name &lt;- ifelse(alpha==0, 'Var Stationary', 'Var Nonstationary')\n    title(paste0(beta_name,', ', alpha_name), font.main=1, adj=0)\n}\n\npar(mfrow = c(2, 2))\nfor(alpha in c(0,.015)){\nfor(beta in c(0,.05)){\n    plot_setup(alpha=alpha, beta=beta)\n    for( sim in c('red','blue')){\n        y_sim &lt;- simulate_series(beta=beta, alpha=alpha)\n        lines(y_sim, col=adjustcolor(sim ,alpha.f=0.5), lwd=2)\n    }\n    plot_title(alpha=alpha, beta=beta)\n}}\n\n\n\n\n\n\n\n\n\n\n\nMeasures of temporal association.\nTime series often exhibit serial dependence—values today are related to past values, and potentially to other processes evolving over time. We can visualize this using correlation-based diagnostics.\nThe Autocorrelation Function (AFC) measures correlation between a time series and its own lagged values:\n\\(ACF_{Y}(k) = \\frac{Cov(Y_{t},Y_{t-k})}{ \\sqrt{Var(Y_{t})Var(Y_{t-k})}}\\)\nThis helps detect temporal persistence (memory). For stationary processes, the ACF typically decays quickly, whereas for nonstationary processes, it typically decays slowly or persists.\n\n\nCode\npar(mfrow = c(2, 2))\nfor(alpha in c(0,.015)){\nfor(beta in c(0,.05)){\n    y_sim &lt;- simulate_series(beta=beta, alpha=alpha)\n    acf(y_sim, main='')\n    plot_title(alpha=alpha, beta=beta)\n}}\n\n\n\n\n\n\n\n\n\nThe Cross-Correlation Function (CCF) measures correlation between two time series at different lags:\n\\(CCF_{YX}(k) = \\frac{Cov(Y_{t},X_{t-k})}{ \\sqrt{Var(Y_t)Var(X_{t-k})}}\\)\nThis is useful for detecting lagged relationships between two series, such as leading indicators or external drivers. (If \\(X\\) is white noise, any visible structure in the CCF likely reflects nonstationarity in \\(Y\\).)\n\n\nCode\nx_sim &lt;- runif(tN, -1,1) # White Noise\npar(mfrow = c(2, 2))\nfor(alpha in c(0,.015)){\nfor(beta in c(0,.05)){\n    y_sim &lt;- simulate_series(beta=beta, alpha=alpha)\n    ccf(y_sim, x_sim, main='')\n    plot_title(alpha=alpha, beta=beta)\n}}",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Observational Data</span>"
    ]
  },
  {
    "objectID": "02_05_ObservationalData.html#spatial-interdependence",
    "href": "02_05_ObservationalData.html#spatial-interdependence",
    "title": "14  Observational Data",
    "section": "14.2 Spatial Interdependence",
    "text": "14.2 Spatial Interdependence\nMany observational datasets exhibit spatial dependence, meaning that values at one location tend to be related to values at nearby locations. This violates the standard assumption of independent observations used in many classical statistical methods.\nFor example, elevation is spatially dependent: if one location is at high elevation, nearby locations are also likely (though not guaranteed) to be high. Similarly, socioeconomic outcomes like disease rates or income often cluster geographically due to shared environmental or social factors.\nJust as stock prices today depend on yesterday, spatial variables often depend on neighboring regions, creating a need for specialized statistical methods that account for spatial autocorrelation.\n\nRaster vs. Vector Data.\nSpatial data typically comes in two formats, each suited to different types of information:\n\nVector data uses geometric shapes (points, lines, polygons) to store data. E.g., a census tract map that stores data on population demographics.\nRaster data uses grid cells (typically squares, but sometimes hexagons) to store data. E.g., an image that stores data on elevation above seawater.\n\n\n\nCode\n# Vector Data\nlibrary(sf)\nnorthcarolina_vector &lt;- st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\n## Reading layer `nc' from data source `/home/Jadamso/R-Libs/sf/shape/nc.shp' using driver `ESRI Shapefile'\n## Simple feature collection with 100 features and 14 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\n## Geodetic CRS:  NAD27\nplot(northcarolina_vector['BIR74'], main='Number of Live Births in 1974')\n\n\n\n\n\n\n\n\n\nCode\n# https://r-spatial.github.io/spdep/articles/sids.html\n\n\n\n\nCode\n# Raster Data\nlibrary(terra)\nluxembourg_elevation_raster &lt;- rast(system.file(\"ex/elev.tif\", package=\"terra\"))\nplot(luxembourg_elevation_raster)\n\n\n\n\n\n\n\n\n\n\n\nStationary.\nJust as with temporal data, stationarity in spatial data means that the statistical properties (like mean, variance, or spatial correlation) are roughly the same across space.\n\nStationary Means: \\(E[y(s)]=E[y(s')]\\) for all locations \\(s,s'\\)\nStationary Vars: \\(V[y(s)]=V[y(s')]\\) for all locations \\(s,s'\\)\n\n\n\nCode\n# Simulated 2D spatial fields\nset.seed(1)\nn &lt;- 20\nx &lt;- y &lt;- seq(0, 1, length.out = n)\ngrid &lt;- expand.grid(x = x, y = y)\n\n# 1. Stationary: Gaussian with constant mean and var\nz_stationary &lt;- matrix(rnorm(n^2, 0, 1), n, n)\n\n# 2. Nonstationary: Mean increases with x and y\nz_nonstationary &lt;- outer(x, y, function(x, y) 3*x*y) + rnorm(n^2, 0, 1)\n\npar(mfrow = c(1, 2))\n# Stationary field\nimage(x, y, z_stationary,\n      main = \"Stationary Field\",\n      col = terrain.colors(100),\n      xlab = \"x\", ylab = \"y\")\n# Nonstationary field\nimage(x, y, z_nonstationary,\n      main = \"Nonstationary Field\",\n      col = terrain.colors(100),\n      xlab = \"x\", ylab = \"y\")\n\n\n\n\n\n\n\n\n\n\n\nMeasures of spatial association.\nJust like temporal data may exhibit autocorrelation, spatial data may show spatial autocorrelation or spatial cross-correlation—meaning that observations located near each other are more (or less) similar than we would expect under spatial independence.\nAutocorrelation. We can measure spatial autocorrelation using Moran’s I, a standard index of spatial dependence. Global Moran’s I summarizes overall spatial association (just like the ACF)\n\n\nCode\n# Raster Data Example\nautocor(luxembourg_elevation_raster, method='moran', global=T)\n## elevation \n## 0.8917057\n\n\nCross-Correlation. We can also assesses the relationship between two variables at varying distances.\n\n\nCode\n# Vector Data Example\ndat &lt;- as.data.frame(northcarolina_vector)[, c('BIR74', 'SID74')]\nmu &lt;- colMeans(dat)\n\n# Format Distances\ndmat &lt;- st_distance( st_centroid(northcarolina_vector) )\ndmat &lt;- units::set_units(dmat, 'km')\n\n# At Which Distances to Compute CCF\n# summary(dmat[,1])\nrdists &lt;- c(-1,seq(0,100,by=25)) # includes 0\nrdists &lt;- units::set_units(rdists , 'km')\n\n# Compute Cross-Covariances\nvarXY &lt;- prod( apply(dat, 2, sd) )\nCCF &lt;- lapply( seq(2, length(rdists)), function(ri){\n    # Which Observations are within (rmin, rmax] distance\n    dmat_r &lt;- dmat\n    d_id &lt;- (dmat_r &gt; rdists[ri-1] & dmat_r &lt;= rdists[ri]) \n    dmat_r[!d_id]  &lt;- NA\n    # Compute All Covariances (Stationary)\n    covs_r &lt;- lapply(1:nrow(dmat_r), function(i){\n        pairsi &lt;- which(!is.na(dmat_r[i,]))        \n        covXiYj &lt;- sapply(pairsi, function(j) {\n            dXi &lt;- dat[i,1] - mu[1]\n            dYj &lt;- dat[j,2] - mu[2]\n            return(dXi*dYj)\n        })\n        return(covXiYj)\n    })\n    corXY &lt;- unlist(covs_r)/varXY\n    return(corXY)\n} )\n\n\n\n\nCode\n# Plot Cross-Covariance Function\nx &lt;- as.numeric(rdists[-1])\n\npar(mfrow=c(1,2))\n\n# Distributional Summary\nboxplot(CCF,\n    outline=F, whisklty=0, staplelty=0,\n    ylim=c(-1,1), #quantile(unlist(CCF), probs=c(.05,.95)),\n    names=x, \n    main='',\n    font.main=1,\n    xlab='Distance [km]',\n    ylab='Cross-Correlation of BIR74 and SID74')\ntitle('Binned Medians and IQRs', font.main=1, adj=0)\nabline(h=0, lty=2)\n\n# Inferential Summary\nCCF_means &lt;- sapply(CCF, mean)\nplot(x, CCF_means,\n    ylim=c(-1,1),\n    type='o', pch=16,\n    main='',\n    xlab='Distance [km]',\n    ylab='Cross-Correlation of BIR74 and SID74')\ntitle('Binned Means + 95% Confidence Band', font.main=1, adj=0)\nabline(h=0, lty=2)    \n# Quick and Dirty Subsampling CI\nCCF_meanCI &lt;- sapply(CCF, function(corXY){\n    ss_size &lt;- floor(length(corXY)*3/4)\n    corXY_boot &lt;- sapply(1:200, function(b){\n        corXY_b &lt;- sample(corXY, ss_size, replace=F)\n        mean(corXY_b, na.rm=T)\n    })\n    quantile(corXY_boot,  probs=c(.025,.975), na.rm=T)\n})\npolygon( c(x, rev(x)), \n    c(CCF_meanCI[1,], rev(CCF_meanCI[2,])), \n    col=grey(0,.25), border=NA)",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Observational Data</span>"
    ]
  },
  {
    "objectID": "02_05_ObservationalData.html#economic-interdependence",
    "href": "02_05_ObservationalData.html#economic-interdependence",
    "title": "14  Observational Data",
    "section": "14.3 Economic Interdependence",
    "text": "14.3 Economic Interdependence\nIn addition to spatial and temporal dependence, many observational datasets exhibit interdependence between variables for economic reasons. In the minds of economists, many variables are endogenous: meaning that they are an economic outcome determined (or caused: \\(\\to\\)) by some other variable.\n\nIf \\(Y \\to X\\), then we have reverse causality\nIf \\(Y \\to X\\) and \\(X \\to Y\\), then we have simultaneity\nIf \\(Z\\to Y\\) and either \\(Z\\to X\\) or \\(X \\to Z\\), then we have omitted a potentially important variable\n\nThese endogeneity issues imply \\(X\\) and \\(\\epsilon\\) are correlated, which is a barrier to interpreting OLS estimates causally. (\\(X\\) and \\(\\epsilon\\) may be correlated for other reasons too, such as when \\(X\\) is measured with error.)\n\n\nCode\n# Simulate data with an endogeneity issue\nn &lt;- 300\nz &lt;- rbinom(n,1,.5)\nxy &lt;- sapply(z, function(zi){\n    y &lt;- rnorm(1,zi,1)\n    x &lt;- rnorm(1,zi*2,1)\n    c(x,y)\n})\nxy &lt;- data.frame(x=xy[1,],y=xy[2,])\nplot(y~x, data=xy, pch=16, col=grey(0,.5))\nabline(lm(y~x,data=xy))\n\n\n\n\n\n\n\n\n\nWith multiple linear regression, endogeneity biases are not just a problem for your main variable of interest. Suppose your interested in how \\(x_{1}\\) affects \\(y\\), conditional on \\(x_{2}\\). Letting \\(X=[x_{1}, x_{2}]\\), you estimate \\[\\begin{eqnarray}\n\\hat{\\beta}_{OLS} = [X'X]^{-1}X'y\n\\end{eqnarray}\\] You paid special attention in your research design to find a case where \\(x_{1}\\) is truly exogenous. Unfortunately, \\(x_{2}\\) is correlated with the error term. (How unfair, I know, especially after all that work). Nonetheless, \\[\\begin{eqnarray}\n\\mathbb{E}[X'\\epsilon] =\n\\begin{bmatrix}\n0 \\\\ \\rho\n\\end{bmatrix}\\\\\n\\mathbb{E}[ \\hat{\\beta}_{OLS} - \\beta] = [X'X]^{-1} \\begin{bmatrix}\n0 \\\\ \\rho\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\rho_{1} \\\\ \\rho_{2}\n\\end{bmatrix}\n\\end{eqnarray}\\] The magnitude of the bias for \\(x_{1}\\) thus depends on the correlations between \\(x_{1}\\) and \\(x_{2}\\) as well as \\(x_{2}\\) and \\(\\epsilon\\).\nI will focus on the seminal economic example to provide some intuition.\n\nCompetitive Market Equilibrium.\nThis model has three structural relationships: (1) market supply is the sum of quantities supplied by individual firms at a given price, (2) market demand is the sum of quantities demanded by individual people at a given price, and (3) market supply equals market demand in equilibrium. Assuming market supply and demand are linear, we can write these three relationships as \\[\\begin{eqnarray}\n\\label{eqn:market_supply}\nQ_{S}(P) &=& A_{S} + B_{S} P + E_{S},\\\\\n\\label{eqn:market_demand}\nQ_{D}(P) &=& A_{D} - B_{D} P + E_{D},\\\\\n\\label{eqn:market_eq}\nQ_{D} &=& Q_{S} = Q.\n%%  $Q_{D}(P) = \\sum_{i} q_{D}_{i}(P)$,\n\\end{eqnarray}\\] This last equation implies a simultaneous “reduced form” relationship where both the price and the quantity are outcomes. With a linear parametric structure to these equations, we can use algebra to solve for the equilibrium price and quantity analytically as \\[\\begin{eqnarray}\nP^{*} &=& \\frac{A_{D}-A_{S}}{B_{D}+B_{S}} + \\frac{E_{D} - E_{S}}{B_{D}+B_{S}}, \\\\\nQ^{*} &=& \\frac{A_{S}B_{D}+ A_{D}B_{S}}{B_{D}+B_{S}} + \\frac{E_{S}B_{D}+ E_{D}B_{S}}{B_{D}+B_{S}}.\n\\end{eqnarray}\\]\n\n\nCode\n# Demand Curve Simulator\nqd_fun &lt;- function(p, Ad=8, Bd=-.8, Ed_sigma=.25){\n    Qd &lt;- Ad + Bd*p + rnorm(1,0,Ed_sigma)\n    return(Qd)\n}\n\n# Supply Curve Simulator\nqs_fun &lt;- function(p, As=-8, Bs=1, Es_sigma=.25){\n    Qs &lt;- As + Bs*p + rnorm(1,0,Es_sigma)\n    return(Qs)\n}\n\n# Quantity Supplied and Demanded at 3 Prices\ncbind(P=8:10, D=qd_fun(8:10), S=qs_fun(8:10))\n##       P          D          S\n## [1,]  8  1.1925652 0.01120111\n## [2,]  9  0.3925652 1.01120111\n## [3,] 10 -0.4074348 2.01120111\n\n# Market Equilibrium Finder\neq_fun &lt;- function(demand, supply, P){\n    # Compute EQ (what we observe)\n    eq_id &lt;- which.min( abs(demand-supply) )\n    eq &lt;- c(P=P[eq_id], Q=demand[eq_id]) \n    return(eq)\n}\n\n\n\n\nCode\n# Simulations Parameters\nN &lt;- 300 # Number of Market Interactions\nP &lt;- seq(5,10,by=.01) # Price Range to Consider\n\n# Generate Data from Competitive Market  \n# Plot Underlying Process\nplot.new()\nplot.window(xlim=c(0,2), ylim=range(P))\nEQ1 &lt;- sapply(1:N, function(n){\n    # Market Data Generating Process\n    demand &lt;- qd_fun(P)\n    supply &lt;- qs_fun(P)\n    eq &lt;- eq_fun(demand, supply, P)    \n    # Plot Theoretical Supply and Demand\n    lines(demand, P, col=grey(0,.01))\n    lines(supply, P, col=grey(0,.01))\n    points(eq[2], eq[1], col=grey(0,.05), pch=16)\n    # Save Data\n    return(eq)\n})\naxis(1)\naxis(2)\nmtext('Quantity',1, line=2)\nmtext('Price',2, line=2)\n\n\n\n\n\n\n\n\n\nSuppose we ask “what is the effect of price on quantity?” You can simply run a regression of quantity (“Y”) on price (“X”): \\(\\widehat{\\beta}_{OLS} = Cov(Q^{*}, P^{*}) / Var(P^{*})\\). You get a number back, but it is hard to interpret meaningfully.\n\n\nCode\n# Analyze Market Data\ndat1 &lt;- data.frame(t(EQ1), cost='1', T=1:N)\nreg1 &lt;- lm(Q~P, data=dat1)\nsummary(reg1)\n## \n## Call:\n## lm(formula = Q ~ P, data = dat1)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.57279 -0.11977 -0.00272  0.11959  0.45525 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)  \n## (Intercept) -0.21323    0.43212  -0.493   0.6221  \n## P            0.12355    0.04864   2.540   0.0116 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1674 on 298 degrees of freedom\n## Multiple R-squared:  0.02119,    Adjusted R-squared:  0.0179 \n## F-statistic: 6.451 on 1 and 298 DF,  p-value: 0.0116\n\n\nThis simple derivation has a profound insight: price-quantity data does not generally tell you how price affects quantity (or vice-versa). The reason is simultaneity: price and quantity mutually cause one another in markets.1\nMoreover, this example also clarifies that our initial question “what is the effect of price on quantity?” is misguided. We could more sensibly ask “what is the effect of price on quantity supplied?” or “what is the effect of price on quantity demanded?”",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Observational Data</span>"
    ]
  },
  {
    "objectID": "02_05_ObservationalData.html#further-reading",
    "href": "02_05_ObservationalData.html#further-reading",
    "title": "14  Observational Data",
    "section": "14.4 Further Reading",
    "text": "14.4 Further Reading",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Observational Data</span>"
    ]
  },
  {
    "objectID": "02_05_ObservationalData.html#footnotes",
    "href": "02_05_ObservationalData.html#footnotes",
    "title": "14  Observational Data",
    "section": "",
    "text": "Although there are many ways this simultaneity can happen, economic theorists have made great strides in analyzing the simultaneity problem as it arises from equilibrium market relationships. In fact, 2SLS arose to understand agricultural markets.↩︎",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Observational Data</span>"
    ]
  },
  {
    "objectID": "02_06_ExperimentalData.html",
    "href": "02_06_ExperimentalData.html",
    "title": "15  Experimental Data",
    "section": "",
    "text": "15.1 Design Basics\nWe will consider with our example from the last chaper\nCode\n# Demand Curve Simulator\nqd_fun &lt;- function(p, Ad=8, Bd=-.8, Ed_sigma=.25){\n    Qd &lt;- Ad + Bd*p + rnorm(1,0,Ed_sigma)\n    return(Qd)\n}\n\n# Supply Curve Simulator\nqs_fun &lt;- function(p, As=-8, Bs=1, Es_sigma=.25){\n    Qs &lt;- As + Bs*p + rnorm(1,0,Es_sigma)\n    return(Qs)\n}\n\n# Quantity Supplied and Demanded at 3 Prices\ncbind(P=8:10, D=qd_fun(8:10), S=qs_fun(8:10))\n##       P         D         S\n## [1,]  8 1.8638052 0.2568647\n## [2,]  9 1.0638052 1.2568647\n## [3,] 10 0.2638052 2.2568647\n\n# Market Equilibrium Finder\neq_fun &lt;- function(demand, supply, P){\n    # Compute EQ (what we observe)\n    eq_id &lt;- which.min( abs(demand-supply) )\n    eq &lt;- c(P=P[eq_id], Q=demand[eq_id]) \n    return(eq)\n}\nCode\nN &lt;- 300 # Number of Market Interactions\nP &lt;- seq(5,10,by=.01) # Price Range to Consider\nEQ1 &lt;- sapply(1:N, function(n){\n    # Market Data Generating Process\n    demand &lt;- qd_fun(P)\n    supply &lt;- qs_fun(P)\n    eq &lt;- eq_fun(demand, supply, P)    \n    return(eq)\n})\ndat1 &lt;- data.frame(t(EQ1), cost='1', T=1:N)",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Experimental Data</span>"
    ]
  },
  {
    "objectID": "02_06_ExperimentalData.html#design-basics",
    "href": "02_06_ExperimentalData.html#design-basics",
    "title": "15  Experimental Data",
    "section": "",
    "text": "Control and Randomize.\nBlocking and Clustering\n\n\nCompetitive Equilibrium Example.\nIf you have exogenous variation on one side of the market, you can get information on the other. For example, lower costs shift out supply (more is produced at given price), allowing you to trace out part of a demand curve.\nTo see this, consider an experiment where student subjects are recruited to a classroom and randomly assigned to be either buyers or sellers in a market for little red balls. In this case, the classroom environment allows the experimenter to control for various factors (e.g., the temperature of the room is constant for all subjects) and the explicit randomization of subjects means that there are not typically systematic differences in different groups of students.\nIn the experiment, sellers are given linear “cost functions” that theoretically yield individual supplies like \\(\\eqref{eqn:market_supply}\\) and are paid “price - cost”. Buyers are given linear “benefit functions” that theoretically yield individual demands like \\(\\eqref{eqn:market_demand}\\), and are paid “benefit - price”. The theoretical predictions are theorefore given in \\(\\eqref{eqn:market_supply}\\). Moreover, experimental manipulation of \\(A_{S}\\) leads to \\[\\begin{eqnarray}\n\\label{eqn:comp_market_statics}\n\\frac{d P^{*}}{d A_{S}} = \\frac{-1}{B_{D}+B_{S}}, \\\\\n\\frac{d Q^{*}}{d A_{S}} = \\frac{B_{D}}{B_{D}+B_{S}}.\n\\end{eqnarray}\\] In this case, the supply shock has identified the demand slope: \\(-B_{D}=d Q^{*}/d P^{*}\\).\n\n\nCode\n# New Observations After Cost Change\nEQ2 &lt;- sapply(1:N, function(n){\n    demand &lt;- qd_fun(P)\n    supply2 &lt;- qs_fun(P, As=-6.5) # More Supplied at Given Price\n    eq &lt;- eq_fun(demand, supply2, P)\n    return(eq)\n    # lines(supply2, P, col=rgb(0,0,1,.01))\n    #points(eq[2], eq[1], col=rgb(0,0,1,.05), pch=16)\n})\ndat2 &lt;- data.frame(t(EQ2), cost='2', T=(1:N) + N)\ndat2 &lt;- rbind(dat1, dat2)\n\n# Plot Simulated Market Data\ncols &lt;- ifelse(as.numeric(dat2$cost)==2, rgb(0,0,1,.2), rgb(0,0,0,.2))\nplot.new()\nplot.window(xlim=c(0,2), ylim=range(P))\npoints(dat2$Q, dat2$P, col=cols, pch=16)\naxis(1)\naxis(2)\nmtext('Quantity',1, line=2)\nmtext('Price',2, line=2)\n\n\n\n\n\n\n\n\n\nIf the function forms for supply and demand are different from what we predicted, we can still measure how much the experimental manipulation of production costs affects the equilibrium quantity sold (and compare that to what was predicted).1",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Experimental Data</span>"
    ]
  },
  {
    "objectID": "02_06_ExperimentalData.html#comparisons-over-time",
    "href": "02_06_ExperimentalData.html#comparisons-over-time",
    "title": "15  Experimental Data",
    "section": "15.2 Comparisons Over Time",
    "text": "15.2 Comparisons Over Time\n\nRegression Discontinuities/Kinks.\nThe basic idea of RDD/RKD is to examine how a variable changes just before and just after a treatment. RDD estimates the difference in the levels of an outcome variable, whereas RKD estimates the difference in the slope. Turning to our canonical competitive market example, the RDD estimate is the difference between the lines at \\(T=300\\).\n\n\nCode\n# Locally Linear Regression \n# (Compare means near break)\n\ncols &lt;- ifelse(as.numeric(dat2$cost)==2, rgb(0,0,1,.5), rgb(0,0,0,.5))\nplot(P~T, dat2, main='Effect of Cost Shock on Price', \n    font.main=1, pch=16, col=cols)\nregP1 &lt;- loess(P~T, dat2[dat2$cost==1,]) \nx1 &lt;- regP1$x\n#lm(): x1 &lt;- regP1$model$T \nlines(x1, predict(regP1), col=rgb(0,0,0), lwd=2)\nregP2 &lt;- loess(P~T, dat2[dat2$cost==2,])\nx2 &lt;- regP2$x #regP1$model$T\nlines(x2, predict(regP2), col=rgb(0,0,1), lwd=2)\n\n\n\n\n\n\n\n\n\nCode\n\nplot(Q~T, dat2, main='Effect of Cost Shock on Quantity',\n    font.main=1, pch=16, col=cols)\nregQ1 &lt;- loess(Q~T, dat2[dat2$cost==1,]) \nlines(x1, predict(regQ1), col=rgb(0,0,0), lwd=2)\nregQ2 &lt;- loess(Q~T, dat2[dat2$cost==2,])\nx2 &lt;- regP2$x #regP1$model$T\nlines(x2, predict(regQ2), col=rgb(0,0,1), lwd=2)\n\n\n\n\n\n\n\n\n\n\nCode\n# Linear Regression Alternative\nsub_id &lt;- (dat2$cost==1 & dat2$T &gt; 250) | (dat2$cost==2 & dat2$T &lt; 300)\ndat2W &lt;- dat2[sub_id,  ]\nregP &lt;- lm(P~T*cost, dat2)\nregQ &lt;- lm(Q~T*cost, dat2)\nstargazer::stargazer(regP, regQ, \n    type='html',\n    title='Recipe RDD',\n    header=F)\n\n\n\nRecipe RDD\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nP\n\n\nQ\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nT\n\n\n0.0002\n\n\n-0.0001\n\n\n\n\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n\n\n\n\n\n\n\n\n\n\ncost2\n\n\n-0.785***\n\n\n0.614***\n\n\n\n\n\n\n(0.066)\n\n\n(0.056)\n\n\n\n\n\n\n\n\n\n\n\n\nT:cost2\n\n\n-0.0002\n\n\n0.0002\n\n\n\n\n\n\n(0.0002)\n\n\n(0.0002)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n8.867***\n\n\n0.912***\n\n\n\n\n\n\n(0.023)\n\n\n(0.020)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n600\n\n\n600\n\n\n\n\nR2\n\n\n0.808\n\n\n0.788\n\n\n\n\nAdjusted R2\n\n\n0.807\n\n\n0.787\n\n\n\n\nResidual Std. Error (df = 596)\n\n\n0.202\n\n\n0.172\n\n\n\n\nF Statistic (df = 3; 596)\n\n\n836.699***\n\n\n739.848***\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\nRemember that this is effect is local: different magnitudes of the cost shock or different demand curves generally yield different estimates.\nMoreover, note that more than just costs have changed over time: subjects in the later periods have history experience behind them while they do not in earlier periods. So hidden variables like “beliefs” are implicitly treated as well. This is one concrete reason to have an explicit control group.\n\n\nDifference in Differences.\nThe basic idea of DID is to examine how a variable changes in response to an exogenous shock, compared to a control group.\n\n\nCode\nEQ3 &lt;- sapply(1:(2*N), function(n){\n\n    # Market Mechanisms\n    demand &lt;- qd_fun(P)\n    supply &lt;- qs_fun(P)\n\n    # Compute EQ (what we observe)\n    eq_id &lt;- which.min( abs(demand-supply) )\n    eq &lt;- c(P=P[eq_id], Q=demand[eq_id]) \n\n    # Return Equilibrium Observations\n    return(eq)\n})\ndat3 &lt;- data.frame(t(EQ3), cost='1', T=1:ncol(EQ3))\ndat3_pre  &lt;- dat3[dat3$T &lt;= N ,]\ndat3_post &lt;- dat3[dat3$T &gt; N ,]\n\n# Plot Price Data\npar(mfrow=c(1,2))\nplot(P~T, dat2, main='Effect of Cost Shock on Price', \n    font.main=1, pch=16, col=cols, cex=.5)\nlines(x1, predict(regP1), col=rgb(0,0,0), lwd=2)\nlines(x2, predict(regP2), col=rgb(0,0,1), lwd=2)\n# W/ Control group\npoints(P~T, dat3, pch=16, col=rgb(1,0,0,.5), cex=.5)\nregP3a &lt;- loess(P~T, dat3_pre)\nx3a &lt;- regP3a$x\nlines(x3a, predict(regP3a), col=rgb(1,0,0), lwd=2)\nregP3b &lt;- loess(P~T, dat3_post)\nx3b &lt;- regP3b$x\nlines(x3b, predict(regP3b), col=rgb(1,0,0), lwd=2)\n\n\n# Plot Quantity Data\nplot(Q~T, dat2, main='Effect of Cost Shock on Quantity',\n    font.main=1, pch=17, col=cols, cex=.5)\nlines(x1, predict(regQ1), col=rgb(0,0,0), lwd=2)\nlines(x2, predict(regQ2), col=rgb(0,0,1), lwd=2)\n# W/ Control group\npoints(Q~T, dat3, pch=16, col=rgb(1,0,0,.5), cex=.5)\nregQ3a &lt;- loess(Q~T, dat3_pre) \nlines(x3a, predict(regQ3a), col=rgb(1,0,0), lwd=2)\nregQ3b &lt;- loess(Q~T, dat3_post) \nlines(x3b, predict(regQ3b), col=rgb(1,0,0), lwd=2)\n\n\n\n\n\n\n\n\n\nLinear Regression Estimates\n\nCode\n# Pool Data\ndat_pooled &lt;- rbind(\n    cbind(dat2, EverTreated=1, PostPeriod=(dat2$T &gt; N)),\n    cbind(dat3, EverTreated=0, PostPeriod=(dat3$T &gt; N)))\ndat_pooled$EverTreated &lt;- as.factor(dat_pooled$EverTreated)\ndat_pooled$PostPeriod &lt;- as.factor(dat_pooled$PostPeriod)\n\n# Estimate Level Shift for Different Groups after T=300\nregP &lt;- lm(P~PostPeriod*EverTreated, dat_pooled)\nregQ &lt;- lm(Q~PostPeriod*EverTreated, dat_pooled)\nstargazer::stargazer(regP, regQ, \n    type='html',\n    title='Recipe DiD',\n    header=F)\n\n\n\nRecipe DiD\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nP\n\n\nQ\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nPostPeriod\n\n\n0.007\n\n\n0.013\n\n\n\n\n\n\n(0.016)\n\n\n(0.014)\n\n\n\n\n\n\n\n\n\n\n\n\nEverTreated1\n\n\n0.018\n\n\n0.022\n\n\n\n\n\n\n(0.016)\n\n\n(0.014)\n\n\n\n\n\n\n\n\n\n\n\n\nPostPeriodTRUE:EverTreated1\n\n\n-0.834***\n\n\n0.651***\n\n\n\n\n\n\n(0.023)\n\n\n(0.020)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n8.873***\n\n\n0.876***\n\n\n\n\n\n\n(0.011)\n\n\n(0.010)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n1,200\n\n\n1,200\n\n\n\n\nR2\n\n\n0.761\n\n\n0.742\n\n\n\n\nAdjusted R2\n\n\n0.760\n\n\n0.741\n\n\n\n\nResidual Std. Error (df = 1196)\n\n\n0.199\n\n\n0.173\n\n\n\n\nF Statistic (df = 3; 1196)\n\n\n1,266.700***\n\n\n1,143.887***\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Experimental Data</span>"
    ]
  },
  {
    "objectID": "02_06_ExperimentalData.html#natural-experiments",
    "href": "02_06_ExperimentalData.html#natural-experiments",
    "title": "15  Experimental Data",
    "section": "15.3 “Natural” Experiments",
    "text": "15.3 “Natural” Experiments\nNatural experiments are historical case studies that remedy the endogeneity issues in observational data. They assume that a historical events is quasi (or psuedo) random. In addition to “RDD” and “DID” methods discussed above, instrumental variables are used in historical event studies. The elementary versions use linear regression, so I can cover them here using our competitive equilibrium example from before.\n\nTwo Stage Least Squares (2SLS).\nConsider the market equilibrium example, which contains a cost shock. We can simply run another regression, but there will still be a problem.\n\n\nCode\n# Not exactly right, but at least right sign\nreg2 &lt;- lm(Q~P, data=dat2)\nsummary(reg2)\n## \n## Call:\n## lm(formula = Q ~ P, data = dat2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.6802 -0.1667 -0.0124  0.1671  0.6334 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  6.58839    0.17719   37.18   &lt;2e-16 ***\n## P           -0.63207    0.02087  -30.29   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.235 on 598 degrees of freedom\n## Multiple R-squared:  0.6054, Adjusted R-squared:  0.6047 \n## F-statistic: 917.4 on 1 and 598 DF,  p-value: &lt; 2.2e-16\n\n\nIt turns out that rhe noisiness of the process within each group affects our OLS estimate: \\(\\widehat{\\beta}_{OLS}=Cov(Q^{*}, P^{*}) / Var(P^{*})\\). For details, see\n\n\nWithin Group Variance\n\nYou can experiment with the effect of different variances on both OLS and IV in the code below. And note that if we had multiple supply shifts and recorded their magnitudes, then we could recover more information about demand, perhaps tracing it out entirely.\n\n\nCode\n# Examine\nEgrid &lt;- expand.grid(Ed_sigma=c(.001, .25, 1), Es_sigma=c(.001, .25, 1))\n\nEgrid_regs &lt;- lapply(1:nrow(Egrid), function(i){\n    Ed_sigma &lt;- Egrid[i,1]\n    Es_sigma &lt;- Egrid[i,2]    \n    EQ1 &lt;- sapply(1:N, function(n){\n        demand &lt;- qd_fun(P, Ed_sigma=Ed_sigma)\n        supply &lt;- qs_fun(P, Es_sigma=Es_sigma)\n        return(eq_fun(demand, supply, P))\n    })\n    EQ2 &lt;- sapply(1:N, function(n){\n        demand &lt;- qd_fun(P,Ed_sigma=Ed_sigma)\n        supply2 &lt;- qs_fun(P, As=-6.5,Es_sigma=Es_sigma)\n        return(eq_fun(demand, supply2, P))\n    })\n    dat &lt;- rbind(\n        data.frame(t(EQ1), cost='1'),\n        data.frame(t(EQ2), cost='2'))\n    return(dat)\n})\nEgrid_OLS &lt;- sapply(Egrid_regs, function(dat) coef( lm(Q~P, data=dat)))\nEgrid_IV &lt;- sapply(Egrid_regs, function(dat) coef( feols(Q~1|P~cost, data=dat)))\n\n#cbind(Egrid, coef_OLS=t(Egrid_OLS)[,2], coef_IV=t(Egrid_IV)[,2])\nlapply( list(Egrid_OLS, Egrid_IV), function(ei){\n    Emat &lt;- matrix(ei[2,],3,3)\n    rownames(Emat) &lt;- paste0('Ed_sigma.',c(.001, .25, 1))\n    colnames(Emat) &lt;- paste0('Es_sigma.',c(.001, .25, 1))\n    return( round(Emat,2))\n})\n\n\n\nTo overcome this issue, we can compute the change in the expected values \\(d \\mathbb{E}[Q^{*}] / d \\mathbb{E}[P^{*}] =-B_{D}\\). Empirically, this is estimated via the change in average value.\n\n\nCode\n# Wald (1940) Estimate\ndat_mean &lt;- rbind(\n    colMeans(dat2[dat2$cost==1,1:2]),\n    colMeans(dat2[dat2$cost==2,1:2]))\ndat_mean\n##             P         Q\n## [1,] 8.891700 0.8977991\n## [2,] 8.065267 1.5609349\nB_est &lt;- diff(dat_mean[,2])/diff(dat_mean[,1])\nround(B_est, 2)\n## [1] -0.8\n\n\nWe can also separately recover \\(d \\mathbb{E}[Q^{*}] / d \\mathbb{E}[A_{S}]\\) and \\(d \\mathbb{E}[P^{*}] / d \\mathbb{E}[A_{S}]\\) from separate regressions.2\n\n\nCode\n# Heckman (2000, p.58) Estimate\nols_1 &lt;- lm(P~cost, data=dat2)\nols_2 &lt;- lm(Q~cost, data=dat2)\nB_est2 &lt;- coef(ols_2)/coef(ols_1)\nround(B_est2[[2]],2)\n## [1] -0.8\n\n\nAlternatively, we can recover the same estimate using an 2SLS regression with two equations: \\[\\begin{eqnarray}\nP &=& \\alpha_{1} + A_{S} \\beta_{1} + \\epsilon_{1} \\\\\nQ &=& \\alpha_{2} + \\hat{P} \\beta_{2} + \\epsilon_{2}.\n\\end{eqnarray}\\] In the first regression, we estimate the average effect of the cost shock on prices. In the second equation, we estimate how the average effect of prices which are exogenous to demand affect quantity demanded. To see this, first substitute the equilibrium condition into the supply equation: \\(Q_{D}=Q_{S}=A_{S}+B_{S} P + E_{S}\\), lets us rewrite \\(P\\) as a function of \\(Q_{D}\\). This yields two theoretical equations \\[\\begin{eqnarray}\n\\label{eqn:linear_supply_iv}\nP &=& -\\frac{A_{S}}{{B_{S}}} + \\frac{Q_{D}}{B_{S}} - \\frac{E_{S}}{B_{S}} \\\\\n\\label{eqn:linear_demand_iv}\nQ_{D} &=&  A_{D} + B_{D} P  + E_{D}.\n\\end{eqnarray}\\]\n\n\nCode\n# Two Stage Least Squares Estimate\nols_1 &lt;- lm(P~cost, data=dat2)\ndat2_new  &lt;- cbind(dat2, Phat=predict(ols_1))\nreg_2sls &lt;- lm(Q~Phat, data=dat2_new)\nsummary(reg_2sls)\n## \n## Call:\n## lm(formula = Q ~ Phat, data = dat2_new)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.50878 -0.11809 -0.00215  0.11295  0.52470 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  8.03256    0.14450   55.59   &lt;2e-16 ***\n## Phat        -0.80241    0.01702  -47.14   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1723 on 598 degrees of freedom\n## Multiple R-squared:  0.7879, Adjusted R-squared:  0.7876 \n## F-statistic:  2222 on 1 and 598 DF,  p-value: &lt; 2.2e-16\n\n# One Stage Instrumental Variables Estimate\nlibrary(fixest)\nreg2_iv &lt;- feols(Q~1|P~cost, data=dat2)\nsummary(reg2_iv)\n## TSLS estimation - Dep. Var.: Q\n##                   Endo.    : P\n##                   Instr.   : cost\n## Second stage: Dep. Var.: Q\n## Observations: 600\n## Standard-errors: IID \n##              Estimate Std. Error  t value  Pr(&gt;|t|)    \n## (Intercept)  8.032561   0.207806  38.6541 &lt; 2.2e-16 ***\n## fit_P       -0.802407   0.024481 -32.7770 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## RMSE: 0.247374   Adj. R2: 0.787576\n## F-test (1st stage), P: stat = 2,510.35, p &lt; 2.2e-16, on 1 and 598 DoF.\n##            Wu-Hausman: stat =   524.51, p &lt; 2.2e-16, on 1 and 597 DoF.\n\n\n\n\nCaveats.\n2SLS regression analysis can be very insightful, but I also want to stress some caveats about their practical application.\nWe always get coefficients back when running feols, and sometimes the computed p-values are very small. The interpretation of those numbers rests on many assumptions:\n\nInstrument exogeneity (Exclusion Restriction): The instrument must affect outcomes only through the treatment variable (e.g., only supply is affected directly, not demand).\nInstrument relevance: The instrument must be strongly correlated with the endogenous regressor, implying the shock creates meaningful variation.\nFunctional form correctness: Supply and demand are assumed linear and additively separable.\nMultiple hypothesis testing risks: We were not repeatedly testing different instruments, which can artificially produce significant findings by chance.\n\nWe are rarely sure that all of these assumptions hold, and this is one reason why researchers often also report their OLS results. But that is insufficient, as spatial and temporal dependence also complicate inference:\n\nExclusion restriction violations: Spatial or temporal spillovers may cause instruments to affect the outcome through unintended channels, undermining instrument exogeneity.\nWeak instruments: Spatial clustering, serial correlation, or network interdependencies can reduce instrument variation, causing weak instruments.\nInference and standard errors: Spatial or temporal interdependence reduces the effective sample size, making conventional standard errors misleadingly small.",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Experimental Data</span>"
    ]
  },
  {
    "objectID": "02_06_ExperimentalData.html#further-reading",
    "href": "02_06_ExperimentalData.html#further-reading",
    "title": "15  Experimental Data",
    "section": "15.4 Further Reading",
    "text": "15.4 Further Reading\nYou are directed to the following resources which discusses endogeneity in more detail and how it applies generally.\n\nCausal Inference for Statistics, Social, and Biomedical Sciences: An Introduction\nhttps://www.mostlyharmlesseconometrics.com/\nhttps://www.econometrics-with-r.org\nhttps://bookdown.org/paul/applied-causal-analysis/\nhttps://mixtape.scunning.com/\nhttps://theeffectbook.net/\nhttps://www.r-causal.org/\nhttps://matheusfacure.github.io/python-causality-handbook/landing-page.html\n\nFor RDD and DID methods in natural experiments, see\n\nhttps://bookdown.org/paul/applied-causal-analysis/rdd-regression-discontinuity-design.html\nhttps://mixtape.scunning.com/06-regression_discontinuity\nhttps://theeffectbook.net/ch-RegressionDiscontinuity.html\nhttps://mixtape.scunning.com/09-difference_in_differences\nhttps://theeffectbook.net/ch-DifferenceinDifference.html\nhttp://www.urfie.net/read/index.html#page/226\n\nFor IV methods in natural experiments, see\n\nhttps://cameron.econ.ucdavis.edu/e240a/ch04iv.pdf\nhttps://mru.org/courses/mastering-econometrics/introduction-instrumental-variables-part-one\nhttps://www.econometrics-with-r.org/12-ivr.html\nhttps://bookdown.org/paul/applied-causal-analysis/estimation-2.html\nhttps://mixtape.scunning.com/07-instrumental_variables\nhttps://theeffectbook.net/ch-InstrumentalVariables.html\nhttp://www.urfie.net/read/index.html#page/247",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Experimental Data</span>"
    ]
  },
  {
    "objectID": "02_06_ExperimentalData.html#footnotes",
    "href": "02_06_ExperimentalData.html#footnotes",
    "title": "15  Experimental Data",
    "section": "",
    "text": "Notice that even in this linear model, however, all effects are conditional: The effect of a cost change on quantity or price depends on the demand curve. A change in costs affects quantity supplied but not quantity demanded (which then affects equilibrium price) but the demand side of the market still matters! The change in price from a change in costs depends on the elasticity of demand.↩︎\nMathematically, we can also do this in a single step by exploiting linear algebra: \\(\\frac{\\frac{ Cov(Q^{*},A_{S})}{ V(A_{S}) } }{\\frac{ Cov(P^{*},A_{S})}{ V(A_{S}) }} = \\frac{Cov(Q^{*},A_{S} )}{ Cov(P^{*},A_{S})}.\\)↩︎",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Experimental Data</span>"
    ]
  },
  {
    "objectID": "02_07_DataScientism.html",
    "href": "02_07_DataScientism.html",
    "title": "16  Data Scientism",
    "section": "",
    "text": "16.1 False Positives\nIn practice, it is hard to find a good natural experiment. For example, suppose we asked “what is the effect of wages on police demanded?” and examined a policy which lowered the educational requirements from 4 years to 2 to become an officer. This increases the labour supply, but it also affects the demand curve through “general equilibrium”: as some of the new officers were potentially criminals and, with fewer criminals, the demand for police shifts down.\nIn practice, it is also easy to find a bad instrument. Paradoxically, natural experiments are something you are supposed to find but never search for. As you search for good instruments, for example, sometimes random noise will appear like a good instrument (spurious instruments). In this age of big data, we are getting increasingly more data and, perhaps surprisingly, this makes it easier to make false discoveries.\nWe will consider three classical ways for false discoveries to arise. After that, there are examples with the latest and greatest empirical recipes—we don’t have so many theoretical results yet but I think you can understand the issue with the numerical example. Although it is difficult to express numerically, you must also know that if you search for a good natural experiment for too long, you can also be led astray from important questions. There are good reasons to be excited about empirical social science, but we would be wise to recall some earlier wisdom from economists on the matter.",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Scientism</span>"
    ]
  },
  {
    "objectID": "02_07_DataScientism.html#false-positives",
    "href": "02_07_DataScientism.html#false-positives",
    "title": "16  Data Scientism",
    "section": "",
    "text": "Data Errors.\nA huge amount of data normally means a huge amount of data cleaning/merging/aggregating. This avoids many copy-paste errors, which are a recipe for disaster, but may also introduce other types of errors. Some spurious results are driven by honest errors in data cleaning. According to one estimate, this is responsible for around one fifth of all medical science retractions (there is even a whole book about this!). Although there are not similar meta-analysis in economics, there are some high-profile examples. This includes papers that are highly influential, like Lott, Levitt and Reinhart and Rogoff as well as others the top economics journals, like the RESTUD and AER. There are some reasons to think such errors are more widespread across the social sciences; e.g., in Census data and Aid data. So be careful!\nNote: one reason to plot your data is to help spot such errors.\n\n\nP-Hacking.\nAnother class of errors pertains to P-hacking (and it’s various synonyms: data drudging, star mining,….). While there are cases of fraudulent data manipulation (which can be considered as a dishonest data error), P-hacking need not even be intentional. You can simply be trying different variable transformations to uncover patterns in the data, for example, without accounting for how easy it is to find patterns when transforming completely random data. P-hacking is pernicious and widespread.\n\n\nCode\n# P-hacking OSLS with different explanatory vars\nset.seed(123)\nn &lt;- 50\nX1 &lt;- runif(n)\n\n# Regression Machine:\n# repeatedly finds covariate, runs regression\n# stops when statistically significant at .1%\np &lt;- 1\ni &lt;- 0\nwhile(p &gt;= .001){ \n    # Get Random Covariate\n    X2 &lt;-  runif(n)\n    # Merge and `Analyze'\n    dat_i &lt;- data.frame(X1,X2)\n    reg_i &lt;- lm(X1~X2, data=dat_i)\n    # update results in global environment\n    p &lt;- summary(reg_i)$coefficients[2,4]\n    i &lt;- i+1\n}\n#summary(reg_i)\n\nplot(X1~X2, data=dat_i,\n    pch=16, col=grey(0,.5), font.main=1,\n    main=paste0('Random Dataset ', i,\":   p=\",\n        formatC(p,digits=2, format='fg')))\nabline(reg_i)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# P-hacking 2SLS with different explanatory vars\n# and different instrumental vars\nlibrary(fixest)\np &lt;- 1\nii &lt;- 0\nset.seed(123)\nwhile(p &gt;= .05){\n    # Get Random Covariates\n    X2 &lt;-  runif(n)    \n    X3 &lt;-  runif(n)\n    # Create Treatment Variable based on Cutoff\n    cutoffs &lt;- seq(0,1,length.out=11)[-c(1,11)]\n    for(tau in cutoffs){\n        T3 &lt;- 1*(X3 &gt; tau)\n        # Merge and `Analyze'\n        dat_i &lt;- data.frame(X1,X2,T3)\n        ivreg_i &lt;- feols(X1~1|X2~T3, data=dat_i)\n        # Update results in global environment\n        ptab &lt;- summary(ivreg_i)$coeftable\n        if( nrow(ptab)==2){\n            p &lt;- ptab[2,4]\n            ii &lt;- ii+1\n        }\n    }\n}\nsummary(ivreg_i)\n## TSLS estimation - Dep. Var.: X1\n##                   Endo.    : X2\n##                   Instr.   : T3\n## Second stage: Dep. Var.: X1\n## Observations: 50\n## Standard-errors: IID \n##              Estimate Std. Error   t value  Pr(&gt;|t|)    \n## (Intercept) -9.95e-14      1e-06 -9.95e-08         1    \n## fit_X2       1.00e+00      1e-06  1.00e+06 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## RMSE: 5.81e-14   Adj. R2: -0.006886\n## F-test (1st stage), X2: stat = 0.66488, p = 0.418869, on 1 and 48 DoF.\n##             Wu-Hausman: stat = 0.23218, p = 0.632145, on 1 and 47 DoF.",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Scientism</span>"
    ]
  },
  {
    "objectID": "02_07_DataScientism.html#spurious-regression",
    "href": "02_07_DataScientism.html#spurious-regression",
    "title": "16  Data Scientism",
    "section": "16.2 Spurious Regression",
    "text": "16.2 Spurious Regression\nEven without any coding errors or p-hacking, you can sometimes make a false discovery. We begin with a motivating empirical example of “US Gov’t Spending on Science”.\nFirst, get and inspect some data from https://tylervigen.com/spurious-correlations\n\n\nCode\n# Your data is not made up in the computer (hopefully!)\nvigen_csv &lt;- read.csv( paste0(\n'https://raw.githubusercontent.com/the-mad-statter/',\n'whysospurious/master/data-raw/tylervigen.csv') ) \nclass(vigen_csv)\n## [1] \"data.frame\"\nnames(vigen_csv)\n##  [1] \"year\"                         \"science_spending\"            \n##  [3] \"hanging_suicides\"             \"pool_fall_drownings\"         \n##  [5] \"cage_films\"                   \"cheese_percap\"               \n##  [7] \"bed_deaths\"                   \"maine_divorce_rate\"          \n##  [9] \"margarine_percap\"             \"miss_usa_age\"                \n## [11] \"steam_murders\"                \"arcade_revenue\"              \n## [13] \"computer_science_doctorates\"  \"noncom_space_launches\"       \n## [15] \"sociology_doctorates\"         \"mozzarella_percap\"           \n## [17] \"civil_engineering_doctorates\" \"fishing_drownings\"           \n## [19] \"kentucky_marriage_rate\"       \"oil_imports_norway\"          \n## [21] \"chicken_percap\"               \"train_collision_deaths\"      \n## [23] \"oil_imports_total\"            \"pool_drownings\"              \n## [25] \"nuclear_power\"                \"japanese_cars_sold\"          \n## [27] \"motor_vehicle_suicides\"       \"spelling_bee_word_length\"    \n## [29] \"spider_deaths\"                \"math_doctorates\"             \n## [31] \"uranium\"\nvigen_csv[1:5,1:5]\n##   year science_spending hanging_suicides pool_fall_drownings cage_films\n## 1 1996               NA               NA                  NA         NA\n## 2 1997               NA               NA                  NA         NA\n## 3 1998               NA               NA                  NA         NA\n## 4 1999            18079             5427                 109          2\n## 5 2000            18594             5688                 102          2\n\n\nExamine some data\n\n\nCode\npar(mfrow=c(1,2), mar=c(2,2,2,1))\nplot.new()\nplot.window(xlim=c(1999, 2009), ylim=c(5,9)*1000)\nlines(science_spending/3~year, data=vigen_csv, lty=1, col=2, pch=16)\ntext(2003, 8200, 'US spending on science, space, technology (USD/3)', col=2, cex=.6, srt=30)\nlines(hanging_suicides~year, data=vigen_csv, lty=1, col=4, pch=16)\ntext(2004, 6500, 'US Suicides by hanging, strangulation, suffocation (Deaths)', col=4, cex=.6, srt=30)\naxis(1)\naxis(2)\n\n\nplot.new()\nplot.window(xlim=c(2002, 2009), ylim=c(0,5))\nlines(cage_films~year, data=vigen_csv[vigen_csv$year&gt;=2002,], lty=1, col=2, pch=16)\ntext(2006, 0.5, 'Number of films with Nicolas Cage (Films)', col=2, cex=.6, srt=0)\nlines(pool_fall_drownings/25~year, data=vigen_csv[vigen_csv$year&gt;=2002,], lty=1, col=4, pch=16)\ntext(2006, 4.5, 'Number of drownings by falling into pool (US Deaths/25)', col=4, cex=.6, srt=0)\naxis(1)\naxis(2)\n\n\n\n\n\n\n\n\n\n\nCode\n# Include an intercept to regression 1\n#reg2 &lt;-  lm(cage_films ~ science_spending, data=vigen_csv)\n#suppressMessages(library(stargazer))\n#stargazer(reg1, reg2, type='html')\n\n\nAnother Example.\nThe US government spending on science is ruining cinema (p&lt;.001)!?\n\n\nCode\n# Drop Data before 1999\nvigen_csv &lt;- vigen_csv[vigen_csv$year &gt;= 1999,] \n\n# Run OLS Regression\nreg1 &lt;-  lm(cage_films ~ -1 + science_spending, data=vigen_csv)\nsummary(reg1)\n## \n## Call:\n## lm(formula = cage_films ~ -1 + science_spending, data = vigen_csv)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.7670 -0.7165  0.1447  0.7890  1.4531 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(&gt;|t|)    \n## science_spending 9.978e-05  1.350e-05    7.39 2.34e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.033 on 10 degrees of freedom\n##   (1 observation deleted due to missingness)\n## Multiple R-squared:  0.8452, Adjusted R-squared:  0.8297 \n## F-statistic: 54.61 on 1 and 10 DF,  p-value: 2.343e-05\n\n\nIt’s not all bad, because people in Maine stay married longer?\n\n\nCode\nplot.new()\nplot.window(xlim=c(1999, 2009), ylim=c(7,9))\nlines(log(maine_divorce_rate*1000)~year, data=vigen_csv)\nlines(log(science_spending/10)~year, data=vigen_csv, lty=2)\naxis(1)\naxis(2)\nlegend('topright', lty=c(1,2), legend=c(\n    'log(maine_divorce_rate*1000)',\n    'log(science_spending/10)'))\n\n\n\n\n\n\n\n\n\nFor more intuition on spurious correlations, try http://shiny.calpoly.sh/Corr_Reg_Game/ The same principles apply to more sophisticated methods.",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Scientism</span>"
    ]
  },
  {
    "objectID": "02_07_DataScientism.html#spurious-causal-impacts",
    "href": "02_07_DataScientism.html#spurious-causal-impacts",
    "title": "16  Data Scientism",
    "section": "16.3 Spurious Causal Impacts",
    "text": "16.3 Spurious Causal Impacts\nIn practice, it is hard to find “good” natural experiments. For example, suppose we asked “what is the effect of wages on police demanded?” and examined a policy which lowered the educational requirements from 4 years to 2 to become an officer. This increases the labour supply, but it also affects the demand curve through “general equilibrium”: as some of the new officers were potentially criminals. With fewer criminals, the demand for likely police shifts down.\nIn practice, it is also surprisingly easy to find “bad” natural experiments. Paradoxically, natural experiments are something you are supposed to find but never search for. As you search for good instruments, for example, sometimes random noise will appear like a good instrument (Spurious instruments). Worse, if you search for a good instrument for too long, you can also be led astray from important questions.\n\nExample: Vigen IV’s.\nWe now run IV regressions for different variable combinations in the dataset of spurious relationships\n\n\nCode\nknames &lt;- names(vigen_csv)[2:11] # First 10 Variables\n#knames &lt;- names(vigen_csv)[-1] # Try All Variables\np &lt;- 1\nii &lt;- 1\nivreg_list &lt;- vector(\"list\", factorial(length(knames))/factorial(length(knames)-3))\n\n# Choose 3 variable\nfor( k1 in knames){\nfor( k2 in setdiff(knames,k1)){\nfor( k3 in setdiff(knames,c(k1,k2)) ){   \n    X1 &lt;- vigen_csv[,k1]\n    X2 &lt;- vigen_csv[,k2]\n    X3 &lt;- vigen_csv[,k3]\n    # Merge and `Analyze'        \n    dat_i &lt;- na.omit(data.frame(X1,X2,X3))\n    ivreg_i &lt;- feols(X1~1|X2~X3, data=dat_i)\n    ivreg_list[[ii]] &lt;- list(ivreg_i, c(k1,k2,k3))\n    ii &lt;- ii+1\n}}}\npvals &lt;- sapply(ivreg_list, function(ivreg_i){ivreg_i[[1]]$coeftable[2,4]})\n\nplot(ecdf(pvals), xlab='p-value', ylab='CDF', font.main=1,\n    main='Frequency IV is Statistically Significant')\nabline(v=c(.01,.05), col=c(2,4))\n\n\n\n\n\n\n\n\n\nCode\n\n# Most Significant Spurious Combinations\npvars &lt;- sapply(ivreg_list, function(ivreg_i){ivreg_i[[2]]})\npdat &lt;- data.frame(t(pvars), pvals)\npdat &lt;- pdat[order(pdat$pvals),]\nhead(pdat)\n##                     X1                 X2            X3        pvals\n## 4     science_spending   hanging_suicides    bed_deaths 3.049883e-08\n## 76    hanging_suicides   science_spending    bed_deaths 3.049883e-08\n## 3     science_spending   hanging_suicides cheese_percap 3.344890e-08\n## 75    hanging_suicides   science_spending cheese_percap 3.344890e-08\n## 485 maine_divorce_rate   margarine_percap cheese_percap 3.997738e-08\n## 557   margarine_percap maine_divorce_rate cheese_percap 3.997738e-08\n\n\n\n\nSimulation Study.\nWe apply the three major credible methods (IV, RDD, DID) to random walks. Each time, we find a result that fits mold and add various extensions that make it appear robust. One could tell a story about how \\(X_{2}\\) affects \\(X_{1}\\) but \\(X_{1}\\) might also affect \\(X_{2}\\), and how they discovered an instrument \\(X_{3}\\) to provide the first causal estimate of \\(X_{2}\\) on \\(X_{1}\\). The analysis looks scientific and the story sounds plausible, so you could probably be convinced if it were not just random noise.\n\n\nCode\nn &lt;- 1000\nn_index &lt;- seq(n)\n\nset.seed(1)\nrandom_walk1 &lt;- cumsum(runif(n,-1,1))\n\nset.seed(2)\nrandom_walk2 &lt;- cumsum(runif(n,-1,1))\n\npar(mfrow=c(1,2))\nplot(random_walk1, pch=16, col=rgb(1,0,0,.25),\n    xlab='Time', ylab='Random Value')\nplot(random_walk2, pch=16, col=rgb(0,0,1,.25),\n    xlab='Time', ylab='Random Value')\n\n\n\n\n\n\n\n\n\nIV. First, find an instrument that satisfy various statistical criterion to provide a causal estimate of \\(X_{2}\\) on \\(X_{1}\\).\n\n\nCode\n# \"Find\" \"valid\" ingredients\nlibrary(fixest)\nrandom_walk3 &lt;- cumsum(runif(n,-1,1))\ndat_i &lt;- data.frame(\n    X1=random_walk1,\n    X2=random_walk2,\n    X3=random_walk3)\nivreg_i &lt;- feols(X1~1|X2~X3, data=dat_i)\nsummary(ivreg_i)\n## TSLS estimation - Dep. Var.: X1\n##                   Endo.    : X2\n##                   Instr.   : X3\n## Second stage: Dep. Var.: X1\n## Observations: 1,000\n## Standard-errors: IID \n##             Estimate Std. Error t value   Pr(&gt;|t|)    \n## (Intercept)  8.53309   1.644285 5.18954 2.5533e-07 ***\n## fit_X2       1.79901   0.472285 3.80916 1.4796e-04 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## RMSE: 6.25733   Adj. R2: 0.032314\n## F-test (1st stage), X2: stat = 10.804, p = 0.001048, on 1 and 998 DoF.\n##             Wu-Hausman: stat = 23.407, p = 1.518e-6, on 1 and 997 DoF.\n\n# After experimenting with different instruments\n# you can find even stronger results!\n\n\nRDD. Second, find a large discrete change in the data that you can associate with a policy. You can use this as an instrument too, also providing a causal estimate of \\(X_{2}\\) on \\(X_{1}\\).\n\n\nCode\n# Let the data take shape\n# (around the large differences before and after)\nn1 &lt;- 290\nwind1 &lt;- c(n1-300,n1+300)\ndat1 &lt;- data.frame(t=n_index, y=random_walk1, d=1*(n_index &gt; n1))\ndat1_sub &lt;- dat1[ n_index&gt;wind1[1] & n_index &lt; wind1[2],]\n\n# Then find your big break\nreg0 &lt;- lm(y~t, data=dat1_sub[dat1_sub$d==0,])\nreg1 &lt;- lm(y~t, data=dat1_sub[dat1_sub$d==1,])\n\n# The evidence should show openly (it's just science)\nplot(random_walk1, pch=16, col=rgb(0,0,1,.25),\n    xlim=wind1, xlab='Time', ylab='Random Value')\nabline(v=n1, lty=2)\nlines(reg0$model$t, reg0$fitted.values, col=1)\nlines(reg1$model$t, reg1$fitted.values, col=1)\n\n\n\n\n\n\n\n\n\n\nCode\n# Dress with some statistics for added credibility\nrdd_sub &lt;- lm(y~d+t+d*t, data=dat1_sub)\nrdd_full &lt;- lm(y~d+t+d*t, data=dat1)\nstargazer::stargazer(rdd_sub, rdd_full, \n    type='html',\n    title='Recipe RDD',\n    header=F,\n    omit=c('Constant'),\n    notes=c('First column uses a dataset around the discontinuity.',\n    'Smaller windows are more causal, and where the effect is bigger.'))\n\n\n\nRecipe RDD\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ny\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nd\n\n\n-13.169***\n\n\n-9.639***\n\n\n\n\n\n\n(0.569)\n\n\n(0.527)\n\n\n\n\n\n\n\n\n\n\n\n\nt\n\n\n0.011***\n\n\n0.011***\n\n\n\n\n\n\n(0.001)\n\n\n(0.002)\n\n\n\n\n\n\n\n\n\n\n\n\nd:t\n\n\n0.009***\n\n\n0.004*\n\n\n\n\n\n\n(0.002)\n\n\n(0.002)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n589\n\n\n1,000\n\n\n\n\nR2\n\n\n0.771\n\n\n0.447\n\n\n\n\nAdjusted R2\n\n\n0.770\n\n\n0.446\n\n\n\n\nResidual Std. Error\n\n\n1.764 (df = 585)\n\n\n3.081 (df = 996)\n\n\n\n\nF Statistic\n\n\n658.281*** (df = 3; 585)\n\n\n268.763*** (df = 3; 996)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\n\nFirst column uses a dataset around the discontinuity.\n\n\n\n\n\n\nSmaller windows are more causal, and where the effect is bigger.\n\n\n\nDID. Third, find a change in the data that you can associate with a policy where the control group has parallel trends. This also provides a causal estimate of \\(X_{2}\\) on \\(X_{1}\\).\n\n\nCode\n# Find a reversal of fortune\n# (A good story always goes well with a nice pre-trend)\nn2 &lt;- 318\nwind2 &lt;- c(n2-20,n2+20)\nplot(random_walk2, pch=16, col=rgb(0,0,1,.5),\n    xlim=wind2, ylim=c(-15,15), xlab='Time', ylab='Random Value')\npoints(random_walk1, pch=16, col=rgb(1,0,0,.5))\nabline(v=n2, lty=2)\n\n\n\n\n\n\n\n\n\n\nCode\n# Knead out any effects that are non-causal (aka correlation)\ndat2A &lt;- data.frame(t=n_index, y=random_walk1, d=1*(n_index &gt; n2), RWid=1)\ndat2B &lt;- data.frame(t=n_index, y=random_walk2, d=0, RWid=2)\ndat2  &lt;- rbind(dat2A, dat2B)\ndat2$RWid &lt;- as.factor(dat2$RWid)\ndat2$tid &lt;- as.factor(dat2$t)\ndat2_sub &lt;- dat2[ dat2$t&gt;wind2[1] & dat2$t &lt; wind2[2],]\n\n# Report the stars for all to enjoy\n# (what about the intercept?)\n# (stable coefficients are the good ones?)\ndid_fe1 &lt;- lm(y~d+tid, data=dat2_sub)\ndid_fe2 &lt;- lm(y~d+RWid, data=dat2_sub)\ndid_fe3 &lt;- lm(y~d*RWid+tid, data=dat2_sub)\nstargazer::stargazer(did_fe1, did_fe2, did_fe3,\n    type='html',\n    title='Recipe DID',\n    header=F,\n    omit=c('tid','RWid', 'Constant'),\n    notes=c(\n     'Fixed effects for time in column 1, for id in column 2, and both in column 3.',\n     'Fixed effects control for most of your concerns.',\n     'Anything else creates a bias in the opposite direction.'))\n\n\n\nRecipe DID\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ny\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n\n\n\n\n\n\nd\n\n\n1.804*\n\n\n1.847***\n\n\n5.851***\n\n\n\n\n\n\n(0.892)\n\n\n(0.652)\n\n\n(0.828)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n78\n\n\n78\n\n\n78\n\n\n\n\nR2\n\n\n0.227\n\n\n0.164\n\n\n0.668\n\n\n\n\nAdjusted R2\n\n\n-0.566\n\n\n0.142\n\n\n0.309\n\n\n\n\nResidual Std. Error\n\n\n2.750 (df = 38)\n\n\n2.035 (df = 75)\n\n\n1.827 (df = 37)\n\n\n\n\nF Statistic\n\n\n0.287 (df = 39; 38)\n\n\n7.379*** (df = 2; 75)\n\n\n1.860** (df = 40; 37)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\n\nFixed effects for time in column 1, for id in column 2, and both in column 3.\n\n\n\n\n\n\nFixed effects control for most of your concerns.\n\n\n\n\n\n\nAnything else creates a bias in the opposite direction.",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Scientism</span>"
    ]
  },
  {
    "objectID": "02_08_MiscTopics.html",
    "href": "02_08_MiscTopics.html",
    "title": "17  Misc. Topics",
    "section": "",
    "text": "17.1 Nonparametric Tests",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Misc. Topics</span>"
    ]
  },
  {
    "objectID": "02_08_MiscTopics.html#nonparametric-tests",
    "href": "02_08_MiscTopics.html#nonparametric-tests",
    "title": "17  Misc. Topics",
    "section": "",
    "text": "Distributional Comparisons.\nWe can also examine whether there are any differences between the entire distributions\n\n\nCode\n# Sample Wage Data\nlibrary(wooldridge)\nx1 &lt;- sort( wage1[wage1$educ == 15,  'wage'])  \nx2 &lt;- sort( wage1[wage1$educ == 16,  'wage'] )\nx &lt;- sort(c(x1, x2))\n\n# Compute Quantiles\nquants &lt;- seq(0,1,length.out=101)\nQ1 &lt;- quantile(x1, probs=quants)\nQ2 &lt;- quantile(x2, probs=quants)\n\n# Compare Distributions via Quantiles\nrx &lt;- range(c(x1, x2))\npar(mfrow=c(1,2))\nplot(rx, c(0,1), type='n', font.main=1,\n    main='Distributional Comparison',\n    xlab=expression(Q[s]),\n    ylab=expression(F[s]))\nlines(Q1, quants, col=2)\nlines(Q2, quants, col=4)\nlegend('bottomright', col=c(2,4), lty=1,\nlegend=c('F1', 'F2'))\n\n# Compare Quantiles\nplot(Q1, Q2, xlim=rx, ylim=rx,\n    main='Quantile-Quantile Plot', font.main=1,\npch=16, col=grey(0,.25))\nabline(a=0,b=1,lty=2)\n\n\n\n\n\n\n\n\n\nThe starting point for hypothesis testing is the Kolmogorov-Smirnov Statistic: the maximum absolute difference between two CDF’s over all sample data \\(x \\in \\{X_1\\} \\cup \\{X_2\\}\\). \\[\\begin{eqnarray}\nKS &=& \\max_{x} |F_{1}(x)- F_{2}(x)|^{p},\n\\end{eqnarray}\\] where \\(p\\) is an integer (typically 1). An intuitive alternative is the Cramer-von Mises Statistic: the sum of absolute differences (raised to an integer, typically 2) between two CDF’s. \\[\\begin{eqnarray}\nCVM=\\sum_{x} |F_{1}(x)- F_{2}(x)|^{p}.\n\\end{eqnarray}\\]\n\n\nCode\n# Distributions\nF1 &lt;- ecdf(x1)(x)\nF2 &lt;- ecdf(x2)(x)\n\nlibrary(twosamples)\n\n# Kolmogorov-Smirnov\nKSq &lt;- which.max(abs(F2 - F1))\nKSqv &lt;- round(twosamples::ks_stat(x1, x2),2)\n\n# Cramer-von Mises Statistic (p=2)\nCVMqv &lt;- round(twosamples::cvm_stat(x1, x2, power=2), 2) \n\n# Visualize Differences\nplot(range(x), c(0,1), type=\"n\", xlab='x', ylab='ECDF')\nlines(x, F1, col=2, lwd=2)\nlines(x, F2, col=4, lwd=2)\n# CVM\nsegments(x, F1, x, F2, lwd=.5, col=grey(0,.2))\n# KS\nsegments(x[KSq], F1[KSq], x[KSq], F2[KSq], lwd=1.5, col=grey(0,.75), lty=2)\n\n\n\n\n\n\n\n\n\nJust as before, you use bootstrapping for hypothesis testing.\n\n\nCode\ntwosamples::cvm_test(x1, x2)\n## Test Stat   P-Value \n##  2.084253  0.084500\n\n\n\n\nComparing Multiple Groups.\nFor multiple groups, we can tests the equality of all distributions (whether at least one group is different). The Kruskal-Wallis test examines \\(H_0:\\; F_1 = F_2 = \\dots = F_G\\) versus \\(H_A:\\; \\text{at least one } F_g \\text{ differs}\\), where \\(F_g\\) is the continuous distribution of group \\(g=1,...G\\). This test does not tell us which group is different.\nTo conduct the test, first denote individuals \\(i=1,...n\\) with overall ranks \\(r_1,....r_{n}\\). Each individual belongs to group \\(g=1,...G\\), and each group \\(g\\) has \\(n_{g}\\) individuals with average rank \\(\\overline{r}_{g} = \\sum_{i} r_{i} /n_{g}\\). The Kruskal Wallis statistic is \\[\\begin{eqnarray}\nKW &=& (N-1) \\frac{\\sum_{g=1}^{G} n_{g}( \\overline{r}_{g} - \\overline{r}  )^2  }{\\sum_{i=1}^{N} ( r_{i} - \\overline{r}  )^2},\n\\end{eqnarray}\\] where \\(\\overline{r} = \\frac{N+1}{2}\\) is the grand mean rank.\nIn the special case with only two groups, \\(G=2\\), the Kruskal Wallis test reduces to the Mann–Whitney U-test (also known as the ). In this case, we can write the hypotheses in terms of individual outcomes in each group, \\(Y_i\\) in one group \\(Y_j\\) in the other; \\(H_0: P(Y_i &gt; Y_j)=P(Y_i &gt; Y_i)\\) versus \\(H_A: P(Y_i &gt; Y_j) \\neq P(Y_i &gt; Y_j)\\). The corresponding test statistic is \\[\\begin{eqnarray}\nU   &=& \\min(U_1,U_2) \\\\\nU_g &=& \\sum_{i\\in g}\\sum_{j\\in -g}\n           \\Bigl[\\mathbf 1(Y_i &gt; Y_j) + \\tfrac12\\mathbf 1(Y_i = Y_j)\\Bigr].\n\\end{eqnarray}\\]\n\n\nCode\nlibrary(AER)\ndata(CASchools)\nCASchools$stratio &lt;- CASchools$students/CASchools$teachers\n\n# Do student/teacher ratio differ for at least 1 county?\n# Single test of multiple distributions\nkruskal.test(CASchools$stratio, CASchools$county)\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  CASchools$stratio and CASchools$county\n## Kruskal-Wallis chi-squared = 161.18, df = 44, p-value = 2.831e-15\n\n# Multiple pairwise tests\n# pairwise.wilcox.test(CASchools$stratio, CASchools$county)",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Misc. Topics</span>"
    ]
  },
  {
    "objectID": "02_08_MiscTopics.html#predictions",
    "href": "02_08_MiscTopics.html#predictions",
    "title": "17  Misc. Topics",
    "section": "17.2 Predictions",
    "text": "17.2 Predictions\n\nDescribe vs. Explain vs. Predict.\n\n\nPrediction Intervals.\nIn addition to confidence intervals, we can also compute a prediction interval which estimate the variability of new data rather than a statistic\nIn this example, we consider a single variable and compute the frequency each value was covered.\n\n\nCode\nx &lt;- runif(1000)\n# Middle 90% of values\nxq0 &lt;- quantile(x, probs=c(.05,.95))\n\nbks &lt;- seq(0,1,by=.01)\nhist(x, breaks=bks, border=NA,\n    main='Prediction Interval', font.main=1)\nabline(v=xq0)\n\n\n\n\n\n\n\n\n\nCode\n\npaste0('we are 90% confident that the a future data point will be between ', \n    round(xq0[1],2), ' and ', round(xq0[2],2) )\n## [1] \"we are 90% confident that the a future data point will be between 0.06 and 0.96\"\n\n\nIn this example, we consider a range for \\(y_{i}(x)\\) rather than for \\(m(x)\\). These intervals also take into account the residuals — the variability of individuals around the mean.\n\n\nCode\n# Bivariate Data from USArrests\nxy &lt;- USArrests[,c('Murder','UrbanPop')]\ncolnames(xy) &lt;- c('y','x')\nxy0 &lt;- xy[order(xy$x),]\n\n\nFor a nice overview of different types of intervals, see https://www.jstor.org/stable/2685212. For an in-depth view, see “Statistical Intervals: A Guide for Practitioners and Researchers” or “Statistical Tolerance Regions: Theory, Applications, and Computation”. See https://robjhyndman.com/hyndsight/intervals/ for constructing intervals for future observations in a time-series context. See Davison and Hinkley, chapters 5 and 6 (also Efron and Tibshirani, or Wehrens et al.)\n\n\nCode\n# From \"Basic Regression\"\nxy0 &lt;- xy[order(xy$x),]\nX0 &lt;- unique(xy0$x)\nreg_lo &lt;- loess(y~x, data=xy0, span=.8)\npreds_lo &lt;- predict(reg_lo, newdata=data.frame(x=X0))\n\n\n# Jackknife CI\njack_lo &lt;- sapply(1:nrow(xy), function(i){\n    xy_i &lt;- xy[-i,]\n    reg_i &lt;- loess(y~x, dat=xy_i, span=.8)\n    predict(reg_i, newdata=data.frame(x=X0))\n})\n\nboot_regs &lt;- lapply(1:399, function(b){\n    b_id &lt;- sample( nrow(xy), replace=T)\n    xy_b &lt;- xy[b_id,]\n    reg_b &lt;- lm(y~x, dat=xy_b)\n})\n\nplot(y~x, pch=16, col=grey(0,.5),\n    dat=xy0, ylim=c(0, 20))\nlines(X0, preds_lo,\n    col=hcl.colors(3,alpha=.75)[2],\n    type='o', pch=2)\n\n# Estimate Residuals CI at design points\nres_lo &lt;- sapply(1:nrow(xy), function(i){\n    y_i &lt;- xy[i,'y']\n    preds_i &lt;- jack_lo[,i]\n    resids_i &lt;- y_i - preds_i\n})\nres_cb &lt;- apply(res_lo, 1, quantile,\n    probs=c(.025,.975), na.rm=T)\n\n# Plot\nlines( X0, preds_lo +res_cb[1,],\n    col=hcl.colors(3,alpha=.75)[2], lt=2)\nlines( X0, preds_lo +res_cb[2,],\n    col=hcl.colors(3,alpha=.75)[2], lty=2)\n\n\n\n# Smooth estimates \nres_lo &lt;- lapply(1:nrow(xy), function(i){\n    y_i &lt;- xy[i,'y']\n    x_i &lt;- xy[i,'x']\n    preds_i &lt;- jack_lo[,i]\n    resids_i &lt;- y_i - preds_i\n    cbind(e=resids_i, x=x_i)\n})\nres_lo &lt;- as.data.frame(do.call(rbind, res_lo))\n\nres_fun &lt;- function(x0, h, res_lo){\n    # Assign equal weight to observations within h distance to x0\n    # 0 weight for all other observations\n    ki &lt;- dunif(res_lo$x, x0-h, x0+h) \n    ei &lt;- res_lo[ki!=0,'e']\n    res_i &lt;- quantile(ei, probs=c(.025,.975), na.rm=T)\n}\nres_lo2 &lt;- sapply(X0, res_fun, h=15, res_lo=res_lo)\n\nlines( X0, preds_lo + res_lo2[1,],\n    col=hcl.colors(3,alpha=.75)[2], lty=1, lwd=2)\nlines( X0, preds_lo + res_lo2[2,],\n    col=hcl.colors(3,alpha=.75)[2], lty=1, lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Bootstrap Prediction Interval\nboot_resids &lt;- lapply(boot_regs, function(reg_b){\n    e_b &lt;- resid(reg_b)\n    x_b &lt;- reg_b$model$x\n    res_b &lt;- cbind(e_b, x_b)\n})\nboot_resids &lt;- as.data.frame(do.call(rbind, boot_resids))\n# Homoskedastic\nehat &lt;- quantile(boot_resids$e_b, probs=c(.025, .975))\nx &lt;- quantile(xy$x,probs=seq(0,1,by=.1))\nboot_pi &lt;- coef(reg)[1] + x*coef(reg)['x']\nboot_pi &lt;- cbind(boot_pi + ehat[1], boot_pi + ehat[2])\n\n# Plot Bootstrap PI\nplot(y~x, dat=xy, pch=16, main='Prediction Intervals',\n    ylim=c(-5,20), font.main=1)\npolygon( c(x, rev(x)), c(boot_pi[,1], rev(boot_pi[,2])),\n    col=grey(0,.2), border=NA)\n\n# Parametric PI (For Comparison)\n#pi &lt;- predict(reg, interval='prediction', newdata=data.frame(x))\n#lines( x, pi[,'lwr'], lty=2)\n#lines( x, pi[,'upr'], lty=2)\n\n\n\n\nCrossvalidation.\nPerhaps the most common approach to selecting a bandwidth is to minimize error. Leave-one-out Cross-validation minimizes the average “leave-one-out” mean square prediction errors: \\[\\begin{eqnarray}\nCV &=& \\min_{\\mathbf{H}} \\quad \\frac{1}{n} \\sum_{i=1}^{n} \\left[ Y_{i} - \\widehat{Y_{[i]}}(\\mathbf{X},\\mathbf{H}) \\right]^2,\n% \\widehat{Y_{[i]}}(\\mathbf{X},\\mathbf{H}) &=& \\sum_{j\\neq i} k(\\mathbf{X}_{j},\\mathbf{X}_{i},\\mathbf{H}) \\left[ \\widehat{\\alpha}(\\mathbf{X}_{j}) +  \\widehat{\\beta}(\\mathbf{X}_{j}) \\mathbf{X}_{i} \\right]\n\\end{eqnarray}\\] where \\(\\widehat{Y_{[i]}}(\\mathbf{X},\\mathbf{H})\\) is the predicted value at \\(\\mathbf{X}_{i}\\) based on a dataset that excludes \\(\\mathbf{X}_{i}\\).\nThere are many types of cross-validation . For example, one extension is , which splits \\(N\\) datapoints into \\(k=1...K\\) groups, each sized \\(B\\), and predicts values for the left-out group. adjusts for the degrees of freedom, whereas the function in R uses by default. You can refer to extensions on a case by case basis.\nMinimizing out-sample prediction error is perhaps the simplest computational approach to choose bandwidths, and it also addresses an issue that plagues observational studies in the social sciences of explanations without predictions. It is a problem if your model explains everything and predicts nothing, but minimizing prediction error is not necessarily “best”.\n\n\nCode\n##################                                         \n# Crossvalidated bandwidth for regression\n##################\ny &lt;- (CASchools$read + CASchools$math) / 2\nxy_mat &lt;- data.frame(y=y, x1=CASchools$income)\nlibrary(np)\n\n## Grid Search\nBWS &lt;- seq(1,10,length.out=20)\nBWS_CV &lt;- sapply(BWS, function(bw){\n    E_bw &lt;- sapply(1:nrow(xy_mat), function(i){\n        llls &lt;- npreg(y~x1, data=xy_mat[-i,], \n            bws=bw, regtype=\"ll\",\n            ckertype='epanechnikov', bandwidth.compute=F)\n        pred_i &lt;- predict(llls, newdata=xy_mat[i,])\n        e &lt;-  (pred_i- xy_mat[i,'y'])\n        return(e)\n    })\n    return( mean(E_bw^2) )\n})\n\n## Plot MSE\npar(mfrow=c(1,2))\nplot(BWS, BWS_CV, ylab='CV', pch=16, \n    xlab='bandwidth (h)',)\n## Plot Resulting Predictions\nbw &lt;- BWS[which.min(BWS_CV)]\nllls &lt;- npreg(y~x1, data=xy_mat, \n    ckertype='epanechnikov',\n    bws=bw, regtype=\"ll\")\nplot(xy_mat$x, predict(llls), pch=16, col=grey(0,.5),\n    xlab='X', ylab='Predictions')\nabline(a=0,b=1, lty=2)\n\n## Built in algorithmic Optimziation\nllls2 &lt;- npreg(y~x1, data=xy_mat, ckertype='epanechnikov', regtype=\"ll\")\npoints(xy_mat$x, predict(llls2), pch=2, col=rgb(1,0,0,.25))\n\n## Add legend\nadd_legend &lt;- function(...) {\n  opar &lt;- par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), \n              mar=c(0, 0, 0, 0), new=TRUE)\n  on.exit(par(opar))\n  plot(0, 0, type='n', bty='n', xaxt='n', yaxt='n')\n  legend(...)\n}\nadd_legend('topright',\n    col=c(grey(0,.5),rgb(1,0,0,.25)), \n    pch=c(16,2),\n    bty='n', horiz=T,\n    legend=c('Grid Search', 'NP-algorithm'))\n\n\n\n\nCode\n##################\n# CV Application\n##################\n\n## Smoothly Estimate X & Y Density\ny &lt;- sort(xy_mat$y)\nfy &lt;- npudens(y, bandwidth.compute=TRUE)\nx1 &lt;- sort(xy_mat$x1)\nfx &lt;- npudens(x1, bandwidth.compute=TRUE)\n## Smoothly Estimate How Y changes with X\nllls2 &lt;- npreg(y~x1,data=xy_mat,\n    ckertype='epanechnikov',\n    regtype=\"ll\", bandwidth.compute=TRUE)\n\n\nlayout( matrix(c(2,0,1,3), ncol=2, byrow=TRUE),\n    widths=c(4/5,1/5), heights=c(1/5,4/5))\n## Joint Distribution\npar(mar=c(4,4,1,1))\nplot(y~x1, data=xy_mat,\n    pch=16, col=grey(0,.25),\n    xlab=\"District Income (1000$)\", \n    ylab=\"Test Score\")\nlines( sort(xy_mat$x), predict(llls2)[order(xy_mat$x1)],\n    pch=16, col=1)\n## Marginal Distribution\npar(mar=c(0,4,1,1))\nplot(x1, predict(fx),\n    col=grey(0,1), type='l', axes=F,\n    xlab='', ylab='')\nrug(x1, col=grey(0,.25))\npar(mar=c(4,0,1,1))\nplot(predict(fy), y,\n    col=grey(0,1), type='l', axes=F,\n    xlab='', ylab='')\nrug(y, col=grey(0,.25), side=2)\n\n\n\n\nFiltering.\nIn some cases, we may want to smooth time series data instead of predict into the future. We can distinguish two types of smoothing, incorporating future observations or not. When only weighting two other observations, the differences can be expressed as trying to estimate the average with different available data:\n\nFiltering: \\(\\mathbb{E}[Y_{t} | X_{t-1}, X_{t-2}]\\)\nSmoothing: \\(\\mathbb{E}[Y_{t} | X_{t-1}, X_{t+1}]\\)\n\nOne example of filtering is Exponential Filtering (sometimes confusingly referred to as “Exponential Smoothing”) which weights only previous observations using an exponential kernel.\n\n\nCode\n##################\n# Time series data\n##################\n\nset.seed(1)\n## Underlying Trend\nx0 &lt;- cumsum(rnorm(500,0,1))\n## Observed Datapoints\nx &lt;- x0 + runif(length(x0),-10,10)\ndat &lt;- data.frame(t=seq(x), x0=x0, x=x)\n\n## Asymmetric Kernel\n#bw &lt;- c(2/3,1/3)\n#s1 &lt;- filter(x, bw/sum(bw), sides=1)\n\n## Symmetric Kernel\n#bw &lt;- c(1/6,2/3,1/6)\n#s2 &lt;- filter(x,  bw/sum(bw), sides=2)\n\n\nThere are several cross-validation procedures for filtering time series data . One is called time series cross-validation (TSCV), which is useful for temporally dependent data .\n\n\nCode\n## Plot Simulated Data\nx &lt;- dat$x\nx0 &lt;- dat$x0\npar(fig = c(0,1,0,1), new=F)\nplot(x, pch=16, col=grey(0,.25))\nlines(x0, col=1, lty=1, lwd=2)\n\n## Work with differenced data?\n#n       &lt;- length(Yt)\n#plot(Yt[1:(n-1)], Yt[2:n],\n#    xlab='d Y (t-1)', ylab='d Y (t)', \n#    col=grey(0,.5), pch=16)\n#Yt &lt;- diff(Yt)\n\n## TSCV One Sided Moving Average\nfilter_bws &lt;- seq(1,20,by=1)\nfilter_mape_bws &lt;- sapply(filter_bws, function(h){\n    bw &lt;- c(0,rep(1/h,h)) ## Leave current one out\n    s2 &lt;- filter(x, bw, sides=1)\n    pe &lt;- s2 - x\n    mape &lt;- mean( abs(pe)^2, na.rm=T)\n})\nfilter_mape_star &lt;- filter_mape_bws[which.min(filter_mape_bws)]\nfilter_h_star &lt;- filter_bws[which.min(filter_mape_bws)]\nfilter_tscv &lt;- filter(x,  c(rep(1/filter_h_star,filter_h_star)), sides=1)\n# Plot Optimization Results\n#par(fig = c(0.07, 0.35, 0.07, 0.35), new=T) \n#plot(filter_bws, filter_mape_bws, type='o', ylab='mape', pch=16)\n#points(filter_h_star, filter_mape_star, pch=19, col=2, cex=1.5)\n\n## TSCV for LLLS\nlibrary(np)\nllls_bws &lt;- seq(8,28,by=1)\nllls_burnin &lt;- 10\nllls_mape_bws &lt;- sapply(llls_bws, function(h){ # cat(h,'\\n')\n    pe &lt;- sapply(llls_burnin:nrow(dat), function(t_end){\n        dat_t &lt;- dat[dat$t&lt;t_end, ]\n        reg &lt;- npreg(x~t, data=dat_t, bws=h,\n            ckertype='epanechnikov',\n            bandwidth.compute=F, regtype='ll')\n        edat &lt;- dat[dat$t==t_end,]\n        pred &lt;- predict(reg, newdata=edat)\n        pe &lt;- edat$x - pred\n        return(pe)    \n    })\n    mape &lt;- mean( abs(pe)^2, na.rm=T)\n})\nllls_mape_star &lt;- llls_mape_bws[which.min(llls_mape_bws)]\nllls_h_star &lt;- llls_bws[which.min(llls_mape_bws)]\n#llls_tscv &lt;- predict( npreg(x~t, data=dat, bws=llls_h_star,\n#    bandwidth.compute=F, regtype='ll', ckertype='epanechnikov'))\nllls_tscv &lt;- sapply(llls_burnin:nrow(dat), function(t_end, h=llls_h_star){\n    dat_t &lt;- dat[dat$t&lt;t_end, ]\n    reg &lt;- npreg(x~t, data=dat_t, bws=h,\n        ckertype='epanechnikov',\n        bandwidth.compute=F, regtype='ll')\n    edat &lt;- dat[dat$t==t_end,]\n    pred &lt;- predict(reg, newdata=edat)\n    return(pred)    \n})\n\n## Compare Fits Qualitatively\nlines(filter_tscv, col=2, lty=1, lwd=1)\nlines(llls_burnin:nrow(dat), llls_tscv, col=4, lty=1, lwd=1)\nlegend('topleft', lty=1, col=c(1,2,4), bty='n',\n    c('Underlying Trend', 'MA-1sided + TSCV', 'LLLS-1sided + TSCV'))\n\n\n\n\n\n\n\n\n\nCode\n\n## Compare Fits Quantitatively\ncbind(\n    bandwidth=c(LLLS=llls_h_star, MA=filter_h_star),\n    mape=round(c(LLLS=llls_mape_star, MA=filter_mape_star),2) )\n##      bandwidth  mape\n## LLLS        18 43.39\n## MA           9 42.28\n\n## See also https://cran.r-project.org/web/packages/smoots/smoots.pdf\n#https://otexts.com/fpp3/tscv.html\n#https://robjhyndman.com/hyndsight/tscvexample/",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Misc. Topics</span>"
    ]
  },
  {
    "objectID": "02_08_MiscTopics.html#decision-theory",
    "href": "02_08_MiscTopics.html#decision-theory",
    "title": "17  Misc. Topics",
    "section": "17.3 Decision Theory",
    "text": "17.3 Decision Theory\n\nStatistical Power\n\n\nQuality Control\n\n\nOptimal Experiment Designs",
    "crumbs": [
      "Introduction to Linear Regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Misc. Topics</span>"
    ]
  }
]