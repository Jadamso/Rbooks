[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introductory Economic Statistics: A Data-Driven Approach using R",
    "section": "",
    "text": "Preface\nThis Rbook introduces business and social science students to statistics with minimal parametric assumptions and formulas. Students are introduced to the basics of statistical programming using R alongside the theoretical analysis of data. In many ways, it is a modern version of “Introductory Econometrics: Using Monte Carlo Simulation with Microsoft Excel” by Barreto and Howland, updated to adhere to modern statistics teaching guidelines and give econometrics students the best tools for their labor market. Altogether, students learn to produce statistical analyses of economic data relevant to both the private and public sector, as well as an intuitive foundation for more advanced courses including nonparametric statistics, program evaluation, forecasting, structural econometrics, and more. This Rbook is organized into three substantive parts: univariate, bivariate, and multivariate data analysis.\nPart I has three notable differences from a typical intro to statistics book. First: students more deeply learn to use and interpret the Histogram, ECDF, and Boxplot (as well as avoid 3D pie charts and chart junk used in business statistics books). They work with actual data before abstract probability theory (initially limited to simple events or intervals, with sums of random variable and transformations available optionally later). Second: students learn the basics of probability theory with real world data and computer simulations. I aimed to replace mathematical proofs with simulations whenever possible. This allows less emphasis on classical probability theory mechanics as well as fewer “t and z drills”. Confidence intervals and hypothesis tests are covered via boostrapping, for example, so students learn the conceptual approach rather than a formula. Three: students learn the theory and practice of univariate statistics before moving to bivariate statistics, rather than mixing uni-and-bivariate content. (Business textbooks often introduce both types of data, cover univariate statistics, and return to bivariate statistics much later, for example, while Mathematics textbooks often introduce students to probability theory long before concrete applications.) These three differences allow for a general conceptual understanding of statistics “out of the gate” and practical skillset that enables students to do more with actual datasets.\nPart II covers bivariate data analysis as its own topic, in depth. After covering the basics, such as correlation and simple regression, this part notably introduces “local relationships” like loess. As such, students learn to incorporate marginal thinking into their empirical analysis early on. Students also learn to quantitatively evaluate their models early on (instead of simply assuming \\(Y=X\\beta+\\epsilon\\), as commonly done in most econometrics textbooks). This rectifies two major shortcomings in econometrics education: a disconnect between theory classes that emphasize marginal effects and empirical classes that emphasize average effects, and model selection is mostly an afterthought. Part II also covers statistical reporting using R + markdown, which research suggests is a good combination for students 1 2. As such, this part includes many practical examples on how to analyze data interactively and communicate results. The boostrapping approach to hypothesis testing covered in part I extends to bivariate data, naturally covering not just canonical statistics like correlation but also other important statistics like differences in medians or distributions. Part II lays the conceptual foundation for understand statistical relationships and provides a powerful skillset for applied data analysis.\nPart III expands the same approach above to multivariate data analysis. Notably, it covers linear models from a “minimum distance” perspective and introduces “local relationships” in multivariate context. This refines material from several introductory econometrics textbooks and actually teaches the maxim “all models are wrong” instead of how to prove unbiasedness. The chapter on observational data introduces students to various interdependence issues (temporal, spatial, economic), followed by a chapter introducing experimental designs and methods relevant to business and economics students. Also included is a novel chapter on “Data scientism” that more clearly illustrates the ways that simplistic approaches can mislead rather than illuminate. (I stress “gun safety” instead of “pull to shoot”, which is missing from many introductory textbooks.) Students gain practical skills in multiple linear regression and a humble appreciation about what we can infer from them. The concepts covered provide strong preperation for more advanced models and methods, as well as a concrete foundation for more advanced theoretical inquiry.\n\nAlthough any interested reader may find it useful, this Rbook is primarily developed for my students.\nIf you use this Rbook, please cite\n@book{Adamson2025_Rbook,\n  title={Introductory Economic Statistics: A Data-Driven Approach using R},\n  author={Adamson, Jordan},\n  year={2025},\n  publisher={Bookdown},\n  url={https://jadamso.github.io/Rbooks/}\n}\nPlease also report any errors or issues at https://github.com/Jadamso/Rbooks/issues.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-00-GettingStarted.html",
    "href": "00-00-GettingStarted.html",
    "title": "Getting Started",
    "section": "",
    "text": "We will use R statistical software, which you will be introduced to in the first chapters.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "01_01_FirstSteps.html",
    "href": "01_01_FirstSteps.html",
    "title": "1  First Steps",
    "section": "",
    "text": "1.1 Why Program in R?\nYou should program your statistical analysis, and we will cover some of the basics of how to do this in R. You also want your work to be replicable\nYou can read more about the distinction in many places, including\nWe focus on R because it is good for complex stats, concise figures, and coherent organization. It is built and developed by applied statisticians for statistics, and used by many in academia and industry. For students, think about labor demand and what may be good for getting a job. Do some of your own research to best understand how much to invest.\nMy main sell to you is that being reproducible is in your own self-interest.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First Steps</span>"
    ]
  },
  {
    "objectID": "01_01_FirstSteps.html#why-program-in-r",
    "href": "01_01_FirstSteps.html#why-program-in-r",
    "title": "1  First Steps",
    "section": "",
    "text": "Replicable: someone collecting new data comes to the same results.\nReproducibile: someone reusing your data comes to the same results.\n\n\n\nhttps://www.annualreviews.org/doi/10.1146/annurev-psych-020821-114157\nhttps://nceas.github.io/sasap-training/materials/reproducible_research_in_r_fairbanks/\n\n\n\n\nAn example workflow.\nFirst Steps…\nStep 1: Some ideas and data about how variable \\(X_{1}\\) affects variable \\(Y_{1}\\), which we denote as \\(X_{1}\\to Y_{1}\\)\n\nYou copy some data into a spreadsheet, manually aggregate\ndo some calculations and tables the same spreadsheet\nsome other analysis from here and there, using this software and that.\n\nStep 2: Pursuing the lead for a week or two\n\nyou extend your dataset with more observations\ncopy in a spreadsheet data, manually aggregate\ndo some more calculations and tables, same as before\n\nA Little Way Down the Road …\n1 month later: someone asks about another factor: \\(X_{2} \\to Y\\).\n\nyou download some other type of data\nYou repeat Step 2 with some data on \\(X_{2}\\).\nThe details from your “point and click” method are a bit fuzzy.\nIt takes a little time, but you successfully redo the analysis.\n\n4 months later: someone asks about yet another factor: \\(X_{3}\\to Y_{1}\\).\n\nYou again repeat Step 2 with some data on \\(X_{3}\\).\nYou’re pretty sure none of tables your tried messed up the order of the rows or columns.\nIt takes more time and effort. The data processing was not transparent, but you eventually redo the analysis.\n\n6 months later: you want to explore another outcome: \\(X_{2} \\to Y_{2}\\).\n\nYou found out Excel had some bugs in it’s statistical calculations (see e.g., https://biostat.app.vumc.org/wiki/pub/Main/TheresaScott/StatsInExcel.TAScot.handout.pdf). You now use a new version of the spreadsheet\nYou’re not sure you merged everything correctly. After much time and effort, most (but not all) of the numbers match exactly.\n\n2 years later: your boss wants you to replicate your work: \\(X_{1}, X_{2}, X_{3} \\to Y_{1}\\).\n\nA rival has proposed something new. Their idea doesn’t actually make any sense, but their figures and statistics look better.\nYou don’t even use that computer anymore and a collaborator who handled the data on \\(X_{2}\\) has moved on.\n\n\n\nAn alternative workflow.\nSuppose you decided to code what you did beginning with Step 2.\nIt does not take much time to update or replicate your results.\n\nYour computer runs for 2 hours and reproduces the figures and tables.\nYou also rewrote your big calculations to use multiple cores, this took two hours to do but saved 6 hours each time you rerun your code.\nYou add some more data. It adds almost no time to see whether much has changed.\n\nYour results are transparent and easier to build on.\n\nYou see the exact steps you took and found an error\n\nGoogle “worst Excel errors” and note the frequency they arise from copy/paste via the “point-and-click” approach. E.g., Fidelity’s $2.6 Billion Dividend Error.\n\nGlad you found a problem before sending your research out! See https://retractionwatch.com/ and https://econjwatch.org/. Future economists should also read https://core.ac.uk/download/pdf/300464894.pdf.\n\n\nYou try out a new plot you found in The Visual Display of Quantitative Information, by Edward Tufte.\n\nIt’s not a standard plot, but google answers most of your questions.\nTutorials help avoid bad practices, such as plotting 2D data as a 3D object (see e.g., https://clauswilke.com/dataviz/no-3d.html).\n\nYou try out an obscure statistical approach that’s hot in your field.\n\nit doesn’t make the report, but you have some confidence that candidate issue isn’t a big problem",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First Steps</span>"
    ]
  },
  {
    "objectID": "01_01_FirstSteps.html#first-steps",
    "href": "01_01_FirstSteps.html#first-steps",
    "title": "1  First Steps",
    "section": "1.2 First Steps",
    "text": "1.2 First Steps\n\nInstall R.\nFirst Install R. Then Install Rstudio.\nFor help setting up, see any of the following links\n\nhttps://learnr-examples.shinyapps.io/ex-setup-r/\nhttps://rstudio-education.github.io/hopr/starting.html\nhttps://a-little-book-of-r-for-bioinformatics.readthedocs.io/en/latest/src/installr.html\nhttps://cran.r-project.org/doc/manuals/R-admin.html\nhttps://courses.edx.org/courses/UTAustinX/UT.7.01x/3T2014/56c5437b88fa43cf828bff5371c6a924/\nhttps://owi.usgs.gov/R/training-curriculum/installr/\nhttps://www.earthdatascience.org/courses/earth-analytics/document-your-science/setup-r-rstudio/\n\nFor Fedora users, note that you need to first enable the repo and then install\n\n\nCode\nsudo dnf install 'dnf-command(copr)'\nsudo dnf copr enable iucar/rstudio\nsudo dnf install rstudio-desktop\n\n\nMake sure you have the latest version of R and Rstudio for class. If not, then reinstall.\n\n\nInterfacing with R Studio.\nRstudio is perhaps the easiest to get going with. (There are other GUI’s.)\nIn Rstudio, there are 4 panes. (If you do not see 4, click “file &gt; new file &gt; R script” on the top left of the toolbar.)\n\n\n\n\n\n\n\n\n\nThe top left pane is where you write your code. For example, type\n\n1+1\n\nThe pane below is where your code is executed. Keep you mouse on the same line as your code, and then click “Run”. You should see\n&gt; 1+1\n[1] 2\nIf you click “Run” again, you should see that same output printed again.\nYou should add comments to your codes, and you do this with hashtags. For example\n# This is my first comment!\n1+1 # The simplest calculation I could think of\nYou can execute each line one-at-a-time. Or you can highlight them both, to take advantage of how R executes commands line-by-line.\n\n\nReading This Textbook.\nAs we proceed, you can see both my source code and output like this:\n\n\nCode\n1+1\n## [1] 2\n\n\nThere are also special boxes\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis box contains need to know examples. Such as\n\n\nCode\n1+1\n## [1] 2\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThis box contains test yourself examples and questions. Such as\n\n\nCode\n2+7\n## [1] 9\n2/7\n## [1] 0.2857143\n\n\n\n\n\n\n\nAssignment.\nYou can create “variables” that store values. For example,\n\n\nCode\nx &lt;- 1 # Make your first variable\nx + 1 # The simplest calculation I could think of\n## [1] 2\n\n\n\n\nCode\nx &lt;- 23 #Another example\nx + 1\n## [1] 24\n\n\n\n\nCode\ny &lt;- x + 1 #Another example\ny\n## [1] 24\n\n\nYour variables must be defined in order to use them. Otherwise you get an error. For example,\n\n\nCode\nX +   1 # notice that R is sensitive to capitalization \n## Error: object 'X' not found\n\n\nYour variable names do not matter technically, but they should be informative\n\n\nCode\none &lt;- 1 # good variable name\none\n## [1] 1\n\none &lt;- 43 # bad variable name\none\n## [1] 43\n\n\nGood names avoid confusion later\n\n\nCode\nx &lt;- 43\nx_plus_two &lt;- x + 2 # better\nx_plus_two\n## [1] 45\n\n\n\n\nScripting.\n\nCreate a folder on your computer to save your scripts\nSave your R Script file as My_First_Script.R in your folder\nClose Rstudio\nOpen your script and re-run it\n\nAs you work through the material, make sure to both execute and save your scripts. Add lots of commentary to your scripts. Name your scripts systematically.\nThere are often many ways to accomplish the same goal. You first scripts will be very basic and rough, but you can edit them later based on what you learn. And you can always ask R for help\n\n\nCode\nsum(x, 2) # x + 2\n?sum\n\n\nWe write script in the top left so that we can edit common mistakes.\n\n\nCode\n# Mistake 1: using undefined objects\nY\n\n#  Mistake 2: spelling and spacing\nY &lt; - 43\nY_plus_z &lt;- Y + z\n\n# Mistake 3: half-completed code\nx + y + \nx_plus_y_plus_z &lt;- x + y + z\n# Seeing \"+\" in the bottom console?\n# press \"Escape\" and try again",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First Steps</span>"
    ]
  },
  {
    "objectID": "01_01_FirstSteps.html#further-reading",
    "href": "01_01_FirstSteps.html#further-reading",
    "title": "1  First Steps",
    "section": "1.3 Further Reading",
    "text": "1.3 Further Reading\nThere are many good and free programming materials online.\nThe most common tasks can be found https://github.com/rstudio/cheatsheets/blob/main/rstudio-ide.pdf\nSome of my programming examples originally come from https://r4ds.had.co.nz/ and I recommend https://intro2r.com.\nI have also used online material from many places over the years, as there are many good yet free-online tutorials and courses specifically on R programming. See e.g.,\n\nhttps://cran.r-project.org/doc/manuals/R-intro.html\nR Graphics Cookbook, 2nd edition. Winston Chang. 2021. https://r-graphics.org/\nR for Data Science. H. Wickham and G. Grolemund. 2017. https://r4ds.had.co.nz/index.html\nAn Introduction to R. W. N. Venables, D. M. Smith, R Core Team. 2017. https://colinfay.me/intro-to-r/\nIntroduction to R for Econometrics. Kieran Marray. https://bookdown.org/kieranmarray/intro_to_r_for_econometrics/\nWollschläger, D. (2020). Grundlagen der Datenanalyse mit R: eine anwendungsorientierte Einführung. http://www.dwoll.de/rexrepos/\nSpatial Data Science with R: Introduction to R. Robert J. Hijmans. 2021. https://rspatial.org/intr/index.html\nhttps://www.econometrics-with-r.org/1.2-a-very-short-introduction-to-r-and-rstudio.html\nhttps://rafalab.github.io/dsbook/\nhttps://moderndive.com/foreword.html\nhttps://rstudio.cloud/learn/primers/1.2\nhttps://cran.r-project.org/manuals.html\nhttps://stats.idre.ucla.edu/stat/data/intro_r/intro_r_interactive_flat.html\nhttps://cswr.nrhstat.org/app-r\n\nFor more on why to program in R, see\n\nhttp://www.r-bloggers.com/the-reproducibility-crisis-in-science-and-prospects-for-r/\nhttp://fmwww.bc.edu/GStat/docs/pointclick.html\nhttps://github.com/qinwf/awesome-R\\#reproducible-research\nA Guide to Reproducible Code in Ecology and Evolution\nhttps://biostat.app.vumc.org/wiki/pub/Main/TheresaScott/ReproducibleResearch.TAScott.handout.pdf",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First Steps</span>"
    ]
  },
  {
    "objectID": "01_02_Mathematics.html",
    "href": "01_02_Mathematics.html",
    "title": "2  Mathematics",
    "section": "",
    "text": "2.1 Objects\nIn R: scalars, vectors, and matrices are different kinds of “objects”.\nThese objects are used extensively in data analysis\nVectors are probably your most common object in R, but we will start with scalars.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mathematics</span>"
    ]
  },
  {
    "objectID": "01_02_Mathematics.html#objects",
    "href": "01_02_Mathematics.html#objects",
    "title": "2  Mathematics",
    "section": "",
    "text": "scalars: summary statistics (average household income).\nvectors: single variables in data sets (the household income of each family in Vancouver).\nmatrices: two variables in data sets (the age and education level of every person in class).\n\n\n\nScalars.\nMake your first scalar\n\n\nCode\nxs &lt;- 2 # Make your first scalar\nxs  # Print the scalar\n## [1] 2\n\n\nPerform simple calculations and see how R is doing the math for you\n\n\nCode\nxs + 2\n## [1] 4\nxs*2 # Perform and print a simple calculation\n## [1] 4\n(xs+1)^2 # Perform and print a simple calculation\n## [1] 9\nxs + NA # often used for missing values\n## [1] NA\n\n\nNow change xs, predict what will happen, then re-run the code.\n\n\nVectors.\nMake your first vector\n\n\nCode\nx &lt;- c(0,1,3,10,6) # Your First Vector\nx # Print the vector\n## [1]  0  1  3 10  6\nx[2] # Print the 2nd Element; 1\n## [1] 1\nx+2 # Print simple calculation; 2,3,5,8,12\n## [1]  2  3  5 12  8\nx*2\n## [1]  0  2  6 20 12\nx^2\n## [1]   0   1   9 100  36\n\n\nApply mathematical calculations elementwise\n\n\nCode\nx+x\n## [1]  0  2  6 20 12\nx*x\n## [1]   0   1   9 100  36\nx^x\n## [1] 1.0000e+00 1.0000e+00 2.7000e+01 1.0000e+10 4.6656e+04\n\n\nIn R, scalars are treated as a vector with one element.\n\n\nCode\nc(1)\n## [1] 1\n\n\nSometimes, we will use vectors that are entirely ordered.\n\n\nCode\nseq(1,7,by=1) #1:7\n## [1] 1 2 3 4 5 6 7\nseq(1,7,by=0.5)\n##  [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0\n\n# Ordering data\nsort(x)\n## [1]  0  1  3  6 10\nx[order(x)]\n## [1]  0  1  3  6 10\n\n\n\n\nMatrices.\nMatrices are also common objects\n\n\nCode\nx1 &lt;- c(1,4,9)\nx2 &lt;- c(3,0,2)\nx_mat &lt;- rbind(x1, x2)\n\nx_mat       # Print full matrix\n##    [,1] [,2] [,3]\n## x1    1    4    9\n## x2    3    0    2\nx_mat[2,]   # Print Second Row\n## [1] 3 0 2\nx_mat[,2]   # Print Second Column\n## x1 x2 \n##  4  0\nx_mat[2,2]  # Print Element in Second Column and Second Row\n## x2 \n##  0\n\n\nThere are elementwise calculations\n\n\nCode\nx_mat+2\n##    [,1] [,2] [,3]\n## x1    3    6   11\n## x2    5    2    4\nx_mat*2\n##    [,1] [,2] [,3]\n## x1    2    8   18\n## x2    6    0    4\nx_mat^2\n##    [,1] [,2] [,3]\n## x1    1   16   81\n## x2    9    0    4\n\nx_mat + x_mat\n##    [,1] [,2] [,3]\n## x1    2    8   18\n## x2    6    0    4\nx_mat*x_mat #NOT classical matrix multiplication\n##    [,1] [,2] [,3]\n## x1    1   16   81\n## x2    9    0    4\nx_mat^x_mat\n##    [,1] [,2]      [,3]\n## x1    1  256 387420489\n## x2   27    1         4\n\n\nAnd you can also use matrix algebra\n\n\nCode\nx_mat1 &lt;- matrix( seq(2,7), 2, 3)\nx_mat1\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n\nx_mat2 &lt;- matrix( seq(4,-1), 2, 3)\nx_mat2\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\ntcrossprod(x_mat1, x_mat2) #x_mat1 %*% t(x_mat2)\n##      [,1] [,2]\n## [1,]   16    4\n## [2,]   22    7\n\ncrossprod(x_mat1, x_mat2)\n##      [,1] [,2] [,3]\n## [1,]   17    7   -3\n## [2,]   31   13   -5\n## [3,]   45   19   -7",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mathematics</span>"
    ]
  },
  {
    "objectID": "01_02_Mathematics.html#functions",
    "href": "01_02_Mathematics.html#functions",
    "title": "2  Mathematics",
    "section": "2.2 Functions",
    "text": "2.2 Functions\n\nSimple Functions.\nFunctions are applied to objects\n\n\nCode\n# Define a function that adds two to any vector\nadd_two &lt;- function(input_vector) { #input_vector is a placeholder\n    output_vector &lt;- input_vector + 2 # new object defined locally \n    return(output_vector) # return new object \n}\n# Apply that function to a vector\nx &lt;- c(0,1,3,10,6)\nadd_two(input_vector=x) #same as add_two(x)\n## [1]  2  3  5 12  8\n\n\nCommon mistakes:\n\n\nCode\nprint(output_vector)\n# This is not available globally\n\n# Double check your spelling\nx &lt; - add_two(input_vector=X) \n\n# Seeing \"+\" in the bottom console\n# often means you forgot to close the function with \"}\" \n# press \"Escape\" and try again\nadd_two &lt;- function(input_vector) { \n    output_vector &lt;- input_vector + 2 \n    return(output_vector)\nx &lt;- c(0,1,3,10,6)\nadd_two(x)\n\n\nThere are many different functions\n\n\nCode\nadd_vec &lt;- function(input_vector1, input_vector2) {\n    output_vector &lt;- input_vector1 + input_vector2\n    return(output_vector)\n}\nadd_vec(x,3)\n## [1]  3  4  6 13  9\nadd_vec(x,x)\n## [1]  0  2  6 20 12\n\nsum_squared &lt;- function(x1, x2) {\n    y &lt;- (x1 + x2)^2\n    return(y)\n}\n\nsum_squared(1, 3)\n## [1] 16\nsum_squared(x, 2)\n## [1]   4   9  25 144  64\nsum_squared(x, NA) \n## [1] NA NA NA NA NA\nsum_squared(x, x)\n## [1]   0   4  36 400 144\nsum_squared(x, 2*x)\n## [1]   0   9  81 900 324\n\n\nFunctions can take functions as arguments. Note that a statistic is defined as a function of data.\n\n\nCode\nstatistic &lt;- function(x, f){\n    y &lt;- f(x)\n    return(y)\n}\nstatistic(x, sum)\n## [1] 20\n\n\nThere are many possible functions you can make and use. More complicated functions often have defaults.\n\n\nCode\nfun_of_seq &lt;- function(f, constant=2){\n    x1 &lt;- seq(1,3, length.out=12)\n    x2 &lt;- x1+constant\n    x &lt;- cbind(x1,x2)\n    y &lt;- f(x)\n    return(y)\n}\nfun_of_seq(sum)\n## [1] 72\nfun_of_seq(sum, 3)\n## [1] 84\nfun_of_seq(prod)\n## [1] 30799645993\nfun_of_seq(prod, 3)\n## [1] 473621744988\n\n\nYou can also apply functions to matrices\n\n\nCode\nsum_squared(x_mat, x_mat)\n##    [,1] [,2] [,3]\n## x1    4   64  324\n## x2   36    0   16\n\n# Apply function to each matrix row\ny &lt;- apply(x_mat, 1, sum)^2 \n# ?apply  #checks the function details\n\n\n\n\nLoops.\nApplying the same function over and over again\n\n\nCode\n# Example 1: simple division\nx &lt;- vector(length=3)\n#Fill empty vector\nfor(i in seq(1,3)){\n    x[i] &lt;- i/2\n}\nx\n## [1] 0.5 1.0 1.5\n# Compare\n\n# Example 2: exponential\n#Create empty vector\nx &lt;- vector(length=3)\n#Fill empty vector\nfor(i in seq(1,3)){\n    x[i] &lt;- exp(i)\n}\n# Compare\nx\n## [1]  2.718282  7.389056 20.085537\nc( exp(1), exp(2), exp(3))\n## [1]  2.718282  7.389056 20.085537\n\n# Example 3: using existing data\nx &lt;- c(1,3,9,2)\ny &lt;- vector(length=length(x))\nfor(i in seq_along(x) ){\n    y[i] &lt;- x[i] + 1\n}\ny\n## [1]  2  4 10  3\n\n\nA more complicated example\n\n\nCode\ncomplicated_fun &lt;- function(i, j=0){\n    x &lt;- i^(i-1)\n    y &lt;- x + mean( seq(j,i) )\n    z &lt;- log(y)/i\n    return(z)\n}\ncomplicated_vector &lt;- vector(length=10)\nfor(i in seq(1,10) ){\n    complicated_vector[i] &lt;- complicated_fun(i)\n}\n\n\nA recursive example\n\n\nCode\nx &lt;- vector(length=4)\nx[1] &lt;- 1\nfor(i in seq(2,4) ){\n    x[i] &lt;- (x[i-1]+1)^2\n}\nx\n## [1]   1   4  25 676",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mathematics</span>"
    ]
  },
  {
    "objectID": "01_02_Mathematics.html#logic-and-counting",
    "href": "01_02_Mathematics.html#logic-and-counting",
    "title": "2  Mathematics",
    "section": "2.3 Logic and Counting",
    "text": "2.3 Logic and Counting\n\nBasic Logic.\nTRUE/FALSE\n\n\nCode\nx &lt;- c(1,2,3,NA)\nx &gt; 2\n## [1] FALSE FALSE  TRUE    NA\nx==2\n## [1] FALSE  TRUE FALSE    NA\n\nany(x==2)\n## [1] TRUE\nall(x==2)\n## [1] FALSE\n2 %in% x\n## [1] TRUE\n\n2==TRUE\n## [1] FALSE\n2==FALSE\n## [1] FALSE\n \nis.numeric(x)\n## [1] TRUE\nis.na(x)\n## [1] FALSE FALSE FALSE  TRUE\n\n\nThe “&” and “|” commands are logical calculations that compare vectors to the left and right.\n\n\nCode\nx &lt;- seq(1,3)\n(x &gt;= 1) & (x &lt; 2)\n## [1]  TRUE FALSE FALSE\n(x &gt;= 1) | (x &lt; 2)\n## [1] TRUE TRUE TRUE\n\nif( all(x &gt;= 1) ){\n    print(\"ok\")\n} else {\n    print(\"not ok\")\n}\n## [1] \"ok\"\n\nlogic_fun &lt;- function(x){\n    if( all(x &gt;= 1) ){\n        print(\"ok\")\n    } else {\n        print(\"not ok\")\n    }\n}\nlogic_fun( seq(1,3) )\n## [1] \"ok\"\nlogic_fun( seq(0,2) )\n## [1] \"not ok\"\n\n\n\n\nBasic Counting.\nFactorials are used for counting the different ways of arranging \\(n\\) distinct objects into a sequence: \\(n!=1\\times2\\times3 ... (n-2)\\times(n-1)\\times n\\).\n\n\nCode\nfactorial(3)\n## [1] 6\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nHow many ways are there to order the numbers \\(\\{1, 2, 3\\}\\)?\n{1,2,3} {1,3,2}\n{2,1,3} {2,3,1}\n{3,2,1} {3,1,2}\n\n\n\nThe binomial coefficient \\(\\tbinom {n}{k}\\) counts the subsets of \\(k\\) elements from a set with \\(n\\) elements.\n\n\nCode\n#Ways to draw k=2 from a set with n=4 \nchoose(4,2)\n## [1] 6\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nHow many subsets with \\(k=2\\) are there for the set \\(\\{1,2,3,4\\}\\)\n{1,2} {1,3}, {3,4}\n{2,3} {2,4}\n{3,4}\n\n\n\nThe exponential function is \\(e^{x}=1+x/1+2^2/2+x^3/6....=\\sum_{k=0}^{\\infty} x^k/k!\\) and the \\(log\\) function is it’s inverse.\n\n\nCode\nx &lt;- exp(4)\nx\n## [1] 54.59815\n\ny &lt;- log(x)\ny\n## [1] 4",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mathematics</span>"
    ]
  },
  {
    "objectID": "01-00-UnivariateData.html",
    "href": "01-00-UnivariateData.html",
    "title": "Univariate Data",
    "section": "",
    "text": "This section introduces basic univariate statistics from a computational rather than mathematical approach. We will also use some basic probability theory, which you can review at the high-school level if this is not already familiar to you.\n\nhttps://www.khanacademy.org/math/cc-sixth-grade-math/cc-6th-data-statistics\nhttps://www.khanacademy.org/math/grade-6-fl-best/x9def9752caf9d75b:data-and-statistics\nhttps://www.khanacademy.org/math/cc-seventh-grade-math/cc-7th-probability-statistics\nhttps://www.khanacademy.org/math/grade-7-virginia/x1e291b30c04dacab:probability-statistics",
    "crumbs": [
      "Univariate Data"
    ]
  },
  {
    "objectID": "01_03_Data.html",
    "href": "01_03_Data.html",
    "title": "3  Data",
    "section": "",
    "text": "3.1 Types",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "01_03_Data.html#types",
    "href": "01_03_Data.html#types",
    "title": "3  Data",
    "section": "",
    "text": "Basic Types.\nThe two basic types of data are cardinal (aka numeric) data and factor data. We can further distinguish between whether cardinal data are discrete or continuous. We can also further distinguish between whether factor data are ordered or not\n\nCardinal (Numeric): the difference between elements always means the same thing.\n\nDiscrete: E.g. \\(\\{ 1,2,3\\}\\) and notice that \\(2-1=3-2\\).\nContinuous: E.g., \\(\\{1.4348, 2.4348, 2.9, 3.9 \\}\\) and notice that \\(2.9-1.4348=3.9-2.4348\\)\n\nFactor: the difference between elements does not always mean the same thing.\n\nOrdered: E.g., \\(\\{1^{st}, 2^{nd}, 3^{rd}\\}\\) place in a race and notice that \\(1^{st}\\) - \\(2^{nd}\\) place does not equal \\(2^{nd}\\) - \\(3^{rd}\\) place for a very competitive person who cares only about winning.\nUnordered (categorical): E.g., \\(\\{Amanda, Bert, Charlie\\}\\) and notice that \\(Amanda - Bert\\) never makes sense.\n\n\nHere are some examples\n\n\nCode\ndat_card1 &lt;- c(1,2,3) # Cardinal data (Discrete)\ndat_card1\n## [1] 1 2 3\n\ndat_card2 &lt;- c(1.1, 2/3, 3) # Cardinal data (Continuous)\ndat_card2\n## [1] 1.1000000 0.6666667 3.0000000\n\ndat_fact1 &lt;- factor( c('A','B','C'), ordered=T) # Factor data (Ordinal)\ndat_fact1\n## [1] A B C\n## Levels: A &lt; B &lt; C\n\ndat_fact2 &lt;- factor( c('Leipzig','Los Angeles','Logan'), ordered=F) # Factor data (Categorical)\ndat_fact2\n## [1] Leipzig     Los Angeles Logan      \n## Levels: Leipzig Logan Los Angeles\n\ndat_fact3 &lt;- factor( c(T,F), ordered=F) # Factor data (Categorical)\ndat_fact3\n## [1] TRUE  FALSE\n## Levels: FALSE TRUE\n\n# Explicitly check the data types:\n#class(dat_card1)\n#class(dat_card2)\n\n\nNote that for theoretical analysis, the types are sometimes grouped differently as\n\nDiscrete (discrete cardinal, ordered factor, and unordered factor data). You can count the potential values. E.g., the set \\(\\{A,B,C\\}\\) has three potential values.\nContinuous (continuous cardinal data). There are uncountably infinite potential values. E.g., Try counting the numbers between \\(0\\) and \\(1\\) including decimal points, and notice that any two numbers have another potential number between them.\n\nIn any case, data are often computationally analyzed as data.frame objects, discussed below.\n\n\nStrings.\nNote that R allows for unstructured plain text, called character strings, which we can then format as factors\n\n\nCode\nc('A','B','C')  # character strings\n## [1] \"A\" \"B\" \"C\"\nc('Leipzig','Los Angeles','Logan')  # character strings\n## [1] \"Leipzig\"     \"Los Angeles\" \"Logan\"\n\n\nAlso note that strings are encounter in a variety of settings, and you often have to format them after reading them into R.1\n\n\nCode\n# Strings\npaste( 'hi', 'mom')\n## [1] \"hi mom\"\npaste( c('hi', 'mom'), collapse='--')\n## [1] \"hi--mom\"\n\nkingText &lt;- \"The king infringes the law on playing curling.\"\ngsub(pattern=\"ing\", replacement=\"\", kingText)\n## [1] \"The k infres the law on play curl.\"\n# advanced usage\n#gsub(\"[aeiouy]\", \"_\", kingText)\n#gsub(\"([[:alpha:]]{3})ing\\\\b\", \"\\\\1\", kingText)",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "01_03_Data.html#datasets",
    "href": "01_03_Data.html#datasets",
    "title": "3  Data",
    "section": "3.2 Datasets",
    "text": "3.2 Datasets\nDatasets can be stored in a variety of formats on your computer. But they can be analyzed in R in three basic ways.\n\nLists.\nLists are probably the most basic type\n\n\nCode\nx &lt;- seq(1,10)\ny &lt;- 2*x\nlist(x, y)  # list of vectors\n## [[1]]\n##  [1]  1  2  3  4  5  6  7  8  9 10\n## \n## [[2]]\n##  [1]  2  4  6  8 10 12 14 16 18 20\n\nx_mat1 &lt;- matrix( seq(2,7), 2, 3)\nx_mat2 &lt;- matrix( seq(4,-1), 2, 3)\nlist(x_mat1, x_mat2)  # list of matrices\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\n\nLists are useful for storing unstructured data\n\n\nCode\nlist(list(x_mat1), list(x_mat2))  # list of lists\n## [[1]]\n## [[1]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## \n## [[2]]\n## [[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\nlist(x_mat1, list(x_mat1, x_mat2)) # list of different objects\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]]\n## [[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]][[2]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\n# ...inception...\nlist(x_mat1,\n    list(x_mat1, x_mat2), \n    list(x_mat1, list(x_mat2)\n    )) \n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]]\n## [[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]][[2]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n## \n## \n## [[3]]\n## [[3]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[3]][[2]]\n## [[3]][[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\n\n\n\nData.frames.\nA data.frame looks like a matrix but each column is actually a list rather than a vector. This allows you to combine different data types into a single object for analysis, which is why it might be your most common object.\n\n\nCode\n# data.frames: your most common data type\n    # matrix of different data-types\n    # well-ordered lists\ndata.frame(x, y)  # list of vectors\n##     x  y\n## 1   1  2\n## 2   2  4\n## 3   3  6\n## 4   4  8\n## 5   5 10\n## 6   6 12\n## 7   7 14\n## 8   8 16\n## 9   9 18\n## 10 10 20\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nCreate a data.frame storing two different types of data. Then show print only the second column\n\n\nCode\nd0 &lt;- data.frame(x=dat_fact2, y=dat_card2)\nd0\n##             x         y\n## 1     Leipzig 1.1000000\n## 2 Los Angeles 0.6666667\n## 3       Logan 3.0000000\n\nd0[,'y']\n## [1] 1.1000000 0.6666667 3.0000000\n\n\n\n\n\n\n\nArrays.\nArrays are generalization of matrices to multiple dimensions. They are a very efficient way to store well-formatted numeric data, and are often used in spatial econometrics and time series (often in the form of “data cubes”).\n\n\nCode\n# data square (matrix)\narray(data = seq(1,24), dim=c(3, 8))\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n## [1,]    1    4    7   10   13   16   19   22\n## [2,]    2    5    8   11   14   17   20   23\n## [3,]    3    6    9   12   15   18   21   24\n\n# data cube\na &lt;- array(data = seq(1,24), dim=c(3, 2, 4))\na\n## , , 1\n## \n##      [,1] [,2]\n## [1,]    1    4\n## [2,]    2    5\n## [3,]    3    6\n## \n## , , 2\n## \n##      [,1] [,2]\n## [1,]    7   10\n## [2,]    8   11\n## [3,]    9   12\n## \n## , , 3\n## \n##      [,1] [,2]\n## [1,]   13   16\n## [2,]   14   17\n## [3,]   15   18\n## \n## , , 4\n## \n##      [,1] [,2]\n## [1,]   19   22\n## [2,]   20   23\n## [3,]   21   24\n\n\n\n\nCode\na[1, , , drop = FALSE]  # Row 1\n#a[, 1, , drop = FALSE]  # Column 1\n#a[, , 1, drop = FALSE]  # Layer 1\n\na[ 1, 1,  ]  # Row 1, column 1\n#a[ 1,  , 1]  # Row 1, \"layer\" 1\n#a[  , 1, 1]  # Column 1, \"layer\" 1\na[1 , 1, 1]  # Row 1, column 1, \"layer\" 1\n\n\nApply extends to arrays\n\n\nCode\napply(a, 1, mean)    # Row means\n## [1] 11.5 12.5 13.5\napply(a, 2, mean)    # Column means\n## [1] 11 14\napply(a, 3, mean)    # \"Layer\" means\n## [1]  3.5  9.5 15.5 21.5\napply(a, c(1,2), mean)  # Row/Column combination \n##      [,1] [,2]\n## [1,]   10   13\n## [2,]   11   14\n## [3,]   12   15\n\n\nOuter products yield arrays\n\n\nCode\nx &lt;- c(1,2,3)\nx_mat1 &lt;- outer(x, x) # x %o% x\nx_mat1\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    2    4    6\n## [3,]    3    6    9\nis.array(x_mat1) # Matrices are arrays\n## [1] TRUE\n\nx_mat2 &lt;- matrix( seq(6,1), 2, 3)\nouter(x_mat2, x)\n## , , 1\n## \n##      [,1] [,2] [,3]\n## [1,]    6    4    2\n## [2,]    5    3    1\n## \n## , , 2\n## \n##      [,1] [,2] [,3]\n## [1,]   12    8    4\n## [2,]   10    6    2\n## \n## , , 3\n## \n##      [,1] [,2] [,3]\n## [1,]   18   12    6\n## [2,]   15    9    3\n# outer(x_mat2, matrix(x))\n# outer(x_mat2, t(x))\n# outer(x_mat1, x_mat2)",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "01_03_Data.html#densities-and-distributions",
    "href": "01_03_Data.html#densities-and-distributions",
    "title": "3  Data",
    "section": "3.3 Densities and Distributions",
    "text": "3.3 Densities and Distributions\n\nInitial Data Inspection.\nRegardless of the data types you have, you typically begin by inspecting your data by examining the first few observations.\nConsider, for example, historical data on crime in the US.\n\n\nCode\nhead(USArrests) # Actual Data\n##            Murder Assault UrbanPop Rape\n## Alabama      13.2     236       58 21.2\n## Alaska       10.0     263       48 44.5\n## Arizona       8.1     294       80 31.0\n## Arkansas      8.8     190       50 19.5\n## California    9.0     276       91 40.6\n## Colorado      7.9     204       78 38.7\n\n# Check NA values\nX &lt;- c(3,3.1,NA,0.02) #Small dataset we will use in numerical examples\nsum(is.na(X))\n## [1] 1\n\n\nTo further examine a particular variable, we look at its distribution. In what follows, we will often work with data as vector \\(\\hat{X}=(\\hat{X}_{1}, \\hat{X}_{2}, ....\\hat{X}_{n})\\), where there are \\(n\\) observations and \\(\\hat{X}_{i}\\) is the value of the \\(i\\)th one. We often analyze observations in comparison to some value \\(x\\).\n\n\nCode\nX &lt;- c(3,3.1,0.02) # Data: \"big X\"\nx &lt;- 2 # particular value: \"little x\"\nsum(X &lt;= x)\n## [1] 1\nsum(X == x)\n## [1] 0\n\n\n\n\nHistogram Density Estimate.\nThe histogram measures the proportion of the data in different bins. It does so by dividing the range of the data into exclusive bins of equal-width \\(h\\), and count the number of observations within each bin. We often rescale the counts, dividing by the number of observations \\(n\\) multiplied by bin-width \\(h\\), so that the total area of the histogram sums to one, which allows us to interpret the numbers as a density.\nMathematically, for an exclusive bin \\(\\left(x-\\frac{h}{2}, x+\\frac{h}{2} \\right]\\) defined by their midpoint \\(x\\) and width \\(h\\), we compute \\[\\begin{eqnarray}\n\\hat{f}(x) &=& \\frac{  \\sum_{i=1}^{n} \\mathbf{1}\\left( \\hat{X}_{i} \\in \\left(x-\\frac{h}{2}, x+\\frac{h}{2} \\right] \\right) }{n h},\n\\end{eqnarray}\\] where \\(\\mathbf{1}()\\) is an indicator function that equals \\(1\\) if the expression inside is TRUE and \\(0\\) otherwise. E.g., if \\(\\hat{X}_{i}=3.8\\) and \\(h=1\\), then for \\(x=1\\) we have \\(\\mathbf{1}\\left( \\hat{X}_{i} \\in \\left(1-\\frac{h}{2}, 1+\\frac{h}{2} \\right] \\right)=\\mathbf{1}\\left( 3.8 \\in \\left(0.5, 1.5\\right] \\right)=0\\) and for \\(x=4\\) \\(\\mathbf{1}\\left( \\hat{X}_{i} \\in \\left(4-\\frac{h}{2}, 4+\\frac{h}{2} \\right] \\right)=\\mathbf{1}\\left(  3.8 \\in \\left(3.5, 4.5\\right] \\right)=1\\).\nNote that the area of the rectangle is “base x height”, which is \\(h \\times \\hat{f}(x)= \\sum_{i=1}^{n} \\mathbf{1}\\left( \\hat{X}_{i} \\in \\left(x-\\frac{h}{2}, x+\\frac{h}{2} \\right] \\right) /n\\). This means the rectangle area equals the proportion of data in the bin. We compute \\(\\hat{f}(x)\\) for each bin midpoint \\(x\\), and the area of all rectangles sums to one. 2\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor example, consider the dataset \\(\\{3,3.1,0.02\\}\\) and use bins \\((0,1], (1,2], (2,3], (3,4]\\). In this case, the midpoints are \\(x=(0.5,1.5,2.5,3.5)\\) and \\(h=1\\). Then the counts at each midpoints are \\((1,0,1,1)\\). Since \\(\\frac{1}{nh}=\\frac{1}{3\\times1}=\\frac{1}{3}\\), we can rescale the counts to compute the density as \\(\\hat{f}(x)=(1,0,1,1) \\frac{1}{3}=(1/3,0,1/3,1/3)\\).\n\n\nCode\n# Intuitive Examples\nX &lt;- c(3,3.1,0.02)\nXhist &lt;- hist(X, breaks=c(0,1,2,3,4), plot=F)\nXhist\n## $breaks\n## [1] 0 1 2 3 4\n## \n## $counts\n## [1] 1 0 1 1\n## \n## $density\n## [1] 0.3333333 0.0000000 0.3333333 0.3333333\n## \n## $mids\n## [1] 0.5 1.5 2.5 3.5\n## \n## $xname\n## [1] \"X\"\n## \n## $equidist\n## [1] TRUE\n## \n## attr(,\"class\")\n## [1] \"histogram\"\n\n# base x height\nbase &lt;- 1\nheight &lt;- Xhist[['density']]\nsum(base*height)\n## [1] 1\n\n\nFor another example, use the bins \\((0,2]\\) and \\((2,4]\\). So the midpoints are \\(1\\) and \\(3\\), and the bin width is \\(2\\). Only one observation, \\(0.02\\), falls in the bin \\((0,2]\\). The other two observations, \\(3\\) and \\(3.1\\), fall into the bin \\((2,4]\\). The scaling factor is \\(\\frac{1}{nh}=\\frac{1}{3\\times 2}=\\frac{1}{6}\\). So the first bin has density \\(f(1)=1\\frac{1}{6}=1/6\\) and the second bin has density \\(f(3)=2\\frac{1}{6}=2/6\\). The area of the first bin’s rectangle is \\(2 \\times f(1)=2/6=1/3\\) and the area of the second rectangle is \\(2 \\times f(3)=4/6=2/3\\).\nNow intuitively work through an example with three bins instead of four. Compute the areas\n\n\nCode\nXhist &lt;- hist(X, breaks=c(0,4/3,8/3,4), plot=F)\nbase &lt;- 4/3\nheight &lt;- Xhist[['density']]\nsum(base*height)\n## [1] 1\n\n\n# as a default, R uses bins (,] instead of [,)\n# but you can change that with \"right=F\"\n# hist(X, breaks=c(0,4/3,8/3,4), plot=F, right=F)\n\n\n\n\n\n\n\nCode\n# Practical Example\nX &lt;- USArrests[,'Murder']\nhist(X,\n    breaks=seq(0,20,by=1), #bin width=1\n    freq=F,\n    border=NA, \n    main='',\n    xlab='Murder Arrests')\n# Raw Observations\nrug(USArrests[,'Murder'], col=grey(0,.5))\n\n\n\n\n\n\n\n\n\nCode\n\n# Since h=1, the density equals the proportion of states in each bin\n# Redo this example with h=2\n\n\nNote that if you your data are factor data, or discrete cardinal data, you can directly plot the proportions in a bar plot. For each unique outcome \\(x\\) we compute \\[\\begin{eqnarray}\n\\hat{p}_{x}=\\sum_{i=1}^{n}\\mathbf{1}\\left(\\hat{X}_{i}=x\\right)/n.\n\\end{eqnarray}\\] where \\(n\\) is the number of observations and \\(\\sum_{i=1}^{n}\\mathbf{1}\\left(\\hat{X}_{i}=x\\right)\\) counts the number of observations equal to \\(x\\). The height of each line equals the proportion of data with a specific value.\n\n\nCode\n# Discretized data\nX &lt;- USArrests[,'Murder']\nXr &lt;- floor(X) #rounded down\n#table(Xr)\nproportions &lt;- table(Xr)/length(Xr)\nplot(proportions, col=grey(0,.5),\n    xlab='Murder Arrests (Discretized)',\n    ylab='Proportion of States with each value')\n\n\n\n\n\n\n\n\n\n\n\nEmpirical Cumulative Distribution Function.\nThe ECDF counts the proportion of observations whose values are less than or equal to \\(x\\); \\[\\begin{eqnarray}\n\\hat{F}(x) = \\frac{1}{n} \\sum_{i}^{n} \\mathbf{1}( \\hat{X}_{i} \\leq x).\n\\end{eqnarray}\\] Typically, we compute this for each unique value of \\(x\\) in the dataset. The ECDF jumps up by \\(1/n\\) at each of the \\(n\\) data points.\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor example, let \\(X=(3,3.1,0.02)\\). We reorder the observations as \\((0.02, 3, 3.1)\\), so that there are discrete jumps of \\(1/n=1/3\\) at each value. Consider the points \\(x \\in \\{0.5,2.5,3.5\\}\\). At \\(x=0.5\\), \\(F(0.5)\\) measures the proportion of the data \\(\\leq 0.5\\). Since only one observations, \\(0.02\\), of three is \\(\\leq 0.5\\), we can compute \\(F(0.5)=1/3\\). Similarly, since only one observations, \\(0.02\\), of three is \\(\\leq 2.5\\), we can compute \\(F(2.5)=1/3\\). Since all observations are \\(\\leq 3.5\\), we can compute \\(F(3.5)=1\\).\n\n\nCode\nX &lt;- c(3,3.1,0.02)\nFhat &lt;- ecdf(X)\n\n# Visualized\nplot(Fhat)\n\n\n\n\n\n\n\n\n\nCode\n\n# Evaluated at the data\nx &lt;- X\nFhat(X)\n## [1] 0.6666667 1.0000000 0.3333333\n#sum(X&lt;=3)/length(X)\n#sum(X&lt;=3.1)/length(X)\n#sum(X&lt;=0.02)/length(X)\n\n# Evaluated at other points\nx &lt;- c(0.5, 2.5, 3.5)\nFhat(x)\n## [1] 0.3333333 0.3333333 1.0000000\n#sum(X&lt;=0.5)/length(X)\n#sum(X&lt;=2.5)/length(X)\n#sum(X&lt;=3.5)/length(X)\n\n\n\n\n\n\n\nCode\nF_murder &lt;- ecdf(USArrests[,'Murder'])\n# proportion of murders &lt;= 10\nF_murder(10)\n## [1] 0.7\n# proportion of murders &lt;= x, for all x\nplot(F_murder, main='', \n    xlab='Murder Arrests (x)',\n    ylab='Proportion of States with Murder Arrests &lt;= x',\n    pch=16, col=grey(0,.5))\nrug(USArrests[,'Murder'])\n\n\n\n\n\n\n\n\n\n\n\nQuantiles.\nYou can summarize the distribution of data using quantiles: the \\(p\\)th quantile is a value of \\(x\\) where \\(p\\) percent of the data are below \\(x\\) and (\\(1-p\\)) percent are above \\(x\\).\n\nThe min is the smallest value (or the most negative value if there are any), where \\(0%\\) of the data has lower values.\nThe median is the middle value, where one half of the data has lower values and the other half has higher values.\nThe max is the smallest value (or the most negative value if there are any), where \\(100%\\) of the data has lower values.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor example, if \\(X=(0,0,0.02,3,5)\\) then the median is \\(0.02\\), the lower quartile is \\(0\\), and the upper quartile is \\(3\\). (The number \\(0\\) is also special: the most frequent observation is called the mode.)\n\n\nCode\nX &lt;-  c(3.1, 3, 0.02)\nquantile(X, probs=c(0,.5,1))\n##   0%  50% 100% \n## 0.02 3.00 3.10\n\n\nNow work through an intuitive example with observations \\(\\{1,2,...,13\\}\\). Hint: split the ordered observations into four groups.\n\n\n\nWe are often also interested in quartiles: where \\(25\\%\\) of the data fall below \\(x\\) (lower quartile) and where \\(75\\%\\) of the data fall below \\(x\\) (upper quartile). Sometimes we are also interested in deciles: where \\(10\\%\\) of the data fall below \\(x\\) (lower decile) and where \\(90\\%\\) of the data fall below \\(x\\) (upper decile). In general, we can use the empirical quantile function \\[\\begin{eqnarray}\n\\hat{Q}(p) = \\hat{F}^{-1}(p),\n\\end{eqnarray}\\] to compute a quantile for any probability \\(p\\in [0,1]\\). The median corresponds to \\(p=0.5\\), upper quartile to \\(p=0.75\\), and upper decile to \\(p=0.9\\).\n\n\nCode\n# common quantiles\nX &lt;- USArrests[,'Murder']\nquantile(X)\n##     0%    25%    50%    75%   100% \n##  0.800  4.075  7.250 11.250 17.400\n\n# All deciles are quantiles\nquantile(X, probs=seq(0,1, by=.1))\n##    0%   10%   20%   30%   40%   50%   60%   70%   80%   90%  100% \n##  0.80  2.56  3.38  4.75  6.00  7.25  8.62 10.12 12.12 13.32 17.40\n\n# Visualized: Inverting the Empirical Distribution\nFX_hat &lt;- ecdf(X)\nplot(FX_hat, lwd=2, xlim=c(0,20),\n    pch=16, col=grey(0,.5), main='')\n# Two Examples of Quantiles \np &lt;- c(.25, .9) # Lower Quartile, Upper Decile\ncols &lt;- c(2,4) \nQX_hat &lt;- quantile(X, p, type=1)\nQX_hat\n##  25%  90% \n##  4.0 13.2\nsegments(QX_hat, p, -10, p, col=cols)\nsegments(QX_hat, p, QX_hat, 0, col=cols)\nmtext( round(QX_hat,2), 1, at=QX_hat, col=cols)\n\n\n\n\n\n\n\n\n\nThere are some issues with quantiles with smaller datasets. E.g., to compute the median of \\(\\{3,3.1,0,1\\}\\), we need some ways to break ties (of which there are many options). Similar issues arise for other quantiles, so quantiles are not used for such small datasets.\n\n\n\n\n\n\nTip\n\n\n\n\n\nTo calculate quantiles, the computer sorts the observations from smallest to largest as \\(\\hat{X}_{(1)}, \\hat{X}_{(2)},... \\hat{X}_{(N)}\\), and then computes quantiles as \\(\\hat{X}_{ (q \\times N) }\\). Note that \\((q \\times N)\\) is rounded and there are different ways to break ties.\n\n\nCode\nX &lt;- USArrests[,'Murder']\nXo &lt;- sort(X)\nXo\n##  [1]  0.8  2.1  2.1  2.2  2.2  2.6  2.6  2.7  3.2  3.3  3.4  3.8  4.0  4.3  4.4\n## [16]  4.9  5.3  5.7  5.9  6.0  6.0  6.3  6.6  6.8  7.2  7.3  7.4  7.9  8.1  8.5\n## [31]  8.8  9.0  9.0  9.7 10.0 10.4 11.1 11.3 11.4 12.1 12.2 12.7 13.0 13.2 13.2\n## [46] 14.4 15.4 15.4 16.1 17.4\n\n# median\nXo[length(Xo)*.5]\n## [1] 7.2\nquantile(X, probs=.5, type=4) #tie break rule #4\n## 50% \n## 7.2\n\n# min\nXo[1]\n## [1] 0.8\nmin(Xo)\n## [1] 0.8\nquantile(Xo, probs=0)\n##  0% \n## 0.8\n\n\n\n\n\n\n\nBoxplots.\nBoxplots also summarize the distribution. The boxplot shows the median (solid black line) and interquartile range (\\(IQR=\\) upper quartile \\(-\\) lower quartile; filled box).3 As a default, whiskers are shown as \\(1.5\\times IQR\\) and values beyond that are highlighted as outliers (so whiskers do not typically show the data range). You can alternatively show all the raw data points instead of whisker+outliers.\n\n\nCode\nboxplot(USArrests[,'Murder'],\n    main='', ylab='Murder Arrests',\n    whisklty=0, staplelty=0, outline=F)\n# Raw Observations\nstripchart(USArrests[,'Murder'],\n    pch='-', col=grey(0,.5), cex=2,\n    vert=T, add=T)",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "01_03_Data.html#further-reading",
    "href": "01_03_Data.html#further-reading",
    "title": "3  Data",
    "section": "3.4 Further Reading",
    "text": "3.4 Further Reading\nECDF\n\nhttps://library.virginia.edu/data/articles/understanding-empirical-cumulative-distribution-functions\n\nHandling Strings\n\nhttps://meek-parfait-60672c.netlify.app/docs/M1_R-intro_03_text.html\nhttps://raw.githubusercontent.com/rstudio/cheatsheets/main/regex.pdf",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "01_03_Data.html#footnotes",
    "href": "01_03_Data.html#footnotes",
    "title": "3  Data",
    "section": "",
    "text": "We will not cover the statistical analysis of text in this course, but strings are amenable to statistical analysis.↩︎\nIf \\(L\\) distinct bins exactly span the range, then \\(h=[\\text{max}(\\hat{X}_{i}) - \\text{min}(\\hat{X}_{i})]/L\\) and \\(x\\in \\left\\{ \\frac{\\ell h}{2} + \\text{min}(\\hat{X}_{i}) \\right\\}_{\\ell=1}^{L}\\).↩︎\nTechnically, the upper and lower hinges use two different versions of the first and third quartile. See https://stackoverflow.com/questions/40634693/lower-and-upper-quartiles-in-boxplot-in-r.↩︎",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "01_04_RandomVariables.html",
    "href": "01_04_RandomVariables.html",
    "title": "4  Random Variables",
    "section": "",
    "text": "Probability.\nIn the last section we computed a distribution given the data, whereas now we generate individual data points given the distribution.\nRandom variables are vectors whose values occur according to a frequency distribution. As such, random variables have a\nWe think of each observation \\(\\hat{X}_{i}\\) before it is actually observed, potentially taking on specific values \\(x\\) from the sample space with known probabilities. For example, we consider flipping a coin before knowing whether it lands on heads or tails. We denote the random variable, in this case the unflipped coin, as \\(X_{i}\\).\nThere are two basic types of sample spaces: discrete (encompassing cardinal-discrete, factor-ordered, and factor-unordered data) and continuous. This leads to two types of random variables: discrete and continuous. However, each type has many different probability distributions.\nThe most common random variables are easily accessible and can be described using the Cumulative Distribution Function (CDF) \\[\\begin{eqnarray}\nF(x) &=& Prob(X_{i} \\leq x).\n\\end{eqnarray}\\] Note that this is just like the Empirical Cumulative Distribution Function (ECDF), \\(\\hat{F}(x)\\), except that it is now theoretically known. You can think of \\(F(x)\\) as the ECDF for a dataset with an infinite number of observations. Equivalently, the ECDF is an empirical version of the CDF that is applied to observed data.\nAfter introducing different random variables, we will also cover some basic implications of their CDF. Intuitively, probabilities must sum up to one. So we can compute \\(Prob(X_{i} &gt; x) = 1- F(x)\\). We also have two “in” and “out” probabilities.\nThe probability of \\(X_{i}\\leq b\\) and \\(X_{i}\\geq a\\) can be written in terms of falling into a range \\(Prob(X_{i} \\in [a,b])=Prob(a \\leq X_{i} \\leq b) = F(b) - F(a)\\).\nThe opposite probability of \\(X_{i} &gt; b\\) or \\(X_{i} &lt; a\\) is \\(Prob(X_{i} &lt; a \\text{ or } X_{i} &gt; b) = F(a) + [1- F(b)]\\). Notice that this opposite probability \\(F(a) + [1- F(b)] =1 - [F(b) - F(a)]\\), so that \\(Prob(X_{i} \\text{ out of } [a,b]) = 1 - Prob( X_{i} \\in [a,b])\\)",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "01_04_RandomVariables.html#discrete",
    "href": "01_04_RandomVariables.html#discrete",
    "title": "4  Random Variables",
    "section": "4.1 Discrete",
    "text": "4.1 Discrete\nA discrete random variable can take one of several values in a set. E.g., any number in \\(\\{1,2,3,...\\}\\) or any letter in \\(\\{A,B,C,...\\}\\). Theoretical proportions are referred to as a probability mass function, which can be thought of as a proportions bar plot for an infinitely large dataset. Equivalently, the bar plot is an empirical version of the probability mass function that is applied to observed data.\n\nBernoulli.\nThink of a Coin Flip: Heads or Tails with equal probability. In general, a Bernoulli random variable denotes heads as the event \\(x=1\\) and tails as the event \\(x=0\\), and allows the probability of heads to vary. \\[\\begin{eqnarray}\nX_{i} &\\in& \\{0,1\\} \\\\\nProb(X_{i} =0) &=& 1-p \\\\\nProb(X_{i} =1) &=& p \\\\\nF(x) &=& \\begin{cases}\n    0   & x&lt;0 \\\\\n    1-p & x \\in [0,1) \\\\\n    1   & x\\geq 1\n\\end{cases}\n\\end{eqnarray}\\]\nHere is an example of the Bernoulli distribution. While you might get all heads (or all tails) in the first few coin flips, the ratios level out to their theoretical values after many flips.\n\n\nCode\nx &lt;- c(0,1)\nx_probs &lt;- c(3/4, 1/4)\n\nsample(x, 1, prob=x_probs, replace=T) # 1 Flip\n## [1] 1\nsample(x, 4, prob=x_probs, replace=T) # 4 Flips \n## [1] 0 0 1 0\nX0 &lt;- sample(x, 400, prob=x_probs, replace=T)\n\n# Plot Cumulative Proportion\nX0_t &lt;- seq(1, length(X0)) #head(X0_t)\nX0_mt &lt;- cumsum(X0)/X0_t #head(X0_mt)\npar(mar=c(4,4,1,4))\nplot(X0_t, X0_mt, type='l',\n    ylab='Cumulative Proportion (p)',\n    xlab='Flip #', \n    ylim=c(0,1), \n    lwd=2)\n# Show individual flip outcomes\npoints(X0_t, X0, col=grey(0,.5),\n    pch='|', cex=.3)\n# Show theoretical proportions\nabline(h=0.25, col='blue')\n\n\n\n\n\n\n\n\n\nCode\n\n# Plot Long run proportions\nproportions &lt;- table(X0)/length(X0)\nplot(proportions, \n    col=grey(0, 0.5),\n    xlab='Flip Outcome', \n    ylab='Proportion',\n    main=NA)\npoints(c(0,1), c(.75, .25), pch=16, col='blue') # Theoretical values\n\n\n\n\n\n\n\n\n\nCode\n\n# Plot CDF\nplot( ecdf(X0), col=grey(0,.5),\n    pch=16, main=NA) #Empirical\n\n\n\n\n\n\n\n\n\nCode\n#points(c(0,1), c(.75, 1), pch=16, col='blue') # Theoretical\n\n\n\n\nDiscrete Uniform.\nDiscrete numbers with equal probability, such as a die with \\(K\\) sides. \\[\\begin{eqnarray}\nX_{i} &\\in& \\{1,...K\\} \\\\\nProb(X_{i} =1) &=& Prob(X_{i} =2) = ... = 1/K\\\\\nF(x) &=& \\begin{cases}\n    0   & x&lt;1 \\\\\n    1/K & x \\in [1,2) \\\\\n    2/K & x \\in [2,3) \\\\\n    \\vdots & \\\\\n    1   & x\\geq K\n\\end{cases}\n\\end{eqnarray}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nHere is an example with \\(K=4\\). E.g., rolling a four-sided die.\nThe probability of a value smaller than or equal to \\(3\\) is \\(Prob(X_{i} \\leq 3)=1/4 + 1/4 + 1/4 = 3/4\\).\nThe probability of a value larger than \\(3\\) is \\(Prob(X_{i} &gt; 3) = 1-Prob(X_{i} \\leq 3)=1/4\\).\nThe probability of a value \\(&gt;\\) 1 and \\(\\leq 3\\) is \\(Prob(1 &lt; X_{i} \\leq 3) = Prob(X_{i} \\leq 3) - \\left[ 1- Prob(X_{i} \\leq 1) \\right] = 3/4 - 1/4 = 2/4\\).1\nThe probability of a value \\(\\leq\\) 1 or \\(&gt; 3\\) is \\(Prob(X_{i} \\leq 1 \\text{ or } X_{i} &gt; 3) =  Prob(X_{i} \\leq 1) +  \\left[ 1- Prob(X_{i} \\leq 3) \\right] = 1/4 + [1 - 3/4]=2/4\\).\n\n\n\n\n\nCode\nx &lt;- c(1,2,3,4)\nx_probs &lt;- c(1/4, 1/4, 1/4, 1/4)\n# sample(x, 1, prob=x_probs, replace=T) # 1 roll\nX1 &lt;- sample(x, 2000, prob=x_probs, replace=T) # 2000 rolls\n\n# Plot Long run proportions\nproportions &lt;- table(X1)/length(X1)\nplot(proportions, col=grey(0,.5),\n    xlab='Outcome', ylab='Proportion', main=NA)\npoints(x, x_probs, pch=16, col='blue') # Theoretical values\n\n\n\n\n\n\n\n\n\nCode\n\n# Hist w/ Theoretical Counts\n# hist(X1, breaks=50, border=NA, main=NA, ylab='Count')\n# points(x, x_probs*length(X1), pch='-') \n\n# Alternative Plot\nplot( ecdf(X1), pch=16, col=grey(0,.5), main=NA)\n\n\n\n\n\n\n\n\n\nCode\n\n# Alternative Plot 2\n#props &lt;- table(X1)\n#barplot(props, ylim = c(0, 0.35), ylab = \"Proportion\", xlab = \"Value\")\n#abline(h = 1/4, lty = 2)\n\n\nNote that the Discrete Uniform distribution generalizes to arbitrary intervals, although we will not exploit the generalization in this class.\n\n\nMultinoulli (aka Categorical).\nNumbers \\(1,...K\\) with unequal probabilities. \\[\\begin{eqnarray}\nX_{i} &\\in& \\{1,...K\\} \\\\\nProb(X_{i} =1) &=& p_{1} \\\\\nProb(X_{i} =2) &=& p_{2} \\\\\n        &\\vdots& \\\\\np_{1} + p_{2} + ... &=& 1\\\\\nF(x) &=& \\begin{cases}\n    0   & x&lt;1 \\\\\n    p_{1} & x \\in [1,2) \\\\\n    p_{1} + p_{2} & x \\in [2,3) \\\\\n    \\vdots & \\\\\n    1   & x\\geq K\n\\end{cases}\n\\end{eqnarray}\\]\nWe can also replace numbers with letters \\((A,...Z)\\) or names \\((John, Jamie, ...)\\) although we must be careful with the CDF when there is no longer a natural ordering. Here is an empirical example with three outcomes\n\n\nCode\nx &lt;- c('A', 'B', 'C')\nx_probs &lt;- c(3/10, 1/10, 6/10)\nsum(x_probs)\n## [1] 1\nX2 &lt;- sample(x, 2000, prob=x_probs, replace=T) # sample of 2000\n\n# Plot Long run proportions\nproportions &lt;- table(X2)/length(X2)\nplot(proportions, col=grey(0,.5),\n    xlab='Outcome', ylab='Proportion', main=NA)\npoints(x_probs, pch=16, col='blue') # Theoretical values\n\n\n\n\n\n\n\n\n\nCode\n\n# Histogram version\n# X2_alt &lt;- X1\n# X2_alt[X2_alt=='A'] &lt;- 1\n#X2_alt[X2_alt=='B'] &lt;- 2\n#X2_alt[X2_alt=='C'] &lt;- 3\n#X2_alt &lt;- as.numeric(X1_alt)\n#hist(X2_alt, breaks=50, border=NA, \n#    main=NA, ylab='Count')\n#points(x, x_probs*length(X2_alt), pch=16) ## Theoretical Counts\n\n# Alternative Plot\n# plot( ecdf(X2), pch=16, col=grey(0,.5), main=NA)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nSuppose there is an experiment with three possible outcomes, \\(\\{A, B, C\\}\\). It was repeated \\(50\\) times and discovered that \\(A\\) occurred \\(10\\) times, \\(B\\) occurred \\(13\\) times, and \\(C\\) occurred \\(27\\) times. The estimated probability of each outcome is found via the bar plot \\(\\hat{p}_{A} = 10/50\\), \\(\\hat{p}_{B} = 13/50\\), \\(\\hat{p}_{A} = 27/50\\). We can also estimate the “in” probabilities as \\(\\hat{Prob}(A \\text{ or } B)=10/50+13/50=23/50\\) and \\(\\hat{Prob}(B \\text{ or } C)=13/50+27/50=40/50\\), as well as the “out” probability as \\(\\hat{Prob}(A \\text{ or } C)=13/50+27/50=37/50\\).\nSuppose there are three possible outcomes of an experiment, \\(\\{\\text{my car dies}, \\text{it rains next Tuesday},\\text{a cat is born}\\}\\), which have corresponding probabilities \\(\\{3/10, 1/10, 6/10 \\}\\) that are known theoretically. Compute the probability that my car dies or a cat is born.",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "01_04_RandomVariables.html#continuous",
    "href": "01_04_RandomVariables.html#continuous",
    "title": "4  Random Variables",
    "section": "4.2 Continuous",
    "text": "4.2 Continuous\nA continuous random variable can take one value out of an uncountably infinite number. E.g., any number between \\(0\\) and \\(1\\) with any number of decimal points. With a continuous random variable, the probability of any individual point is zero, so we describe these variables with the cumulative distribution function (CDF), \\(F\\), or the probability density function (PDF), \\(f\\). Just as \\(F\\) can be thought of as the ECDF, \\(\\hat{F}\\), with an infinite amount of data, \\(f\\) can be thought of as a histogram, \\(\\hat{f}\\), with an infinite amount of data. Equivalently, the histogram is an empirical version of the PDF that is applied to observed data.\nOften, the PDF helps you intuitively understand a random variable whereas the CDF helps you calculate numerical values. This is because probabilities are depicted as areas in the PDF and the CDF accumulates those areas: \\(F(x)\\) equals the area under the PDF from \\(-\\infty\\) to \\(x\\). For example, \\(Prob(X_{i} \\leq 1)\\) is depicted by the PDF as the area under \\(f(x)\\) from the lowest possible value until \\(x=1\\), which is numerically calculated simply as \\(F(1)\\).\n\nContinuous Uniform.\nAny number on a unit interval allowing for any number of decimal points, with every interval of the same size having the same probability. \\[\\begin{eqnarray}\nX_{i} &\\in& [0,1] \\\\\nf(x) &=& \\begin{cases}\n    1 & x \\in [0,1] \\\\\n    0 & \\text{Otherwise}\n\\end{cases}\\\\\nF(x) &=& \\begin{cases}\n    0 & x &lt; 0 \\\\\n    x & x \\in [0,1] \\\\\n    1 & x &gt; 1.\n\\end{cases}\n\\end{eqnarray}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe probability of a value being exactly \\(0.25\\) is \\(Prob(X_{i} =0.25)=0\\).\nThe probability of a value smaller than \\(0.25\\) is \\(F(0.25)=0.25\\).\nThe probability of a value larger than \\(0.25\\) is \\(1-F(0.25)=0.75\\).\nThe probability of a value in \\((0.25,0.75]\\) is \\(Prob(0.25 &lt; X_{i} \\leq 0.75) = Prob(X_{i} \\leq 0.75) - \\left[ 1- Prob(X_{i} \\leq 0.25) \\right] = 0.75 - 0.25 = 0.5\\).\nThe probability of a value in \\((0.2,0.7]\\) is \\(Prob(0.2 &lt; X_{i} \\leq 0.7) = Prob(X_{i} \\leq 0.7) - \\left[ 1- Prob(X_{i} \\leq 0.2) \\right] = 0.7 - 0.2 = 0.5\\).\nThe probability of a value outside of \\((0.2,0.7]\\) is \\(Prob(X_{i} \\leq 0.2 \\text{ or } x &gt; 0.7) = 0.2 + [1-0.7]=0.5\\). Alternatively, you can compute \\(1- Prob(0.2 &lt; X_{i} \\leq 0.7)=1-0.5=0.5\\).\n\n\nCode\n# Prob. &lt; 0.25\npunif(0.25)\n## [1] 0.25\n\n# Prob. &gt; 0.25\n1-punif(0.25)\n## [1] 0.75\n\n# Prob. in (0.2,0.7]\nF_two &lt;- punif( c(0.2, 0.7) )\nF_in &lt;- F_two[2] - F_two[1]\nF_in\n## [1] 0.5\n\n# Prob. out of (0.2,0.7]\nF_out &lt;- 1- F_in\nF_out\n## [1] 0.5\n\n\n\n\n\n\n\nCode\nrunif(3) # 3 draws\n## [1] 0.6184793 0.6324535 0.8858883\n\n# Empirical Density \nX3 &lt;- runif(2000)\nhist(X3, breaks=20, border=NA, main=NA, freq=F)\n# Theoretical Density\nx &lt;- seq(-0.1,1.1,by=.001)\nfx &lt;- dunif(x)\nlines(x, fx, col='blue')\n\n\n\n\n\n\n\n\n\nCode\n\n# CDF example 1\nP_low &lt;- punif(0.25)\nP_low\n## [1] 0.25\n# Uncomment to show via PDF\n# x_low &lt;- seq(0,0.25,by=.001)\n# fx_low &lt;- dunif(x_low)\n# polygon( c(x_low, rev(x_low)), c(fx_low,fx_low*0),\n#    col=rgb(0,0,1,.25), border=NA)\n    \n# CDF example 2\nP_high &lt;- 1-punif(0.25)\nP_high\n## [1] 0.75\n# Uncomment to show via PDF\n# x_high &lt;- seq(0.25,1,by=.001)\n# fx_high &lt;- dunif(x_high)\n# polygon( c(x_high, rev(x_high)), c(fx_high,fx_high*0),\n#    col=rgb(0,0,1,.25), border=NA)\n    \n# CDF example 3\nP_mid &lt;- punif(0.75) - punif(0.25)\nP_mid\n## [1] 0.5\n# Uncomment to show via PDF\n# x_mid &lt;-  seq(0.25,0.75,by=.001)\n# fx_mid &lt;- dunif(x_mid)\n# polygon( c(x_mid, rev(x_mid)), c(fx_mid,fx_mid*0),\n#    col=rgb(0,0,1,.25), border=NA)\n\n\nNote that the Continuous Uniform distribution generalizes to an arbitrary interval, \\(X_{i} \\in [a,b]\\). In this case, \\(f(x)=1/[b-a]\\) if \\(x \\in [a,b]\\) and \\(F(x)=[x-a]/[b-a]\\) if \\(x \\in [a,b]\\).\n\n\n\n\n\n\nNote\n\n\n\n\n\nSuppose \\(X_{i}\\) is a random variable continuously distributed over \\(a=-2\\) and \\(b=2\\). What is the probability of a value larger than \\(0.25\\)? First use the computer to suggest an answer: simulate \\(1000\\) draws and then make a histogram and an ECDF. Then find the answer mathematically using the CDF. Finally, verify the answer is intuitively correct in a figure of the PDF. You should draw by hand both the CDF and the PDF with correct axes labels and marking clearly the probability of a value larger than \\(0.25\\).\n\n\nCode\n# Simulation\nX &lt;- runif(1000, -2, 2)\n# ... \n\n# Draw by computer the CDF, F(x)\nx &lt;- seq(-3,3,by=0.005)\nFx &lt;- punif(x, -2, 2)\nplot(x, Fx, type='l', col='blue')\n# Answer\nFstar &lt;- punif(0.25, -2, 2)\n# Visualize Answer\nsegments(0.25, 0, 0.25, Fstar,\n    col=rgb(0,0,1, 0.25))\n\n\n\n\n\n\n\n\n\nCode\n\n# Draw by computer the PDF, f(x)\nfx &lt;- dunif(x, -2, 2)\nplot(x, fx, type='l', col='blue')\n# Visualize Answer\nfstar &lt;- dunif(0.25, -2, 2)\npolygon(\n    c(-2, 0.25, 0.25, -2), \n    c(0, 0, fstar,fstar),\n    col=rgb(0,0,1, 0.25), border=NA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nSuppose the flight time between Calgary and Kamloops is Uniformly distributed between \\(68\\) and \\(78\\) minutes. According to Air Canada the flight takes \\(70\\) minutes. What is the probability that the flight will be late?\n\n\n\n\n\nBeta.\nThe sample space is any number on the unit interval, \\(X_{i} \\in [0,1]\\), but with non-uniform probabilities.\n\n\nCode\nX4 &lt;- rbeta(2000,2,2) ## two shape parameters\nhist(X4, breaks=20, border=NA, main=NA, freq=F)\n\n#See the underlying probabilities\n#f_25 &lt;- dbeta(.25, 2, 2)\n\nx &lt;- seq(0,1,by=.01)\nfx &lt;- dbeta(x, 2, 2)\nlines(x, fx, col='blue')\n\n\n\n\n\n\n\n\n\nThe Beta distribution is mathematically complicated to write, and so we omit it. However, we can find the probability graphically using either the probability density function or cumulative distribution function.\n\n\n\n\n\n\nTip\n\n\n\n\n\nSuppose \\(X_{i}\\) is a random variable with a beta distribution. Intuitively depict \\(Prob(X_{i} \\in [0.2, 0.8])\\) by drawing an area under the density function. Numerically estimate that same probability using the CDF.\n\n\nCode\nplot( ecdf(X4), main=NA) # Empirical\n\nx &lt;- seq(0,1,by=.01) # Theoretical\nFx &lt;- pbeta(x, 2, 2)\nlines(x, Fx, col='blue')\n\n# Middle Interval Example \nF2 &lt;- pbeta(0.2, 2, 2)\nF8 &lt;- pbeta(0.8, 2, 2)\nF_2_8 &lt;- F8 - F2\nF_2_8\n## [1] 0.792\n\n# Visualize\ntitle('Middle between 0.2 and 0.8')\nsegments( 0.2, F2, -1, F2, col='red')\nsegments( 0.8, F8, -1, F8, col='red')\n\n\n\n\n\n\n\n\n\n\n\n\nThis distribution is often used, as the probability density function has two parameters that allow it to take many different shapes.\n\n\n\n\n\n\nTip\n\n\n\n\n\nFor each example below, intuitively depict \\(Prob(X_{i} \\leq 0.5)\\) using the PDF. Repeat the exercise using a CDF instead of a PDF to calculate a numerical value.\n\n\nCode\nop &lt;- par(no.readonly = TRUE); on.exit(par(op), add = TRUE)\nx &lt;- seq(0,1,by=.01)\npars &lt;- expand.grid( c(.5,1,2), c(.5,1,2) )\npar(mfrow=c(3,3))\napply(pars, 1, function(p){\n    fx &lt;- dbeta( x,p[1], p[2])\n    plot(x, fx, type='l', xlim=c(0,1), ylim=c(0,4), lwd=2, col='blue')\n    #hist(rbeta(2000, p[1], p[2]), breaks=50, border=NA, main=NA, freq=F)\n})\ntitle('Beta densities', outer=T, line=-1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExponential.\nThe sample space is any positive number.2 An Exponential random variable has a single parameter, \\(\\lambda&gt;0\\), that governs its shape \\[\\begin{eqnarray}\nX_{i} &\\in& [0,\\infty) \\\\\nf(x) &=& \\lambda exp\\left\\{ -\\lambda x \\right\\} \\\\\nF(x) &=& \\begin{cases}\n    0 & x &lt; 0 \\\\\n    1-  exp\\left\\{ -\\lambda x \\right\\} & x \\geq 0.\n\\end{cases}\n\\end{eqnarray}\\]\n\n\nCode\nrexp(3) # 3 draws\n## [1] 3.1877697 0.2425307 0.3188751\n\nX5 &lt;- rexp(2000)\nhist(X5, breaks=20,\n    border=NA, main=NA,\n    freq=F, ylim=c(0,1), xlim=c(0,10))\n    \nx &lt;- seq(0,10,by=.1)\nfx &lt;- dexp(x)\nlines(x, fx, col='blue')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nSuppose the lifetime of a battery is an exponential random variable with \\(\\lambda=1/50\\). Using the computer, find the probability that the lifetime is \\(&lt; 10\\) hours. Find the probability that the lifetime is \\(\\geq 100\\) hours. Use the computer to find the probability that the lifetime is between \\(10\\) and \\(100\\) hours.\n\n\nCode\npexp(10, 1/50)\n## [1] 0.1812692\n\n\n\n\n\n\n\nNormal (Gaussian).\nThis distribution is for any number on the real line, with bell shaped probabilities. The Normal distribution is mathematically complex and sometimes called the Gaussian distribution. We call it “Normal” because we will encounter it again and again and again. The probability density function \\(f\\) has two parameters \\(\\mu \\in (\\infty,\\infty)\\) and \\(\\sigma &gt; 0\\). \\[\\begin{eqnarray}\nX_{i} &\\in& (\\infty,\\infty) \\\\\nf(x) &=& \\frac{1}{\\sqrt{2\\pi \\sigma^2}} exp\\left\\{ \\frac{-(x-\\mu)^2}{2\\sigma^2} \\right\\}\n\\end{eqnarray}\\]\n\n\nCode\nrnorm(3) # 3 draws\n## [1] -0.5561154 -0.5016472 -1.2277121\n\nX6 &lt;- rnorm(2000)\nhist(X6, breaks=20,\n    border=NA, main=NA,\n    freq=F, ylim=c(0,.4), xlim=c(-4,4))\n\nx &lt;- seq(-10,10,by=.025)\nfx &lt;- dnorm(x)\nlines(x, fx, col='blue')\n\n\n\n\n\n\n\n\n\nEven thought the distribution function is complex, we can compute CDF values using the computer.\n\n\nCode\npnorm( c(-1.645, 1.645) ) # 10%\n## [1] 0.04998491 0.95001509\npnorm( c(-2.576, 2.576) ) #  1%\n## [1] 0.004997532 0.995002468\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nSuppose \\(X_{i}\\) is a random variable with a normal distribution with \\(\\mu=0\\) and \\(\\sigma=1\\). Intuitively depict \\(Prob(X_{i} \\in [0.2, 0.8])\\) by drawing an area under the density function. Numerically estimate that same probability using the CDF.\n\n\nCode\nplot( ecdf(X6), main=NA) # Empirical\n\nx &lt;- seq(-4,4,by=.01) # Theoretical\nFx &lt;- pnorm(x, 0, 1)\nlines(x, Fx, col='blue')\n\n# Middle Interval Example \nF2 &lt;- pnorm(0.2, 0, 1)\nF8 &lt;- pnorm(0.8, 0, 1)\nF_2_8 &lt;- F8 - F2\nF_2_8\n## [1] 0.2088849\n\n# Visualize\ntitle('Middle between 0.2 and 0.8')\nsegments( 0.2, F2, -5, F2, col='red')\nsegments( 0.8, F8, -5, F8, col='red')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nSuppose that your health status is a normally distributed random variable with \\(\\mu=2\\) and \\(\\sigma=3\\). If we randomly sample one person, what is the probability there health status is higher than \\(4\\)?\n\n\nCode\n# Start with a simulation of 1000 people to build intuition\nX &lt;- rnorm(1000,2,3)\nhist(X, freq=F, border=NA, main=NA)\n\n\n\n\n\n\n\n\n\nCode\nsum(X&gt;4)/1000\n## [1] 0.249\n\n# Do an exact calculation\n1-pnorm(4,2,3)\n## [1] 0.2524925\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nDraw the Middle \\(90\\%\\) of a normal distribution\n\n\nCode\n# PDF\nx &lt;- seq(-10,10,by=.025)\nfx &lt;- dnorm(x)\nplot(x, fx,\n    col='blue', type='l',\n    ylab='Theoretical Density: f(x)',\n    main='Middle 90%')\n# Show Middle 90%\nx_90 &lt;- seq(-1.645,1.645,by=.025)\nfx_90 &lt;- dnorm(x_90)\npolygon( c(x_90, rev(x_90)), c(fx_90,fx_90*0),\n    col=rgb(0,0,1,.25), border=NA)\n\n\n\n\n\n\n\n\n\nCode\npnorm(1.645)-pnorm(-1.645)\n## [1] 0.9000302\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nSuppose scores in math class are approximately normally distributed with \\(\\mu=50, \\sigma=1\\). If you selected one student randomly, what is the probability their score is higher than \\(90\\). Is \\(Prob(X_{i}\\geq 90)\\) higher if \\(\\mu=25, \\sigma=2\\)? What about \\(\\mu=10, \\sigma=5\\)?",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "01_04_RandomVariables.html#further-reading",
    "href": "01_04_RandomVariables.html#further-reading",
    "title": "4  Random Variables",
    "section": "4.3 Further Reading",
    "text": "4.3 Further Reading\nNote that many random variables are related to each other\n\nhttps://en.wikipedia.org/wiki/Relationships_among_probability_distributions\nhttps://www.math.wm.edu/~leemis/chart/UDR/UDR.html\nhttps://qiangbo-workspace.oss-cn-shanghai.aliyuncs.com/2018-11-11-common-probability-distributions/distab.pdf\n\nAlso note that numbers randomly generated on your computer cannot be truly random, they are “Pseudorandom”.",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "01_04_RandomVariables.html#footnotes",
    "href": "01_04_RandomVariables.html#footnotes",
    "title": "4  Random Variables",
    "section": "",
    "text": "This is the general formula using CDFs, and you can verify it works in this instance by directly adding the probability of each 2 or 3 event: \\(Prob(X_{i} = 2) +  Prob(X_{i} = 3) = 1/4 + 1/4 = 2/4\\).↩︎\nIn other classes, you may further distinguish types of random variables based on whether their maximum value is theoretically finite or infinite.↩︎",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html",
    "href": "01_05_Statistics.html",
    "title": "5  Statistics",
    "section": "",
    "text": "5.1 Mean and Variance\nWe often summarize distributions with statistics: functions of data. The most basic way to do this is with summary, whose values can all be calculated individually.\nTogether, the mean and variance statistics summarize the central tendency and dispersion of a distribution. In some special cases, such as with the normal distribution, they completely describe the distribution. Other distributions are better described with other statistics, either as an alternative or in addition to the mean and variance. After discussing those other statistics, we will return to the two most basic statistics in theoretical detail.\nThe mean and variance are the two most basic statistics that summarize the center and how spread apart the values are. As before, we represent data as vector \\(\\hat{X}=(\\hat{X}_{1}, \\hat{X}_{2}, ....\\hat{X}_{n})\\), where there are \\(n\\) observations and \\(\\hat{X}_{i}\\) is the value of the \\(i\\)th one.",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html#mean-and-variance",
    "href": "01_05_Statistics.html#mean-and-variance",
    "title": "5  Statistics",
    "section": "",
    "text": "Mean.\nPerhaps the most common statistic is the mean, which is the [sum of all values] divided by [number of values]; \\[\\begin{eqnarray}\n\\hat{M} &=& \\frac{\\sum_{i=1}^{n}\\hat{X}_{i}}{n},\n\\end{eqnarray}\\] where \\(\\hat{X}_{i}\\) denotes the value of the \\(i\\)th observation.\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor example, a dataset of \\(\\{1,4,10\\}\\) has a mean of \\([1+4+10]/3=5\\).\n\n\nCode\nX &lt;- c(1,4,10)\nsum(X)/length(X)\n## [1] 5\nmean(X)\n## [1] 5\n\n\n\n\n\n\n\nCode\n# compute the mean of a random sample\nX1 &lt;- USArrests[,'Murder']\nX1_mean &lt;- mean(X1)\nX1_mean\n## [1] 7.788\n\n# visualize on a histogram\nhist(X1, border=NA, main=NA)\nabline(v=X1_mean, col=2, lwd=2)\ntitle(paste0('mean= ', round(X1_mean,2)), font.main=1)\n\n\n\n\n\n\n\n\n\n\n\nVariance.\nPerhaps the second most common statistic is the variance: the average squared deviation from the mean \\[\\begin{eqnarray}\n\\hat{V} &=&\\frac{\\sum_{i=1}^{n} [\\hat{X}_{i} - \\hat{M}]^2}{n}.\n\\end{eqnarray}\\] The standard deviation is simply \\(\\hat{S} = \\sqrt{\\hat{V} }\\), which can be interpreted as the average distance from the mean.\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor example, a dataset of \\(\\{1,4,10\\}\\) has a mean of \\([1+4+10]/3=5\\). The variance is \\([(1-5)^2+(4-5)^2+(10-5)^2]/3=14\\) and the standard deviation is \\(\\sqrt{14}\\).\n\n\nCode\nX &lt;- c(1,4,10)\nX_mean &lt;- mean(X)\nX_var &lt;- mean( (X - X_mean)^2 )\nsqrt(X_var)\n## [1] 3.741657\n\n\n\n\n\n\n\nCode\nX1_s &lt;- sd(X1) # sqrt(var(X))\nhist(X1, border=NA, main=NA, freq=F)\nX1_s_lh &lt;- c(X1_mean - X1_s,  X1_mean + X1_s)\nabline(v=X1_s_lh, col=4)\ntext(X1_s_lh, -.02,\n    c( expression(bar(X)-s[X]), expression(bar(X)+s[X])),\n    col=4, adj=0)\ntitle(paste0('sd= ', round(X1_s,2)), font.main=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nNote that a “corrected version” is used by R and many statisticians: \\(\\hat{V}' =\\frac{\\sum_{i=1}^{n} [\\hat{X}_{i} - \\hat{M}]^2}{n-1}\\) and \\(\\hat{S}' = \\sqrt{\\hat{V}'}\\). In this class, we use the “unocorrect version” as defined previously. There is hardly any difference when \\(n\\) is large: e.g., \\(\\frac{1}{n}\\approx \\frac{1}{n-1}\\) for \\(n=100,000\\).\n\n\nCode\nX &lt;- c(1,4,10)\n\nX_mean &lt;- mean(X)\nX_var &lt;- mean( (X - X_mean)^2 )\nX_var\n## [1] 14\n\n# Corrected Version\nn &lt;- length(X)  \nX_var2 &lt;- sum( (X - X_mean)^2 )/(n-1)\nX_var2\n## [1] 21\n\nvar(X) # R-Version\n## [1] 21",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html#other-centerspread-statistics",
    "href": "01_05_Statistics.html#other-centerspread-statistics",
    "title": "5  Statistics",
    "section": "5.2 Other Center/Spread Statistics",
    "text": "5.2 Other Center/Spread Statistics\nA general rule of applied statistics is that there are multiple ways to measure something. Mean and Variance are measurements of Center and Spread, but there are others that have different theoretical properties and may be better suited for your dataset.\n\nMedians and Absolute Deviations.\nWe can use the Median as a “robust alternative” to means that is especially useful for data with asymmetric distributions and extreme values. Recall that the \\(q\\)th quantile is the value where \\(q\\) percent of the data are below and (\\(1-q\\)) percent are above. The median (\\(q=.5\\)) is the point where half of the data is lower values and the other half is higher. This means that median is not sensitive to extreme values (whereas the mean is).\n\n\nCode\nX &lt;- rexp(50, 0.4)\nX\n##  [1]  2.99516321  0.54847140  1.67200880  0.28610426  0.66454128  1.90108908\n##  [7]  2.42909965  3.37545438  1.66230620  6.16513407  0.50564456  2.43181155\n## [13]  8.44081135  2.87374191  0.44784890  3.12252597  5.30875360  2.94995183\n## [19]  1.17419179  0.25127888  1.41727894  3.32507180  0.10864295  5.76131656\n## [25] 14.60311290  2.23078648  4.84401994  0.84367128  0.88584361  2.63805991\n## [31]  3.63315348  0.10055005  3.69759307  1.10012706  2.02576645  3.04211289\n## [37]  3.20955086  0.02346518  7.82371269  2.32731487  1.34548805  0.09378121\n## [43]  0.55052488  0.03900745  1.88015966  2.52420331  0.51485764  0.13490447\n## [49]  4.06035884  0.24801571\nhist(X, freq=F, breaks=seq(0,18),\n    border=NA, main='')\n\n\n\n\n\n\n\n\n\nCode\n\n# For discrete numbers\n# X &lt;- rgeom(50, .4)\n#proportions &lt;- table(X)/length(X)\n#plot(proportions, ylab='proportion')\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nExamine robustness to an extreme value\n\n\nCode\n\nX_extreme &lt;- c(X, 1000) # add one extreme value\n#par(mfrow=c(1,2)) # visualize side-by-side\n#hist(X)\n#hist(X_extreme)\n\n\n# Which measures of central tendency are robust\n# to a single extreme value?\nmean(X)\n## [1] 2.484768\nmean( X_extreme )\n## [1] 22.04389\n\nquantile(X, prob=0.5)\n##      50% \n## 1.963428\nquantile(X_extreme, prob=0.5)\n##      50% \n## 2.025766\n\n\n\n\n\nWe can also use the Interquartile Range or Median Absolute Deviation as an alternative to variance. The difference between the first and third quartiles (quantiles \\(q=.25\\) and \\(q=.75\\)) measure is range of the middle \\(50%\\) of the data, which is how the boxplot measures “spread”. The median absolute deviation is another statistic that also measures “spread”. \\[\\begin{eqnarray}\n\\tilde{M} &=& \\text{Med}( \\hat{X}_{i}) \\\\\n\\hat{\\text{MAD}} &=& \\text{Med}\\left( | \\hat{X}_{i} - \\tilde{M} | \\right).\n\\end{eqnarray}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nCompute the \\(IQR\\) statistic for the dataset \\(\\{-100,1,4,10,10\\}\\).\n\\(IQR =\\) Upper quartile \\(-\\) Lower quartile \\(= 10 - 1 = 9\\).\n\n\nCode\nX &lt;- c(-100,1,4,10,10)\n# An intuitive alternative to sd(X), used in the boxplot\nquants &lt;- quantile(X, probs=c(.25,.75))\nquants[2]-quants[1]\n## 75% \n##   9\nIQR(X)\n## [1] 9\n\n\nCompute the \\(\\text{MAD}\\) statistic for the dataset \\(\\{1,4,10\\}\\). First compute the median, \\(\\text{Med}(1,4,10)=1\\). Then compute \\(\\text{Med}( |1-4|,~ |4-4|,~ |10-4| )= \\text{Med}( 3, 0, 6 ) = 3\\).\n\n\nCode\nX &lt;- c(1,4,10)\n#Another alternative to sd(X)\nmad(X, constant=1)\n## [1] 3\n\n# Computationally equivalent\n# x_med &lt;- quantile(X, probs=.5)\n# x_absdev &lt;- abs(X -x_med)\n# quantile(x_absdev, probs=.5)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nCompare the robustness of various “spread” metrics to an extreme value\n\n\nCode\nX &lt;- rgeom(50, .4)\nX_extreme &lt;- c(X, 1000) # add one extreme value\n\nsd(X)\n## [1] 2.545985\nsd(X_extreme)\n## [1] 139.8071\n\nmad(X, constant=1)\n## [1] 1\nmad(X_extreme, constant=1)\n## [1] 1\n\nIQR(X)\n## [1] 2\nIQR(X_extreme)\n## [1] 2.5\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nNote that there other “absolute deviation” statistics\n\n\nCode\n# sometimes seen elsewhere\nmean( abs(X - mean(X)) )\nmean( abs(X - median(X)) )\nmedian( abs(X - mean(X)) )\n\n\n\n\n\n\n\nMode and Share Concentration.\nSometimes, none of the above work well. With categorical data, for example, distributions are easier to describe with other statistics. The mode is the most common observation: the value with the highest observed frequency. We can also measure the spread of the frequencies or concentration at the mode vs elsewhere.\n\n\nCode\n# Draw 3 Random Letters\nx &lt;- LETTERS\nx_probs &lt;- seq(1, length(x))\nx_probs &lt;- x_probs/sum(x_probs) # probs must sum to 1\nX &lt;- sample(x, 1, prob=x_probs, replace=T)\nX\n## [1] \"R\"\n\n# Draw Random Letters 100 Times\nX &lt;- sample(x, 1000, prob=x_probs, replace=T)\nX &lt;- factor(unlist(X), levels=LETTERS, ordered=F)\n\nproportions &lt;- table(X)/length(X)\nplot(proportions, col=grey(0,0.5))\n\n\n\n\n\n\n\n\n\nCode\n\n# mode(s)\nmode_id &lt;- which(proportions==max(proportions))\nnames(proportions)[ mode_id ]\n## [1] \"Z\"\n\n# freq. spreads\nsd(proportions)\n## [1] 0.02128329\nsum(proportions^2)\n## [1] 0.049786\n\n# freq. concentration at mode\nmax(proportions)/mean(proportions)\n## [1] 2.054",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html#shape-statistics",
    "href": "01_05_Statistics.html#shape-statistics",
    "title": "5  Statistics",
    "section": "5.3 Shape Statistics",
    "text": "5.3 Shape Statistics\nCentral tendency and dispersion are often insufficient to describe a distribution. To further describe shape, we can compute skew and kurtosis to measure asymmetry and extreme values. There are many other statistics we could compute on an ad-hoc basis.\n\nSkewness.\nThe skew statistic captures how asymmetric the distribution is by measuring the average cubed deviation from the mean, normalized the standard deviation cubed \\[\\begin{eqnarray}\n\\frac{\\sum_{i=1}^{n} [\\hat{X}_{i} - \\hat{M}]^3 / n}{ s^3 }\n\\end{eqnarray}\\]\n\n\nCode\nX &lt;- rweibull(1000, shape=1)\nhist(X, border=NA, main=NA, freq=F, breaks=20)\n\n\n\n\n\n\n\n\n\nCode\n\nskewness &lt;-  function(X){\n X_mean &lt;- mean(X)\n m3 &lt;- mean((X - X_mean)^3)\n s3 &lt;- sd(X)^3\n skew &lt;- m3/s3\n return(skew)\n}\n\nskewness( rweibull(1000, shape=1))\n## [1] 1.975545\nskewness( rweibull(1000, shape=10) )\n## [1] -0.4896092\n\n\nWe can automatically compare against the normal distribution, which has a skew of 0.\n\n\nKurtosis.\nThis statistic captures how many “outliers” there are like skew but looking at quartic deviations instead of cubed ones. \\[\\begin{eqnarray}\n\\frac{\\sum_{i=1}^{n} [\\hat{X}_{i} - \\hat{M}]^4 / n}{ s^4 }.\n\\end{eqnarray}\\] Some authors further subtract \\(3\\) to explicitly compare against the normal distribution (the normal distribution has a kurtosis of \\(3\\)).\n\n\nCode\nX &lt;- rweibull(1000, shape=1)\nboxplot(X, main=NA)\n\n\n\n\n\n\n\n\n\nCode\n\nkurtosis &lt;- function(X){  \n X_mean &lt;- mean(X)\n m4 &lt;- mean((X - X_mean)^4) \n s4 &lt;- sd(X)^4\n kurt &lt;- m4/s4\n excess_kurt &lt;- kurt - 3 # compare against normal\n return(kurt)\n}\n\nkurtosis( rweibull(1000, shape=1) )\n## [1] 8.597539\nkurtosis( rweibull(1000, shape=10) )\n## [1] 3.392387\n\n\n\n\nClusters/Gaps.\nYou can also describe distributions in terms of how clustered the values are, including the number of modes, bunching, and many other statistics. But remember that “a picture is worth a thousand words”.\n\n\nCode\n# Complicated Random Variable\nr_ugly0 &lt;- function(n, populationMean=c(-2, 3), populationVar=c(1, 4), prob=c(.5,.5)){\n  # Which distribution is each observation coming from\n  di &lt;- sample(c(1,2), size=n, replace=TRUE, prob=prob)\n  rnorm(n, mean=populationMean[di], sd=sqrt(populationVar)[di])\n}\n\nX &lt;- r_ugly0(6000)\nhist(X, breaks=60,\n    freq=F, border=NA,\n    xlab=\"x\", main='')\n\n\n\n\n\n\n\n\n\nCode\n\n#hist( rbeta(1000, .6, .6), border=NA, main=NA, freq=F, breaks=20)\n\n\n\n\nCode\n# Another Complicated Random Variable\nr_ugly1 &lt;- function(n, theta1=c(-8,-1), theta2=c(-2,2), rho=.25){\n    omega   &lt;- rbinom(n, size=1, rho)\n    epsilon &lt;- omega * runif(n, theta1[1], theta2[1]) +\n        (1-omega) * rnorm(n, theta1[2], theta2[2])\n    return(epsilon)\n}\n# Very Large Sample\nX &lt;- r_ugly1(1000000)\nhist(X, breaks=1000,  freq=F, border=NA,\n    xlab=\"x\", main='')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Show True Density\nd_ugly1 &lt;- function(x, theta1=c(-8,-1), theta2=c(-2,2), rho=.25){\n    rho     * dunif(x, theta1[1], theta2[1]) +\n    (1-rho) * dnorm(x, theta1[2], theta2[2]) }\nx &lt;- seq(-12,6,by=.001)\ndx &lt;- d_ugly1(x)\nlines(x, dx, col=1)",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html#probability-theory",
    "href": "01_05_Statistics.html#probability-theory",
    "title": "5  Statistics",
    "section": "5.4 Probability Theory",
    "text": "5.4 Probability Theory\nWe will now dig a little deeper theoretically into the statistics we compute. When we know how the data are generated theoretically, we can often compute the theoretical value of the two most basic and often-used statistics: the mean and variance. To see this, we separately analyze how they are computed for discrete and continuous random variables.\nHere, we denote \\(X_{i}\\) as a random variable that can take on specific values \\(x\\) from the sample space with known probabilities.\n\nDiscrete Random Variables.\nIf the sample space is discrete, we can compute the theoretical mean (or expected value) as \\[\\begin{eqnarray}\n\\mathbb{E}[X_{i}] = \\sum_{x} x Prob(X_{i}=x),\n\\end{eqnarray}\\] where \\(Prob(X_{i}=x)\\) is the probability the random variable \\(X_{i}\\) takes the particular value \\(x\\). Similarly, we can compute the theoretical variance as \\[\\begin{eqnarray}\n\\mathbb{V}[X_{i}] = \\sum_{x} \\left(x - \\mathbb{E}[X_{i}] \\right)^2 Prob(X_{i}=x).\n\\end{eqnarray}\\] The theoretical standard deviation is \\(\\mathbb{s}[X_{i}] = \\sqrt{\\mathbb{V}[X_{i}]}\\).\nFor example, consider a five-sided die with equal probability of landing on each side. I.e., \\(X_{i}\\) is a Discrete Uniform random variable with outcomes \\(x \\in \\{1,2,3,4,5\\}\\). What is the theoretical mean? What is the variance and standard deviation? \\[\\begin{eqnarray}\n\\mathbb{E}[X_{i}] &=& 1\\frac{1}{5} + 2\\frac{1}{5} + 3\\frac{1}{5} + 4\\frac{1}{5} + 5\\frac{1}{5} = 15/5 = 3\\\\\n\\mathbb{V}[X_{i}] &=& (1-3)^2\\frac{1}{5} +\n  (2 - 3)^2\\frac{1}{5} +\n  (3 - 3)^2\\frac{1}{5} +\n  (4 - 3)^2\\frac{1}{5} +\n  (5 - 3)^2\\frac{1}{5} \\\\\n  &=& 2^2 \\frac{1}{5} + 1^2\\frac{1}{5} + 0^2 \\frac{1}{5} + + 1^2\\frac{1}{5} +  2^2 \\frac{1}{5}\n  = (4 + 1 + 1 + 4)/5 = 10/5 = 2\\\\\n\\mathbb{s}[X_{i}] &=& \\sqrt{2}\n\\end{eqnarray}\\]\n\n\nCode\n# Computerized Way to Compute Mean\nx &lt;- c(1,2,3,4,5)\nx_probs &lt;- c(1/5, 1/5, 1/5, 1/5, 1/5)\nX_mean &lt;- sum(x*x_probs)\nX_mean\n## [1] 3\n\n# Computerized Way to Compute SD\nXvar &lt;- sum((x-X_mean)^2*x_probs)\nXvar\n## [1] 2\nXsd &lt;- sqrt(Xvar)\nXsd\n## [1] 1.414214\n\n# Verified by simulation\nXsim &lt;- sample(x, prob=x_probs,\n          size=10000, replace=T)\nmean(Xsim)\n## [1] 3.0066\nsd(Xsim)\n## [1] 1.427712\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor example, consider an unfair coin with a \\(3/4\\) probability of heads (\\(x=1\\)) and a \\(1/4\\) probability of tails (\\(x=0\\)) has a theoretical mean of \\[\\begin{eqnarray}\n\\mathbb{E}[X_{i}] = 0\\frac{1}{4} + 1\\frac{3}{4} = \\frac{3}{4}\n\\end{eqnarray}\\] and a theoretical variance of \\[\\begin{eqnarray}\n\\mathbb{V}[X_{i}] = [0 - \\frac{3}{4}]^2 \\frac{1}{4} + [1 - \\frac{3}{4}]^2 \\frac{3}{4}\n= \\frac{9}{64} + \\frac{3}{64}\n= \\frac{12}{64}\n= \\frac{3}{16}.\n\\end{eqnarray}\\] Verify both the mean and variance by simulation.\n\n\nCode\n# A simulation of many coin flips\nx &lt;- c(0,1)\nx_probs &lt;- c(1/4, 3/4)\nX &lt;- sample(x, 1000, prob=x_probs, replace=T)\n\nround( mean(X), 4)\n## [1] 0.754\n\nround( var(X), 4)\n## [1] 0.1857\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nConsider a four-sided die with outcomes \\(\\{1,2,3,4 \\}\\) and corresponding probabilities \\(\\{ 1/8, 2/8, 1/8, 4/8 \\}\\) . What is the mean? \\[\\begin{eqnarray}\n\\mathbb{E}[X_{i}] = 1 \\frac{1}{8} + 2 \\frac{2}{8} + 3 \\frac{1}{8} + 4 \\frac{4}{8} = 3\n\\end{eqnarray}\\] What is the variance? Verify both the mean and variance by simulation.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nConsider an unfair coin with a \\(p\\) probability of heads (\\(x=1\\)) and a \\(1-p\\) probability of tails (\\(x=0\\)), where \\(p\\) is a parameter between \\(0\\) and \\(1\\). The theoretical mean is \\[\\begin{eqnarray}\n\\mathbb{E}[X_{i}] = 1[p] + 0[1-p] = p\n\\end{eqnarray}\\] and the theoretical variance is \\[\\begin{eqnarray}\n\\mathbb{V}[X_{i}]\n= [1 - p]^2 p + [0 - p]^2 [1-p] = [1 - p]^2 p + p^2 [1-p]\n= [1-p]p\\left( [1-p] + p\\right) = [1-p] p.\n\\end{eqnarray}\\] So the theoretical standard deviation is \\(\\mathbb{s}[X_{i}]=\\sqrt{[1-p] p}\\).\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nSuppose \\(X_{i}\\) is a discrete random variable with this probability mass function: \\[\\begin{eqnarray}\nX_{i} &=& \\begin{cases}\n-1 &  \\text{ with probability } \\frac{1}{2 \\lambda^2} \\\\\n0  & \\text{ with probability } 1-\\frac{1}{\\lambda^2} \\\\\n+1 & \\text{ with probability } \\frac{1}{2\\lambda^2}\n\\end{cases},\n\\end{eqnarray}\\] where \\(\\lambda&gt;0\\) is a parameter. What is the theoretical standard deviation?\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nFor a discrete uniform random variable with th sample space \\(\\{1,2,3,4,5\\}\\), calculate the theoretical median and IQR.\n\n\n\n\n\nWeighted Data.\nSometimes, you may have a dataset of values and probability weights. Othertimes, you can calculate them yourself. In either case, you can explicitly do the computations for discrete data. Given data on unique outcome \\(x\\) and their frequency \\(\\hat{p}_{x}=\\sum_{i=1}^{n}\\mathbf{1}\\left(X_{i}=x\\right)/n\\), we compute \\[\\begin{eqnarray}\n\\hat{M} &=& \\sum_{x} x \\hat{p}_{x}.\n\\end{eqnarray}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nSuppose we flipped a coin \\(100\\) times and found that \\(76\\) were heads and \\(23\\) were tails. The estimated probabilities are \\(76/100\\) for the outcome \\(X_{1}=1\\) and \\(24/100\\) for the outcome \\(X_{i}=0\\). We compute the mean as \\(\\hat{M}=1\\times 0.76 + 0 \\times 0.24 = 0.76\\).\n\n\nCode\n# Compute a sample estimate using probability weights\nP  &lt;- table(X) #table of counts\np &lt;- c(P)/length(X) #frequencies (must sum to 1)\nx &lt;- as.numeric(names(p)) #unique values\ncbind(x,p)\n##   x     p\n## 0 0 0.246\n## 1 1 0.754\n\n# Sample Mean Estimate\nX_mean &lt;- sum(x*p)\nX_mean\n## [1] 0.754\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nTry estimating the sample mean the two different ways for another random sample\n\n\nCode\nx &lt;- c(0,1,2)\nx_probs &lt;- c(1/3,1/3,1/3)\nX  &lt;-  sample(x, prob=x_probs, 1000, replace=T)\n\n# First Way (Computerized)\nmean(X)\n## [1] 1.032\n\n# Second Way (Explicit Calculations)\n# start with a table of counts like the previous example\n\n\n\n\n\nThis idea generalizes the mean to a weighted mean: an average where different values contribute to the final result with varying levels of importance. For each outcome \\(x\\) we have a weight \\(W_{x}\\) and compute \\[\\begin{eqnarray}\n\\hat{M} &=& \\frac{\\sum_{x} x W_{x}}{\\sum_{x} W_{x}} = \\sum_{x} x w_{x},\n\\end{eqnarray}\\] where \\(w_{x}=\\frac{W_{x}}{\\sum_{x'} W_{x'}}\\) is normalized version of \\(W_{x}\\) that implies \\(\\sum_{x}w_{x}=1\\).1\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor example, suppose a student has these scores\n\n\nCode\nHomework1 &lt;- c(score=88, weight=0.25)\nHomework2 &lt;- c(score=92, weight=0.25)\nExam1 &lt;- c(score=67, weight=0.2)\nExam2 &lt;- c(score=90, weight=0.3)\n\nGrades &lt;- rbind(Homework1, Homework2, Exam1, Exam2)\nGrades\n##           score weight\n## Homework1    88   0.25\n## Homework2    92   0.25\n## Exam1        67   0.20\n## Exam2        90   0.30\n\n\nWe can compute the final grade as a weighted mean\n\n\nCode\n# Manual Way\n88*0.25 + 92*0.25 + 67*0.2 + 90*0.3\n## [1] 85.4\n\n# Computerized Way\nValues &lt;- Grades[,'score'] * Grades[,'weight']\nFinalGrade &lt;- sum(Values)\nFinalGrade\n## [1] 85.4\n\n\n\n\n\nThe weighting idea generalizes to other statistics. E.g., we can also computed a weighted variance or weighted quantile.\n\n\n\n\n\n\nTip\n\n\n\n\n\nProvide an example of computing a weighted variance building on this code below\n\n\nCode\nx_diff &lt;- (x - x_mean)^2\np &lt;- P/sum(P)\nx_var &lt;- sum(p * x_diff)\n\n\nSee that we can also compute weighted quantiles\n\n\nCode\nweighted.quantile &lt;- function(x, w, probs){\n    #See spatstat.univar::weighted.quantile\n    oo &lt;- order(x)\n    x &lt;- x[oo]\n    w &lt;- w[oo]\n    Fx &lt;- cumsum(w)/sum(w)\n    quantile_id &lt;- max(which(Fx &lt;= probs))+1\n    xq &lt;- x[quantile_id] \n    return(xq)\n}\n\n## Unweighted\nquantile(X, probs=.5)\n## 50% \n##   1\nweights &lt;- rep(1, length(X))\nweighted.quantile(x=X, w=weights, probs=.5)\n## [1] 1\n\n## Weighted\nweights &lt;- seq(X)\nweights &lt;- weights/sum(weights) # normalize\nweighted.quantile(x=X, w=weights, probs=.5)\n## [1] 1\n\n\n\n\n\n\n\nContinuous Random Variables.\nMany continuous random variables are parameterized by their means and variances. For example, the exponential distribution has parameter \\(\\lambda\\), which corresponds to the theoretical mean \\(1/\\lambda\\) and variance \\(1/\\lambda^2\\). For another example, the two parameters of the normal distribution are \\(\\mu\\) and \\(\\sigma\\), which corresponds to the theoretical mean and variance.\n\n\nCode\n# Exponential Random Variable\nX &lt;- rexp(5000, 2)\nm &lt;- mean(X)\nround(m, 2)\n## [1] 0.5\n\n# Normal Random Variable\nX &lt;- rnorm(5000, 1, 2)\nround(mean(X), 2)\n## [1] 0.94\nround(var(X), 2)\n## [1] 4.01\n\n\n\n\nAdvanced and Optional\n\n\nIf the sample space is continuous, we can compute the theoretical mean (or expected value) as \\[\\begin{eqnarray}\n\\mathbb{E}[X] = \\int x f(x) d x,\n\\end{eqnarray}\\] where \\(f(x)\\) is the probability the random variable takes the particular value \\(x\\). Similarly, we can compute the theoretical variance as \\[\\begin{eqnarray}\n\\mathbb{V}[X_{i}]= \\int \\left(x - \\mathbb{E}[X_{i}] \\right)^2 f(x) d x,\n\\end{eqnarray}\\]\nFor example, consider a random variable with a continuous uniform distribution over \\([-1, 1]\\). In this case, \\(f(x)=1/[1 - (-1)]=1/2\\) for each \\(x \\in [-1, 1]\\) and \\[\\begin{eqnarray}\n\\mathbb{E}[X_{i}] = \\int_{-1}^{1} \\frac{x}{2} d x = \\int_{-1}^{0} \\frac{x}{2} d x + \\int_{0}^{1} \\frac{x}{2} d x = 0\n\\end{eqnarray}\\] and \\[\\begin{eqnarray}\n\\mathbb{V}[X_{i}]= \\int_{-1}^{1} x^2 \\frac{1}{2} d x = \\frac{1}{2} \\frac{x^3}{3}|_{-1}^{1} = \\frac{1}{6}[1 - (-1)] = 2/6 =1/3\n\\end{eqnarray}\\]\n\n\nCode\nX &lt;- USArrests[,'Murder']\nround( mean(X), 4)\n## [1] 7.788\nround( var(X), 4)\n## [1] 18.9705\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nYou can estimate means and variances for continuous random variables with weights, but here we have an additional approximation error\n\n\nCode\n# values and probabilities\nh  &lt;- hist(X, plot=F)\nwt &lt;- h[['counts']]/length(X) \nxt &lt;- h[['mids']]\n# Weighted mean\nX_mean &lt;- sum(wt*xt)\nX_mean\n## [1] 7.8\n\n# Compare to \"mean(x)\"\n\n\nTry it yourself with\n\n\nCode\nX &lt;- runif(2000, -1, 1)",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html#further-reading",
    "href": "01_05_Statistics.html#further-reading",
    "title": "5  Statistics",
    "section": "5.5 Further Reading",
    "text": "5.5 Further Reading\nProbability Theory\n\n[Refresher] https://www.khanacademy.org/math/statistics-probability/probability-library/basic-theoretical-probability/a/probability-the-basics\nhttps://book.stat420.org/probability-and-statistics-in-r.html\nhttps://bookdown.org/speegled/foundations-of-statistics/\nhttps://math.dartmouth.edu/~prob/prob/prob.pdf\nhttps://bookdown.org/probability/beta/discrete-random-variables.html\nhttps://www.econometrics-with-r.org/2.1-random-variables-and-probability-distributions.html\nhttps://probability4datascience.com/ch02.html\nhttps://statsthinking21.github.io/statsthinking21-R-site/probability-in-r-with-lucy-king.html\nhttps://bookdown.org/probability/statistics/\nhttps://www.atmos.albany.edu/facstaff/timm/ATM315spring14/R/IPSUR.pdf\nhttps://rc2e.com/probability\nhttps://bookdown.org/probability/beta/\nhttps://bookdown.org/a_shaker/STM1001_Topic_3/\nhttps://bookdown.org/fsancier/bookdown-demo/\nhttps://bookdown.org/kevin_davisross/probsim-book/\nhttps://bookdown.org/machar1991/ITER/2-pt.html\nhttps://www.atmos.albany.edu/facstaff/timm/ATM315spring14/R/IPSUR.pdf\nhttps://math.dartmouth.edu/~prob/prob/prob.pdf\n\nFor weighted statistics, see\n\nhttps://seismo.berkeley.edu/~kirchner/Toolkits/Toolkit_12.pdf\nhttps://www.bookdown.org/rwnahhas/RMPH/survey-desc.html",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_05_Statistics.html#footnotes",
    "href": "01_05_Statistics.html#footnotes",
    "title": "5  Statistics",
    "section": "",
    "text": "Note that if there are \\(K\\) unique outcomes and \\(W_{x}=1\\) then \\(\\sum_{x}W_{x}=K\\) and \\(w_{x}=1/K\\). This means \\(\\hat{M} = \\sum_{x} x w_{x} = \\sum_{x} x /K\\), which is just a simple mean.↩︎",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html",
    "href": "01_06_Sampling.html",
    "title": "6  (Re)Sampling",
    "section": "",
    "text": "6.1 Sample Distributions\nA sample is a subset of the population. A simple random sample is a sample where each possible sample of size n has the same probability of being selected.\nOften, we think of the population as being infinitely large. This is an approximation that makes mathematical and computational work much simpler.\nIntuition for infinite populations: imagine drawing names from a giant urn. If the urn has only \\(10\\) names, then removing one name slightly changes the composition of the urn, and the probabilities shift for the next name you draw. Now imagine the urn has \\(100\\) billion names, so that removing one makes no noticeable difference. We can pretend the composition never changes: each draw is essentially identical and independent (iid). We can actually guarantee the names are iid by putting any names drawn back into the urn (sampling with replacement).\nThe sampling distribution of a statistic shows us how much a statistic varies from sample to sample.\nFor example, the sampling distribution of the mean shows how the sample mean varies from sample to sample to sample. The sampling distribution of mean can also be referred to as the probability distribution of the sample mean.\nCode\n# Three Sample Example w/ Visual\npar(mfrow=c(1,3))\nfor(b in 1:3){\n    x &lt;- runif(100) \n    m &lt;-  mean(x)\n    hist(x,\n        breaks=seq(0,1,by=.1), #for comparability\n        freq=F, main=NA, border=NA)\n    abline(v=m, col=2, lwd=2)\n    title(paste0('mean= ', round(m,2)),  font.main=1)\n}\nExamine the sampling distribution of the mean\nCode\n# Many sample example\nsample_means &lt;- vector(length=500)\nfor(i in seq_along(sample_means)){\n    x &lt;- runif(1000)\n    m &lt;- mean(x)\n    sample_means[i] &lt;- m\n}\nhist(sample_means, \n    breaks=seq(0.45,0.55,by=.001),\n    border=NA, freq=F,\n    col=2, font.main=1, \n    xlab=expression(hat(M)),\n    main='Sampling Distribution of the mean')\nIn this figure, you see two the most profound results known in statistics",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html#sample-distributions",
    "href": "01_06_Sampling.html#sample-distributions",
    "title": "6  (Re)Sampling",
    "section": "",
    "text": "Note\n\n\n\n\n\nGiven ages for population of \\(4\\) students, compute the sampling distribution for the mean with samples of \\(n=2\\).\n\n\nCode\nX &lt;- c(18,20,22,24) # Ages for student population\n# six possible samples\nm1 &lt;- mean( X[c(1,2)] ) #{1,2}\nm2 &lt;- mean( X[c(1,3)] ) #{1,3}\nm3 &lt;- mean( X[c(1,4)] ) #{3,4}\nm4 &lt;- mean( X[c(2,3)] ) #{2,3}\nm5 &lt;- mean( X[c(2,4)] ) #{2,4}\nm6 &lt;- mean( X[c(3,4)] ) #{3,4}\n# sampling distribution\nsample_means &lt;- c(m1, m2, m3, m4, m5, m6)\nhist(sample_means,\n    freq=F, breaks=100,\n    main='', border=F)\n\n\n\n\n\n\n\n\n\nNow compute the sampling distribution for the median with samples of \\(n=3\\).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nTo consider an infinite population, expand the loop below to consider 3000 samples and then make a histogram.\n\n\nCode\n# Three Sample Example from infinite population\nx1 &lt;- runif(100)\nm1 &lt;- mean(x1)\nx2 &lt;- runif(100)\nm2 &lt;- mean(x2)\nx3 &lt;- runif(100)\nm3 &lt;- mean(x3)\nsample_means &lt;- c(m1, m2, m3)\nsample_means\n## [1] 0.4877457 0.5284887 0.4656983\n\n# An Equivalent Approach: fill vector in a loop\nsample_means &lt;- vector(length=3)\nfor(i in seq(sample_means)){\n    x &lt;- runif(100)\n    m &lt;- mean(x)\n    sample_means[i] &lt;- m\n}\nsample_means\n## [1] 0.4193916 0.4608601 0.5039221\n\n\nFor more on loops, see https://jadamso.github.io/Rbooks/01_02_Mathematics.html#loops.\n\n\n\n\n\n\n\n\nLaw of Large Numbers: the sample mean is centered around the true mean, and more tightly centered with more data\nCentral Limit Theorem: the sampling distribution of the mean is approximately Normal.\n\n\nLaw of Large Numbers.\nThere are different variants of the Law of Large Numbers (LLN), but they all say some version of “the sample mean is centered around the true mean, and more tightly centered with more data”.\n\n\n\n\n\n\nNote\n\n\n\n\n\nNotice where the sampling distribution is centered\n\n\nCode\nm_LLLN &lt;- mean(sample_means)\nround(m_LLLN, 3)\n## [1] 0.5\n\n\nand more tightly centered with more data\n\n\nCode\npar(mfrow=c(1,3))\nfor(n in c(5,50,500)){\n    sample_means_n &lt;- vector(length=299)\n    for(i in seq_along(sample_means_n)){\n        x &lt;- runif(n)\n        m &lt;- mean(x)\n        sample_means_n[i] &lt;- m\n    }\n    hist(sample_means_n, \n        breaks=seq(0,1,by=.01),\n        border=NA, freq=F,\n        col=2, font.main=1, \n        xlab=expression(hat(M)),\n        main=paste0('n=',n) )\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nPlot the variability of the sample mean as a function of sample size\n\n\nCode\nn_seq &lt;- seq(1, 40)\nsd_seq &lt;- vector(length=length(n_seq))\nfor(n in seq_along(sd_seq)){\n    sample_means_n &lt;- vector(length=499)\n    for(i in seq_along(sample_means_n)){\n        x &lt;- runif(n)\n        m &lt;- mean(x)\n        sample_means_n[i] &lt;- m\n    }\n    sd_seq[n] &lt;- sd(sample_means_n)\n}\nplot(n_seq, sd_seq, pch=16, col=grey(0,0.5),\n    xlab='n', ylab='sd of sample means', main='')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem.\nThere are different variants of the central limit theorem (CLT), but they all say some version of “the sampling distribution of a statistic is approximately normal”. For example, the sampling distribution of the mean, shown above, is approximately normal.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nCode\nhist(sample_means,\n    breaks=seq(0.45,0.55,by=.001),\n    border=NA, freq=F,\n    col=2, font.main=1,\n    xlab=expression(hat(M)),\n    main='Sampling Distribution of the mean')\n    \n## Approximately normal?\nmu &lt;- mean(sample_means)\nmu_sd &lt;- sd(sample_means)\nx &lt;- seq(0.1, 0.9, by=0.001)\nfx &lt;- dnorm(x, mu, mu_sd)\nlines(x, fx, col='red')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nFor an example with another statistic, let’s the sampling distribution of the standard deviation.\n\n\nCode\n# CLT example of the \"sd\" statistic\nsample_sds &lt;- vector(length=1000)\nfor(i in seq_along(sample_sds)){\n    x &lt;- runif(100) # same distribution\n    s &lt;- sd(x) # different statistic\n    sample_sds[i] &lt;- s\n}\nhist(sample_sds,\n    breaks=seq(0.2,0.4,by=.01),\n    border=NA, freq=F,\n    col=4, font.main=1,\n    xlab=expression(hat(S)),\n    main='Sampling Distribution of the sd')\n\n## Approximately normal?\nmu &lt;- mean(sample_sds)\nmu_sd &lt;- sd(sample_sds)\nx &lt;- seq(0.1, 0.9, by=0.001)\nfx &lt;- dnorm(x, mu, mu_sd)\nlines(x, fx, col='blue')\n\n\n\n\n\n\n\n\n\nCode\n\n# Try another function, such as\nmy_function &lt;- function(x){ diff(range(exp(x))) }\n\n# try another random variable, such as rexp(100) instead of runif(100)\n\n\n\n\n\nIt is beyond this class to prove this result mathematically, but you should know that not all sampling distributions are standard normal. The CLT approximation is better for “large \\(n\\)” datasets with “well behaved” variances. The CLT also does not apply to “extreme” statistics.\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor example of “extreme” statistics, examine the sampling distribution of min and max statistics.\n\n\nCode\n# Create 300 samples, each with 1000 random uniform variables\nx_samples &lt;- matrix(nrow=300, ncol=1000)\nfor(i in seq(1,nrow(x_samples))){\n    x_samples[i,] &lt;- runif(1000)\n}\n# Each row is a new sample\nlength(x_samples[1,])\n## [1] 1000\n\n# Compute min and max for each sample\nx_mins &lt;- apply(x_samples, 1, quantile, probs=0)\nx_maxs &lt;- apply(x_samples, 1, quantile, probs=1)\n\n# Plot the sampling distributions of min, median, and max\n# Median looks normal. Maximum and Minumum do not!\npar(mfrow=c(1,2))\nhist(x_mins, breaks=100, main='Min', font.main=1,\n    xlab=expression(hat(Q)[0]), border=NA, freq=F)\nhist(x_maxs, breaks=100, main='Max', font.main=1,\n    xlab=expression(hat(Q)[1]), border=NA, freq=F)\ntitle('Sampling Distributions', outer=T, line=-1, adj=0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nHere is an example where variance is not “well behaved” .\n\n\nCode\nsample_means &lt;- vector(length=999)\nfor(i in seq_along(sample_means)){\n    x &lt;- rcauchy(1000,0,10)\n    m &lt;- mean(x)\n    sample_means[i] &lt;- m\n} )\nhist(sample_means, breaks=100,\n    main='',\n    border=NA, freq=F) # Tails look too \"fat\"\n\n\n\n\n\n\n\nThe Fundamental Theorem of Statistics.\nThe Law of Large Numbers generalizes to many other statistics, like median or sd.1 In fact, the Glivenko-Cantelli Theorem (GCT) shows entire empirical distribution converges: the ECDF gets increasingly close to the CDF as the sample sizes grow. This result is often termed the The Fundamental Theorem of Statistics.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 3))\n\nfor (n in c(50, 500, 5000)) {\n  x &lt;- runif(n)\n  Fx &lt;- ecdf(x)\n  plot(Fx)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,3))\nfor(n in c(5,50,500)){\n    sample_quants_n &lt;- vector(length=299)\n    for(i in seq_along(sample_quants_n)){\n        x &lt;- runif(n)\n        m &lt;- quantile(x, probs=0.75) #upper quartile\n        sample_quants_n[i] &lt;- m\n    }\n    hist(sample_quants_n, \n        breaks=seq(0,1,by=.01),\n        border=NA, freq=F,\n        col=2, font.main=1, \n        xlab=\"Sample Quantile\",\n        main=paste0('n=',n) )\n}",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html#resampling",
    "href": "01_06_Sampling.html#resampling",
    "title": "6  (Re)Sampling",
    "section": "6.2 Resampling",
    "text": "6.2 Resampling\nOften, we only have one sample. How then can we estimate the sampling distribution of a statistic?\n\n\nCode\nsample_dat &lt;- USArrests[,'Murder']\nsample_mean &lt;- mean(sample_dat)\nsample_mean\n## [1] 7.788\n\n\nWe can “resample” our data. Hesterberg (2015) provides a nice illustration of the idea. The two most basic versions are the jackknife and the bootstrap, which are discussed below.\n\n\n\n\n\n\n\n\n\nNote that we do not use the mean of the resampled statistics as a replacement for the original estimate. This is because the resampled distributions are centered at the observed statistic, not the population parameter. (The bootstrapped mean is centered at the sample mean, for example, not the population mean.) This means that we cannot use resampling to improve on \\(\\hat{M}\\). We use resampling to estimate sampling variability.\n\nJackknife Distribution.\nHere, we compute all “leave-one-out” estimates. Specifically, for a dataset with \\(n\\) observations, the jackknife uses \\(n-1\\) observations other than \\(i\\) for each unique subsample.\n\n\n\n\n\n\nNote\n\n\n\n\n\nTaking the mean, for example, we have\n\njackknifed estimates: \\(\\hat{M}_{i}^{\\text{jack}}=\\frac{1}{n-1} \\sum_{j \\neq i}^{n-1} \\hat{X}_{j}\\).\nmean of the jackknife: \\(\\bar{\\hat{M}}^{\\text{jack}}=\\frac{1}{n} \\sum_{i}^{n} \\hat{M}_{i}^{\\text{jack}}\\).\nstandard deviation of the jackknife: \\(\\hat{S}^{\\text{jack}}= \\sqrt{ \\frac{1}{n} \\sum_{i}^{n} \\left[\\hat{M}_{i}^{\\text{jack}} - \\bar{\\hat{M}}^{\\text{jack}} \\right]^2 }\\).\n\nGiven the sample \\(\\{1,6,7,22\\}\\), compute the jackknife estimate of the median. Show the result mathematically by hand and also with the computer.\n\n\n\n\n\nCode\nsample_dat &lt;- USArrests[,'Murder']\nsample_mean &lt;- mean(sample_dat)\n\n# Jackknife Estimates\nn &lt;- length(sample_dat)\njackknife_means &lt;- vector(length=n)\nfor(i in seq_along(jackknife_means)){\n    dat_noti &lt;- sample_dat[-i]\n    mean_noti &lt;- mean(dat_noti)\n    jackknife_means[i] &lt;- mean_noti\n}\nhist(jackknife_means, breaks=25,\n    border=NA, freq=F,\n    main='', xlab=expression(hat(M)[-i]))\nabline(v=sample_mean, col='red', lty=2)\n\n\n\n\n\n\n\n\n\n\n\nBootstrap Distribution.\nHere, we draw \\(n\\) observations with replacement from the original data to create a bootstrap sample and calculate a statistic. Each bootstrap sample \\(b=1...B\\) uses a random subset of observations to compute a statistic. We repeat that many times, say \\(B=9999\\), to estimate the sampling distribution.\n\n\n\n\n\n\nNote\n\n\n\n\n\nTaking the mean, for example, we have\n\nbootstrapped estimate: \\(\\hat{M}_{b}^{\\text{boot}}= \\frac{1}{n} \\sum_{i=1}^{n} \\hat{X}_{i}^{(b)}\\), for resampled data \\(\\hat{X}_{i}^{(b)}\\)\nmean of the bootstrap: \\(\\bar{\\hat{M}}^{\\text{boot}}= \\frac{1}{B} \\sum_{b} \\hat{M}_{b}^{\\text{boot}}\\).\nstandard deviation of the bootstrap: \\(\\hat{S}^{\\text{boot}}= \\sqrt{ \\frac{1}{B} \\sum_{b=1}^{B} \\left[\\hat{M}_{b}^{\\text{boot}} - \\bar{\\hat{M}}^{\\text{boot}} \\right]^2}\\).\n\nGiven the sample \\(\\{1,6,7,22\\}\\), compute the bootstrap estimate of the median, with \\(B=8\\). Show the result with the computer and then show what is happening in each sample by hand.\n\n\n\n\n\nCode\n# Bootstrap estimates\nbootstrap_means &lt;- vector(length=9999)\nfor(b in seq_along(bootstrap_means)){\n    dat_id &lt;- seq(1,n)\n    boot_id &lt;- sample(dat_id , replace=T)\n    dat_b  &lt;- sample_dat[boot_id] # c.f. jackknife\n    mean_b &lt;- mean(dat_b)\n    bootstrap_means[b] &lt;-mean_b\n}\n\nhist(bootstrap_means, breaks=25,\n    border=NA, freq=F,\n    main='', xlab=expression(hat(M)[b]))\nabline(v=sample_mean, col='red', lty=2)\n\n\n\n\n\n\n\n\n\nWhy does this work? The sample: \\(\\{\\hat{X}_{1}, \\hat{X}_{2}, ... \\hat{X}_{n}\\}\\) is drawn from a CDF \\(F\\). Each bootstrap sample: \\(\\{\\hat{X}_{1}^{(b)}, \\hat{X}_{2}^{(b)}, ... \\hat{X}_{n}^{(b)}\\}\\) is drawn from the ECDF \\(\\hat{F}\\). With \\(\\hat{F} \\approx F\\), each bootstrap sample is approximately a random sample. So when we compute a statistic on each bootstrap sample, we approximate the sampling distribution of the statistic.\n\n\n\n\n\n\nTip\n\n\n\n\n\nHere is an intuitive example with Bernoulli random variables (unfair coin flips)\n\n\nCode\n# theoretical probabilities\nx &lt;- c(0,1)\nx_probs &lt;- c(1/4, 3/4)\n# sample draws\ncoin_sample &lt;- sample(x, prob=x_probs, 1000, replace=T)\nFhat &lt;- ecdf(coin_sample)\n\nx_probs_boot &lt;- c(Fhat(0), 1-Fhat(0))\nx_probs_boot # approximately the theoretical value\n## [1] 0.239 0.761\ncoin_resample &lt;- sample(x, prob=x_probs_boot, 999, replace=T)\n# any draw from here is almost the same as the original process\n\n\n\n\n\nUsing either the bootstrap or jackknife distribution, we can estimate variability via the Standard Error: the standard deviation of your statistic across different samples (square rooted). In either case, this differs from the standard deviation of the data within your sample.\n\n\nCode\nsd(sample_dat) # standard deviation\n## [1] 4.35551\nsd(bootstrap_means) # standard error\n## [1] 0.6040716\n\n\nAlso note that each additional data point you have provides more information, which ultimately decreases the standard error of your estimates. This is why statisticians will often recommend that you to get more data. However, the improvement in the standard error increases at a diminishing rate. In economics, this is known as diminishing returns and why economists may recommend you do not get more data.\n\n\nCode\nB &lt;- 1000 # number of bootstrap samples\nNseq &lt;- seq(1,100, by=1) # different sample sizes\n\nSE &lt;- vector(length=length(Nseq))\nfor(n in Nseq){\n    sample_statistics_n &lt;- vector(length=B)\n    for(b in seq(1,B)){\n        x_b &lt;- rnorm(n) # Sample of size n\n        x_stat_b &lt;- quantile(x_b, probs=.4) # Stat of interest\n        sample_statistics_n[b] &lt;- x_stat_b\n    }\n    se_n &lt;- sd(sample_statistics_n) # How much the stat varies across samples\n    SE[n] &lt;- se_n\n}\n\nplot(Nseq, SE, pch=16, col=grey(0,.5),\n    ylab='standard error', xlab='sample size')",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html#probability-theory",
    "href": "01_06_Sampling.html#probability-theory",
    "title": "6  (Re)Sampling",
    "section": "6.3 Probability Theory",
    "text": "6.3 Probability Theory\n\nMeans.\nThe LLN follows from a famous theoretical result in statistics, Linearity of Expectations: the expected value of a sum of random variables equals the sum of their individual expected values. To be concrete, suppose we take \\(n\\) random variables, each one denoted as \\(X_{i}\\). Then, for constants \\(a,b,1/n\\), we have \\[\\begin{eqnarray}\n\\mathbb{E}[a X_{1}+ b X_{2}] &=& a \\mathbb{E}[X_{1}]+ b \\mathbb{E}[X_{2}]\\\\\n\\mathbb{E}\\left[M\\right] &=& \\mathbb{E}\\left[ \\sum_{i=1}^{n} X_{i}/n \\right] = \\sum_{i=1}^{n} \\mathbb{E}[X_{i}]/n\n\\end{eqnarray}\\] Assuming each data point has identical means; \\(\\mathbb{E}[X_{i}]=\\mu\\), the expected value of the sample average is the mean; \\(\\mathbb{E}\\left[M\\right] = \\sum_{i=1}^{n} \\mu/n = \\mu\\).\nNote that the estimator \\(M\\) differs from the particular estimate you calculated for your sample, \\(\\hat{M}\\). For example, consider flipping a coin three times: \\(M\\) corresponds to a theoretical value before you flip the coins and \\(\\hat{M}\\) corresponds to a specific value after you flip the coins.\n\n\n\n\n\n\nTip\n\n\n\n\n\nFor example, consider a coin flip with Heads \\(X_{i}=1\\) having probability \\(p\\) and Tails \\(X_{i}=0\\) having probability \\(1-p\\). First notice that \\(\\mathbb{E}[X_{i}]=p\\). Then notice we can first \\[\\begin{align*}\n\\mathbb{E}[X_{1}+X_{2}]\n&= [1+1][p \\times p] + [1+0][p \\times (1-p)] + [0+1][(1-p) \\times p] + [0+0][(1-p) \\times (1-p)] &  \\text{``HH + HT + TH + TT''} \\\\\n&= [1][p \\times p] + [1][p \\times (1-p)] + [0][(1-p) \\times p] + [0][(1-p) \\times (1-p)] &  \\text{first outcomes times prob.} \\\\\n&+ [1][p \\times p] + [0][p \\times (1-p)] + [1][(1-p) \\times p] + [0][(1-p) \\times (1-p)] &\n\\text{+second outcomes times prob.} \\\\\n&= [1][p \\times p] + [1][p \\times (1-p)] + [1][p \\times p] + [1][(1-p) \\times p] & \\text{drop zeros}\\\\\n&= 1p (p + [1-p]) +  1p (p + [1-p]) = p+p & \\text{algebra}\\\\\n&= \\mathbb{E}[X_{1}] +  \\mathbb{E}[X_{2}] .\n\\end{align*}\\] The theoretical mean is \\(\\mathbb{E}[\\frac{X_{1}+X_{2}}{2}]=\\frac{p+p}{2}=p\\).\n\n\n\n\n\nVariances.\nAnother famous theoretical result in statistics is that if we have independent and identical data (i.e., that each random variable \\(X_{i}\\) has the same mean \\(\\mu\\), same variance \\(\\sigma^2\\), and is drawn without any dependence on the previous draws), then the standard error of the sample mean is “root n” proportional to the theoretical standard error. Intuitively, this follows from thinking of the variance as a type of mean (the mean squared deviation from \\(\\mu\\)). \\[\\begin{eqnarray}\n\\mathbb{V}\\left( M \\right)\n&=& \\mathbb{V}\\left( \\frac{\\sum_{i}^{n} X_{i}}{n} \\right)\n= \\sum_{i}^{n} \\mathbb{V}\\left(\\frac{X_{i}}{n}\\right)\n= \\sum_{i}^{n} \\frac{\\sigma^2}{n^2}\n= \\sigma^2/n\\\\\n\\mathbb{s}\\left(M\\right) &=& \\sqrt{\\mathbb{V}\\left( M \\right) } = \\sqrt{\\sigma^2/n} = \\sigma/\\sqrt{n}.\n\\end{eqnarray}\\]\nNote that the standard deviation refers to variance within a single sample, and is hence different from the standard error. Nonetheless, it can be used to estimate the variability of the mean: we can estimate \\(\\mathbb{s}\\left(M\\right)\\) with \\(\\hat{S}/\\sqrt{n}\\), where \\(\\hat{S}\\) is the standard deviation of the sample. This estimate is often a little different from than the bootstrap estimate, as it is based on idealistic theoretical assumptions whereas the bootstrap estimate is driven by data that are often not ideal.\n\n\nCode\nboot_se &lt;- sd(bootstrap_means)\n\ntheory_se &lt;- sd(sample_dat)/sqrt(n)\n\nc(boot_se, theory_se)\n## [1] 0.6040716 0.4355510\n\n\n\n\nShape.\nSometimes, the sampling distribution is approximately normal (according to the CLT). In this case, you can use a standard error and the normal distribution to get a confidence interval.\n\n\nCode\n# Standard Errors\ntheory_sd &lt;- sd(sample_dat1)/sqrt(length(sample_dat1))\n## Normal CI\ntheory_quantile &lt;- qnorm(c(0.025, 0.975))\ntheory_ci &lt;- mean(sample_dat1) + theory_quantile*theory_sd\n\n# compare with: boot_ci, jack_ci",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html#further-reading",
    "href": "01_06_Sampling.html#further-reading",
    "title": "6  (Re)Sampling",
    "section": "6.4 Further Reading",
    "text": "6.4 Further Reading\nSee\n\nhttps://dlsun.github.io/probability/linearity.html",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_06_Sampling.html#footnotes",
    "href": "01_06_Sampling.html#footnotes",
    "title": "6  (Re)Sampling",
    "section": "",
    "text": "When a statistic converges in probability to the quantity they are meant to estimate, they are called consistent.↩︎",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>(Re)Sampling</span>"
    ]
  },
  {
    "objectID": "01_07_Intervals.html",
    "href": "01_07_Intervals.html",
    "title": "7  Intervals",
    "section": "",
    "text": "7.1 Confidence Intervals\nWe can also estimate variability using a Confidence Interval: range your statistic varies across different samples.",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Intervals</span>"
    ]
  },
  {
    "objectID": "01_07_Intervals.html#confidence-intervals",
    "href": "01_07_Intervals.html#confidence-intervals",
    "title": "7  Intervals",
    "section": "",
    "text": "Percentile Intervals.\nThis type of confidence interval is simply the upper and lower quantiles of the sampling distribution.\nFor example, consider the sample mean. We simulate the sampling distribution of the sample mean and construct a \\(90\\%\\) confidence interval by taking the \\(5^{th}\\) and \\(95^{th}\\) percentiles of the simulated means.\n\n\nCode\n# Create 300 samples, each with 1000 random uniform variables\nx_samples &lt;- matrix(nrow=300, ncol=1000)\nfor(i in seq(1,nrow(x_samples))){\n    x_samples[i,] &lt;- runif(1000)\n}\nsample_means &lt;- apply(x_samples, 1, mean) # mean for each sample (row)\n\n# Middle 90%\nmq &lt;- quantile(sample_means, probs=c(.05,.95))\npaste0('we are 90% confident that the mean is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\n## [1] \"we are 90% confident that the mean is between 0.49 and 0.51\"\n\nhist(sample_means,\n    breaks=seq(.4,.6, by=.001), \n    border=NA, freq=F,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='90% Confidence Interval for the Mean')\nabline(v=mq)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor another example, consider the median. We now repeat the above process to estimate the median for each sample, instead of the mean.\n\n\nCode\n## Sample Quantiles (medians)\nsample_quants &lt;- apply(x_samples, 1, quantile, probs=0.5) #quantile for each sample (row)\n\n# Middle 90% of estimates\nmq &lt;- quantile(sample_quants, probs=c(.05,.95))\npaste0('we are 90% confident that the median is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\n## [1] \"we are 90% confident that the median is between 0.47 and 0.52\"\n\nhist(sample_quants,\n    breaks=seq(.4,.6, by=.001),\n    border=NA, freq=F,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='90% Confidence Interval for the Median')\nabline(v=mq)\n\n\n\n\n\n\n\n\n\n\n\n\nNote that \\(Z\\%\\) confidence intervals do not generally cover \\(Z\\%\\) of the data (those types of intervals are covered later). In the examples above, notice the confidence interval for the mean differs from the confidence interval of the median, and so both cannot cover \\(90\\%\\) of the data. The confidence interval for the mean is roughly \\([0.48, 0.52]\\), which theoretically covers only a \\(0.52-0.48=0.04\\) proportion of uniform random data, much less than the proportion 0.9.\n\n\nInterval Size.\nConfidence intervals shrink with more data, as averaging washes out random fluctuations. Here is the intuition for estimating the weight of an apple:\n\nWith \\(n=1\\) apple, your estimate depends entirely on that one draw. If it happens to be unusually large or small, your estimate can be far off.\nWith \\(n=2\\) apples, the estimate averages out their idiosyncrasies. An unusually heavy apple can be balanced by a lighter one, lowering how far off you can be. You are less likely to get two extreme values than just one.\nWith \\(n=100\\) apples, individual apples barely move the needle. The average becomes stable.\n\n\n\nCode\nX &lt;- c(18,20,22,24) #student ages\n# six possible samples of size 2\nm1 &lt;- mean( X[c(1,2)] ) #{1,2}\nm2 &lt;- mean( X[c(1,3)] ) #{1,3}\nm3 &lt;- mean( X[c(1,4)] ) #{3,4}\nm4 &lt;- mean( X[c(2,3)] ) #{2,3}\nm5 &lt;- mean( X[c(2,4)] ) #{2,4}\nm6 &lt;- mean( X[c(3,4)] ) #{3,4}\nmeans_2 &lt;- c(m1, m2, m3, m4, m5, m6)\nsd(means_2)\n## [1] 1.414214\n\n# four possible samples of size 3\nm1 &lt;- mean( X[c(1,2,3)] ) \nm2 &lt;- mean( X[c(1,2,4)] ) \nm3 &lt;- mean( X[c(1,3,4)] ) \nm4 &lt;- mean( X[c(2,3,4)] ) \nmeans_3 &lt;- c(m1, m2, m3, m4)\nsd(means_3)\n## [1] 0.860663\n\n\n\n\nCode\n# Create 300 samples, each of size n\nn &lt;- 10000\nx_samples &lt;- matrix(nrow=300, ncol=n)\nfor(i in seq(1,nrow(x_samples))){\n    x_samples[i,] &lt;- runif(1000)\n}\n# Compute means for each row (for each sample)\nsample_means &lt;- apply(x_samples, 1, mean)\nmq &lt;- quantile(sample_means, probs=c(.05,.95))\npaste0('we are 90% confident that the mean is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\nhist(sample_means,\n    breaks=seq(.4,.6, by=.001), \n    border=NA, freq=F,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='90% Confidence Interval for the Mean (larger n)')\nabline(v=mq)\n\n\nNote that both resampling methods provide imperfect estimates, and can give different numbers. Jackknife resamples are systematically less variable than they should be and sample \\(n-1\\) instead of \\(n\\). Bootstrap resamples have the right \\(n\\) but often have duplicated data. Until you know more, a conservative approach is to take the larger estimate (often the bootstrap). That is also good advice when considering theoretically derived confidence intervals too.\n\n\nCode\nsample_dat &lt;- USArrests[,'Murder']\nsample_mean &lt;- mean(sample_dat)\nsample_mean\n## [1] 7.788\n\n# Jackknife Distribution\nn &lt;- length(sample_dat)\njackknife_means &lt;- vector(length=n)\nfor(i in seq_along(jackknife_means)){\n    dat_noti &lt;- sample_dat[-i]\n    mean_noti &lt;- mean(dat_noti)\n    jackknife_means[i] &lt;- mean_noti\n}\n\n# Bootstrap Distribution\nset.seed(1) # to be replicable\nbootstrap_means &lt;- vector(length=9999)\nfor(b in seq_along(bootstrap_means)){\n    dat_id &lt;- seq(1,n)\n    boot_id &lt;- sample(dat_id , replace=T)\n    dat_b  &lt;- sample_dat[boot_id] # c.f. jackknife\n    mean_b &lt;- mean(dat_b)\n    bootstrap_means[b] &lt;-mean_b\n}\n\n# Jack CI\njack_ci &lt;- quantile(jackknife_means, probs=c(.025, .975))\njack_ci\n##     2.5%    97.5% \n## 7.621582 7.904082\n\n# Boot CI\nboot_ci &lt;- quantile(bootstrap_means, probs=c(.025, .975))\nboot_ci\n##   2.5%  97.5% \n## 6.6039 8.9420\n\n# more conservative estimate\nci &lt;- boot_ci",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Intervals</span>"
    ]
  },
  {
    "objectID": "01_07_Intervals.html#hypothesis-testing",
    "href": "01_07_Intervals.html#hypothesis-testing",
    "title": "7  Intervals",
    "section": "7.2 Hypothesis Testing",
    "text": "7.2 Hypothesis Testing\nIn this section, we test hypotheses using data-driven methods that assume much less about the data generating process. There are two main ways to conduct a hypothesis test to do so: inverting a confidence interval and imposing the null. The first treats the distribution of estimates directly; the second explicitly enforces the null hypothesis to evaluate how unusual the observed statistic is. Both approaches rely on the bootstrap: resampling the data to approximate sampling variability. The most typical case is hypothesizing about about the mean.\n\nInvert a CI.\nOne main way to conduct hypothesis tests is to examine whether a confidence interval contains a hypothesized value. We then use this decision rule\n\nreject the null if value falls outside of the interval\nfail to reject the null if value falls inside of the interval\n\nWe typically use a \\(95\\%\\) confidence interval to create a rejection region.\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor example, suppose you hypothesize the mean is \\(9\\). You then construct a bootstrap distribution with \\(95\\%\\) confidence interval, and find your hypothesized value falls outside of the confidence interval. Then, after accounting for sampling variability (which you estimate), it still seems extremely unlikely that the theoretical mean actually equals \\(9\\), so you reject that that hypothesis. (If the theoretical value landed in the interval, you would “fail to reject” the theoretical mean equals \\(9\\).)\n\n\nCode\nhist(bootstrap_means, breaks=25,\n    border=NA,\n    main='',\n    xlab='Bootstrap Samples')\n# CI\nci_95 &lt;- quantile(bootstrap_means, probs=c(.025, .975))\nabline(v=ci_95, lwd=2)\n# H0: mean=9\nabline(v=9, col=2, lwd=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThe above procedure also generalizes to many other statistics. Perhaps the most informative additional statistics for spread or shape. E.g., you can conduct hypothesis tests for sd and IQR, or skew and kurtosis.\n\n\nCode\n# Bootstrap Distribution for SD\nsd_obs &lt;- sd(sample_dat)\nbootstrap_sd &lt;- vector(length=999)\nfor(b in seq_along(bootstrap_sd)){\n    x_b &lt;- sample(sample_dat, replace=T)\n    sd_b &lt;- sd(x_b)\n    bootstrap_sd[b] &lt;- sd_b\n}\n\n# Test for SD Differences (Invert CI)\nsd_null &lt;- 3.6\nhist(bootstrap_sd, freq=F,\n    border=NA, xlab='Bootstrap', font.main=1,\n    main='Standard Deviations (Invert CI)')\nsd_ci &lt;- quantile(bootstrap_sd, probs=c(0.25,.975) )\nabline(v=sd_ci, lwd=2)\nabline(v=sd_null, lwd=2, col=2)\n\n\n\n\n\n\n\n\n\nTo better your understanding, try redoing the above for any function (such as IQR(x_b)/median(x_b))\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nSuppose you scored \\(83\\%\\) on your exam with \\(50\\) questions, but think you are really a \\(90\\%\\) student. Explain how you might test your hypothesis to your professor who insists your claim be supported by evidence. What would be the issue if we could not reject your hypothesis? Provide a computer simulation illustrating the issue.\n\n\n\n\n\nImpose the Null.\nWe can also compute a null distribution: the sampling distribution of the statistic under the null hypothesis (assuming your null hypothesis was true). We use the bootstrap to loop through a large number of “resamples”. In each iteration of the loop, we impose the null hypothesis and re-estimate the statistic of interest. We then calculate the range of the statistic across all resamples and compare how extreme the original value we observed is.\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor example, suppose you hypothesize the mean is \\(9\\). You then construct a \\(95\\%\\) confidence interval around the null bootstrap distribution (resamples centered around \\(9\\)). If your sample mean falls outside of that interval, then even after accounting for sampling variability (which you estimate), it seems extremely unlikely that the theoretical mean actually equals \\(9\\), so you reject that that hypothesis. (If the sample mean landed in the interval, you would “fail to reject” the theoretical mean equals \\(9\\).)\n\n\nCode\nsample_dat &lt;- USArrests[,'Murder']\nsample_mean &lt;- mean(sample_dat)\n\n# Bootstrap NULL: mean=9\n# Bootstrap shift: center each bootstrap resample so that the distribution satisfies the null hypothesis on average.\nset.seed(1)\nmu &lt;- 9\nbootstrap_means_null &lt;- vector(length=999)\nfor(b in seq_along(bootstrap_means_null)){\n    dat_b &lt;- sample(sample_dat, replace=T) \n    mean_b &lt;- mean(dat_b) + (mu - sample_mean) # impose the null via Bootstrap shift\n    bootstrap_means_null[b] &lt;- mean_b\n}\nhist(bootstrap_means_null, breaks=25, border=NA,\n    main='',\n    xlab='Null Bootstrap Samples')\nci_95 &lt;- quantile(bootstrap_means_null, probs=c(.025, .975)) # critical region\nabline(v=ci_95, lwd=2)\nabline(v=sample_mean, lwd=2, col=4)",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Intervals</span>"
    ]
  },
  {
    "objectID": "01_07_Intervals.html#probability-theory",
    "href": "01_07_Intervals.html#probability-theory",
    "title": "7  Intervals",
    "section": "7.3 Probability Theory",
    "text": "7.3 Probability Theory\n\nCI Coverage.\nOften, a \\(Z\\%\\) confidence interval means that \\(Z\\%\\) of the intervals we generate will contain the true mean. E.g., suppose that we repeatedly sample data and construct \\(95\\%\\) bootstrap confidence interval for the mean, then we expect that \\(95\\%\\) of our constructed confidence intervals contain the theoretical population mean. Given the sampling distribution is approximately normally, confidence intervals are symmetric. For the sample mean \\(M\\), we can construct the interval \\([M - E, M + E]\\), where \\(E\\) is some “margin of error” on either side of \\(M\\). A coverage level of \\(1-\\alpha\\) means \\(Prob( M - E &lt; \\mu &lt; M + E)=1-\\alpha\\). I.e., if the same sampling procedure were repeated \\(100\\) times from the same population, approximately \\(95\\) of the resulting intervals would be expected to contain the true population mean.1 Note that a \\(95\\%\\) coverage level does not imply a \\(95\\%\\) probability that the true parameter lies within a particular calculated interval. E.g., if you compute \\(\\hat{M}=9\\) for your particular sample, a coverage level of \\(1-\\alpha=95\\%\\) does not mean \\(Prob(9 - E &lt; \\mu &lt; 9 + E)=95\\%\\). The interval you computed either contains the true mean or it does not.\n\n\n\n\n\n\nTip\n\n\n\n\n\nGiven the sample size, \\(n\\), is large enough for the sample mean to be approximately normally distributed, what confidence interval satisfies the following: the theoretical mean \\(\\mu\\) is inside of the interval with probability \\(95%\\) (i.e., for \\((1 - 0.05)%\\) of samples)?\n\n\n\nFor a fixed sample size \\(n\\), there is a trade-off between precision: the width of a confidence interval, and accuracy: the probability that a confidence interval contains the theoretical value.\n\n\nCode\n# Confidence Interval for each sample\nxq &lt;- apply(x_samples, 1, function(r){ #theoretical se's \n    mean(r) + c(-1,1)*sd(r)/sqrt(length(r))\n})\n# First 3 interval estimates\nxq[, c(1,2,3)]\n##           [,1]      [,2]      [,3]\n## [1,] 0.5059216 0.4959424 0.4958189\n## [2,] 0.5243759 0.5144455 0.5140334\n\n# Explicit calculation\nmu_true &lt;- 0.5 # theoretical result for uniform samples\n# Logical vector: whether the true mean is in each CI\ncovered &lt;- mu_true &gt;= xq[1, ] & mu_true &lt;= xq[2, ]\n# Empirical coverage rate\ncoverage_rate &lt;- mean(covered)\ncat(sprintf(\"Estimated coverage probability: %.2f%%\\n\", 100 * coverage_rate))\n## Estimated coverage probability: 71.00%\n\n# Theoretically: [-1 sd, +1 sd] has 2/3 coverage\n# Change to [-2 sd, +2 sd] to see Precision-Accuracy tradeoff.\n\n\n\n\nCode\n# Visualize first 100 confidence intervals\nn &lt;- 100\nplot.new()\nplot.window(xlim = range(xq), ylim = c(0, n))\nfor (i in seq(1,n) ) {\n  col_i &lt;- if (covered[i]) rgb(0, 0, 0, 0.3) else rgb(1, 0, 0, 0.5)\n  segments(xq[1, i], i, xq[2, i], i, col = col_i, lwd = 2)\n}\nabline(v = mu_true, col = \"blue\", lwd = 2)\naxis(1)\ntitle(\"Visualizing CI Coverage (Red = Missed)\", font.main=1)",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Intervals</span>"
    ]
  },
  {
    "objectID": "01_07_Intervals.html#further-reading",
    "href": "01_07_Intervals.html#further-reading",
    "title": "7  Intervals",
    "section": "7.4 Further Reading",
    "text": "7.4 Further Reading\nSee\n\nhttps://www.r-bloggers.com/2025/02/bootstrap-vs-standard-error-confidence-intervals/\nhttps://learningstatisticswithr.com/book/hypothesistesting.html\nhttps://okanbulut.github.io/rbook/part5.html",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Intervals</span>"
    ]
  },
  {
    "objectID": "01_07_Intervals.html#footnotes",
    "href": "01_07_Intervals.html#footnotes",
    "title": "7  Intervals",
    "section": "",
    "text": "Notice that \\(Prob( M - E &lt; \\mu &lt; M + E) = Prob( - E &lt; \\mu - M &lt; + E) = Prob( E + \\mu &gt; M &gt; \\mu - E)\\). So if the interval \\([\\mu - 10, \\mu + 10]\\) contains \\(95\\%\\) of all \\(M\\), then the interval \\([M-10, M+10]\\) will also contain \\(\\mu\\) in \\(95\\%\\) of the samples because whenever \\(M\\) is within \\(10\\) of \\(\\mu\\), the value \\(\\mu\\) is also within \\(10\\) of \\(M\\). But for any particular sample, the interval \\([\\hat{M}-10, \\hat{M}+10]\\) either does or does not contain \\(\\mu\\).↩︎",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Intervals</span>"
    ]
  },
  {
    "objectID": "01_08_HypothesisTests.html",
    "href": "01_08_HypothesisTests.html",
    "title": "8  \\(p\\)-values",
    "section": "",
    "text": "8.1 \\(p\\)-values\nA \\(p\\)-value is the frequency you see something as extreme as your statistic when sampling from the null distribution. We want the probability mass in both tails: the random variable \\(M\\) that is at least as extreme (far from the null mean of \\(9\\)) as our observed sample mean \\(\\hat{M}\\). \\[\\begin{eqnarray}\nProb( |M - \\mu| \\geq |\\hat{M} - \\mu| \\mid \\mu = 9 )\n&\\approx& Prob( |M^{\\text{boot}}- \\mu^{\\text{boot}}| \\geq |\\hat{M}- \\mu^{\\text{boot}}|  \\mid \\mu^{\\text{boot}} = 9) \\\\\n&=& 1-\\hat{F}^{|\\text{boot}|}_{0}(|\\hat{M}-9|),\n\\end{eqnarray}\\] where \\(\\hat{F}^{|\\text{boot}|}_{0}\\) is the ECDF of \\(|M^{\\text{boot}}- \\mu^{\\text{boot}}|\\).\nThe bootstrap idea here is to approximate \\(M-\\mu\\), the difference between the sample mean \\(M\\) and the unknown theoretical mean \\(\\mu\\), with the difference between the bootstrap mean \\(M^{\\text{boot}}\\) and the sample mean, \\(M^{\\text{boot}}-M\\).",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>$p$-values</span>"
    ]
  },
  {
    "objectID": "01_08_HypothesisTests.html#p-values",
    "href": "01_08_HypothesisTests.html#p-values",
    "title": "8  \\(p\\)-values",
    "section": "",
    "text": "Note\n\n\n\n\n\n\n\nCode\nsample_dat &lt;- USArrests[,'Murder']\nsample_mean &lt;- mean(sample_dat)\n\nset.seed(1)\n# Bootstrap NULL: mean=9\n# Bootstrap shift: center each bootstrap resample so that the distribution satisfies the null hypothesis on average.\nmu &lt;- 9\nbootstrap_means_null &lt;- vector(length=999)\nfor(b in seq_along(bootstrap_means_null)){\n    dat_b &lt;- sample(sample_dat, replace=T) \n    mean_b &lt;- mean(dat_b) + (mu - sample_mean) # impose the null via Bootstrap shift\n    bootstrap_means_null[b] &lt;- mean_b\n}\nhist(bootstrap_means_null, breaks=25, border=NA,\n    main='',\n    xlab='Null Bootstrap Samples')\nci_95 &lt;- quantile(bootstrap_means_null, probs=c(.025, .975)) # critical region\nabline(v=ci_95, lwd=2)\nabline(v=sample_mean, lwd=2, col=4)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Two-Sided Test, ALTERNATIVE: mean &lt; 9 or mean &gt;9\n# Visualize Two Sided Prob. & reject region boundary\npar(mfrow=c(1,2))\nhist(bootstrap_means_null-mu,\n    freq=F, breaks=20,\n    border=NA,\n    main='',\n    xlab=expression('Null Bootstrap for M - '~mu))\nabline(v=sample_mean-mu, col=4)\nci_95 &lt;- quantile(bootstrap_means_null-mu, probs=c(0.025,.975))\nabline(v=ci_95, lwd=2)\n\n# Equivalent Visualization\nboot_absval &lt;- abs(bootstrap_means_null-mu)\nFhat_abs0 &lt;- ecdf(boot_absval)\nplot(Fhat_abs0,\n    main='',\n    xlab=expression('Null Bootstrap for |M - '~mu~'|'))\nabline(v=abs(sample_mean-mu), col=4)\n# with Two Sided Probability\np2 &lt;- 1 - Fhat_abs0( abs(sample_mean-mu) )\ntitle( paste0('p=', round(p2,3)))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaveats.\nBeware that a common misreading of the \\(p\\)-value as “the probability the null is true”. That is false.\nOften, one may also see or hear “\\(p&lt;.05\\): statistically significant” and “\\(p&gt;.05\\): not statistically significant”. That is decision making on purely statistical grounds, and it may or may not be suitable for your context. You simply need to know that whoever says those things is using \\(5\\%\\) as a critical value to reject an alternative hypothesis.\n\n\nCode\n# Purely-Statistical Decision Making \n# via Two Sided Test\nif(p2 &gt;.05){\n    print('fail to reject the null that mean=9, at the 5% level')\n} else {\n    print('reject the null that mean=9 in favor of either &lt;9 or &gt;9, at the 5% level')\n}\n## [1] \"reject the null that mean=9 in favor of either &lt;9 or &gt;9, at the 5% level\"\n\n\nAlso note that the \\(p\\)-value is itself a function of data, and hence a random variable that changes from sample to sample. Given that the \\(5\\%\\) level is somewhat arbitrary, and that the \\(p\\)-value both varies from sample to sample and is often misunderstood, it makes sense to give \\(p\\)-values a limited role in decision making.\n\n\nCode\np_values &lt;- vector(length=300)\nfor(b2 in seq(p_values)){\n    bootstrap_means_null_p &lt;- vector(length=999)\n    for(b in seq_along(bootstrap_means_null_p)){\n        dat_b &lt;- sample(sample_dat, replace=T) \n        mean_b &lt;- mean(dat_b) + (mu - sample_mean) # impose the null\n        bootstrap_means_null_p[b] &lt;- mean_b\n    }\n    Fhat_abs0 &lt;- ecdf( abs(bootstrap_means_null_p-mu) )\n    p2 &lt;- 1- Fhat_abs0( abs(sample_mean-mu) )\n    p_values[b2] &lt;- p2\n}\n\nhist(p_values, freq=F,\n    border=NA, main='')",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>$p$-values</span>"
    ]
  },
  {
    "objectID": "01_08_HypothesisTests.html#other-statistics",
    "href": "01_08_HypothesisTests.html#other-statistics",
    "title": "8  \\(p\\)-values",
    "section": "8.2 Other Statistics",
    "text": "8.2 Other Statistics\n\n\\(t\\)-values.\nA \\(t\\)-value standardizes the approach for hypothesis tests of the mean. For any specific sample, we compute the estimate \\[\\begin{eqnarray}\n\\hat{t}=(\\hat{M}-\\mu)/\\hat{S},\n\\end{eqnarray}\\] which corresponds to the estimator \\(t = (M - \\mu) / \\mathbb{s}(M)\\), which varies from sample to sample. We use bootstrapping to estimate the variability of the \\(t\\) statistic, just like we did with the mean.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\nCode\n#null hypothesis\nmu &lt;- 9\n\n# t statistic \njackknife_means &lt;- vector(length=length(sample_dat))\nfor(i in seq_along(jackknife_means)){\n    jackknife_means[i] &lt;- mean(dat_b[-i])\n}\nsample_t &lt;- (sample_mean - mu)/sd(jackknife_means)\n\n# Boostrap Null Distribution\nbootstrap_t_null &lt;- vector(length=999)\nfor(b in seq_along(bootstrap_t_null)){\n    dat_b &lt;- sample(sample_dat, replace=T) \n    mean_b &lt;- mean(dat_b) + (mu - sample_mean) # impose the null by recentering\n    # Compute t stat using jackknife ses (same as above)\n    jackknife_means_b &lt;- vector(length=length(dat_b))\n    for(i in seq_along(jackknife_means_b)){\n        jackknife_means_b[i] &lt;- mean(dat_b[-i])\n    }\n    jackknife_se_b &lt;- sd( jackknife_means_b )\n    jackknife_t_b &lt;- (mean_b - mu)/jackknife_se_b\n    bootstrap_t_null[b] &lt;- jackknife_t_b\n}\n\n# Plot the null distribution and CI\npar(mfrow=c(1,2))\nhist(bootstrap_t_null, border=NA, breaks=50,\n    freq=F, main=NA, xlab='Null Bootstrap for t')\nabline(v=sample_t, col=4)\nci_95 &lt;- quantile(bootstrap_t_null, probs=c(0.025,0.975) )\nabline(v=ci_95, lwd=2)\n\n# Compute the p-value for two-sided test\nFhat0 &lt;- ecdf(abs(bootstrap_t_null))\nplot(Fhat0, \n    xlim=range(bootstrap_t_null, sample_t),\n    xlab='Null Bootstrap for |t|',\n    main='')\nabline(v=abs(sample_t), col=4)\np &lt;- 1 - Fhat0( abs(sample_t) )\ntitle( paste0('p=', round(p,3)) )\n\n\n\n\n\n\n\n\n\nCode\n\nif(p &gt;.05){\n    print('fail to reject the null that mean=9, at the 5% level')\n} else {\n    print('reject the null that mean=9 in favor of either &lt;9 or &gt;9, at the 5% level')\n}\n## [1] \"reject the null that mean=9 in favor of either &lt;9 or &gt;9, at the 5% level\"\n\n\n\n\n\nThere are several benefits to this statistic:\n\nuses the same statistic for different hypothesis tests\nmakes the statistic comparable across different studies\nremoves dependence on unknown parameters by normalizing with a standard error\nmakes the null distribution theoretically known asymptotically (approximately)\n\nThe last point implies we are typically dealing with a normal distribution that is well-studied, or another well-studied distribution derived from it.1\n\n\nQuantiles and Shape Statistics.\nBootstrap allows hypothesis tests for any statistic, not just the mean, without relying on parametric theory. For example, the above procedures generalize from means to quantile statistics like medians.\n\n\nCode\n# Test for Median Differences (Impose the Null)\n# Bootstrap Null Distribution for the median\n# Each Bootstrap shifts medians so that median = q_null\n\nq_obs &lt;- quantile(sample_dat, probs=.5)\nq_null &lt;- 7.8\nbootstrap_quantile_null &lt;- vector(length=999)\nfor(b in seq_along(bootstrap_quantile_null)){\n    x_b &lt;- sample(sample_dat, replace=T) #bootstrap sample\n    q_b &lt;- quantile(x_b, probs=.5) # median\n    d_b &lt;- q_b - (q_obs-q_null) #impose the null\n    bootstrap_quantile_null[b] &lt;- d_b \n}\n\n    # Note that you could also standardize like the t value. E.g., \n    # jackknife_quantiles_b &lt;- vector(length=length(dat_b))\n    # se_b &lt;- sd(jackknife_quantiles_b)\n    # d_b &lt;- d_b/se_b\n    \n# 2-Sided Test for Medians\nhist(bootstrap_quantile_null-q_null, \n    border=NA, freq=F, xlab='Null Bootstrap',\n    font.main=1, main='Medians (Impose Null)')\nmedian_ci &lt;- quantile(bootstrap_quantile_null-q_null, probs=c(.025, .975))\nabline(v=median_ci, lwd=2)\nabline(v=q_obs-q_null, lwd=2, col=4)\n\n\n\n\n\n\n\n\n\nCode\n\n# 2-Sided Test for Median Difference\n## Null: No Median Difference\n1 - ecdf( abs(bootstrap_quantile_null-q_null))( abs(q_obs-q_null) ) \n## [1] 0.5695696",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>$p$-values</span>"
    ]
  },
  {
    "objectID": "01_08_HypothesisTests.html#one-sided-tests",
    "href": "01_08_HypothesisTests.html#one-sided-tests",
    "title": "8  \\(p\\)-values",
    "section": "8.3 One-Sided Tests",
    "text": "8.3 One-Sided Tests\nAbove, we tested whether the observed statistic is either extremely high or low. This is known as a two-sided test. There are also two one-sided tests (left tail: observed statistic is extremely low, right tail: observed statistic is extremely high). For a concrete example, consider whether the mean statistic, \\(M\\), is centered on a theoretical value of \\(\\mu=9\\) for the population. If your null hypothesis is that the theoretical mean is nine, \\(H_{0}: \\mu =9\\), and you calculated the mean for your sample as \\(\\hat{M}\\), then you can consider any one of these three alternative hypotheses:\n\n\\(H_{A}​: \\mu \\neq 9\\), a two-tail test\n\\(H_{A}: \\mu &lt; 9\\), a left-tail test\n\\(H_{A​}: \\mu &gt; 9\\), a right-tail test\n\n\nOne-Sided Intervals.\nA one-sided test is associated with an interval that shifted to one side, containing one tail rather than the middle. For example, a left tail test that “inverts a CI” uses the interval \\((-\\infty, q_{0.95}]\\), where \\(q_{0.95}\\) is the \\(95^{\\text{th}}\\) percentile of the bootstrap distribution. If the hypothesized value falls inside that interval, then we fail to reject the null (at the \\(5\\%\\) level), otherwise we reject the null (it seems extremely unlikely that we would find such a large value). A left tail test that “inverts a CI” uses the interval \\([q_{0.05}, \\infty)\\), where \\(q_{0.05}\\) is the \\(5^{\\text{th}}\\) percentile of the bootstrap distribution.\n\n\n\n\n\n\nTip\n\n\n\n\n\nHere is right-tail Test example\n\n\nCode\n# Bootstrap Distribution\nset.seed(1) # to be replicable\nbootstrap_means &lt;- vector(length=9999)\nfor(b in seq_along(bootstrap_means)){\n    dat_id &lt;- seq(1,length(sample_dat))\n    boot_id &lt;- sample(dat_id, replace=T)\n    dat_b  &lt;- sample_dat[boot_id] # c.f. jackknife\n    mean_b &lt;- mean(dat_b)\n    bootstrap_means[b] &lt;-mean_b\n}\n\n# ALTERNATIVE: mean &gt; 9\n# Visualize One Sided Prob. & reject region boundary\nhist(bootstrap_means, border=NA, breaks=50,\n    freq=F, main=NA, xlab='Null Bootstrap')\nabline(v=sample_mean, col=4)\nci_95 &lt;- quantile(bootstrap_means, probs=c(0.05,1) )\nabline(v=ci_95, lwd=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced and Optional\n\n\nFor me at least, “invert a CI” is the most intuitive way to conduct hypothesis test using intervals. However, there are “Impose the Null” intervals, which are reversed, that correspond more closely to \\(p\\)-values.\nA right tail test that “imposes the null” uses the interval \\((-\\infty, q_{0.95}]\\), where \\(q_{0.95}\\) is the \\(95^{\\text{th}}\\) percentile of the null bootstrap distribution: If the observed value falls inside that interval, then we fail to reject the null (at the \\(5\\%\\) level), otherwise we reject the null (it seems extremely unlikely that we would find such a small value). A left tail test that “imposes the null” uses the interval \\([q_{0.05}, \\infty)\\), where \\(q_{0.05}\\) is the \\(5^{\\text{th}}\\) percentile of the null bootstrap distribution. Referring to the hypothetical value generally as \\(\\mu_0\\) instead of the particular number \\(9\\), we can summarize the one-sided decision rules as\n\n\n\n\n\n\n\n\n\n\nTail\nReject when\nFail to reject when\n\n\n\n\nImpose the Null(shifted/bootstrap-null)\nRight-tail \\(H_A: \\mu &gt; \\mu_0\\)\n\\(\\hat{M} &gt; q^{\\text{null}}_{0.95}\\)\n\\(\\hat{M} \\le q^{\\text{null}}_{0.95}\\)\n\n\n\nLeft-tail\\(H_A: \\mu &lt; \\mu_0\\)\n\\(\\hat{M} &lt; q^{\\text{null}}_{0.05}\\)\n\\(\\hat{M} \\ge q^{\\text{null}}_{0.05}\\)\n\n\nInvert a CI(percentile CI)\nRight-tail\\(H_A: \\mu &gt; \\mu_0\\)\n\\(\\mu_0 &lt; q^{\\text{boot}}_{0.05}\\)\n\\(\\mu_0 \\ge q^{\\text{boot}}_{0.05}\\)\n\n\n\nLeft-tail \\(H_A: \\mu &lt; \\mu_0\\)\n\\(\\mu_0 &gt; q^{\\text{boot}}_{0.95}\\)\n\\(\\mu_0 \\le q^{\\text{boot}}_{0.95}\\)\n\n\n\n\n\n\n\nOne-Sided \\(p\\)-values.\nThe \\(p\\)-value for a one-sided test is more straightforward to implement via a bootstrap null distribution.\nFor a left-tail test, we examine \\[\\begin{eqnarray}\np = Prob( M &lt; \\hat{M} \\mid \\mu = 9 )\n    &\\approx& Prob( M^{\\text{boot}} &lt; \\hat{M} \\mid  \\mu^{\\text{boot}} = 9 ) = \\hat{F}^{\\text{boot}}_{0}(\\hat{M}),\n\\end{eqnarray}\\] where \\(\\hat{F}^{\\text{boot}}_{0}\\) is the ECDF of the bootstrap null distribution. We reject the null if \\(p &lt; 0.05\\) at the \\(5\\%\\) level, and otherwise fail to reject.\nFor a right-tail test, we examine \\(p=Prob( M &gt; \\hat{M} \\mid \\mu = 9 ) \\approx 1-\\hat{F}^{\\text{boot}}_{0}(\\hat{M})\\).\n\n\nCode\n# Right-tail Test, ALTERNATIVE: mean &gt; 9\n# Equivalent Visualization with p-value\nFhat0 &lt;- ecdf(bootstrap_means_null) # Look at right tail\nplot(Fhat0,\n    main='',\n    xlab='Null Bootstrap')\nabline(v=sample_mean, col=4)\np1 &lt;- 1- Fhat0(sample_mean) #Compute right tail prob: 0.987\ntitle( paste0('p=', round(p1,3)))\n\n\n\n\n\n\n\n\n\nCode\n\nif(p1 &gt;.05){\n    print('fail to reject the null that mean=9, at the 5% level')\n} else {\n    print('reject the null that mean=9 in favor of &gt;9, at the 5% level')\n}\n## [1] \"fail to reject the null that mean=9, at the 5% level\"\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nNotice that the recentering adjustment affects two-sided tests (because they depend on distance from the null mean) but not one-sided tests (because adding a constant does not change rank order). Specifically, \\(p = Prob( M &lt; \\hat{M} \\mid \\mu = 9 ) =  Prob( M - \\mu &lt; \\hat{M} - \\mu \\mid \\mu = 9 )\\). That is intuitively also why the \\(t\\)-value can be used for both one and two-sided hypothesis tests.\n\n\nCode\n# See that the \"recentering\" matters for two-sided tests\necdf( abs(bootstrap_means_null-mu) )( abs(sample_mean-mu) )\n## [1] 0.966967\necdf( abs(bootstrap_means_null) )( abs(sample_mean) )\n## [1] 0.01301301\n\n# See that the \"recentering\" doesn't matter for one-sided ones\necdf( bootstrap_means_null-mu)( sample_mean-mu)\n## [1] 0.01301301\necdf( bootstrap_means_null )( sample_mean)\n## [1] 0.01301301",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>$p$-values</span>"
    ]
  },
  {
    "objectID": "01_08_HypothesisTests.html#further-reading",
    "href": "01_08_HypothesisTests.html#further-reading",
    "title": "8  \\(p\\)-values",
    "section": "8.4 Further Reading",
    "text": "8.4 Further Reading\n\nhttps://learningstatisticswithr.com/book/hypothesistesting.html\nhttps://okanbulut.github.io/rbook/part5.html",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>$p$-values</span>"
    ]
  },
  {
    "objectID": "01_08_HypothesisTests.html#footnotes",
    "href": "01_08_HypothesisTests.html#footnotes",
    "title": "8  \\(p\\)-values",
    "section": "",
    "text": "In another statistics class, you will learn the math behind the null t-distribution. In this class, we skip this because we can simply bootstrap the \\(t\\) statistic too.↩︎",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>$p$-values</span>"
    ]
  },
  {
    "objectID": "01_09_MiscTopics.html",
    "href": "01_09_MiscTopics.html",
    "title": "9  Misc. Univariate Topics",
    "section": "",
    "text": "9.1 Data Transformations\nTransformations can stabilize variance, reduce skewness, and make model errors closer to Gaussian.\nPerhaps the most common examples are power transformations: \\(y= x^\\lambda\\), which includes \\(\\sqrt{x}\\) and \\(x^2\\).\nOther examples include the exponential transformation: \\(y=\\exp(x)\\) for any \\(x\\in (-\\infty, \\infty)\\) and logarithmic transformation: \\(y=\\log(x)\\) for any \\(x&gt;0\\).\nThe Box–Cox Transform nests many cases. For \\(x&gt;0\\) and parameter \\(\\lambda\\), \\[\\begin{eqnarray}\ny=\\begin{cases}\n\\dfrac{x^\\lambda-1}{\\lambda}, & \\lambda\\neq 0,\\\\\n\\log\\left(x\\right) & \\lambda=0.\n\\end{cases}\n\\end{eqnarray}\\] This function is continuous over \\(\\lambda\\).\nCode\n# Box–Cox transform and inverse\nbc_transform &lt;- function(x, lambda) {\n  if (any(x &lt;= 0)) stop(\"Box-Cox requires x &gt; 0\")\n  if (abs(lambda) &lt; 1e-8) log(x) else (x^lambda - 1)/lambda\n}\nbc_inverse &lt;- function(t, lambda) {\n  if (abs(lambda) &lt; 1e-8) exp(t) else (lambda*t + 1)^(1/lambda)\n}\n\nX &lt;- USArrests[,'Murder']\nhist(X, main='', border=NA, freq=F)\n\n\n\n\n\n\n\n\n\nCode\n\npar(mfrow=c(1,3))\nfor(lambda in c(-1,0,1)){\n    Y &lt;- bc_transform(X, lambda)\n    hist(Y, \n        main=bquote(paste(lambda,'=',.(lambda))),\n        border=NA, freq=F)\n}",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Misc. Univariate Topics</span>"
    ]
  },
  {
    "objectID": "01_09_MiscTopics.html#data-transformations",
    "href": "01_09_MiscTopics.html#data-transformations",
    "title": "9  Misc. Univariate Topics",
    "section": "",
    "text": "Law of the Unconscious Statistician (LOTUS).\nAs before, we will represent the random variable as \\(X_{i}\\), which can take on values \\(x\\) from the sample space. If \\(X_{i}\\) is a discrete random variable (a random variable with a discrete sample space) and \\(g\\) is a function, then \\(\\mathbb E[g(X_{i})] = \\sum_x g(x)Prob(X_{i}=x)\\).\n\n\n\n\n\n\nTip\n\n\n\n\n\nLet \\(X_{i}\\) take values \\(\\{1,2,3\\}\\) with \\[\\begin{eqnarray}\nPr(X_{i}=1)=0.2,\\quad Prob(X_{i}=2)=0.5,\\quad Prob(X_{i}=3)=0.3.\n\\end{eqnarray}\\] Let \\(g(x)=x^2+1\\). Then \\(g(1)=1^2+1=2\\), \\(g(2)=2^2+1=5\\), \\(g(3)=3^2+1=10\\).\nThen, by LOTUS, \\[\\begin{eqnarray}\n\\mathbb E[g(X_{i})]=\\sum_x g(x)Prob(X_{i}=x)\n&=& g(1)\\cdot 0.2 + g(2)\\cdot 0.5 + g(3)\\cdot 0.3 \\\\\n&=& 2 \\cdot 0.2 + 5 \\cdot 0.5 + 10 \\cdot 0.3 \\\\\n&=& 0.4 + 2.5 + 3 = 5.9.\n\\end{eqnarray}\\]\n\n\nCode\nx  &lt;- c(1,2,3)\nx_probs &lt;- c(0.2,0.5,0.3)\ng  &lt;- function(x) x^2 + 1\nsum(g(x) * x_probs) \n## [1] 5.9\n\n\n\n\n\n\n\nCode\ng  &lt;- function(x) x^2 + 1\n\n# A theoretical example\nx &lt;- c(1,2,3,4)\nx_probs &lt;- c(1/4, 1/4, 1/4, 1/4)\nsum(g(x) * x_probs) \n## [1] 8.5\n\n# A simulation example\nX &lt;- sample(x, x_probs, size=1000, replace=T)\nmean(g(X))\n## [1] 8.397\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nIf \\(X_{i}\\) is a continuous random variable (a random variable with a continuous sample space) and \\(g\\) is a function, then \\(\\mathbb E[g(X_{i})] = \\int_{-\\infty}^{\\infty} g(x)f(x) dx\\).\n\n\nCode\nx &lt;- rexp(5e5, rate = 1)           # X ~ Exp(1)\nmean(sqrt(x))                      # LOTUS Simulation\n## [1] 0.8868115\nsqrt(pi) / 2                       # Exact via LOTUS integral\n## [1] 0.8862269\n\n\n\n\n\nNote that you have already seen the special case where \\(g(X_{i})=\\left(X_{i}-\\mathbb{E}[X_{i}]\\right)^2\\).\n\n\nJensen’s inequality.\nConcave functions curve inwards, like the inside of a cave. Convex functions curve outward, the opposite of concave.\nIf \\(g\\) is a concave function, then \\(g(\\mathbb E[X_{i}]) \\geq \\mathbb E[g(X_{i})]\\).\n\n\nCode\n# Continuous Example 1\nmean( sqrt(x) )\n## [1] 0.8868115\nsqrt( mean(x) ) \n## [1] 1.000621\n\n# Continuous Example 2\nmean( log(x) )\n## [1] -0.5761631\nlog( mean(x) ) \n## [1] 0.001241502\n\n\n\n\nCode\n## Discrete Example\nx  &lt;- c(1,2,3)\npx &lt;- c(0.2,0.5,0.3)\nEX &lt;- sum(x * px)\nEX\n## [1] 2.1\n\ng  &lt;- sqrt\ngEX &lt;- g(EX)\nEgX &lt;- sum(g(x) * px)\nc(gEX, EgX)\n## [1] 1.449138 1.426722\n\n\nIf \\(g\\) is a convex function, then the inequality reverses: \\(g(\\mathbb E[X_{i}]) \\leq \\mathbb E[g(X_{i})]\\).\n\n\nCode\nmean( exp(x) )\n## [1] 10.06429\nexp( mean(x) )  \n## [1] 7.389056",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Misc. Univariate Topics</span>"
    ]
  },
  {
    "objectID": "01_09_MiscTopics.html#drawing-samples",
    "href": "01_09_MiscTopics.html#drawing-samples",
    "title": "9  Misc. Univariate Topics",
    "section": "9.2 Drawing Samples",
    "text": "9.2 Drawing Samples\nTo generate a random variable from known distributions, you can use some type of physical machine. E.g., you can roll a fair die to generate Discrete Uniform data or you can roll weighted die to generate Categorical data.\nThere are also several ways to computationally generate random variables from a probability distribution. Perhaps the most common one is “inverse sampling”. To generate a random variable using inverse sampling, first sample \\(p\\) from a uniform distribution and then find the associated quantile quantile function \\(\\hat{F}^{-1}(p)\\).1\n\nUsing Data.\nYou can generate a random variable from a known empirical distribution. Inverse sampling randomly selects observations from the dataset with equal probabilities. To implement this, we\n\norder the data and associate each observation with an ECDF value\ndraw \\(p \\in [0,1]\\) as a uniform random variable\nfind the associated quantile via the ECDF\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nHere is an example of generating random murder rates for US states.\n\n\nCode\n# Empirical Distribution\nX &lt;- USArrests[,'Murder']\nFX_hat &lt;- ecdf(X)\nplot(FX_hat, lwd=2, xlim=c(0,20),\n    pch=16, col=grey(0,.5), main='')\n\n\n\n\n\n\n\n\n\nCode\n\n# Generating random variables via inverse ECDF\np &lt;- runif(3000) ## Multiple Draws\nQX_hat &lt;- quantile(FX_hat, p, type=1)\nQX_hat[c(1,2,3)]\n## 47.53182358% 16.73507320% 82.80265993% \n##          6.8          3.2         12.7\n\n## Can also do directly from the data\nQX_hat &lt;- quantile(X, p, type=1)\nQX_hat[c(1,2,3)]\n## 47.53182358% 16.73507320% 82.80265993% \n##          6.8          3.2         12.7\n\n\n\n\n\n\n\nUsing Math.\nIf you know the distribution function that generates the data, then you can derive the quantile function and do inverse sampling. Here is an in-depth example of the Dagum distribution. The distribution function is \\(F(x)=(1+(x/b)^{-a})^{-c}\\). For a given probability \\(p\\), we can then solve for the quantile as \\(F^{-1}(p)=\\frac{ b p^{\\frac{1}{ac}} }{(1-p^{1/c})^{1/a}}\\). Afterwhich, we sample \\(p\\) from a uniform distribution and then find the associated quantile.\n\n\nCode\n# Theoretical Quantile Function (from VGAM::qdagum)\nqdagum &lt;- function(p, scale.b=1, shape1.a, shape2.c) {\n  # Quantile function (theoretically derived from the CDF)\n  ans &lt;- scale.b * (expm1(-log(p) / shape2.c))^(-1 / shape1.a)\n  # Special known cases\n  ans[p == 0] &lt;- 0\n  ans[p == 1] &lt;- Inf\n  # Safety Checks\n  ans[p &lt; 0] &lt;- NaN\n  ans[p &gt; 1] &lt;- NaN\n  if(scale.b &lt;= 0 | shape1.a &lt;= 0 | shape2.c &lt;= 0){ ans &lt;- ans*NaN }\n  # Return\n  return(ans)\n}\n\n# Generate Random Variables (VGAM::rdagum)\nrdagum &lt;-function(n, scale.b=1, shape1.a, shape2.c){\n    p &lt;- runif(n) # generate random probabilities\n    x &lt;- qdagum(p, scale.b=scale.b, shape1.a=shape1.a, shape2.c=shape2.c) #find the inverses\n    return(x)\n}\n\n# Example\nset.seed(123)\nX &lt;- rdagum(3000,1,3,1)\nX[c(1,2,3)]\n## [1] 0.7390476 1.5499868 0.8845006",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Misc. Univariate Topics</span>"
    ]
  },
  {
    "objectID": "01_09_MiscTopics.html#set-theory",
    "href": "01_09_MiscTopics.html#set-theory",
    "title": "9  Misc. Univariate Topics",
    "section": "9.3 Set Theory",
    "text": "9.3 Set Theory\nLet the sample space contain events \\(A\\) and \\(B\\), with their probability denoted as \\(Prob(A)\\) and \\(Prob(B)\\). Then\n\nthe union: \\(A\\cup B\\), refers to either event occurring\nthe intersection: \\(A\\cap B\\), refers to both events occurring\nthe events \\(A\\) and \\(B\\) are mutually exclusive if and only if \\(A\\cap B\\) is empty\nthe inclusion–exclusion rule is: \\(Prob(A\\cup B)=Prob(A)+Prob(B)-Prob(A\\cap B)\\)\nthe events \\(A\\) and \\(B\\) are mutually independent if and only if \\(Prob(A\\cap B)=Prob(A) Prob(B)\\)\nthe conditional probability is \\(Prob(A \\mid B) = Prob(A \\cap B)/ P(B)\\)\n\nFor example, consider a six sided die where events are the number \\(&gt;\\!2\\) or the number is even.\n\nThe sets are \\(A=\\{3,4,5,6\\}\\) and \\(B=\\{2,4,6\\}\\)\nTo check mutually exclusive: notice \\(A\\cap B=\\{4,6\\}\\) which is not empty. So event \\(A\\) and event \\(B\\) are not mutually exclusive\n\nFurther assuming the die are fair\n\n\\(Prob(A)=4/6=2/3\\) and \\(Prob(B)=3/6=1/2\\)\n\\(A\\cup B=\\{2,3,4,5,6\\}\\), which implies \\(Prob(A\\cup B)=5/6\\)\n\\(A\\cap B=\\{4,6\\}\\), which implies \\(Prob(A\\cap B)=2/6=1/3\\)\nTo check independence: notice \\(Prob(A\\cap B)=1/3 = (2/3) (1/2) = Prob(A)Prob(B)\\). So event \\(A\\) and event \\(B\\) are mutually independent\nTo check inclusion–exclusion: notice \\(Prob(A) + Prob(B) - Prob(A\\cap B) = 2/3 + 1/2 - 1/3 = 4/6 + 3/6 - 2/6 = 5/6 = Prob(A\\cup B)\\)\nFinally, the conditional probability of \\(Prob(\\text{die is} &gt;2 \\mid \\text{die shows even number}) = Prob(A \\mid B) = \\frac{Prob(A \\cap B)}{P(B)} = \\frac{1/3}{1/2} = 2/3\\)\n\n\n\nCode\n# Simulation verification\nset.seed(123)\nx &lt;- seq(1,6)\nrolls &lt;- sample(x, 1e6, replace = TRUE)\nA &lt;- rolls &gt; 2\nB &lt;- rolls %% 2 == 0\nP_AcondB &lt;- mean(A & B) / mean(B)\nround(P_AcondB, 3)\n## [1] 0.667\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nConsider a six sided die with sample space \\(\\{1,2,3,4,5,6\\}\\) where each outcome is each equally likely. What is \\(Prob(\\text{die is odd or }&lt;5)\\)? Denote the odd events as \\(A=\\{1,3,5\\}\\) and the less than five events as \\(B=\\{1,2,3,4\\}\\). Then\n\n\\(A\\cup B=\\{1,3\\}\\), which implies \\(Prob(A\\cap B)=2/6=1/3\\).\n\\(Prob(A)=3/6=1/2\\) and \\(Prob(B)=4/6=2/3\\).\nBy inclusion–exclusion: \\(Prob(A\\cup B)=Prob(A)+Prob(B)-Prob(A\\cap B)=1/2+2/3-1/3=5/6\\).\n\nNow find \\(Prob(\\text{die is even and }&lt;5)\\). Verify your answer with a computer simulation.",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Misc. Univariate Topics</span>"
    ]
  },
  {
    "objectID": "01_09_MiscTopics.html#count-distributions",
    "href": "01_09_MiscTopics.html#count-distributions",
    "title": "9  Misc. Univariate Topics",
    "section": "9.4 Count Distributions",
    "text": "9.4 Count Distributions\n\nBinomial.\nThe sum of \\(n\\) Bernoulli trials (number of successes)\n\nDiscrete, support \\(\\{0,1,\\ldots,n\\}\\)\nProbability Mass Function: \\(Prob(X_{i}=x)=\\binom{n}{x}p^{x}(1-p)^{n-x}\\)\nSee https://en.wikipedia.org/wiki/Binomial_distribution\nA common use case: How many heads will I get when I flip a coin twice?\n\n\n\nCode\n## Minimal example\nn &lt;- 15\np &lt;- 0.3\n\n# PMF plot\nx &lt;- seq(0, n)\nf_x &lt;- dbinom(x, n, p)\nplot(x, f_x, type = \"h\", col = \"blue\",\n    main='',  xlab = \"x\", ylab = \"Prob(X = x)\")\ntitle(bquote(paste('Binom(',.(n),', ',.(p), ')' ) ))\npoints(x, f_x, pch = 16, col = \"blue\")\n\n\n\n\n\n\n\n\n\nCode\n\n# Simulate:  Compare empirical vs theoretical\n#X &lt;- rbinom(1e4, size = n, prob = p)\n#c(emp_mean = mean(X), th_mean = n*p)\n#c(emp_var  = var(X),  th_var  = n*p*(1-p))\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nSuppose that employees at a company are \\(70%\\) female and \\(30%\\) male. If we select a random sample of eight employees, what is the probability that more than \\(2\\) in the sample are female?\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nShow that \\(\\mathbb{E}[X_{i}]=np\\) and \\(\\mathbb{V}[X_{i}]=np(1-p)\\). Start with the case \\(n=1\\), then the case \\(n=2\\), then case \\(n=3\\), then the general case of any \\(n\\).\n\n\n\nThe Binomial Limit Theorem (de Moivre–Laplace theorem) says that as \\(n\\) grows large, with \\(p \\in (0,1)\\) staying fixed, the Binomial distribution is approximately normal with mean \\(np\\) and variance \\(np(1-p)\\)\n\n\n\n\n\n\nTip\n\n\n\n\n\nThe unemployment rate is \\(10%\\). Suppose that \\(100\\) employable people are selected randomly. What is the probability that this sample contains between \\(9\\) and \\(12\\) unemployed people. Use the normal approximation to binomial probabilities (parameters \\(\\mu=100, \\sigma=9.49\\)).\n\n\n\n\n\nPoisson.\nThe number of events in a fixed interval\n\nDiscrete, support \\(\\{0,1,2,\\ldots\\}\\)\nProbability Mass Function: \\(Prob(X_{i}=x)=e^{-\\lambda}\\lambda^x/x!\\)\nSee https://en.wikipedia.org/wiki/Poisson_distribution\nA common use case: How many cars will show up in the lot tomorrow?\n\n\n\nCode\n## Minimal example\nlambda &lt;- 3.5\n\n# PMF plot\nx &lt;- seq(0, 15)\nf_x &lt;- dpois(x, lambda)\nplot(x, f_x, type=\"h\", col=\"blue\",\n     xlab = \"x\", ylab = \"Prob(X = x)\")\npoints(x, f_x, pch = 16, col = \"blue\")\ntitle(bquote(paste('Pois(',.(lambda), ')')))\n\n\n\n\n\n\n\n\n\nCode\n\n\n# Simulate: Compare empirical vs theoretical\n#X &lt;- rpois(1e4, lambda)\n#c(emp_mean = mean(X), th_mean = lambda)\n#c(emp_var  = var(X),  th_var  = lambda)\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nShow that \\(\\mathbb{E}[X_{i}] = \\mathbb{V}[X_{i}]= \\lambda\\).\n\n\n\n\n\nIrwin–Hall.\nThe sum of \\(n\\) i.i.d. \\(\\text{Uniform}(0,1)\\).\n\nContinuous, support \\([0,n]\\)\nProbability Density Function: \\(f(x) = \\dfrac{1}{(n-1)!}\n\\displaystyle\\sum_{k=0}^{\\lfloor x \\rfloor} (-1)^k\n\\binom{n}{k} (x - k)^{n-1}\\) for \\(x \\in [0,n]\\), and \\(0\\) otherwise\nSee https://en.wikipedia.org/wiki/Irwin%E2%80%93Hall_distribution\nA common use case: representing the sum of many small independent financial shocks\n\n\n\nCode\n## Minimal example (base R)\n# Irwin–Hall PDF function\ndirwinhall &lt;- function(x, n) {\n    f_x &lt;- vector(length=length(x))\n    for(i in seq(x)) {\n        xx &lt;- x[i]\n        if(xx &lt; 0 | xx &gt; n){\n            f_x[i] &lt;- 0 \n        } else {\n            k &lt;- seq(0, floor(xx))\n            f_k &lt;- sum((-1)^k*choose(n, k)*(xx-k)^(n-1))/factorial(n-1)\n            f_x[i] &lt;- f_k\n        }\n    }\n    return(f_x)\n}\n\n# Parameters\nn &lt;- 2\nx &lt;- seq(0, n, length.out = 500)\n\n# Compute and plot PDF\nf_x &lt;- dirwinhall(x, n)\nplot(x, f_x, type=\"l\", col=\"blue\",\n    main='', xlab = \"x\", ylab = \"f(x)\")\ntitle(bquote(paste('IrwinHall(',.(n), ')')))\n\n\n\n\n\n\n\n\n\nSee also the Bates Distribution.",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Misc. Univariate Topics</span>"
    ]
  },
  {
    "objectID": "01_09_MiscTopics.html#beyond-basic-programming",
    "href": "01_09_MiscTopics.html#beyond-basic-programming",
    "title": "9  Misc. Univariate Topics",
    "section": "9.5 Beyond Basic Programming",
    "text": "9.5 Beyond Basic Programming\n\nPackages.\nExpansion “packages” allow for more functionality.\nMost packages can be easily installed from the internet via CRAN.\n\n\nCode\n# common packages for tables/figures\ninstall.packages(\"plotly\")\ninstall.packages(\"reactable\")\ninstall.packages(\"stargazer\")\n\n# common packages for data/teaching\ninstall.packages(\"AER\")\ninstall.packages(\"Ecdat\")\ninstall.packages('wooldridge')\n\n# other packages for statistics and data handling\ninstall.packages(\"extraDistr\")\ninstall.packages(\"twosamples\")\ninstall.packages(\"data.table\")\n\n\nYou only need to install packages once. But you need to load the package via library every time you want the extended functionality. For example, to generate exotic probability distributions\n\n\nCode\n# install.packages(\"extraDistr\"), once ever\nlibrary(extraDistr) # every time you need it\n\npar(mfrow=c(1,2))\nfor(p in c(-.5,0)){\n    x &lt;- rgev(2000, mu=0, sigma=1, xi=p)\n    hist(x, breaks=50, border=NA, main=NA, freq=F)\n}\ntitle('GEV densities', outer=T, line=-1)\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(extraDistr)\n\npar(mfrow=c(1,3))\nfor(p in c(-1, 0,2)){\n    x &lt;- rtlambda(2000, p)\n    hist(x, breaks=100, border=NA, main=NA, freq=F)\n}\ntitle('Tukey-Lambda densities', outer=T, line=-1)\n\n\n\n\n\n\n\n\n\nThe most common packages also have cheatsheets you can use.\n\n\nUpdating.\nMake sure R and your packages are up to date. The current version of R and any packages used can be found (and recorded) with\n\n\nCode\nsessionInfo()\n\n\nTo update your R packages, use\n\n\nCode\nupdate.packages()\n\n\nAfter updating R, you can update all packages stored on your computer (in all .libPaths())\n\n\nCode\nupdate.packages(checkBuilt=T, ask=F)\n# install.packages(old.packages(checkBuilt=T)[,\"Package\"])\n\n\n\n\nBase.\nWhile additional packages can make your code faster, they also create dependencies that can lead to problems. So learn base R well before becoming dependent on other packages\n\nhttps://bitsofanalytics.org/posts/base-vs-tidy/\nhttps://jtr13.github.io/cc21fall2/comparison-among-base-r-tidyverse-and-datatable.html\n\n\n\nGithub Packages.\n\n\nAdvanced and Optional\n\n\nSometimes you will want to install a package from GitHub. For this, you can use devtools or its light-weight version remotes\nTo color terminal output on Linux systems, for example, you can use the colorout package\n\n\nCode\n# install.packages(\"remotes\")\nlibrary(remotes)\n# Install &lt;https://github.com/jalvesaq/colorout\n# to .libPaths()[1]\ninstall_github('jalvesaq/colorout')\nlibrary(colorout)\n\n\nTo use devtools instead, you also need to have software developer tools installed on your computer.\n\nWindows: Rtools\nMac: Xcode",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Misc. Univariate Topics</span>"
    ]
  },
  {
    "objectID": "01_09_MiscTopics.html#footnotes",
    "href": "01_09_MiscTopics.html#footnotes",
    "title": "9  Misc. Univariate Topics",
    "section": "",
    "text": "Drawing random uniform samples with computers is actually quite complex and beyond the scope of this course.↩︎",
    "crumbs": [
      "Univariate Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Misc. Univariate Topics</span>"
    ]
  },
  {
    "objectID": "02-00-BivariateData.html",
    "href": "02-00-BivariateData.html",
    "title": "Bivariate Data",
    "section": "",
    "text": "This section introduces basic bivariate statistics, building on the approach in the previous part.",
    "crumbs": [
      "Bivariate Data"
    ]
  },
  {
    "objectID": "02_01_BivariateDistributions.html",
    "href": "02_01_BivariateDistributions.html",
    "title": "10  Bivariate Distributions",
    "section": "",
    "text": "10.1 Types of Distributions\nWe will now study two variables. The data for each observation data can be grouped together as a vector \\((\\hat{X}_{i}, \\hat{Y}_{i})\\).\nThe vector \\((\\hat{X}_{i}, \\hat{Y}_{i})\\) has a joint distribution that describes the relationship between \\(\\hat{X}_{i}\\) and \\(\\hat{Y}_{i}\\).",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate Distributions</span>"
    ]
  },
  {
    "objectID": "02_01_BivariateDistributions.html#types-of-distributions",
    "href": "02_01_BivariateDistributions.html#types-of-distributions",
    "title": "10  Bivariate Distributions",
    "section": "",
    "text": "Joint Distributions.\nScatterplots are used frequently to summarize the joint relationship between two variables, multiple observations of \\((\\hat{X}_{i}, \\hat{Y}_{i})\\). They can be enhanced in several ways. As a default, use semi-transparent points so as not to hide any points (and perhaps see if your observations are concentrated anywhere). You can also add other features that help summarize the relationship, although I will defer this until later.\n\n\nCode\nplot(Murder~UrbanPop, USArrests, pch=16, col=grey(0.,.5))\n\n\n\n\n\n\n\n\n\n\n\nMarginal Distributions.\nYou can also show the distributions of each variable along each axis.\n\n\nCode\n# Setup Plot\nlayout( matrix(c(2,0,1,3), ncol=2, byrow=TRUE),\n    widths=c(9/10,1/10), heights=c(1/10,9/10))\n\n# Scatterplot\npar(mar=c(4,4,1,1))\nplot(Murder~UrbanPop, USArrests, pch=16, col=rgb(0,0,0,.5))\n\n# Add Marginals\npar(mar=c(0,4,1,1))\nxhist &lt;- hist(USArrests[,'UrbanPop'], plot=FALSE)\nbarplot(xhist[['counts']], axes=FALSE, space=0, border=NA)\n\npar(mar=c(4,0,1,1))\nyhist &lt;- hist(USArrests[,'Murder'], plot=FALSE)\nbarplot(yhist[['counts']], axes=FALSE, space=0, horiz=TRUE, border=NA)\n\n\n\n\n\n\n\n\n\n\n\nConditional Distributions.\nWe can show how distributions and densities change according to a second (or even third) variable using data splits. E.g.,\n\n\nCode\n# Tailored Histogram \nylim &lt;- c(0,8)\nxbks &lt;-  seq(min(USArrests[,'Murder'])-1, max(USArrests[,'Murder'])+1, by=1)\n\n# Also show more information\n# Split Data by Urban Population above/below mean\npop_mean &lt;- mean(USArrests[,'UrbanPop'])\npop_cut &lt;- USArrests[,'UrbanPop']&lt; pop_mean\nmurder_lowpop &lt;- USArrests[pop_cut,'Murder']\nmurder_highpop &lt;- USArrests[!pop_cut,'Murder']\ncols &lt;- c(low=rgb(0,0,1,.75), high=rgb(1,0,0,.75))\n\npar(mfrow=c(1,2))\nhist(murder_lowpop,\n    breaks=xbks, col=cols[1],\n    main='Urban Pop &gt;= Mean', font.main=1,\n    xlab='Murder Arrests',\n    border=NA, ylim=ylim)\n\nhist(murder_highpop,\n    breaks=xbks, col=cols[2],\n    main='Urban Pop &lt; Mean', font.main=1,\n    xlab='Murder Arrests',\n    border=NA, ylim=ylim)\n\n\n\n\n\n\n\n\n\nIt is sometimes it is preferable to show the ECDF instead. And you can glue various combinations together to convey more information all at once\n\n\nCode\npar(mfrow=c(1,2))\n# Full Sample Density\nhist(USArrests[,'Murder'], \n    main='Density Function Estimate', font.main=1,\n    xlab='Murder Arrests',\n    breaks=xbks, freq=F, border=NA)\n\n# Split Sample Distribution Comparison\nF_lowpop &lt;- ecdf(murder_lowpop)\nplot(F_lowpop, col=cols[1],\n    pch=16, xlab='Murder Arrests',\n    main='Distribution Function Estimates',\n    font.main=1, bty='n')\nF_highpop &lt;- ecdf(murder_highpop)\nplot(F_highpop, add=T, col=cols[2], pch=16)\n\nlegend('bottomright', col=cols,\n    pch=16, bty='n', inset=c(0,.1),\n    title='% Urban Pop.',\n    legend=c('Low (&lt;= Mean)','High (&gt;= Mean)'))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Simple Interactive Scatter Plot\n# plot(Assault~UrbanPop, USArrests, col=grey(0,.5), pch=16,\n#    cex=USArrests[,'Murder']/diff(range(USArrests[,'Murder']))*2,\n#    main='US Murder arrests (per 100,000)')\n\n\nYou can also split data into grouped boxplots in the same way\n\n\nCode\nlayout( t(c(1,2,2)))\nboxplot(USArrests[,'Murder'], main='',\n    xlab='All Data', ylab='Murder Arrests')\n\n# K Groups with even spacing\nK &lt;- 3\nUSArrests[,'UrbanPop_Kcut'] &lt;- cut(USArrests[,'UrbanPop'],K)\nKcols &lt;- hcl.colors(K,alpha=.5)\nboxplot(Murder~UrbanPop_Kcut, USArrests,\n    main='', col=Kcols,\n    xlab='Urban Population', ylab='')\n\n\n\n\n\n\n\n\n\nCode\n\n# 4 Groups with equal numbers of observations\n#Qcuts &lt;- c(\n#    '0%'=min(USArrests[,'UrbanPop'])-10*.Machine[['double.eps']],\n#    quantile(USArrests[,'UrbanPop'], probs=c(.25,.5,.75,1)))\n#USArrests[,'UrbanPop']_cut &lt;- cut(USArrests[,'UrbanPop'], Qcuts)\n#boxplot(Murder~UrbanPop_cut, USArrests, col=hcl.colors(4,alpha=.5))",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate Distributions</span>"
    ]
  },
  {
    "objectID": "02_01_BivariateDistributions.html#probability-theory",
    "href": "02_01_BivariateDistributions.html#probability-theory",
    "title": "10  Bivariate Distributions",
    "section": "10.2 Probability Theory",
    "text": "10.2 Probability Theory\nWe now consider a bivariate random vector \\((X_{i}, Y_{i})\\), which is a theoretical version of the bivariate observations \\((\\hat{X}_{i}, \\hat{Y}_{i})\\). E.g., If we are going to flip two coins, then \\((X_{i}, Y_{i})\\) corresponds to the unflipped coins and \\((\\hat{X}_{i}, \\hat{Y}_{i})\\) corresponds to concrete values after they are flipped.\n\nDefinitions for Discrete Data.\nThe joint distribution is defined as \\[\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y)\n\\end{eqnarray}\\] Variables are statistically independent if \\(Prob(X_{i} = x, Y_{i} = y)= Prob(X_{i} = x) Prob(Y_{i} = y)\\) for all \\(x, y\\). Independence is sometimes assumed for mathematical simplicity, not because it generally fits data well.1\nThe conditional distributions are defined as \\[\\begin{eqnarray}\nProb(X_{i} = x | Y_{i} = y) = \\frac{ Prob(X_{i} = x, Y_{i} = y)}{ Prob( Y_{i} = y )}\\\\\nProb(Y_{i} = y | X_{i} = x) = \\frac{ Prob(X_{i} = x, Y_{i} = y)}{ Prob( X_{i} = x )}\n\\end{eqnarray}\\] The marginal distributions are then defined as \\[\\begin{eqnarray}\nProb(X_{i} = x) = \\sum_{y} Prob(X_{i} = x | Y_{i} = y) Prob( Y_{i} = y ) \\\\\nProb(Y_{i} = y) = \\sum_{x} Prob(Y_{i} = y | X_{i} = x) Prob( X_{i} = x ),\n\\end{eqnarray}\\] which is also known as the law of total probability.\n\n\nCoin Flips Example.\nFor one example, Consider flipping two coins, where we mark whether “heads” is face up with a \\(1\\) and “tail” with a \\(0\\). E.g., the first coin has a value of \\(x=1\\) if it shows heads and \\(x=0\\) if it shows tails. This table shows both the joint distribution and also each marginal distribution.\n\n\n\n\n\n\n\n\n\n\n\\(x=0\\)\n\\(x=1\\)\nMarginal\n\n\n\n\n\\(y=0\\)\n\\(Prob(X_{i}=0,Y_{i}=0)\\)\n\\(Prob(X_{i}=1,Y_{i}=0)\\)\n\\(Prob(Y_{i}=0)\\)\n\n\n\\(y=1\\)\n\\(Prob(X_{i}=0,Y_{i}=1)\\)\n\\(Prob(X_{i}=1,Y_{i}=1)\\)\n\\(Prob(Y_{i}=1)\\)\n\n\nMarginal\n\\(Prob(X_{i}=0)\\)\n\\(Prob(X_{i}=1)\\)\n\\(1\\)\n\n\n\nNote that different joint distributions can have the same marginal distributions.\n\n\n\n\n\n\nNote\n\n\n\n\n\nSuppose both coins are “fair”: \\(Prob(X_{i}=1)= 1/2\\) and \\(Prob(Y_{i}=1|X_{i}=x)=1/2\\) for either \\(x=1\\) or \\(x=0\\), then the four potential outcomes have equal probabilities. \\[\\begin{eqnarray}\nProb(X_{i} = 0, Y_{i} = 0) &=& 1/2 \\times 1/2 = 1/4 \\\\\nProb(X_{i} = 0, Y_{i} = 1) &=& 1/4 \\\\\nProb(X_{i} = 1, Y_{i} = 0) &=& 1/4 \\\\\nProb(X_{i} = 1, Y_{i} = 1) &=& 1/4 .\n\\end{eqnarray}\\] The joint distribution is written generally as \\[\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y) &=& Prob(X_{i} = x) Prob(Y_{i} = y).\n\\end{eqnarray}\\]\nThe marginal distribution of the second coin is \\[\\begin{eqnarray}\nProb(Y_{i} = 0) &=& Prob(Y_{i} = 0 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 0 | X_{i} = 1) Prob(X_{i}=1)\\\\\n&=& 1/2 (1/2) + 1/2 (1/2) = 1/2\\\\\nProb(Y_{i} = 1) &=& Prob(Y_{i} = 1 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 1 | X_{i} = 1) Prob(X_{i}=1)\\\\\n&=& 1/2 (1/2) + 1/2 (1/2) = 1/2\n\\end{eqnarray}\\]\nThe marginal distribution of the first coin is found in the exact same way \\[\\begin{eqnarray}\nProb(X_{i} = 0) &=& Prob(X_{i} = 0 | Y_{i} = 0) Prob(Y_{i}=0) + Prob(X_{i} = 0 | Y_{i} = 1) Prob(Y_{i}=1)\\\\\n&=& 1/2 (1/2) + 1/2 (1/2) = 1/2\\\\\nProb(X_{i} = 1) &=& Prob(X_{i} = 1 | Y_{i} = 0) Prob(Y_{i}=0) + Prob(X_{i} = 1 | Y_{i} = 1) Prob(Y_{i}=1)\\\\\n&=& 1/2 (1/2) + 1/2 (1/2) = 1/2\n\\end{eqnarray}\\]\nAltogether, we find\n\n\n\n\n\\(x=0\\)\n\\(x=1\\)\nMarginal\n\n\n\n\n\\(y=0\\)\n\\(1/4\\)\n\\(1/4\\)\n\\(1/2\\)\n\n\n\\(y=1\\)\n\\(1/4\\)\n\\(1/4\\)\n\\(1/2\\)\n\n\nMarginal\n\\(1/2\\)\n\\(1/2\\)\n\\(1\\)\n\n\n\n\n\nCode\n# Create a 2x2 matrix for the joint distribution.\n# Rows correspond to X1 (coin 1), and columns correspond to X2 (coin 2).\nP_fair &lt;- matrix(1/4, nrow = 2, ncol = 2)\nrownames(P_fair) &lt;- c(\"X1=0\", \"X1=1\")\ncolnames(P_fair) &lt;- c(\"X2=0\", \"X2=1\")\nP_fair\n##      X2=0 X2=1\n## X1=0 0.25 0.25\n## X1=1 0.25 0.25\n\n# Compute the marginal distributions.\n# Marginal for X1: sum across columns.\nP_X1 &lt;- rowSums(P_fair)\nP_X1\n## X1=0 X1=1 \n##  0.5  0.5\n# Marginal for X2: sum across rows.\nP_X2 &lt;- colSums(P_fair)\nP_X2\n## X2=0 X2=1 \n##  0.5  0.5\n\n# Compute the conditional probabilities Prob(X2 | X1).\ncond_X2_given_X1 &lt;- matrix(0, nrow = 2, ncol = 2)\nfor (j in c(1,2)) {\n  cond_X2_given_X1[, j] &lt;- P_fair[, j] / P_X1[j]\n}\nrownames(cond_X2_given_X1) &lt;- c(\"X2=0\", \"X2=1\")\ncolnames(cond_X2_given_X1) &lt;- c(\"given X1=0\", \"given X1=1\")\ncond_X2_given_X1\n##      given X1=0 given X1=1\n## X2=0        0.5        0.5\n## X2=1        0.5        0.5\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nNow consider a second example, where the second coin is “Completely Unfair”, so that it is always the same as the first. The outcomes generated with a Completely Unfair coin are the same as if we only flipped one coin. \\[\\begin{eqnarray}\nProb(X_{i} = 0, Y_{i} = 0) &=& 1/2 \\\\\nProb(X_{i} = 0, Y_{i} = 1) &=& 0 \\\\\nProb(X_{i} = 1, Y_{i} = 0) &=& 0 \\\\\nProb(X_{i} = 1, Y_{i} = 1) &=& 1/2 .\n\\end{eqnarray}\\] The joint distribution is written generally as \\[\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y) &=& Prob(X_{i} = x) \\mathbf{1}( x=y ),\n\\end{eqnarray}\\] where \\(\\mathbf{1}(X_{i}=1)\\) means \\(X_{i}= 1\\) and \\(0\\) if \\(X_{i}\\neq0\\). The marginal distribution of the second coin is \\[\\begin{eqnarray}\nProb(Y_{i} = 0)\n&=& Prob(Y_{i} = 0 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 0 | X_{i} = 1) Prob(X_{i} = 1)\\\\\n&=& 1 (1/2) + 0(1/2) = 1/2 .\\\\\nProb(Y_{i} = 1)\n&=& Prob(Y_{i} = 1 | X_{i} =0) Prob( X_{i} = 0) + Prob(Y_{i} = 1 | X_{i} = 1) Prob( X_{i} = 1)\\\\\n&=& 0 (1/2) + 1 (1/2) = 1/2 .\n\\end{eqnarray}\\] which is the same marginal as in the first example!\nThe marginal distribution of the first coin is found in the exact same way (show this yourself).\nAlltogether, we find\n\n\n\n\n\\(x=0\\)\n\\(x=1\\)\nMarginal\n\n\n\n\n\\(y=0\\)\n\\(1/2\\)\n\\(0\\)\n\\(1/2\\)\n\n\n\\(y=1\\)\n\\(0\\)\n\\(1/2\\)\n\\(1/2\\)\n\n\nMarginal\n\\(1/2\\)\n\\(1/2\\)\n\\(1\\)\n\n\n\n\n\nCode\n# Create the joint distribution matrix for the unfair coin case.\nP_unfair &lt;- matrix(c(0.5, 0, 0, 0.5), nrow = 2, ncol = 2, byrow = TRUE)\nrownames(P_unfair) &lt;- c(\"X1=0\", \"X1=1\")\ncolnames(P_unfair) &lt;- c(\"X2=0\", \"X2=1\")\nP_unfair\n##      X2=0 X2=1\n## X1=0  0.5  0.0\n## X1=1  0.0  0.5\n\n# Compute the marginal distribution for X2 in the unfair case.\nP_X2_unfair &lt;- colSums(P_unfair)\nP_X1_unfair &lt;- rowSums(P_unfair)\n\n# Compute the conditional probabilities Prob(X1 | X2) for the unfair coin.\ncond_X2_given_X1_unfair &lt;- matrix(NA, nrow = 2, ncol = 2)\nfor (j in c(1,2)) {\n  if (P_X1_unfair[j] &gt; 0) {\n    cond_X2_given_X1_unfair[, j] &lt;- P_unfair[, j] / P_X1_unfair[j]\n  }\n}\nrownames(cond_X2_given_X1_unfair) &lt;- c(\"X2=0\", \"X2=1\")\ncolnames(cond_X2_given_X1_unfair) &lt;- c(\"given X1=0\", \"given X1=1\")\ncond_X2_given_X1_unfair\n##      given X1=0 given X1=1\n## X2=0          1          0\n## X2=1          0          1\n\n\n\n\n\n\n\nDefinitions for Continuous Data.\nThe joint distribution is defined as \\[\\begin{eqnarray}\nF(x, y) &=& Prob(X_{i} \\leq x, Y_{i} \\leq y)\n\\end{eqnarray}\\] The marginal distributions are then defined as \\[\\begin{eqnarray}\nF_{X}(x) &=& F(x, \\infty)\\\\\nF_{Y}(y) &=& F(\\infty, y).\n\\end{eqnarray}\\] which is also known as the law of total probability. Variables are statistically independent if \\(F(x, y) = F_{X}(x)F_{Y}(y)\\) for all \\(x, y\\).\nFor example, suppose \\((X_{i},Y_{i})\\) is bivariate normal with means \\((\\mu_{X}, \\mu_{Y})\\), variances \\((\\sigma_{X}, \\sigma_{Y})\\) and covariance \\(\\rho\\).\n\n\nCode\n# Simulate Bivariate Data\nN &lt;- 10000\nMu &lt;- c(2,2) ## Means\n\nSigma1 &lt;- matrix(c(2,-.8,-.8,1),2,2) ## CoVariance Matrix\nMVdat1 &lt;- mvtnorm::rmvnorm(N, Mu, Sigma1)\ncolnames(MVdat1) &lt;- c('X','Y')\n\nSigma2 &lt;- matrix(c(2,.4,.4,1),2,2) ## CoVariance Matrix\nMVdat2 &lt;- mvtnorm::rmvnorm(N, Mu, Sigma2)\ncolnames(MVdat2) &lt;- c('X','Y')\n\npar(mfrow=c(1,2))\n## Different diagonals\nplot(MVdat2, col=rgb(1,0,0,0.02), pch=16,\n    main='Joint Distributions', font.main=1,\n    ylim=c(-4,8), xlim=c(-4,8),\n    xlab='X', ylab='Y')\npoints(MVdat1,col=rgb(0,0,1,0.02),pch=16)\n## Same marginal distributions\nxbks &lt;- seq(-4,8,by=.2)\nhist(MVdat2[,2], col=rgb(1,0,0,0.5),\n    breaks=xbks, border=NA, \n    xlab='Y',\n    main='Marginal Distributions', font.main=1)\nhist(MVdat1[,2], col=rgb(0,0,1,0.5),\n    add=T, breaks=xbks, border=NA)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# See that independent data are a special case\nn &lt;- 2e4\n## 2 Indepenant RV\nXYiid &lt;- cbind( rnorm(n),  rnorm(n))\n## As a single Joint Draw\nXYjoint &lt;- mvtnorm::rmvnorm(n, c(0,0))\n## Plot\npar(mfrow=c(1,2))\nplot(XYiid, xlab=\n    col=grey(0,.05), pch=16, xlim=c(-5,5), ylim=c(-5,5))\nplot(XYjoint,\n    col=grey(0,.05), pch=16, xlim=c(-5,5), ylim=c(-5,5))\n\n# Compare densities\n#d1 &lt;- dnorm(XYiid[,1],0)*dnorm(XYiid[,2],0)\n#d2 &lt;- mvtnorm::dmvnorm(XYiid, c(0,0))\n#head(cbind(d1,d2))\n\n\nThe multivariate normal is a workhorse for analytical work on multivariate random variables, but there are many more. See e.g., https://cran.r-project.org/web/packages/NonNorMvtDist/NonNorMvtDist.pdf\n\n\nImportant Applications.\nNote Simpson’s Paradox:\nAlso note Bayes’ Theorem: \\[\\begin{eqnarray}\nProb(X_{i} = x | Y_{i} = y)  Prob( Y_{i} = y)\n    &=& Prob(X_{i} = x, Y_{i} = y) = Prob(Y_{i} = y | X_{i} = x) Prob(X_{i} = x).\\\\\nProb(X_{i} = x | Y_{i} = y)\n    &=& \\frac{ Prob(Y_{i} = y | X_{i} = x) Prob(X_{i}=x) }{ Prob( Y_{i} = y) }.\n\\end{eqnarray}\\]\n\n\nCode\n# Verify Bayes' theorem for the unfair coin case:\n# Compute Prob(X1=1 | X2=1) using the formula:\n#   Prob(X1=1 | X2=1) = [Prob(X2=1 | X1=1) * Prob(X1=1)] / Prob(X2=1)\n\nP_X1_1 &lt;- 0.5\nP_X2_1_given_X1_1 &lt;- 1  # Since coin 2 copies coin 1.\nP_X2_1 &lt;- P_X2_unfair[\"X2=1\"]\n\nbayes_result &lt;- (P_X2_1_given_X1_1 * P_X1_1) / P_X2_1\nbayes_result\n## X2=1 \n##    1",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate Distributions</span>"
    ]
  },
  {
    "objectID": "02_01_BivariateDistributions.html#further-reading",
    "href": "02_01_BivariateDistributions.html#further-reading",
    "title": "10  Bivariate Distributions",
    "section": "10.3 Further Reading",
    "text": "10.3 Further Reading\nFor plotting histograms and marginal distributions, see\n\nhttps://www.r-bloggers.com/2011/06/example-8-41-scatterplot-with-marginal-histograms/\nhttps://r-graph-gallery.com/histogram.html\nhttps://r-graph-gallery.com/74-margin-and-oma-cheatsheet.html\nhttps://jtr13.github.io/cc21fall2/tutorial-for-scatter-plot-with-marginal-distribution.html\n\nMany introductory econometrics textbooks have a good appendix on probability and statistics. There are many useful statistical texts online too\nSee the Further reading about Probability Theory in the Statistics chapter.\n\nhttps://www.r-bloggers.com/2024/03/calculating-conditional-probability-in-r/",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate Distributions</span>"
    ]
  },
  {
    "objectID": "02_01_BivariateDistributions.html#footnotes",
    "href": "02_01_BivariateDistributions.html#footnotes",
    "title": "10  Bivariate Distributions",
    "section": "",
    "text": "The same can be said about assuming normally distributed errors, although at least that can be motivated by the Central Limit Theorems.↩︎",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate Distributions</span>"
    ]
  },
  {
    "objectID": "02_02_BivariateStatistics.html",
    "href": "02_02_BivariateStatistics.html",
    "title": "11  Bivariate Statistics",
    "section": "",
    "text": "11.1 Statistics of Association\nAll of the univariate statistics we have covered apply to marginal distributions. For joint distributions, there are several ways to statistically describe the relationship between two variables. The major differences surround whether the data are cardinal or an ordered/unordered factor.",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bivariate Statistics</span>"
    ]
  },
  {
    "objectID": "02_02_BivariateStatistics.html#statistics-of-association",
    "href": "02_02_BivariateStatistics.html#statistics-of-association",
    "title": "11  Bivariate Statistics",
    "section": "",
    "text": "Two Cardinals.\nPearson (Linear) Correlation. Suppose you have two vectors, \\(\\hat{X}\\) and \\(\\hat{Y}\\), that are both cardinal data. As such, you can compute the most famous measure of association, the covariance. Letting \\(\\hat{M}_{X}\\) and \\(\\hat{M}_{Y}\\) denote the mean of \\(\\hat{X}\\) and \\(\\hat{Y}\\), we have \\[\\begin{eqnarray}\n\\hat{C}_{XY} =  \\sum_{i=1}^{n} [\\hat{X}_{i} - \\hat{M}_{X}] [\\hat{Y}_i - \\hat{M}_{Y}] / n\n\\end{eqnarray}\\]\nNote that covariance of \\(\\hat{X}\\) and \\(\\hat{X}\\) is just the variance of \\(\\hat{X}\\); \\(\\hat{C}_{XX}=\\hat{V}_{X}\\), and recall that the standard deviation is \\(\\hat{S}_{X}=\\sqrt{\\hat{V}_X}\\). For ease of interpretation and comparison, we rescale the correlation statistic to always lay on a scale \\(-1\\) and \\(+1\\). A value close to \\(-1\\) suggests negative association, a value close to \\(0\\) suggests no association, and a value close to \\(+1\\) suggests positive association. \\[\\begin{eqnarray}\n\\hat{R}_{XY} = \\frac{ \\hat{C}_{XY} }{ \\hat{S}_{X} \\hat{S}_{Y}}\n\\end{eqnarray}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf \\(\\hat{X}=\\left(0,1,2\\right)\\) and \\(\\hat{Y}=\\left(0.1, 0.3, 0.2\\right)\\), what is the correlation? Find the answer both mathematically and computationally.\nMathematically, there are five steps.\nStep 1: Compute the means \\[\\begin{eqnarray}\n\\hat{M}_{X} &=& \\frac{0+1+2}{3} = 1 \\\\\n\\hat{M}_{Y} &=& \\frac{0.1+0.3+0.2}{3} = 0.2\n\\end{eqnarray}\\]\nStep 2: Compute the deviances \\[\\begin{eqnarray}\n\\begin{array}{c|rrrr}\n\\hat{X}_i   & 0   & 1 & 2 \\\\\n\\hat{X}_i-\\hat{M}_{X} & -1  & 0 & 1 \\\\\n\\hat{Y}_i   & 0.1 & 0.3 & 0.2 \\\\\n\\hat{Y}_i-\\hat{M}_{Y}   & -0.1 & 0.1 & 0\n\\end{array}\n\\end{eqnarray}\\]\nStep 3: Compute the Covariance \\[\\begin{eqnarray}\n\\hat{C}_{XY} &=&\n\\sum  (\\hat{X}_i-\\hat{M}_{X})(\\hat{Y}_i-\\hat{M}_{Y})/n\n= \\left[ (-1)(-0.1) + 0(0.1) + 1(0) \\right] \\frac{1}{3}\n=  (-0.1) \\frac{1}{3}\n= 1/30\n\\end{eqnarray}\\]\nStep 4: Compute Standard Deviations \\[\\begin{eqnarray}\n\\hat{V}_{X} &=& \\sum_{i=1}^n \\left(\\hat{X}_i-\\hat{M}_{X}\\right)^2 / n\n= \\left[(-1)^2+0^2+1^2 \\right]/3\n= 2/3 \\\\\n\\hat{S}_{X} &=& \\sqrt{2/3} \\\\\n\\hat{V}_{Y} &=& \\sum_{i=1}^n \\left( \\hat{Y}_i-\\hat{M}_{Y} \\right)^2 / n\n= \\left[ (-0.1)^2+(0.1)^2+0^2 \\right]/3\n= \\left[0.01+0.01\\right]/3\n= \\frac{2}{100} \\frac{1}{3} = 2/300 \\\\\n\\hat{S}_{Y} &=& \\sqrt{2/300}\n\\end{eqnarray}\\]\nStep 5: Compute the Correlation \\[\\begin{eqnarray}\n\\frac{\\hat{C}_{XY}}{\\hat{S}_X \\hat{S}_Y}\n&=& \\frac{1/30}{ \\sqrt{2/3}  \\sqrt{2/300}}\n= \\frac{1/30}{ 2 /\\sqrt{900}}\n= \\frac{1/30}{2/30} = 1/2\n\\end{eqnarray}\\]\nNote that this value suggests a positive relationship between the variables.\nComputationally, we do the same steps\n\n\nCode\n# Create the Data\nX &lt;- c(0,1,2)\nX\n## [1] 0 1 2\nY &lt;- c(0.1,0.3,0.2)\nY\n## [1] 0.1 0.3 0.2\n\n# Compute the Means\nmX &lt;- mean(X)\nmY &lt;- mean(Y)\n\n# Compute the Deviances\ndev_X &lt;- X - mX\ndev_Y &lt;- Y - mY\n\n# Compute the Covariance\ncov_manual &lt;-  sum(dev_X * dev_Y) / length(X)\n\n# Compute the Standard Deviations\nvar_X &lt;- sum(dev_X^2) / length(X)\nsd_X &lt;- sqrt(var_X)\nvar_Y &lt;- sum(dev_Y^2) / length(Y)\nsd_Y &lt;- sqrt(var_Y)\n\n# Compute the Correlation\ncor_manual &lt;- cov_manual / (sd_X * sd_Y)\ncor_manual\n## [1] 0.5\n\n# Verify with the built-in function\ncor(X,Y)\n## [1] 0.5\n\n\n\n\n\nYou can conduct hypothesis tests for these statistics using the same procedures we learned for univariate data. For example, by inverting a confidence interval.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nCode\n# Load the Data\nxy &lt;- USArrests[,c('Murder','UrbanPop')]\nxy_cor &lt;- cor(xy[, 1], xy[, 2])\n#plot(xy, pch=16, col=grey(0,.25))\n    \n# Bootstrap Distribution of Correlation\nn &lt;- nrow(xy)\nbootstrap_cor &lt;- vector(length=9999)\nfor(b in seq(bootstrap_cor) ){\n    xy_b &lt;- xy[sample(n, replace=T),]\n    xy_cor_b &lt;- cor(xy_b[, 1], xy_b[, 2])\n    bootstrap_cor[b] &lt;- xy_cor_b\n}\nhist(bootstrap_cor, breaks=100,\n    border=NA, font.main=1,\n    xlab='Correlation',\n    main='Bootstrap Distribution')\n\n## Test whether correlation is statistically different from 0\nboot_ci &lt;- quantile(bootstrap_cor, probs=c(0.025, 0.975))\nabline(v=boot_ci)\nabline(v=0, col='red')\n\n\n\n\n\n\n\n\n\n\n\n\nImportantly, we can also impose the null of hypothesis of no association by reshuffling the data. If we resample without replacement, this is known as a permutation test.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nCode\nxy &lt;- USArrests[,c('Murder','UrbanPop')]\nxy_cor &lt;- cor(xy[, 1], xy[, 2])\n#plot(xy, pch=16, col=grey(0,.25))\n    \n# Null Bootstrap Distribution of Correlation\nn &lt;- nrow(xy)\nnull_bootstrap_cor &lt;- vector(length=9999)\nfor(b in seq(null_bootstrap_cor) ){\n    xy_b &lt;- xy\n    xy_b[,'UrbanPop'] &lt;- xy[sample(n, replace=T),'UrbanPop'] ## Reshuffle X\n    xy_cor_b &lt;- cor(xy_b[, 1], xy_b[, 2])\n    null_bootstrap_cor[b] &lt;- xy_cor_b\n}\nhist(null_bootstrap_cor, breaks=100,\n    border=NA, font.main=1,\n    xlab='Correlation',\n    main='Null Bootstrap Distribution')\n\n## Test whether correlation is statistically different from 0\nboot_ci &lt;- quantile(null_bootstrap_cor, probs=c(0.025, 0.975))\nabline(v=boot_ci)\nabline(v=xy_cor, col='blue')\n\n\n\n\n\n\n\n\n\nBecause all dependence resides in the pairing, and permuting one margin destroys the pairing completely.\nTo construct a permutation test, we need to use replace=F. Rework the above code to make a Permutation Null Distribution and conduct a permutation test.\n\n\n\n\nA bootstrap CI evaluates sampling variability in the world that generated your data.\nA permutation test constructs the distribution of the statistic in a world where the null is true.\n\nFalk Codeviance. The Codeviance, \\(\\tilde{C}_{XY}\\), is a robust alternative to Covariance. Instead of relying on means (which can be sensitive to outliers), it uses medians.1 We can also scale the Codeviance by the median absolute deviation to compute the median correlation, \\(\\tilde{R}_{XY}\\), which typically lies in \\([-1,1]\\) but not always. Letting \\(\\tilde{M}_{X}\\) and \\(\\tilde{M}_{Y}\\) denote the median of \\(\\hat{X}\\) and \\(\\hat{Y}\\), we have \\[\\begin{eqnarray}\n\\tilde{C}_{XY} = \\text{Med}\\left\\{ |\\hat{X}_{i} - \\tilde{M}_{X}| |\\hat{Y}_i - \\tilde{M}_{Y}| \\right\\} \\\\\n\\tilde{R}_{XY} = \\frac{ \\tilde{C}_{XY} }{ \\hat{\\text{MAD}}_{X} \\hat{\\text{MAD}}_{Y}}.\n\\end{eqnarray}\\]\n\n\nCode\ncodev &lt;- function(xy) {\n  # Compute medians for each column\n  med &lt;- apply(xy, 2, median)\n  # Subtract the medians from each column\n  xm &lt;- sweep(xy, 2, med, \"-\")\n  # Compute CoDev\n  CoDev &lt;- median(xm[, 1] * xm[, 2])\n  # Compute the medians of absolute deviation\n  MadProd &lt;- prod( apply(abs(xm), 2, median) )\n  # Return the robust correlation measure\n  return( CoDev / MadProd)\n}\nxy_codev &lt;- codev(xy)\nxy_codev\n## [1] 0.005707763\n\n\nYou construct sampling distributions and conduct hypothesis tests for Falk’s Codeviance statistic in the same way you do for Pearson’s Correlation statistic.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\nCode\nxy &lt;- USArrests[,c('Murder','UrbanPop')]\nxy_cor &lt;- cor(xy[, 1], xy[, 2])\n#plot(xy, pch=16, col=grey(0,.25))\n    \n# Null Permutation Distribution of Codeviance\nn &lt;- nrow(xy)\nnull_permutation_codev &lt;- vector(length=9999)\nfor(b in seq(null_permutation_codev) ){\n    xy_b &lt;- xy\n    xy_b[,'UrbanPop'] &lt;- xy[sample(n, replace=F),'UrbanPop'] ## Reshuffle X\n    xy_codev_b &lt;- codev(xy_b)\n    null_permutation_codev[b] &lt;- xy_codev_b\n}\nhist(null_permutation_codev, breaks=100,\n    border=NA, font.main=1,\n    xlab='Codeviance',\n    main='Null Permutation Distribution')\n\n## Test whether correlation is statistically different from 0\nabline(v=quantile(null_permutation_codev, probs=c(0.025, 0.975)))\nabline(v=xy_codev, col='blue')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo Ordered Factors.\nSuppose now that \\(\\hat{X}\\) and \\(\\hat{Y}\\) are both ordered variables. Kendall’s rank correlation statistic measures the strength and direction of association by counting the number of concordant pairs (where the ranks agree) versus discordant pairs (where the ranks disagree). A value closer to \\(1\\) suggests positive association in rankings, a value closer to \\(-1\\) suggests a negative association, and a value of \\(0\\) suggests no association in the ordering. \\[\\begin{eqnarray}\n\\hat{KT} = \\frac{2}{n(n-1)} \\sum_{i} \\sum_{j &gt; i} \\text{sgn} \\Bigl( (\\hat{X}_{i} - \\hat{X}_{j})(\\hat{Y}_i - \\hat{Y}_j) \\Bigr),\n\\end{eqnarray}\\] where the sign function is: \\[\\begin{eqnarray}\n\\text{sgn}(z) =\n\\begin{cases}\n    +1 & \\text{if } z &gt; 0\\\\\n    0  & \\text{if } z = 0 \\\\\n    -1 & \\text{if } z &lt; 0\n\\end{cases}.\n\\end{eqnarray}\\]\n\n\nCode\nxy &lt;- USArrests[,c('Murder','UrbanPop')]\nxy[,1] &lt;- rank(xy[,1] )\nxy[,2] &lt;- rank(xy[,2] )\n# plot(xy, pch=16, col=grey(0,.25))\nKT &lt;- cor(xy[, 1], xy[, 2], method = \"kendall\")\nround(KT, 3)\n## [1] 0.074\n\n\nYou construct sampling distributions and conduct hypothesis tests for Kendall’s rank correlation statistic in the same way you do as for Pearson’s Correlation statistic and Falk’s Codeviance statistic.\n\n\n\n\n\n\nTip\n\n\n\n\n\nTest whether Kendal’s correlation statistic is statistically different from \\(0\\).\n\n\nCode\nxy &lt;- USArrests[,c('Murder','UrbanPop')]\nKT &lt;- cor(xy[, 1], xy[, 2], method=\"kendall\")\n\n\n\n\n\nKendall’s rank correlation coefficient can also be used for non-linear relationships, where Pearson’s correlation coefficient often falls short. It almost always helps to visual your data first before summarizing it with a statistic.\n\n\n\n\n\n\n\n\n\n\n\nTwo Unordered Factors.\nSuppose \\(\\hat{X}\\) and \\(\\hat{Y}\\) are both categorical variables; the value of \\(\\hat{X}\\) is one of \\(1...K\\) categories and the value of \\(\\hat{Y}\\) is one of \\(1...J\\) categories. We organize such data as a contingency table with \\(K\\) rows and \\(J\\) columns and use Cramer’s V to quantify the strength of association by adjusting a chi-squared statistic to provide a measure that ranges from \\(0\\) to \\(1\\); \\(0\\) suggests no association while a value closer to \\(1\\) suggests a strong association.\nFirst, compute the chi-square statistic: \\[\\begin{eqnarray}\n\\hat{\\chi}^2 = \\sum_{k=1}^{K} \\sum_{j=1}^{J} \\frac{(\\hat{O}_{kj} - \\hat{E}_{kj})^2}{\\hat{E}_{kj}}.\n\\end{eqnarray}\\]\nwhere\n\n\\(\\hat{O}_{kj}\\) denote the observed frequency in cell \\((k, j)\\),\n\\(\\hat{E}_{kj} = \\hat{RF}_{k} \\cdot \\hat{CF}_j / n\\) is the expected frequency for each cell if \\(\\hat{X}\\) and \\(\\hat{Y}\\) are independent\n\\(\\hat{RF}_{k}\\) denotes the total frequency for row \\(k\\) (i.e., \\(\\hat{RF}_i = \\sum_{j=1}^{J} \\hat{O}_{kj}\\)),\n\\(\\hat{CF}_{j}\\) denotes the total frequency for column \\(j\\) (i.e., \\(\\hat{CF}_{j} = \\sum_{k=1}^{K} \\hat{O}_{kj}\\)),\n\nSecond, normalize the chi-square statistic with the sample size and the degrees of freedom to compute Cramer’s V. Recalling that \\(I\\) is the number of categories for \\(\\hat{X}\\), and \\(J\\) is the number of categories for \\(\\hat{Y}\\), the statistic is \\[\\begin{eqnarray}\n\\hat{CV} = \\sqrt{\\frac{\\hat{\\chi}^2 / n}{\\min(J - 1, \\, K - 1)}}.\n\\end{eqnarray}\\]\n\n\nCode\nxy &lt;- USArrests[,c('Murder','UrbanPop')]\nxy[,1] &lt;- cut(xy[,1],3)\nxy[,2] &lt;- cut(xy[,2],4)\ntable(xy)\n##               UrbanPop\n## Murder         (31.9,46.8] (46.8,61.5] (61.5,76.2] (76.2,91.1]\n##   (0.783,6.33]           4           5           8           5\n##   (6.33,11.9]            0           4           7           6\n##   (11.9,17.4]            2           4           2           3\n\nCV &lt;- function(xy){\n    # Create a contingency table from the categorical variables\n    tbl &lt;- table(xy)\n    # Compute the chi-square statistic (without Yates' continuity correction)\n    chi2 &lt;- chisq.test(tbl, correct=FALSE)[['statistic']]\n    # Total sample size\n    n &lt;- sum(tbl)\n    # Compute the minimum degrees of freedom (min(rows-1, columns-1))\n    df_min &lt;- min(nrow(tbl) - 1, ncol(tbl) - 1)\n    # Calculate Cramer's V\n    V &lt;- sqrt((chi2 / n) / df_min)\n    return(V)\n}\nCV(xy)\n## X-squared \n## 0.2307071\n\n# DescTools::CramerV( table(xy) )\n\n\nYou construct sampling distributions and conduct hypothesis tests for Cramer’s V statistic in the same way you do as the other statistics.",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bivariate Statistics</span>"
    ]
  },
  {
    "objectID": "02_02_BivariateStatistics.html#probability-theory",
    "href": "02_02_BivariateStatistics.html#probability-theory",
    "title": "11  Bivariate Statistics",
    "section": "11.2 Probability Theory",
    "text": "11.2 Probability Theory\nWe will now dig a little deeper theoretically into the statistics we compute. When we know how the data are generated theoretically, we can often compute the theoretical value of the most basic and often-used bivariate statistic: the Pearson correlation. To see this, we focus on two discrete random variables, first showing their covariance, \\(\\mathbb{C}[X_{i}, Y_{i}]\\), and then their correlation \\(\\mathbb{R}[X_{i}, Y_{i}]\\); \\[\\begin{eqnarray}\n\\mathbb{C}[X_{i}, Y_{i}]\n&=& \\mathbb{E}[(X_{i} – \\mathbb{E}[X_{i}])(Y_{i} – \\mathbb{E}[Y_{i}])]\n= \\sum_{x}\\sum_{y}\n\\mathbb{E}[(x – \\mathbb{E}[X_{i}])(y – \\mathbb{E}[Y_{i}])] Prob(X_{i} = x, Y_{i} = y)\n\\\\\n\\mathbb{R}[X_{i}, Y_{i}] &=& \\frac{\\mathbb{C}[X_{i}, Y_{i}] }{ \\mathbb{s}[X_{i}] \\mathbb{s}[Y_{i}] }\n\\end{eqnarray}\\]\nFor example, suppose we have discrete data with the following outcomes and probabilities. Note that cells reflect the probabilities of the outcomes depicted on the row and column lables, e.g. \\(Prob(X_{i}=1, Y_{i}=0)=0.1\\).\n\n\n\n\n\\(x=0\\)\n\\(x=1\\)\n\\(x=2\\)\n\n\n\n\n\\(y=0\\)\n\\(0.0\\)\n\\(0.1\\)\n\\(0.0\\)\n\n\n\\(y=10\\)\n\\(0.1\\)\n\\(0.3\\)\n\\(0.1\\)\n\n\n\\(y=20\\)\n\\(0.1\\)\n\\(0.1\\)\n\\(0.2\\)\n\n\n\nYou can see this information qualitatively in the plot below, where higher probability events are reflected with deeper colors.\n\n\nCode\nprob_table &lt;- expand.grid(x=c(0,1,2), y=c(0,10,20))\nprob_table[,'probabilities'] &lt;- c(\n    0.0, 0.1, 0.0,\n    0.1, 0.3, 0.1,\n    0.1, 0.2, 0.2)\nplot(prob_table[,'x'], prob_table[,'y'],\n     xlim=c(-0.5,2.5), ylim=c(-5, 25),\n     pch=21, cex=8, col='blue',\n     bg=rgb(0,0,1, prob_table[,'probabilities']),\n     xlab = \"X\", ylab = \"Y\")\n\n\n\n\n\n\n\n\n\nAfter verifying that the probabilities sum to \\(1\\), we then compute the marginal distributions \\[\\begin{eqnarray}\nProb(X_{i}=0)=0.2,\\quad Prob(X_{i}=1)=0.5,\\quad Prob(X_{i}=2) = 0.3 \\\\\nProb(Y_{i}=0)=0.1,\\quad Prob(Y_{i}=10)=0.5,\\quad Prob(Y_{i}=20) = 0.4\n\\end{eqnarray}\\] which allows us to compute the means: \\[\\begin{eqnarray}\n\\mathbb{E}[X_{i}] &=& 0(0.2)+1(0.5)+2(0.3) = 1.1 \\\\\n\\mathbb{E}[Y_{i}] &=& 0(0.1)+10(0.5)+20(0.4) = 13\n\\end{eqnarray}\\] We can then compute the cell-by-cell contributions: \\(Prob(X_{i} = x, Y_{i} = y) (x-\\mathbb{E}[X_{i}])(y-\\mathbb{E}[Y_{i}])\\), \\[\\begin{eqnarray}\n\\begin{array}{l l r r r r r}\n\\hline\nx & y & Prob(X_{i}=x, Y_{i}=y) & x-\\mathbb{E}[X_{i}] & y-\\mathbb{E}[Y_{i}] & (x-\\mathbb{E}[X_{i}])(y-\\mathbb{E}[Y_{i}]) & \\text{Contribution}\\\\\n\\hline\n0 & 0  & 0.0 & -1.1 & -13 & 14.3  & 0\\\\\n0 & 10 & 0.1 & -1.1 & -3  & 3.3   & 0.330\\\\\n0 & 20 & 0.1 & -1.1 & 7   & -7.7  & -0.770\\\\\n1 & 0  & 0.1 & -0.1 & -13 & 1.3   & 0.130\\\\\n1 & 10 & 0.3 & -0.1 & -3  & 0.3   & 0.090\\\\\n1 & 20 & 0.1 & -0.1 & 7   & -0.7  & -0.070\\\\\n2 & 0  & 0.0 & 0.9  & -13 & -11.7 & 0\\\\\n2 & 10 & 0.1 & 0.9  & -3  & -2.7  & -0.270\\\\\n2 & 20 & 0.2 & 0.9  & 7   & 6.3   & 1.260\\\\\n\\hline\n\\end{array}\n\\end{eqnarray}\\] and plug them into the covariance \\[\\begin{eqnarray}\n\\mathbb{C}[X_{i},Y_{i}] &=& \\sum_{x} \\sum_{y} \\left(x-\\mathbb{E}[X_{i}]\\right)\\left(y-\\mathbb{E}[Y_{i}]\\right) Prob\\left(X_{i} = x, Y_{i} = y\\right) \\\\\n&=& 0 + 0.330 -0.770 + 0.130 + 0.090  -0.070 +0 -0.270 + 1.260\n= 0.7\n\\end{eqnarray}\\]\nTo compute the correlation value, we first need the standard deviations \\[\\begin{eqnarray}\n\\mathbb{V}[X_{i}] &=& \\sum_{x} (x-\\mathbb{E}[X_{i}])^2 Prob(X_{i} = x) \\\\\n&=& (0-1.1)^2(0.2)+(1-1.1)^2(0.5)+(2-1.1)^2(0.3)=0.49 \\\\\n\\mathbb{s}[X_{i}] &=& \\sqrt{0.49} \\\\\n\\mathbb{V}[Y_{i}] &=& \\sum_{y}  (y-\\mathbb{E}[Y_{i}])^2 Prob(Y_{i} = y) \\\\\n&=& (0-13)^2(0.1)+(10-13)^2(0.5)+(20-13)^2(0.4)=41 \\\\\n\\mathbb{s}[Y_{i}] &=& \\sqrt{41}.\n\\end{eqnarray}\\] Then we can find the correlation as \\[\\begin{eqnarray}\n\\frac{\\mathbb{C}[X_{i},Y_{i}]}{\\mathbb{s}[X_{i}]\\mathbb{s}[Y_{i}]}\n&=& \\frac{0.7}{\\sqrt{0.49} \\sqrt{41}} \\approx 0.156,\n\\end{eqnarray}\\] which suggests a weak positive association between the variables.\n\nUsing the Computer.\nNote that you can do all of the above calculations using the computer instead of by hand.\n\n# Make a Probability Table\nx &lt;- c(0,1,2)\ny &lt;- c(0,10,20)\nxy_probs &lt;- matrix(c(\n    0.0, 0.1, 0.0,\n    0.1, 0.3, 0.1,\n    0.1, 0.1, 0.2\n), nrow=3, ncol=3, byrow=TRUE)\nrownames(xy_probs) &lt;- paste0('y=',y)\ncolnames(xy_probs) &lt;-  paste0('x=',x)\nxy_probs\n##      x=0 x=1 x=2\n## y=0  0.0 0.1 0.0\n## y=10 0.1 0.3 0.1\n## y=20 0.1 0.1 0.2\n\n# Compute Marginals and Means\npX  &lt;- colSums(xy_probs)\npY  &lt;- rowSums(xy_probs)\nEX  &lt;- sum(x * pX)\nEY  &lt;- sum(y * pY)\n\n# Compute Covariance\ndxy_grid &lt;- expand.grid(dy=y-EY, dx=x-EX)[,c(2,1)]\ndxy_grid[,'p'] &lt;- as.vector(xy_probs)\ndxy_grid[,'contribution'] &lt;- dxy_grid[,'dx'] * dxy_grid[,'dy'] * dxy_grid[,'p']\nCovXY &lt;- sum(dxy_grid[,'contribution'])\nCovXY\n## [1] 0.7\n\n# Compute Variances\nVX &lt;- sum( (x-EX)^2 * pX)\nSX &lt;- sqrt(VX)\nVY &lt;- sum( (y-EY)^2 * pY)\nSY &lt;- sqrt(VY)\n\n# Compute Correlation\nCorXY &lt;- CovXY / (SX * SY)\nCorXY\n## [1] 0.1561738\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nCompute the correlation for bivariate data with these probabilities\n\n\n\n\n\\(x=-10\\)\n\\(x=10\\)\n\n\n\n\n\\(y=0\\)\n\\(0.05\\)\n\\(0.20\\)\n\n\n\\(y=1\\)\n\\(0.05\\)\n\\(0.20\\)\n\n\n\\(y=2\\)\n\\(0.05\\)\n\\(0.20\\)\n\n\n\\(y=3\\)\n\\(0.05\\)\n\\(0.20\\)\n\n\n\nAlso compute the correlation for bivariate data with these probabilities\n\n\n\n\n\\(x=-10\\)\n\\(x=10\\)\n\n\n\n\n\\(y=0\\)\n\\(0.05\\)\n\\(0.05\\)\n\n\n\\(y=1\\)\n\\(0.10\\)\n\\(0.10\\)\n\n\n\\(y=2\\)\n\\(0.15\\)\n\\(0.15\\)\n\n\n\\(y=3\\)\n\\(0.20\\)\n\\(0.20\\)\n\n\n\nAlso compute the correlation for bivariate data with these probabilities\n\n\n\n\n\\(x=-10\\)\n\\(x=10\\)\n\n\n\n\n\\(y=0\\)\n\\(0.05\\)\n\\(0.15\\)\n\n\n\\(y=1\\)\n\\(0.05\\)\n\\(0.15\\)\n\n\n\\(y=2\\)\n\\(0.10\\)\n\\(0.20\\)\n\n\n\\(y=3\\)\n\\(0.10\\)\n\\(0.20\\)\n\n\n\nFinally, explain intuitively when the correlation equals \\(0\\) and when it does not.",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bivariate Statistics</span>"
    ]
  },
  {
    "objectID": "02_02_BivariateStatistics.html#further-reading",
    "href": "02_02_BivariateStatistics.html#further-reading",
    "title": "11  Bivariate Statistics",
    "section": "11.3 Further Reading",
    "text": "11.3 Further Reading\nOther Statistics\n\nhttps://cran.r-project.org/web/packages/qualvar/vignettes/wilcox1973.html",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bivariate Statistics</span>"
    ]
  },
  {
    "objectID": "02_02_BivariateStatistics.html#footnotes",
    "href": "02_02_BivariateStatistics.html#footnotes",
    "title": "11  Bivariate Statistics",
    "section": "",
    "text": "See also Theil-Sen Estimator, which may be seen as a precursor.↩︎",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bivariate Statistics</span>"
    ]
  },
  {
    "objectID": "02_03_TwoSamples.html",
    "href": "02_03_TwoSamples.html",
    "title": "12  Comparing Groups",
    "section": "",
    "text": "12.1 Differences\nFor mixed data, \\(\\hat{Y}_{i}\\) is a cardinal variable and \\(\\hat{X}_{i}\\) is a factor variable (typically unordered). For such data, we analyze associations via group comparisons. The basic idea is best seen in a comparison of two samples, which corresponds to an \\(\\hat{X}_{i}\\) with two categories. For example, the heights of men and women in Canada or the homicide rates in two different American states. For another example, the wages for people with and without completing a degree.\nThere may be several differences between these samples. Often, the first summary statistic we investigate is the difference in means.",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparing Groups</span>"
    ]
  },
  {
    "objectID": "02_03_TwoSamples.html#differences",
    "href": "02_03_TwoSamples.html#differences",
    "title": "12  Comparing Groups",
    "section": "",
    "text": "Mean Differences.\nWe often want to know if the means of different sample are different. To test this hypothesis, we compute the means separately for each sample and then examine the differences term \\[\\begin{eqnarray}\n\\hat{D} = \\hat{M}_{Y1} - \\hat{M}_{Y2},\n\\end{eqnarray}\\] with a null hypothesis that there is no difference in the population means.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nCode\n# Differences between means\nm1 &lt;- mean(Y1)\nm2 &lt;- mean(Y2)\nd &lt;- m1-m2\n    \n# Bootstrap Distribution\nbootstrap_diff &lt;- vector(length=9999)\nfor(b in seq(bootstrap_diff) ){\n    Y1_b &lt;- sample(Y1, replace=T)\n    Y2_b &lt;- sample(Y2, replace=T)\n    m1_b &lt;- mean(Y1_b)\n    m2_b &lt;- mean(Y2_b)\n    d_b &lt;- m1_b - m2_b\n    bootstrap_diff[b] &lt;- d_b\n}\nhist(bootstrap_diff,\n    border=NA, font.main=1,\n    main='Difference in Means')\n\n# 2-Sided Test via Confidence Interval\nboot_ci &lt;- quantile(bootstrap_diff, probs=c(.025, .975))\nabline(v=boot_ci, lwd=2)\nabline(v=0, lwd=2, col=2)\n\n\n\n\n\n\n\n\n\n\n\n\nJust as with one sample tests, we can compute a standardized differences, where \\(D\\) is converted into a \\(t\\) statistic. Note, however, that we have to compute the standard error for the difference statistic, which is a bit more complicated. However, this allows us to easily conduct one or two sided hypothesis tests using a standard normal approximation.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\nCode\nse_hat &lt;- sqrt(var(Y1)/n1 + var(Y2)/n2);\nt_obs &lt;- d/se_hat\n\nt_2sample &lt;- function(Y1, Y2){\n    # Differences between means\n    m1 &lt;- mean(Y1)\n    m2 &lt;- mean(Y2)\n    d &lt;- (m1-m2)\n\n    # SE estimate\n    n1  &lt;- length(Y1)\n    n2  &lt;- length(Y2)\n    s1  &lt;- var(Y1)\n    s2  &lt;- var(Y2)\n    s   &lt;- ((n1-1)*s1 + (n2-1)*s2)/(n1+n2-2)\n    d_se &lt;- sqrt(s*(1/n1+1/n2))\n\n    # t stat\n    t_stat &lt;- d/d_se\n    return(t_stat)\n}\n \ntstat &lt;- twosam(data[,'male'], data[,'female'])\ntstat\n\n\n\n\n\n\n\nQuantile Differences.\nThe above procedure generalized from differences in means to other quantiles statistics like medians.\n\n\nCode\n# Quantile Comparison\n\n## Distribution 1\nF1 &lt;- ecdf(Y1)\nplot(F1, col=cols[1],\n     pch=16, xlab='Wages',\n     main='Comparing Medians',\n     font.main=1, bty='n')\n## Median 1\nmed1 &lt;- quantile(F1, probs=0.5)\nsegments(med1, 0, med1, 0.5, col=cols[1], lty=2)\nabline(h=0.5, lty=2)\n\n## Distribution 2\nF2 &lt;- ecdf(Y2)\nplot(F2, add=TRUE, col=cols[2], pch=16)\n## Median 2\nmed2 &lt;- quantile(F2, probs=0.5)\nsegments(med2, 0, med2, 0.5, col=cols[2], lty=2)\n\n## Legend\nlegend('bottomright',\n       col=cols, pch=15,\n       legend=c('Grade 15', 'Grade 16'),\n       title='School Completed')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nCode\n# Bootstrap Distribution Function\nboot_quant &lt;- function(Y1, Y2, B=9999, prob=0.5, ...){\n    bootstrap_diff &lt;- vector(length=B)\n    for(b in seq(bootstrap_diff)){\n        Y1_b &lt;- sample(Y1, replace=T)\n        Y2_b &lt;- sample(Y2, replace=T)\n        q1_b &lt;- quantile(Y1_b, probs=0.5, ...)\n        q2_b &lt;- quantile(Y2_b, probs=0.5, ...)\n        d_b &lt;- q1_b - q2_b\n        bootstrap_diff[b] &lt;- d_b\n    }\n    return(bootstrap_diff)\n}\n\n# 2-Sided Test for Median Differences\n# d &lt;- median(Y2) - median(Y1)\nboot_d &lt;- boot_quant(Y1, Y1, B=999, prob=0.5)\nhist(boot_d, border=NA, font.main=1,\n    main='Difference in Medians')\nabline(v=quantile(boot_d, probs=c(.025, .975)), lwd=2)\nabline(v=0, lwd=2, col=2)\n\n\n\n\n\n\n\n\n\nCode\n1 - ecdf(boot_d)(0)\n## [1] 0.4464464\n\n\n\n\n\nIf we want to test for the differences in medians across groups with independent observations, we can also use notches in the boxplot. If the notches of two boxes do not overlap, then there is rough evidence that the difference in medians is statistically significant. The square root of the sample size is also shown as the bin width in each boxplot.1\n\n\nCode\nboxplot(Y1, Y2,\n    col=c(2,4),\n    notch=T,\n    varwidth=T)\n\n\n\n\n\n\n\n\n\nNote that bootstrap tests can perform poorly with highly unequal variances or skewed data. To see this yourself, make a simulation with skewed data and unequal variances.\nIn principle, we can also examine whether there are differences in spread or shape statistics such as sd and IQR, or skew and kurtosis. More often, however, we examine whether there are any differences in the distributions.\n\n\n\n\n\n\nTip\n\n\n\n\n\nHere is an example to look at differences in “spread”\n\n\nCode\nboot_fun &lt;- function( fun, B=9999, ...){\n    bootstrap_diff &lt;- vector(length=B)\n    for(b in seq(bootstrap_diff)){\n        Y1_b &lt;- sample(Y1, replace=T)\n        Y2_b &lt;- sample(Y2, replace=T)\n        f1_b &lt;- fun(Y1_b, ...)\n        f2_b &lt;- fun(Y2_b, ...)\n        d_b &lt;- f1_b - f2_b\n        bootstrap_diff[b] &lt;- d_b\n    }\n    return(bootstrap_diff)\n}\n\n# 2-Sided Test for SD Differences\n#d &lt;- sd(Y2) - sd(Y1)\nboot_d &lt;- boot_fun(sd)\nhist(boot_d, border=NA, font.main=1,\n    main='Difference in Standard Deviations')\nabline(v=quantile(boot_d, probs=c(.025, .975)), lwd=2)\nabline(v=0, lwd=2, col=2)\n1 - ecdf(boot_d)(0)\n\n\n# Try any function!\n# boot_fun( function(xs) { IQR(xs)/median(xs) } )",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparing Groups</span>"
    ]
  },
  {
    "objectID": "02_03_TwoSamples.html#distributional-comparisons",
    "href": "02_03_TwoSamples.html#distributional-comparisons",
    "title": "12  Comparing Groups",
    "section": "12.2 Distributional Comparisons",
    "text": "12.2 Distributional Comparisons\nWe can also examine whether there are any differences between the entire distributions. Often, we simply plot the two CDF’s. Another useful visualization is to plot the quantiles against one another: a quantile-quantile plot. I.e., the first data point on the bottom left shows the first quantile for both distributions.\n\n\nCode\n# Wage Data (same as from before)\n#library(wooldridge)\n#Y1 &lt;- sort( wage1[wage1[,'educ'] == 15,  'wage'])  \n#Y2 &lt;- sort( wage1[wage1[,'educ'] == 16,  'wage'] )\n\n# Compute Quantiles\nquants &lt;- seq(0,1,length.out=101)\nQ1 &lt;- quantile(Y1, probs=quants)\nQ2 &lt;- quantile(Y2, probs=quants)\n\n# Compare Distributions via Quantiles\n#ry &lt;- range(c(Y1, Y2))\n#plot(ry, c(0,1), type='n', font.main=1,\n#    main='Distributional Comparison',\n#    xlab=\"Quantile\",\n#    ylab=\"Probability\")\n#lines(Q1, quants, col=2)\n#lines(Q2, quants, col=4)\n#legend('bottomright', col=c(2,4), lty=1,\n#   legend=c(\n#       expression(hat(F)[1]),\n#       expression(hat(F)[2]) \n#))\n\n# Compare Quantiles\nry &lt;- range(c(Y1, Y2))\nplot(Q1, Q2, xlim=ry, ylim=ry,\n    xlab=expression(Q[1]),\n    ylab=expression(Q[2]),\n    main='Quantile-Quantile Plot',\n    font.main=1,\n    pch=16, col=grey(0,.25))\nabline(a=0,b=1,lty=2)\n\n\n\n\n\n\n\n\n\nThe starting point for hypothesis testing is the Kolmogorov-Smirnov Statistic: the maximum absolute difference between two CDF’s over all sample data \\(y \\in \\{Y_1\\} \\cup \\{Y_2\\}\\). \\[\\begin{eqnarray}\n\\hat{KS} &=& \\max_{y} |\\hat{F}_{1}(y)- \\hat{F}_{2}(y)|^{p},\n\\end{eqnarray}\\] where \\(p\\) is an integer (typically 1). An intuitive alternative is the Cramer-von Mises Statistic: the sum of absolute differences (raised to an integer, typically 2) between two CDF’s. \\[\\begin{eqnarray}\n\\hat{CVM} &=& \\sum_{y} | \\hat{F}_{1}(y)- \\hat{F}_{2}(y)|^{p}.\n\\end{eqnarray}\\]\n\n\nCode\n# Distributions\ny &lt;- sort(c(Y1, Y2))\nF1 &lt;- ecdf(Y1)(y)\nF2 &lt;- ecdf(Y2)(y)\n\nlibrary(twosamples)\n\n# Kolmogorov-Smirnov\nKSq &lt;- which.max(abs(F2 - F1))\nKSqv &lt;- round(twosamples::ks_stat(Y1, Y2),2)\n\n# Cramer-von Mises Statistic (p=2)\nCVMqv &lt;- round(twosamples::cvm_stat(Y1, Y2, power=2), 2) \n\n# Visualize Differences\nplot(range(y), c(0,1), type=\"n\", xlab='x', ylab='ECDF')\nlines(y, F1, col=2, lwd=2)\nlines(y, F2, col=4, lwd=2)\n\n# KS\ntitle( paste0('KS: ', KSqv), adj=0, font.main=1)\nsegments(y[KSq], F1[KSq], y[KSq], F2[KSq], lwd=1.5, col=grey(0,.75), lty=2)\n\n# CVM\ntitle( paste0('CVM: ', CVMqv), adj=1, font.main=1)\nsegments(y, F1, y, F2, lwd=.5, col=grey(0,.2))\n\n\n\n\n\n\n\n\n\nJust as before, you use bootstrapping for hypothesis testing.\n\n\nCode\ntwosamples::ks_test(Y1, Y2)\n## Test Stat   P-Value \n## 0.2892157 0.0960000\n\ntwosamples::cvm_test(Y1, Y2)\n## Test Stat   P-Value \n##  2.084253  0.082500\n\n\n\nComparing Multiple Groups.\nFor multiple groups, we can tests the equality of all distributions (whether at least one group is different). The Kruskal-Wallis test examines \\(H_0:\\; F_1 = F_2 = \\dots = F_G\\) versus \\(H_A:\\; \\text{at least one } F_g \\text{ differs}\\), where \\(F_g\\) is the continuous distribution of group \\(g=1,...G\\). This test does not tell us which group is different.\nTo conduct the test, first denote individuals \\(i=1,...n\\) with overall ranks \\(\\hat{r}_1,....\\hat{r}_{n}\\). Each individual belongs to group \\(g=1,...G\\), and each group \\(g\\) has \\(n_{g}\\) individuals with average rank \\(\\bar{r}_{g} = \\sum_{i} \\hat{r}_{i} /n_{g}\\). The Kruskal Wallis statistic is \\[\\begin{eqnarray}\n\\hat{KW} &=& (N-1) \\frac{\\sum_{g=1}^{G} n_{g}( \\bar{r}_{g} - \\bar{r}  )^2  }{\\sum_{i=1}^{n} ( \\hat{r}_{i} - \\bar{r}  )^2},\n\\end{eqnarray}\\] where \\(\\bar{r} = \\frac{n+1}{2}\\) is the grand mean rank.\nIn the special case with only two groups, \\(G=2\\), the Kruskal Wallis test reduces to the Mann–Whitney U test (also known as the Wilcoxon rank-sum test). In this case, we can write the hypotheses in terms of individual outcomes in each group, \\(Y_i\\) in one group \\(Y_j\\) in the other; \\(H_0: Prob(Y_i &gt; Y_j)=Prob(Y_i &gt; Y_i)\\) versus \\(H_A: Prob(Y_i &gt; Y_j) \\neq Prob(Y_i &gt; Y_j)\\). The corresponding test statistic is \\[\\begin{eqnarray}\n\\hat{U}   &=& \\min(\\hat{U}_1, \\hat{U}_2) \\\\\n\\hat{U}_g &=& \\sum_{i\\in g}\\sum_{j\\in -g}\n           \\Bigl[\\mathbf 1( \\hat{Y}_{i} &gt; \\hat{Y}_{j}) + \\tfrac12\\mathbf 1(\\hat{Y}_{i} = \\hat{Y}_{j})\\Bigr].\n\\end{eqnarray}\\]\n\n\nCode\nlibrary(AER)\ndata(CASchools)\nCASchools[,'stratio'] &lt;- CASchools[,'students']/CASchools[,'teachers']\n\n# Do student/teacher ratio differ for at least 1 county?\n# Single test of multiple distributions\nkruskal.test(CASchools[,'stratio'], CASchools[,'county'])\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  CASchools[, \"stratio\"] and CASchools[, \"county\"]\n## Kruskal-Wallis chi-squared = 161.18, df = 44, p-value = 2.831e-15\n\n# Multiple pairwise tests\n# pairwise.wilcox.test(CASchools[,'stratio'], CASchools[,'county'])",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparing Groups</span>"
    ]
  },
  {
    "objectID": "02_03_TwoSamples.html#theory",
    "href": "02_03_TwoSamples.html#theory",
    "title": "12  Comparing Groups",
    "section": "12.3 Theory",
    "text": "12.3 Theory\nUnder the assumption that both populations are normally distributed, we can analytically derive the sampling distribution for the difference-in-means.\n\n\nCode\n# Sample 1 (e.g., males)\nn1 &lt;- 100\nY1 &lt;- rnorm(n1, 0, 2)\n# Sample 2 (e.g., females)\nn2 &lt;- 80\nY2 &lt;- rnorm(n2, 1, 1)\n\npar(mfrow=c(1,2))\nbks &lt;- seq(-8,8, by=.5)\nhist(Y1, border=NA, breaks=bks,\n    main='Sample 1', font.main=1)\nhist(Y2, border=NA, breaks=bks, \n    main='Sample 2', font.main=1)",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparing Groups</span>"
    ]
  },
  {
    "objectID": "02_03_TwoSamples.html#further-reading",
    "href": "02_03_TwoSamples.html#further-reading",
    "title": "12  Comparing Groups",
    "section": "12.4 Further Reading",
    "text": "12.4 Further Reading\nOther Statistics\n\nhttps://cran.r-project.org/web/packages/qualvar/vignettes/wilcox1973.html",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparing Groups</span>"
    ]
  },
  {
    "objectID": "02_03_TwoSamples.html#footnotes",
    "href": "02_03_TwoSamples.html#footnotes",
    "title": "12  Comparing Groups",
    "section": "",
    "text": "Let each group \\(g\\) have median \\(\\tilde{M}_{g}\\), interquartile range \\(\\hat{IQR}_{g}\\), observations \\(n_{g}\\). We can compute standard deviation of the median as \\(\\tilde{S}_{g}= \\frac{1.25 \\hat{IQR}_{g}}{1.35 \\sqrt{n_{g}}}\\). As a rough guess, the interval \\(\\tilde{M}_{g} \\pm 1.7 \\tilde{S}_{g}\\) is the historical default and displayed as a notch in the boxplot. See also https://www.tandfonline.com/doi/abs/10.1080/00031305.1978.10479236.↩︎",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparing Groups</span>"
    ]
  },
  {
    "objectID": "02_04_BasicRegression.html",
    "href": "02_04_BasicRegression.html",
    "title": "13  Simple Regression",
    "section": "",
    "text": "13.1 Simple Linear Regression\nSuppose we have some bivariate data: \\(\\hat{X}_{i}, \\hat{Y}_{i}\\). First, we inspect it as in Part I.\nNow we will assess the association between variables by fitting a line through the data points using a “regression”.\nThis refers to fitting a linear model to bivariate data. Specifically, our model is \\[\\begin{eqnarray}\n\\hat{Y}_{i}=b_{0}+b_{1} \\hat{X}_{i}+e_{i},\n\\end{eqnarray}\\] where \\(b_{0}\\) and \\(b_{1}\\) are parameters, often referred to as “coefficients”, and \\(\\hat{X}_{i}, \\hat{Y}_{i}\\) are data for observation \\(i\\), and \\(e_{i}\\) is a residual error term that represents the difference between the model and the data.\nWe then find the parameters which best-fit the data. Specifically, our objective function is \\[\\begin{eqnarray}\n\\min_{b_{0}, b_{1}} \\sum_{i=1}^{n} \\left( e_{i} \\right)^2 &=& \\min_{b_{0}, b_{1}} \\sum_{i=1}^{n} \\left( \\hat{Y}_{i} - [b_{0}+b_{1} \\hat{X}_{i}] \\right)^2.\n\\end{eqnarray}\\] Minimizing the sum of squared errors then yields two parameter estimates: \\[\\begin{eqnarray}\n0 &=& \\sum_{i=1}^{n} 2\\left( \\hat{Y}_{i} - [b_{0}+b_{1} \\hat{X}_{i}] \\right)\\\\\n\\hat{b}_{0} &=& \\hat{M}_{Y}-\\hat{b}_{1}\\hat{M}_{X} \\\\\n\\end{eqnarray}\\] where \\(\\hat{M}_{Y}\\) and \\(\\hat{M}_{X}\\) are sample means. Similarly, \\[\\begin{eqnarray}\n0 &=& \\sum_{i=1}^{n} 2\\left( \\hat{Y}_{i} - [b_{0}+b_{1} \\hat{X}_{i}] \\right) \\hat{X}_{i} \\\\\n\\hat{b}_{1} &=& \\frac{\\sum_{i}^{n}(\\hat{X}_{i}-\\hat{M}_{X})(\\hat{Y}_{i}-\\hat{M}_{Y})}{\\sum_{i}^{}(\\hat{X}_{i}-\\hat{M}_{X})^2} = \\frac{\\hat{C}_{XY}}{\\hat{V}_{X}},\n\\end{eqnarray}\\] the latter term being the estimated covariance between \\(X\\) and \\(Y\\) divided by the variance of \\(X\\). This in turn yields model predictions \\[\\begin{eqnarray}\n\\hat{y}_{i} &=& \\hat{b}_{0}+\\hat{b}_{1}\\hat{X}_{i}\\\\\n\\hat{e}_i &=& \\hat{Y}_{i}-\\hat{y}_{i}\n\\end{eqnarray}\\]\nCode\n# Run a Regression Coefficients\nreg &lt;- lm(y~x, dat=xy)\n# predict(reg)\n# resid(reg)\n# coef(reg)",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple Regression</span>"
    ]
  },
  {
    "objectID": "02_04_BasicRegression.html#simple-linear-regression",
    "href": "02_04_BasicRegression.html#simple-linear-regression",
    "title": "13  Simple Regression",
    "section": "",
    "text": "Goodness of Fit.\nFirst, we qualitatively analyze the ‘’Goodness of fit’’ of our model, we plot our predictions for a qualitative analysis\n\n\nCode\n# Plot Data and Predictions\nlibrary(plotly)\nxy[,'ID'] &lt;- rownames(USArrests)\nxy[,'pred'] &lt;- predict(reg)\nxy[,'resid'] &lt;- resid(reg)\nfig &lt;- plotly::plot_ly(\n  xy, x=~x, y=~y,\n  mode='markers',\n  type='scatter',\n  hoverinfo='text',\n  marker=list(color=grey(0,.25), size=10),\n  text=~paste('&lt;b&gt;', ID, '&lt;/b&gt;',\n              '&lt;br&gt;Urban  :', x,\n              '&lt;br&gt;Murder :', y,\n              '&lt;br&gt;Predicted Murder :', round(pred,2),\n              '&lt;br&gt;Residual :', round(resid,2)))              \n# Add Legend\nfig &lt;- plotly::layout(fig,\n          showlegend=F,\n          title='Crime and Urbanization in America 1975',\n          xaxis = list(title='Percent of People in an Urban Area'),\n          yaxis = list(title='Homicide Arrests per 100,000 People'))\n# Plot Model Predictions\nadd_trace(fig, x=~x, y=~pred,\n    inherit=F, hoverinfo='none',\n    mode='lines+markers', type='scatter',\n    color=I('black'),\n    line=list(width=1/2),\n    marker=list(symbol=134, size=5))\n\n\n\n\n\n\nFor a quantitative summary, we can also compute the linear correlation between the model predictions and the sample data: \\(\\hat{R}_{yY} = \\hat{C}_{yY}/[\\hat{S}_{y} \\hat{S}_{Y}]\\). With linear models, we typically compute the “R-squared” statistic \\(\\hat{R}_{yY}^2\\), also known as the “coefficient of determination”, using the sums of squared errors (Total, Explained, and Residual) \\[\\begin{eqnarray}\n\\underbrace{\\sum_{i}(\\hat{Y}_{i}-\\hat{M}_{Y})^2}_\\text{TSS}\n=\\underbrace{\\sum_{i}(\\hat{y}_i-\\hat{M}_{Y})^2}_\\text{ESS}+\\underbrace{\\sum_{i}\\hat{e}_{i}^2}_\\text{RSS}\\\\\n\\hat{R}_{yY}^2 = \\frac{\\hat{ESS}}{\\hat{TSS}}=1-\\frac{\\hat{RSS}}{\\hat{TSS}}\n\\end{eqnarray}\\]\n\n\nCode\n# Manually Compute R2\nEhat &lt;- resid(reg)\nRSS  &lt;- sum(Ehat^2)\nY &lt;- xy[,'y']\nTSS  &lt;- sum((Y-mean(Y))^2)\nR2 &lt;- 1 - RSS/TSS\nR2\n## [1] 0.00484035\n\n# Check R2\nsummary(reg)$r.squared\n## [1] 0.00484035\n\n# Double Check R2\nR &lt;- cor(xy[,'y'], predict(reg))\nR^2\n## [1] 0.00484035",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple Regression</span>"
    ]
  },
  {
    "objectID": "02_04_BasicRegression.html#variability-estimates",
    "href": "02_04_BasicRegression.html#variability-estimates",
    "title": "13  Simple Regression",
    "section": "13.2 Variability Estimates",
    "text": "13.2 Variability Estimates\nA regression coefficient is a statistic. And, just like all statistics, we can calculate\n\nstandard deviation: variability within a single sample.\nstandard error: variability across different samples.\nconfidence interval: range your statistic varies across different samples.\n\nNote that values reported by your computer do not necessarily satisfy this definition. To calculate these statistics, we will estimate variability using data-driven methods. (For some theoretical background, see https://www.sagepub.com/sites/default/files/upm-binaries/21122_Chapter_21.pdf.)\n\nJackknife.\nWe first consider the simplest, the jackknife. In this procedure, we loop through each row of the dataset. And, in each iteration of the loop, we drop that observation from the dataset and reestimate the statistic of interest. We then calculate the standard deviation of the statistic across all ``subsamples’’.\n\n\nCode\n# Jackknife Standard Errors for OLS Coefficient\njack_regs &lt;- lapply(1:nrow(xy), function(i){\n    xy_i &lt;- xy[-i,]\n    reg_i &lt;- lm(y~x, dat=xy_i)\n})\njack_coefs &lt;- sapply(jack_regs, coef)['x',]\njack_se &lt;- sd(jack_coefs)\n# classic_se &lt;- sqrt(diag(vcov(reg)))[['x']]\n\n\n# Jackknife Sampling Distribution\nhist(jack_coefs, breaks=25,\n    main=paste0('SE est. = ', round(jack_se,4)),\n    font.main=1, border=NA,\n    xlab=expression(hat(b)[-i]))\n# Original Estimate\nabline(v=coef(reg)['x'], lwd=2)\n# Jackknife Confidence Intervals\njack_ci_percentile &lt;- quantile(jack_coefs, probs=c(.025,.975))\nabline(v=jack_ci_percentile, lty=2)\n\n\n\n\n\n\n\n\n\nCode\n\n\n# Plot Normal Approximation\n# jack_ci_normal &lt;- jack_mean+c(-1.96, +1.96)*jack_se\n# abline(v=jack_ci_normal, col=\"red\", lty=3)\n\n\n\n\nBootstrap.\nThere are several resampling techniques. The other main one is the bootstrap, which resamples with replacement for an arbitrary number of iterations. When bootstrapping a dataset with \\(n\\) observations, you randomly resample all \\(n\\) rows in your data set \\(B\\) times.\n\n\nCode\n# Bootstrap\nboot_regs &lt;- lapply(1:399, function(b){\n    b_id &lt;- sample( nrow(xy), replace=T)\n    xy_b &lt;- xy[b_id,]\n    reg_b &lt;- lm(y~x, dat=xy_b)\n})\nboot_coefs &lt;- sapply(boot_regs, coef)['x',]\nboot_se &lt;- sd(boot_coefs)\n\nhist(boot_coefs, breaks=25,\n    main=paste0('SE est. = ', round(boot_se,4)),\n    font.main=1, border=NA,\n    xlab=expression(hat(b)[b]))\nboot_ci_percentile &lt;- quantile(boot_coefs, probs=c(.025,.975))\nabline(v=boot_ci_percentile, lty=2)\nabline(v=coef(reg)['x'], lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nSubsampling.\nRandom subsampling is one of many hybrid approaches that tries to combine the best of the core methods: Bootstrap and Jacknife.\n\n\n\n\nSample Size per Iteration\nNumber of Iterations\nResample\n\n\n\n\nBootstrap\n\\(n\\)\n\\(B\\)\nWith Replacement\n\n\nJackknife\n\\(n-1\\)\n\\(n\\)\nWithout Replacement\n\n\nRandom Subsample\n\\(m &lt; n\\)\n\\(B\\)\nWithout Replacement\n\n\n\n\n\nCode\n# Random Subsamples\nrs_regs &lt;- lapply(1:399, function(b){\n    b_id &lt;- sample( nrow(xy), nrow(xy)-10, replace=F)\n    xy_b &lt;- xy[b_id,]\n    reg_b &lt;- lm(y~x, dat=xy_b)\n})\nrs_coefs &lt;- sapply(rs_regs, coef)['x',]\nrs_se &lt;- sd(rs_coefs)\n\nhist(rs_coefs, breaks=25,\n    main=paste0('SE est. = ', round(rs_se,4)),\n    font.main=1, border=NA,\n    xlab=expression(hat(b)[b]))\nabline(v=coef(reg)['x'], lwd=2)\nrs_ci_percentile &lt;- quantile(rs_coefs, probs=c(.025,.975))\nabline(v=rs_ci_percentile, lty=2)\n\n\n\n\n\n\n\n\n\nWe can also bootstrap other statistics, such as \\(\\hat{t}\\) or \\(\\hat{R}^2\\). We do such things to test a null hypothesis, which is often ``no relationship’’. We are rarely interested in computing standard errors and conducting hypothesis tests for simple linear regressions, but work through the ideas with two variables before moving to analyze multiple variables.",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple Regression</span>"
    ]
  },
  {
    "objectID": "02_04_BasicRegression.html#hypothesis-tests",
    "href": "02_04_BasicRegression.html#hypothesis-tests",
    "title": "13  Simple Regression",
    "section": "13.3 Hypothesis Tests",
    "text": "13.3 Hypothesis Tests\n\nInvert a CI.\nOne main way to conduct hypothesis tests is to examine whether a confidence interval contains a hypothesized value. Does the slope coefficient equal \\(0\\)? For reasons we won’t go into in this class, we typically normalize the coefficient by its standard error: \\(\\hat{t} = \\frac{\\hat{b}}{\\hat{s}_{\\hat{b}}}\\), where \\(\\hat{s}_{\\hat{b}}\\) is the estimated standard error of the coefficient.\n\n\nCode\ntvalue &lt;- coef(reg)['x']/jack_se\n\njack_t &lt;- sapply(jack_regs, function(reg_b){\n    # Data\n    xy_b &lt;- reg_b[['model']]\n    # Coefficient\n    coef_b &lt;- coef(reg_b)[['x']]\n    t_hat_b &lt;- coef_b/jack_se\n    return(t_hat_b)\n})\n\nhist(jack_t, breaks=25,\n    main='Jackknife t Density',\n    font.main=1, border=NA,\n    xlab=expression(hat(t)[b]), \n    xlim=range(c(0, jack_t)) )\nabline(v=quantile(jack_t, probs=c(.025,.975)), lty=2)\nabline(v=0, col=\"red\", lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nImpose the Null.\nWe can also compute a null distribution. We focus on the simplest: bootstrap simulations that each impose the null hypothesis and re-estimate the statistic of interest. Specifically, we compute the distribution of t-values on data with randomly reshuffled outcomes (imposing the null), and compare how extreme the observed value is.\n\n\nCode\n# Null Distribution for Reg Coef\nboot_t0 &lt;- sapply( 1:399, function(b){\n    xy_b &lt;- xy\n    xy_b[,'y'] &lt;- sample( xy_b[,'y'], replace=T)\n    reg_b &lt;- lm(y~x, dat=xy_b)\n    coef_b &lt;- coef(reg_b)[['x']]\n    t_hat_b &lt;- coef_b/jack_se\n    return(t_hat_b)\n})\n\n# Null Bootstrap Distribution\nboot_ci_percentile0 &lt;- quantile(boot_t0, probs=c(.025,.975))\nhist(boot_t0, breaks=25,\n    main='Null Bootstrap Density',\n    font.main=1, border=NA,\n    xlab=expression(hat(t)[b]),\n    xlim=range(boot_t0))\nabline(v=boot_ci_percentile0, lty=2)\nabline(v=tvalue, col=\"red\", lwd=2)\n\n\n\n\n\n\n\n\n\nAlternatively, you can impose the null by recentering the sampling distribution around the theoretical value; \\(\\hat{t} = \\frac{\\hat{b} - \\beta }{\\hat{s}_{\\hat{b}}}\\).1 In any case, we can calculate a p-value: the probability you would see something as extreme as your statistic under the null (assuming your null hypothesis was true). We can always calculate a p-value from an explicit null distribution.\n\n\nCode\n# One Sided Test for P(t &gt; boot_t | Null) = 1 - P(t &lt; boot_t | Null)\nThat_NullDist1 &lt;- ecdf(boot_t0)\nPhat1  &lt;- 1-That_NullDist1(jack_t)\n\n\n# Two Sided Test for P(t &gt; jack_t or t &lt; -jack_t | Null)\nThat_NullDist2 &lt;- ecdf(abs(boot_t0))\nPhat2  &lt;-  1-That_NullDist2( abs(tvalue))\nPhat2\n## [1] 0.6065163\nplot(That_NullDist2, xlim=range(boot_t0, jack_t),\n    xlab=expression( abs(hat(t)[b]) ),\n    main='Null Bootstrap Distribution', font.main=1)\nabline(v=tvalue, col='red')",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple Regression</span>"
    ]
  },
  {
    "objectID": "02_04_BasicRegression.html#footnotes",
    "href": "02_04_BasicRegression.html#footnotes",
    "title": "13  Simple Regression",
    "section": "",
    "text": "Under some assumptions, the null distribution follows a t-distribution. (For more on parametric t-testing based on statistical theory, see https://www.econometrics-with-r.org/4-lrwor.html.)↩︎",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple Regression</span>"
    ]
  },
  {
    "objectID": "02_05_KernelIntro.html",
    "href": "02_05_KernelIntro.html",
    "title": "14  Local Regression",
    "section": "",
    "text": "14.1 Local Relationships",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Local Regression</span>"
    ]
  },
  {
    "objectID": "02_05_KernelIntro.html#local-relationships",
    "href": "02_05_KernelIntro.html#local-relationships",
    "title": "14  Local Regression",
    "section": "",
    "text": "The Effect.\nThe interpretation of regression coefficients as ``the effect’’ assumes the linear model is true. If you fit a line to a non-linear relationship then you will get back a number, but there is no singular the effect if the true relationship is non-linear! Consider a classic example, Anscombe’s Quartet, which shows four very different datasets that give the same linear regression coefficient. You understand the problem because we used scatterplots to visual the data.1\n\n\nCode\n##################\n# Anscombe\n##################\n\npar(mfrow=c(2,2))\nfor(i in 1:4){\n    xi &lt;- anscombe[,paste0('x',i)]\n    yi &lt;- anscombe[,paste0('y',i)]\n    plot(xi, yi, ylim=c(4,13), xlim=c(4,20),\n        pch=16, col=grey(0,.6))\n    reg &lt;- lm(yi~xi)\n    b &lt;- round(coef(reg)[2],2)\n    p &lt;- round(summary(reg)$coefficients[2,4],4)\n    abline(reg, col='orange')\n    title(paste0(\"Slope=\", b,', p=',p), font.main=1)\n}\n\n\n\n\n\n\n\n\n\nCode\n\n## For an even better example, see `Datasaurus Dozen'\n#browseURL(\n#'https://bookdown.org/paul/applied-data-visualization/\n#why-look-the-datasaurus-dozen.html')\n\n\nIt is true that “OLS is the best linear predictor of the nonlinear regression function if the mean-squared error is used as the loss function.” . But this is not a carte-blanche justification for OLS, as the best of the bad predictors is still a bad predictor. For many economic applications, it is more helpful to think and speak of “dose response curves” instead of “the effect”.\nWhile adding interaction terms or squared terms allows one incorporate heterogeneity and non-linearity, they change several features of the model (most of which are not intended). Often, there are nonsensical predicted values. For example, if the most of your age data are between \\([23,65]\\), a quadratic term can imply silly things for people aged \\(10\\) or \\(90\\).\nNonetheless, OLS provides an important piece of quantitative information that is understood by many. All models are an approximation, and sometimes only unimportant nuances are missing from a vanilla linear model. Other times, that model can be seriously misleading. (This is especially true if your making policy recommondations based on a universal ``the effect’’.) As an exploratory tool, OLS is a good guess but one whose point estimates should not be taken too seriously (in which case, the standard errors are also much less important). Before trying to find a regression specification that makes sense for the entire dataset, explore local relationships.\n\n\nLocal Relationships.\nScatterplots are a great and simplest plot for bivariate data that simply plots each observation. There are many extensions and similar tools. The example below shows two ways of summarizing the information; in both cases helping you understand how the central tendency and dispersion change.\n\n\nCode\n##################\n# Application: Summarizing wages\n##################\nlibrary(wooldridge)\n\n## Plot 1\nplot(wage~educ, data=wage1, pch=16, col=grey(0,.1))\neduc_means &lt;- aggregate(wage1[,c(\"wage\",\"educ\")], list(wage1[,'educ']), mean)\npoints(wage~educ, data=educ_means, pch=17, col='blue', type='b')\ntitle(\"Grouped Means and Scatterplot\", font.main=1)\n\n\n\n\n\n\n\n\n\nCode\n\n## Plot 2 (Less informative!)\n#barplot(wage~educ, data=educ_means)\n#title(\"Bar Plot of Grouped Means\")\n\n\n\n\nRegressograms.\nJust as we use histograms to describe the distribution of a random variable, we can use a regressogram for conditional relationships. Specifically, we can use dummies for exclusive intervals or bins to estimate how the average value of \\(Y\\) varies with \\(X\\).\nAfter dividing \\(X\\) into \\(1,...L\\) exclusive bins of width \\(h\\). Each bin has a midpoint, \\(x\\), and an associated dummy variable \\(\\hat{D}_{i}(x,h) = \\mathbf{1}\\left(\\hat{X}_{i} \\in \\left(x-h/2,x+h/2\\right] \\right)\\).2 Then conduct a dummy variable regression \\[\\begin{eqnarray}\n\\hat{Y}_{i} &=& \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} b_{0}(x,h) \\hat{D}_{i}(x,h)  + e_{i},\n\\end{eqnarray}\\] where each bin has a coefficient \\(b_{0}(x,h)\\).\nConsider this two-bin example of how age affects wage for people aged \\(10\\) to \\(70\\). \\[\\begin{eqnarray}\n\\text{Wage}_{i} &=& b_{0}(x=25, h=15) \\mathbf{1}\\left(\\text{Age}_{i} \\in (10,40]\\right) + b_{0}(x=55, h=15) \\mathbf{1}\\left(\\text{Age}_{i} \\in (40,70] \\right) + e_{i}.\n\\end{eqnarray}\\] You could also look at yearly bins and see if a tri-part grouping emerges naturally or not (e.g., whether the main effect on wages is whether your not in school or retired). You can choose other bins as well.\n\n\nCode\n##################\n# Regressogram\n##################\n\n## Ages\nXmx &lt;- 70\nXmn &lt;- 15\n\n##Generate N Observations\ndat_sim &lt;- function(n=1000){\n    n  &lt;- 1000\n    X &lt;- seq(Xmn,Xmx,length.out=n)\n    ## Random Productivity\n    e &lt;- runif(n, 0, 1E6)\n    beta &lt;-  1E-10*exp(1.4*X -.015*X^2)\n    Y    &lt;-  (beta*X + e)/10\n    return(data.frame(Y,X))\n}\n\n\ndat &lt;- dat_sim(1000)\n## Plot\nplot(Y~X, data=dat, pch=16, col=grey(0,.1),\n    ylab='Yearly Productivity ($)', xlab='Age' )\n\n## Regression Estimates\nreg1  &lt;- lm(Y~X, data=dat) ## OLS\nX &lt;- dat[,'X']\npred1 &lt;- cbind( Y=predict(reg1), X)[order(X),]\n\ndat[,'xcc'] &lt;- cut(X, seq(Xmn-1,Xmx,length.out=6)) ## Course Age Bins\nreg2  &lt;- lm(Y~xcc, data=dat)\npred2 &lt;- cbind( Y=predict(reg2), X)[order(X),]\n\ndat[,'xcf']   &lt;- cut(X, seq(Xmn-1, Xmx, length.out=31)) ## Fine Age Bins\nreg3  &lt;- lm(Y~xcf, data=dat)\npred3 &lt;- cbind( Y=predict(reg3), X)[order(X),]\n\n## Compare Models\nlines(Y~X, data=pred1, lwd=2, col=2)\nlines(Y~X, data=pred2, lwd=2, col=3)\nlines(Y~X, data=pred3, lwd=2, col=4)\nlegend('topleft',\n    legend=c('Linear Regression','Regressogram (5)','Regressogram (30)'),\n    lty=1, col=2:4, cex=.8)\n\n\n\n\n\n\n\n\n\nNotice that each bin has \\(n(x,h) = \\sum_{i}^{n}\\hat{D}_{i}(x,h)\\) observations. This means se can split the dataset into parts associated with each bin \\[\\begin{eqnarray}\n\\label{eqn:regressogram1}\n\\sum_{i}^{n}\\left[e_{i}\\right]^2\n&=& \\sum_{i}^{n}\\left[\\hat{Y}_{i}- \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} b_{0}(x,h) \\hat{D}_{i}(x,h) \\right]^2 \\\\\n&=& \\sum_{i}^{n(x_{1},h)}\\left[\\hat{Y}_{i}- \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} b_{0}(x,h) \\hat{D}_{i}(x,h) \\right]^2 + ...  \\nonumber  \\\\\n& & \\sum_{i}^{n(x_{L},h)}\\left[\\hat{Y}_{i}- \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} b_{0}(x,h) \\hat{D}_{i}(x,h) \\right]^2 \\\\\n&=& \\sum_{i}^{n(x_{1},h)}\\left[\\hat{Y}_{i}- b_{0}\\left(x_1,h\\right) \\right]^2 + ... \\sum_{i}^{n(x_L,h)}\\left[\\hat{Y}_{i}-b_{0}\\left(x_L,h\\right) \\right]^2 % +~ (N-1)\\sum_{i}\\hat{Y}_{i}.\n\\end{eqnarray}\\] This separation allows us optimize for each bin separately \\[\\begin{eqnarray}\n\\label{eqn:regressogram2}\n\\min_{ \\left\\{ b_{0}(x,h) \\right\\} } \\sum_{i}^{n}\\left[e_{i}\\right]^2\n&=& \\min_{ \\left\\{ b_{0}(x,h) \\right\\} } \\sum_{i}^{n(x,h)}\\left[\\hat{Y}_{i}- b_{0}\\left(x,h\\right) \\right]^2,\n\\end{eqnarray}\\] since, in either case, minimizing yields \\[\\begin{eqnarray}\n0 &=& -2 \\sum_{i}^{n(x,h)}\\left[ \\hat{Y}_{i} - b_{0}(x,h)  \\right] \\\\\n\\hat{b}_{0}(x,h) &=& \\frac{\\sum_{i}^{n(x,h)} \\hat{Y}_{i}}{ n(x,h) } = \\hat{M}_{Y}(x,h) .\n\\end{eqnarray}\\] As such, the OLS regression yields coefficients that are interpreted as the conditional mean: \\(\\hat{M}_{Y}(x,h)\\). We can directly compute the same statistic directly by simply takes the average value of \\(\\hat{Y}_{i}\\) for all \\(i\\) observations in a particular bin. The values predicted by the model are then found as \\(\\hat{y}_{i} = \\sum_{x} \\hat{b}_{0}(x,h) \\hat{D}_{i}(x,h)\\).\nInterestingly, we can obtain the same statistic from weighted least squares regression. For some specific design point, \\(x\\), we can find \\(\\hat{b}(x, h)\\) by minimizing \\[\\begin{eqnarray}\n\\sum_{i}^{n}\\left[ e_{i} \\right]^2  \\hat{D}_{i}(x,h) &=& \\sum_{i}^{n}\\left[ \\hat{Y}_{i}- b_{0}(x,h) \\right]^2  \\hat{D}_{i}(x,h) \\\\\n&=& \\sum_{i}^{n(x_{1},h)}\\left[ \\hat{Y}_{i}- b_{0}(x_{1},h) \\right]^2  \\hat{D}_{i}(x_{1},h) + ... \\sum_{i}^{n(x_{L},h)}\\left[ \\hat{Y}_{i}- b_{0}(x_{L},h) \\right]^2  \\hat{D}_{i}(x_{L},h) \\\\\n&=& \\sum_{i}^{n(x,h)}\\left[\\hat{Y}_{i}- b_{0}\\left(x,h\\right) \\right]^2\n\\end{eqnarray}\\]\n\n\nPiecewise Regression.\nThe regressogram depicts locally constant relationships. We can also included slope terms within each bin to allow for locally linear relationships. This is often called segmented/piecewise regression, which runs a separate regression for different subsets of the data.\n\\[\\begin{eqnarray}\n\\hat{Y}_{i} &=& \\sum_{x} \\left[b_{0}(x,h) + b_{1}(x,h)\\hat{X}_{i} \\right] \\hat{D}_{i}(x,h) + e_{i}.\n\\end{eqnarray}\\]\n\n\nCode\n##################\n# Regressogram w/ Slopes\n##################\n\n## Plot\ndat &lt;- dat_sim(1000)\nplot(Y~X, data=dat, pch=16, col=grey(0,.1),\n    ylab='Yearly Productivity ($)', xlab='Age' )\n\n## Course Age Bins\nX &lt;- dat[,'X']\n#### Single Regression\ndat[,'xcc']   &lt;- cut(X, seq(Xmn-1,Xmx,length.out=6)) ## Course Age Bins\npred4 &lt;- cbind( Y=predict( lm(Y~xcc*X, data=dat) ), X)[order(X),]\n#### Split Sample Regressions\ndat4 &lt;- split( dat, dat[,'xcc'])\npred4_B &lt;- lapply(dat4, function(d){\n    pred_d &lt;- cbind( Y=predict( lm(Y~X, d)), X=d[,'X'])\n})\npred4_B &lt;- as.data.frame(do.call('rbind', pred4_B))\npred4_B &lt;- pred4_B[order(pred4_B[,'X']),]\n\nlines(Y~X, data=pred4, lwd=2, col=5, lty=1)\nlines(Y~X, data=pred4_B, lwd=4, col=5, lty=3)\n\n\n## Fine Age Bins\n#### Single Regression\ndat[,'xcf']  &lt;- cut(X, seq(Xmn-1,Xmx,length.out=31)) ## Course Age Bins\npred5 &lt;- cbind( Y=predict(lm(Y~xcf*X,data=dat)), X)[order(X),]\n#### Split Sample Regressions\ndat5 &lt;- split(dat, dat[,'xcf'])\npred5_B &lt;- lapply(dat5, function(d){\n    pred_d &lt;- cbind( Y=predict(lm(Y~X, d)), X=d[,'X'])\n})\npred5_B &lt;- as.data.frame(do.call('rbind', pred5_B))\npred5_B &lt;- pred5_B[order(pred5_B[,'X']),]\n\n## Compare Models\nlines(Y~X, data=pred5, lwd=2, col=6, lty=1)\nlines(Y~X, data=pred5_B, lwd=4, col=6, lty=3)\nlegend('topleft', \n    legend=c('5 bins','30 bins'),\n    lty=1, col=5:6, cex=.8)\n\n\n\n\n\n\n\n\n\nHere is another example with a real dataset\n\n\nCode\nxy &lt;- USArrests[,c('Murder','UrbanPop')]\ncolnames(xy) &lt;- c('y','x')\n\n# Globally Linear\nreg &lt;- lm(y~x, data=xy)\n\n# Diagnose Fit\n#plot( fitted(reg), resid(reg), pch=16, col=grey(0,.5))\n#plot( xy[,'x'], resid(reg), pch=16, col=grey(0,.5))\n\n# Linear in 2 Pieces (subsets)\nxcut2 &lt;- cut(xy[,'x'],2)\nxy_list2 &lt;- split(xy, xcut2)\nregs2 &lt;- lapply(xy_list2, function(xy_s){\n    lm(y~x, data=xy_s)\n})\nsapply(regs2, coef)\n##             (31.9,61.5] (61.5,91.1]\n## (Intercept)  -0.2836303  4.15337509\n## x             0.1628157  0.04760783\n\n# Linear in 3 Pieces (subsets or bins)\nxcut3 &lt;- cut(xy[,'x'], seq(32,92,by=20)) # Finer Bins\nxy_list3 &lt;- split(xy, xcut3)\nregs3 &lt;- lapply(xy_list3, function(xy_s){\n    lm(y~x, data=xy_s)\n})\nsapply(regs3, coef)\n##                (32,52]    (52,72]      (72,92]\n## (Intercept) 4.60313390 2.36291848  8.653829140\n## x           0.08233618 0.08132841 -0.007174454\n\n\nCompare Predictions\n\n\nCode\npred1 &lt;- data.frame(yhat=predict(reg), x=reg[['model']][,'x'])\npred1 &lt;- pred1[order(pred1[,'x']),]\n\npred2 &lt;- lapply(regs2, function(reg){\n    data.frame(yhat=predict(reg), x=reg[['model']][,'x'])\n})\npred2 &lt;- do.call(rbind,pred2)\npred2 &lt;- pred2[order(pred2[,'x']),]\n\npred3 &lt;- lapply(regs3, function(reg){\n    data.frame(yhat=predict(reg), x=reg[['model']][,'x'])\n})\npred3 &lt;- do.call(rbind,pred3)\npred3 &lt;- pred3[order(pred3[,'x']),]\n\n# Compare Predictions\nplot(y ~ x, pch=16, col=grey(0,.5), dat=xy)\nlines(yhat~x, pred1, lwd=2, col=2)\nlines(yhat~x, pred2, lwd=2, col=4)\nlines(yhat~x, pred3, lwd=2, col=3)\nlegend('topleft',\n    legend=c('Globally Linear', 'Peicewise Linear (2)','Peicewise Linear (3)'),\n    lty=1, col=c(2,4,3), cex=.8)\n\n\n\n\n\n\n\n\n\nFor many things, a simple linear regression, regressograms, or piecewise regression is “good enough”. Simple linear regressions struggle with nonlinear relationships but are very easy to run with a computer. Regressograms and piecewise regressions are intuitive ways to capture nonlinear relationships that are computationally efficient but have obvious problems where the bins change. Sometimes we want smoother predictions or to estimate derivatives (gradients). To cover more advanced regression methods that do those things, we will need to first learn about kernel density estimation.",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Local Regression</span>"
    ]
  },
  {
    "objectID": "02_05_KernelIntro.html#kernel-density-estimation",
    "href": "02_05_KernelIntro.html#kernel-density-estimation",
    "title": "14  Local Regression",
    "section": "14.2 Kernel Density Estimation",
    "text": "14.2 Kernel Density Estimation\nA kernel density is generally a “smooth” version of a histogram. We estimate the density at many points (e.g., all unique values \\(x\\) in the dataset), not just the midpoints of exclusive bins. The uniform kernel and density estimator is \\[\\begin{eqnarray}\n\\label{eqn:uniform}\n\\hat{f}_{U}(x) &=& \\frac{1}{n} \\sum_{i}^{n} \\frac{k_{U}(\\hat{X}_{i}, x, h) }{2h} \\\\\nk_{U}\\left( \\hat{X}_{i}, x, h \\right) &=& \\mathbf{1}\\left(\\frac{|\\hat{X}_{i}-x|}{h}&lt;1\\right)\n= \\mathbf{1}\\left( \\hat{X}_{i} \\in \\left( x-h, x + h\\right) \\right).\n\\end{eqnarray}\\] Comparing equations \\(\\ref{eqn:uniform}\\) to \\(\\ref{eqn:indicator}\\), we can see the uniform kernel is essentially the histogram but without the restriction that \\(x\\) must be a midpoint of exclusive bins. Typically, the points \\(x\\) are chosen to be either the unique observations or some equidistant set of “design points”.\nWe can also replace the uniform kernel with a more general kernel function \\(k\\), which is then normalized to an easier to read and program \\(K\\) function: \\(k\\left( \\hat{X}_{i}, x, h \\right)= K\\left( \\frac{|\\hat{X}_{i}-x|}{h} \\right)\\). We can then define a general kernel function as a non-negative real-valued function \\(K\\) that integrates to unity: \\[\\begin{eqnarray}\n\\int_{-\\infty}^{\\infty} K(v) dv &=& 1\n\\end{eqnarray}\\] The general idea behind kernel density is to use windows around each \\(x\\) that potentially overlap, rather than partitioning the range of \\(X\\) into exclusive bins.3 For examples of some common kernels, see https://en.wikipedia.org/wiki/Kernel_(statistics)#In_non-parametric_statistics. In my view, these are the most intuitive and common.\n\n\nCode\n##################\n# Kernel Density Functions\n##################\n\nX &lt;- seq(-2,2, length.out=1001)\n\nplot.new()\nplot.window(xlim=c(-1.2,1.2), ylim=c(0,1))\n\nh &lt;- 1\nlines( dunif(X,-h,h)~X, col=1, lty=1)\n\nh &lt;- 1/2\nlines( dnorm(X,0,h)~X, col=2, lty=1)\n\ndtricub &lt;- function(X, x=0, h){\n    u &lt;- abs(X-x)/h\n    fu &lt;- 70/81*(1-u^3)^3/h*(u &lt;= 1)\n    return(fu)\n}\nh &lt;- 1\nlines( dtricub(X,0,h)~X, col=3, lty=1)\n\nh &lt;- 1/2\nlines(density(x=0, bw=h, kernel=\"epanechnikov\"), col=4, lty=1)\n## Note that \"density\" defines h slightly differently\n\nrug(0, lwd=2)\naxis(1)\naxis(2)\n\nlegend('topright', lty=1, col=1:4,\n    legend=c('uniform(1)', 'gaussian(1/2)', 'tricubic(1)', 'epanechnikov(1)'))\n\n\n\n\n\n\n\n\n\nCode\n\n\n## Try others:\n## lines(density(x=0, bw=1/2, kernel=\"triangular\"),col=4, lty=1)\n\n\nOnce we have picked a kernel (which particular one is not particularly important) we can use it to compute density estimates.\n\n\nCode\n##################\n# Kernel Density Estimation\n##################\n\nN &lt;- 1000\ne &lt;- rweibull(N,100,100)\nebins &lt;- seq(floor(min(e)), ceiling(max(e)), length.out=12)\n\n## Histogram Estimates at 12 points\nxbks &lt;- c(ebins[1]-diff(ebins)[1]/2, ebins+diff(ebins)[1]/2)\nhist(e, freq=F, main='', breaks=xbks, ylim=c(0,.4), border=NA)\nrug(e, lwd=.07, col=grey(0,.5))  ## Sample\n\n\n## Manually Compute Uniform Estimate at X=100 with h=2\n# w100 &lt;- (e &lt; 101)*(e &gt; 99)\n# sum(w100)/(N*2)\n\n## Gaussian Estimates at same points as histogram\nF_hat &lt;- sapply(ebins, function(x,h=.5){\n    kx &lt;- dnorm( abs(e-x)/h )\n    fx &lt;- sum(kx,na.rm=T)/(h*N)\n    fx\n})\n## Verify the same\nfhat1 &lt;- density(e, n=12, from=min(ebins), to=max(ebins), bw=.5)\npoints(fhat1[['x']], fhat1[['y']], pch=16, col=rgb(0,0,1,.5), cex=1.5)\n\n## Gaussian Estimates at all sample points\nfhat2 &lt;- density(e, n=1000, from=min(ebins), to=max(ebins), bw=.5)\npoints(fhat2[['x']], fhat2[['y']], pch=16, col=rgb(1,0,0,.25), cex=.5)\n\nlegend('topleft', pch=c(15,16,16),\n    col=c(grey(0,.5),rgb(0,0,1,.5), rgb(1,0,0,.25)),\n    title='Type (# Design Points)', bty='n',\n    legend=c('Histogram (12)',\n    'Gaussian-Kernel (12)',\n    'Gaussian-Kernel (1000)'))",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Local Regression</span>"
    ]
  },
  {
    "objectID": "02_05_KernelIntro.html#local-linear-regression",
    "href": "02_05_KernelIntro.html#local-linear-regression",
    "title": "14  Local Regression",
    "section": "14.3 Local Linear Regression",
    "text": "14.3 Local Linear Regression\nIt is safer to assume that you could be analyzing data with nonlinear relationships. A general nonparametric model is written as \\[\\begin{eqnarray}\n\\hat{Y}_{i} = m(\\hat{X}_{i}) + \\epsilon_{i}\n\\end{eqnarray}\\] where \\(m\\) is an unknown continuous function and \\(\\epsilon\\) is white noise. (As such, the linear model is a special case.) You can estimate the mean of \\(Y_{i}\\) conditional on \\(X_{i}=x\\) with a regressogram or a variety of other least-squares procedures.\n\nLocally Constant.\nConsider a point \\(x\\) and suppose \\(\\hat{Y}_{i} = b(x,h) + e_{i}\\) locally. Then notice a weighted OLS estimator with uniform kernel weights yields \\[\\begin{eqnarray} \\label{eqn:lcls}\n& & \\min_{b(x,h)}~ \\sum_{i}^{n}\\left[e_{i} \\right]^2 k_{U}\\left( \\hat{X}_{i}, x, h \\right) \\\\\n\\Rightarrow & & -2 \\sum_{i}^{n}\\left[\\hat{Y}_{i}- b(x,h) \\right] k_{U}\\left(\\hat{X}_{i}, x, h\\right) = 0\\\\\n\\label{eqn:lcls1}\n\\Rightarrow & & \\hat{b}_{U}(x)\n= \\frac{\\sum_{i} \\hat{Y}_{i} k_{U} \\left( \\hat{X}_{i}, x, h \\right) }{ \\sum_{i} k_{U}\\left( \\hat{X}_{i}, x, h \\right) }\n= \\sum_{i} \\hat{Y}_{i} \\left[ \\frac{ k_{U} \\left( \\hat{X}_{i}, x, h \\right) }{ \\sum_{i} k_{U}\\left( \\hat{X}_{i}, x, h \\right)} \\right] =  \\sum_{i} \\hat{Y}_{i} w_{i},\n\\end{eqnarray}\\] where weight \\(w_{i} = \\mathbf{1}\\left( |\\hat{X}_{i} - x| &lt; h \\right)/N\\). The last equality is derived analogously to equation \\(\\ref{eqn:sum}\\); where \\(k_{U} \\left( \\hat{X}_{i}, x, h \\right)\\) is either one or zero, and \\(\\sum_{i} k_{U} \\left( \\hat{X}_{i}, x, h \\right) = n(x,h)\\).\nWhen \\(n\\) is small, \\(\\hat{b}_{U}(x)\\) is typically estimated for each unique observed value: \\(x \\in \\{ x_{1},...x_{n} \\}\\). For large datasets, you can select a subset or evenly spaced values of \\(x\\) for which to estimate \\(\\hat{b}_{U}(x)\\). If we use exclusive bins, then equation \\(\\ref{eqn:regressogram1}\\) equals \\(\\ref{eqn:lcls1}\\), which shows the regressogram is a kernel regression weights that recovers the conditional mean. This regressogram is more crude but can be estimated with OLS.\n\n\nCode\n##################\n# LCLS\n##################\n## Generate Sample Data\nx &lt;- 1:5\ny &lt;- runif(length(x))\n## plot(x,y)\n\n## Manually Compute Estimate at X=3\nw3 &lt;- dunif(x-3,-1,1) #(x &lt; 4)*(x &gt; 2)\nyhat_3 &lt;- sum(w3*y)\nyhat_3\n## [1] 0.6145845\n\n\nThe basic idea also generalizes other kernels. As such, a kernel regression using uniform weights is often called a ``naive kernel regression’’. Typically, kernel regressions use kernels that weight nearby observations more heavily. We can also add a slope term to improve the fit.\nIf \\(X\\) represents time, then the local constant regressions is also called a moving average. With non-uniform kernel weights, we have a weighted moving average.\n\n\nLocally Linear.\nA less simple case is a local linear regression which conducts a linear regression for each data point using a subsample of data around it. Consider a point \\(x\\) and suppose \\(\\hat{Y}_{i} = b_{0}(x,h) + b_{1}(x) \\hat{X}_{i} + e_{i}\\) for data near \\(x\\). The weighted OLS estimator with kernel weights is \\[\\begin{eqnarray}\n& & \\min_{b_{0}(x,h),b_{1}(x,h)}~ \\sum_{i}^{n}\\left[\\hat{Y}_{i}- b_{0}(x,h) - b_{1}(x,h) \\hat{X}_{i} \\right]^2 K\\left(\\frac{|\\hat{X}_{i}-x|}{h}\\right)\n\\end{eqnarray}\\] Deriving the optimal values \\(\\hat{b}_{0}(x,h)\\) and \\(\\hat{b}_{1}(x,h)\\) for \\(k_{U}\\) is left as a homework exercise.4\n\n\nCode\n# ``Naive\" Smoother\npred_fun &lt;- function(x0, h, xy){\n    # Assign equal weight to observations within h distance to x0\n    # 0 weight for all other observations\n    ki   &lt;- dunif(xy[,'x'], x0-h, x0+h) \n    llls &lt;- lm(y~x, data=xy, weights=ki)\n    yhat_i &lt;- predict(llls, newdata=data.frame(x=x0))\n}\n\nX0 &lt;- sort(unique(xy[,'x']))\npred_lo1 &lt;- sapply(X0, pred_fun, h=2, xy=xy)\npred_lo2 &lt;- sapply(X0, pred_fun, h=20, xy=xy)\n\nplot(y~x, pch=16, data=xy, col=grey(0,.5),\n    ylab='Murder Rate', xlab='Population Density')\ncols &lt;- c(rgb(.8,0,0,.5), rgb(0,0,.8,.5))\nlines(X0, pred_lo1, col=cols[1], lwd=1, type='o')\nlines(X0, pred_lo2, col=cols[2], lwd=1, type='o')\nlegend('topleft', title='Locally Linear',\n    legend=c('h=2 ', 'h=20'),\n    lty=1, col=cols, cex=.8)\n\n\n\n\n\n\n\n\n\nNote that there are more complex versions of local linear regressions (see https://shinyserv.es/shiny/kreg/ for a nice illustration.) An even more complex (and more powerful) version is loess, which uses adaptive bandwidths in order to have a similar number of data points in each subsample (especially useful when \\(X\\) is not uniform.)\n\n\nCode\n# Adaptive-width subsamples with non-uniform weights\nxy0 &lt;- xy[order(xy[,'x']),]\nplot(y~x, pch=16, col=grey(0,.5), dat=xy0)\n\nreg_lo4 &lt;- loess(y~x, data=xy0, span=.4)\nreg_lo8 &lt;- loess(y~x, data=xy0, span=.8)\n\ncols &lt;- hcl.colors(3,alpha=.75)[-3]\nlines(xy0[,'x'], predict(reg_lo4),\n    col=cols[1], type='o', pch=2)\nlines(xy0[,'x'], predict(reg_lo8),\n    col=cols[2], type='o', pch=2)\n\nlegend('topleft', title='Loess',\n    legend=c('span=.4 ', 'span=.8'),\n    lty=1, col=cols, cex=.8)\n\n\n\n\n\n\n\n\n\n\n\nConfidence Bands.\nThe smoothed predicted values estimate the local means. So we can also construct confidence bands\n\n\nCode\n# Loess\nxy0 &lt;- xy[order(xy[,'x']),]\nX0 &lt;- unique(xy0[,'x'])\nreg_lo &lt;- loess(y~x, data=xy0, span=.8)\n\n# Jackknife CI\njack_lo &lt;- sapply(1:nrow(xy), function(i){\n    xy_i &lt;- xy[-i,]\n    reg_i &lt;- loess(y~x, dat=xy_i, span=.8)\n    predict(reg_i, newdata=data.frame(x=X0))\n})\njack_cb &lt;- apply(jack_lo,1, quantile,\n    probs=c(.025,.975), na.rm=T)\n\n# Plot\nplot(y~x, pch=16, col=grey(0,.5), dat=xy0)\npreds_lo &lt;- predict(reg_lo, newdata=data.frame(x=X0))\nlines(X0, preds_lo,\n    col=hcl.colors(3,alpha=.75)[2],\n    type='o', pch=2)\n# Plot CI\npolygon(\n    c(X0, rev(X0)),\n    c(jack_cb[1,], rev(jack_cb[2,])),\n    col=hcl.colors(3,alpha=.25)[2],\n    border=NA)",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Local Regression</span>"
    ]
  },
  {
    "objectID": "02_05_KernelIntro.html#footnotes",
    "href": "02_05_KernelIntro.html#footnotes",
    "title": "14  Local Regression",
    "section": "",
    "text": "The same principles holds when comparing two groups: http://www.stat.columbia.edu/~gelman/research/published/causal_quartet_second_revision.pdf↩︎\nThe bins do not all need to have the same width, but that is a good default and more notationally convenient than letting the bandwidth depend on the bin; \\(h(x)\\).↩︎\nWe only examine symmetric kernels, as some texts also include symmetric in the definition of a kernel; \\(K(v) = K(-v)\\).↩︎\nNote that one general benefit of LLLS is with edge effects (see homework). Another is that it is theoretically motivated: assuming that \\(Y_{i}=m(X_{i}) + \\epsilon_{i}\\), we can then take a Taylor approximation: \\(m(X_{i}) + \\epsilon_{i} \\approx m(x) + m'(x)[X_{i}-x] + \\epsilon_{i} = [m(x) - m'(x)x ] + m'(x)X_{i} + \\epsilon_{i} = b_{0}(x,h) + b_{1}(x,h) X_{i}\\). As such, a third benefit is that the estimated slope coefficient \\(\\hat{b}_{1}(x,h)\\) can be interpreted as the estimated gradient at \\(x\\).↩︎",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Local Regression</span>"
    ]
  },
  {
    "objectID": "02_06_DataAnalysis.html",
    "href": "02_06_DataAnalysis.html",
    "title": "15  Data Analysis",
    "section": "",
    "text": "15.1 Inputs",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "02_06_DataAnalysis.html#inputs",
    "href": "02_06_DataAnalysis.html#inputs",
    "title": "15  Data Analysis",
    "section": "",
    "text": "Reading Data.\nThe first step in data analysis is getting data into R. There are many ways to do this, depending on your data structure. Perhaps the most common case is reading in a csv file.\n\n\nCode\n# Read in csv (downloaded from online)\n# download source 'http://www.stern.nyu.edu/~wgreene/Text/Edition7/TableF19-3.csv'\n# download destination '~/TableF19-3.csv'\nread.csv('~/TableF19-3.csv')\n \n# Can read in csv (directly from online)\n# dat_csv &lt;- read.csv('http://www.stern.nyu.edu/~wgreene/Text/Edition7/TableF19-3.csv')\n\n\nReading in other types of data can require the use of “packages”. For example, the “wooldridge” package contains datasets on crime. To use this data, we must first install the package on our computer. Then, to access the data, we must first load the package.\n\n\nCode\n# Install R Data Package and Load in\ninstall.packages('wooldridge') # only once\nlibrary('wooldridge') # anytime you want to use the data\n\ndata('crime2') \ndata('crime4')\n\n\nWe can use packages to access many different types of data. To read in a Stata data file, for example, we can use the “haven” package.\n\n\nCode\n# Read in stata data file from online\n#library(haven)\n#dat_stata &lt;- read_dta('https://www.ssc.wisc.edu/~bhansen/econometrics/DS2004.dta')\n#dat_stata &lt;- as.data.frame(dat_stata)\n\n# For More Introductory Econometrics Data, see \n# https://www.ssc.wisc.edu/~bhansen/econometrics/Econometrics%20Data.zip\n# https://pages.stern.nyu.edu/~wgreene/Text/Edition7/tablelist8new.htm\n# R packages: wooldridge, causaldata, Ecdat, AER, ....\n\n\n\n\nCleaning Data.\nData transformation is often necessary before analysis, so remember to be careful and check your code is doing what you want. (If you have large datasets, you can always test out the code on a sample.)\n\n\nCode\n# Function to Create Sample Datasets\nmake_noisy_data &lt;- function(n, b=0){\n    # Simple Data Generating Process\n    x &lt;- seq(1,10, length.out=n) \n    e &lt;- rnorm(n, mean=0, sd=10)\n    y &lt;- b*x + e \n    # Obervations\n    xy_mat &lt;- data.frame(ID=seq(x), x=x, y=y)\n    return(xy_mat)\n}\n\n# Two simulated datasets\ndat1 &lt;- make_noisy_data(6)\ndat2 &lt;- make_noisy_data(6)\n\n# Merging data in long format\ndat_merged_long &lt;- rbind(\n    cbind(dat1,DF=1),\n    cbind(dat2,DF=2))\n\n\nNow suppose we want to transform into wide format\n\n\nCode\n# Merging data in wide format, First Attempt\ndat_merged_wide &lt;- cbind( dat1, dat2)\nnames(dat_merged_wide) &lt;- c(paste0(names(dat1),'.1'), paste0(names(dat2),'.2'))\n\n# Merging data in wide format, Second Attempt\n# higher performance\ndat_merged_wide2 &lt;- merge(dat1, dat2,\n    by='ID', suffixes=c('.1','.2'))\n## CHECK they are the same.\nidentical(dat_merged_wide, dat_merged_wide2)\n## [1] FALSE\n# Inspect any differences\n\n# Merging data in wide format, Third Attempt with dedicated package\n# (highest performance but with new type of object)\nlibrary(data.table)\ndat_merged_longDT &lt;- as.data.table(dat_merged_long)\ndat_melted &lt;- melt(dat_merged_longDT, id.vars=c('ID', 'DF'))\ndat_merged_wide3 &lt;- dcast(dat_melted, ID~DF+variable)\n\n## CHECK they are the same.\nidentical(dat_merged_wide, dat_merged_wide3)\n## [1] FALSE\n\n\nOften, however, we ultimately want data in long format\n\n\nCode\n# Merging data in long format, Second Attempt with dedicated package \ndat_melted2 &lt;- melt(dat_merged_wide3, measure=c(\"1_x\",\"1_y\",\"2_x\",\"2_y\"))\nmelt_vars &lt;- strsplit(as.character(dat_melted2[['variable']]),'_')\ndat_melted2[,'DF'] &lt;- sapply(melt_vars, `[[`,1)\ndat_melted2[,'variable'] &lt;- sapply(melt_vars, `[[`,2)\ndat_merged_long2 &lt;- dcast(dat_melted2, DF+ID~variable)\ndat_merged_long2 &lt;- as.data.frame(dat_merged_long2)\n\n## CHECK they are the same.\nidentical( dat_merged_long2, dat_merged_long)\n## [1] FALSE\n\n# Further Inspect\ndat_merged_long2 &lt;- dat_merged_long2[,c('ID','x','y','DF')]\nmapply( identical, dat_merged_long2, dat_merged_long)\n##    ID     x     y    DF \n##  TRUE  TRUE  TRUE FALSE\n\n\n\n\nInspecting Data.\nYou can find a value by a particular criterion\n\n\nCode\ny &lt;- 1:10\n\n# Return Y-value with minimum absolute difference from 3\nabs_diff_y &lt;- abs( y - 3 ) \nabs_diff_y # is this the luckiest number?\n##  [1] 2 1 0 1 2 3 4 5 6 7\n\nmin(abs_diff_y)\n## [1] 0\nwhich.min(abs_diff_y)\n## [1] 3\ny[ which.min(abs_diff_y) ]\n## [1] 3\n\n\nThere are also some useful built in functions for standardizing data\n\n\nCode\nm &lt;- matrix(c(1:3,2*(1:3)),byrow=TRUE,ncol=3)\nm\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    2    4    6\n\n# normalize rows\nm/rowSums(m)\n##           [,1]      [,2] [,3]\n## [1,] 0.1666667 0.3333333  0.5\n## [2,] 0.1666667 0.3333333  0.5\n\n# normalize columns\nt(t(m)/colSums(m))\n##           [,1]      [,2]      [,3]\n## [1,] 0.3333333 0.3333333 0.3333333\n## [2,] 0.6666667 0.6666667 0.6666667\n\n# de-mean rows\nsweep(m,1,rowMeans(m), '-')\n##      [,1] [,2] [,3]\n## [1,]   -1    0    1\n## [2,]   -2    0    2\n\n# de-mean columns\nsweep(m,2,colMeans(m), '-')\n##      [,1] [,2] [,3]\n## [1,] -0.5   -1 -1.5\n## [2,]  0.5    1  1.5\n\n\nYou can also easily bin and aggregate data\n\n\nCode\nx &lt;- 1:10\ncut(x, 4)\n##  [1] (0.991,3.25] (0.991,3.25] (0.991,3.25] (3.25,5.5]   (3.25,5.5]  \n##  [6] (5.5,7.75]   (5.5,7.75]   (7.75,10]    (7.75,10]    (7.75,10]   \n## Levels: (0.991,3.25] (3.25,5.5] (5.5,7.75] (7.75,10]\nsplit(x, cut(x, 4))\n## $`(0.991,3.25]`\n## [1] 1 2 3\n## \n## $`(3.25,5.5]`\n## [1] 4 5\n## \n## $`(5.5,7.75]`\n## [1] 6 7\n## \n## $`(7.75,10]`\n## [1]  8  9 10\n\n\n\n\nCode\nxs &lt;- split(x, cut(x, 4))\nsapply(xs, mean)\n## (0.991,3.25]   (3.25,5.5]   (5.5,7.75]    (7.75,10] \n##          2.0          4.5          6.5          9.0\n\n# shortcut\naggregate(x, list(cut(x,4)), mean)\n##        Group.1   x\n## 1 (0.991,3.25] 2.0\n## 2   (3.25,5.5] 4.5\n## 3   (5.5,7.75] 6.5\n## 4    (7.75,10] 9.0\n\n\nSee also https://bookdown.org/rwnahhas/IntroToR/logical.html",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "02_06_DataAnalysis.html#outputs",
    "href": "02_06_DataAnalysis.html#outputs",
    "title": "15  Data Analysis",
    "section": "15.2 Outputs",
    "text": "15.2 Outputs\n\nInteractive Figures.\nNotably, histograms, boxplots, and scatterplots\n\n\nCode\nlibrary(plotly) #install.packages(\"plotly\")\nUSArrests[,'ID'] &lt;- rownames(USArrests)\n\n# Scatter Plot\nfig &lt;- plot_ly(\n    USArrests, x = ~UrbanPop, y = ~Assault,\n    mode='markers',\n    type='scatter',\n    hoverinfo='text',\n    marker=list( color='rgba(0, 0, 0, 0.5)'),\n    text = ~paste('&lt;b&gt;', ID, '&lt;/b&gt;',\n        \"&lt;br&gt;Urban  :\", UrbanPop,\n        \"&lt;br&gt;Assault:\", Assault))\nfig &lt;- layout(fig,\n    showlegend=F,\n    title='Crime and Urbanization in America 1975',\n    xaxis = list(title = 'Percent of People in an Urban Area'),\n    yaxis = list(title = 'Assault Arrests per 100,000 People'))\nfig\n\n\n\n\n\n\n\n\nCode\n# Box Plot\nfig &lt;- plot_ly(USArrests,\n    y=~Murder, color=~cut(UrbanPop,4),\n    alpha=0.6, type=\"box\",\n    pointpos=0, boxpoints = 'all',\n    hoverinfo='text',    \n    text = ~paste('&lt;b&gt;', ID, '&lt;/b&gt;',\n        \"&lt;br&gt;Urban  :\", UrbanPop,\n        \"&lt;br&gt;Assault:\", Assault,\n        \"&lt;br&gt;Murder :\", Murder))    \nfig &lt;- layout(fig,\n    showlegend=FALSE,\n    title='Crime and Urbanization in America 1975',\n    xaxis = list(title = 'Percent of People in an Urban Area'),\n    yaxis = list(title = 'Murders Arrests per 100,000 People'))\nfig\n\n\n\n\n\n\n\n\nCode\npop_mean &lt;- mean(USArrests[,'UrbanPop'])\npop_cut &lt;- USArrests[,'UrbanPop'] &lt; pop_mean\nmurder_lowpop &lt;- USArrests[ pop_cut,'Murder']\nmurder_highpop &lt;- USArrests[ !pop_cut,'Murder']\n\n# Overlapping Histograms\nfig &lt;- plot_ly(alpha=0.6, hovertemplate=\"%{y}\")\nfig &lt;- add_histogram(fig, murder_lowpop, name='Low Pop. (&lt; Mean)',\n    histnorm = \"probability density\",\n    xbins = list(start=0, size=2))\nfig &lt;- add_histogram(fig, murder_highpop, name='High Pop (&gt;= Mean)',\n    histnorm = \"probability density\",\n    xbins = list(start=0, size=2))\nfig &lt;- layout(fig,\n    barmode=\"overlay\",\n    title=\"Crime and Urbanization in America 1975\",\n    xaxis = list(title='Murders Arrests per 100,000 People'),\n    yaxis = list(title='Density'),\n    legend=list(title=list(text='&lt;b&gt; % Urban Pop. &lt;/b&gt;')) )\nfig\n\n\n\n\n\n\nCode\n\n# Possible, but less preferable, to stack histograms\n# barmode=\"stack\", histnorm=\"count\"\n\n\nNote that many plots can be made interactive via https://plotly.com/r/. For more details, see examples and then applications.\nIf you have many points, for example, you can make a 2D histogram.\n\n\nCode\nlibrary(plotly)\nfig &lt;- plot_ly(\n    USArrests, x = ~UrbanPop, y = ~Assault)\nfig &lt;- add_histogram2d(fig, nbinsx=25, nbinsy=25)\nfig\n\n\n\n\nInteractive Tables.\nYou can create an interactive table to explore raw data.\n\n\nCode\ndata(\"USArrests\")\nlibrary(reactable)\nreactable(USArrests, filterable=T, highlight=T)\n\n\n\n\n\n\nYou can create an interactive table that summarizes the data too.\n\n\nCode\n# Compute summary statistics\nvars &lt;- names(USArrests)\nstats_list &lt;- lapply(vars, function(v) {\n  x &lt;- USArrests[[v]]\n  c(\n    Variable = v,\n    N       = sum(!is.na(x)),\n    Mean    = mean(x, na.rm = TRUE),\n    SD      = sd(x, na.rm = TRUE),\n    Min     = min(x, na.rm = TRUE),\n    Q1      = as.numeric(quantile(x, 0.25, na.rm = TRUE)),\n    Median  = median(x, na.rm = TRUE),\n    Q3      = as.numeric(quantile(x, 0.75, na.rm = TRUE)),\n    Max     = max(x, na.rm = TRUE)\n  )\n})\n\n# Convert list to data frame with numeric columns \nstats_df &lt;- as.data.frame(do.call(rbind, stats_list), stringsAsFactors = FALSE)\nnum_cols &lt;- setdiff(names(stats_df), \"Variable\")\nstats_df[num_cols] &lt;- lapply(stats_df[num_cols], function(i){\n    round(as.numeric(i), 3)\n})\n\n# Display interactively\nreactable(stats_df)\n\n\n\n\n\n\n\n\nPolishing.\nYour first figures are typically standard, and probably not as good as they should be. Edit your plot to focus on the most useful information. For others to easily comprehend your work, you must also polish the plot. When polishing, you must do two things:\n\nAdd details that are necessary.\nRemove details that are not necessary.\n\n\n\nCode\n# Random Data\nx &lt;- seq(1, 10, by=.0002)\ne &lt;- rnorm(length(x), mean=0, sd=1)\ny &lt;- .25*x + e \n\n# First Draft\n# plot(x, y)\n\n# Second Draft: Focus\n# (In this example: relationship magnitude)\nxs &lt;- scale(x)\nys &lt;- scale(y)\nplot(ys, xs, \n    xlab='', ylab='',\n    pch=16, cex=.5, col=grey(0,.2))\nmtext(expression('['~X[i]-hat(M)[X]~'] /'~hat(S)[X]), 1, line=2.5)\nmtext(expression('['~Y[i]-hat(M)[Y]~'] /'~hat(S)[Y]), 2, line=2.5)\n# Add a 45 degree line\nabline(a=0, b=1, lty=2, col='red')\nlegend('topleft', \n    legend=c('data point', '45 deg. line'),\n    pch=c(16,NA), lty=c(NA,2), col=c(grey(0,.2), 'red'), \n    bty='n')\ntitle('Standardized Relationship')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Another Example\nxy_dat &lt;- data.frame(x=x, y=y)\npar(fig=c(0,1,0,0.9), new=F)\nplot(y~x, xy_dat, pch=16, col=rgb(0,0,0,.05), cex=.5,\n    xlab='', ylab='') # Format Axis Labels Seperately\nmtext( 'y=0.25 x + e\\n e ~ standard-normal', 2, line=2.2)\nmtext( expression(x%in%~'[0,10]'), 1, line=2.2)\n#abline( lm(y~x, data=xy_dat), lty=2)\ntitle('Plot with good features, but too excessive in several ways',\n    adj=0, font.main=1)\n\n# Outer Legend (https://stackoverflow.com/questions/3932038/)\nouter_legend &lt;- function(...) {\n  opar &lt;- par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), \n    mar=c(0, 0, 0, 0), new=TRUE)\n  on.exit(par(opar))\n  plot(0, 0, type='n', bty='n', xaxt='n', yaxt='n')\n  legend(...)\n}\nouter_legend('topright', legend='single data point',\n    title='do you see the normal distribution?',\n    pch=16, col=rgb(0,0,0,.1), cex=1, bty='n')\n\n\n\n\n\n\n\n\n\nLearn to edit your figures:\n\nhttps://websites.umich.edu/~jpboyd/eng403_chap2_tuftegospel.pdf\nhttps://jtr13.github.io/cc19/tuftes-principles-of-data-ink.html\nhttps://github.com/cxli233/FriendsDontLetFriends\nhttps://www.edwardtufte.com/notebook/chartjunk/\nhttps://www.biostat.wisc.edu/~kbroman/topten_worstgraphs/\nhttps://www.businessinsider.com/the-27-worst-charts-of-all-time-2013-6\n\nWhich features are most informative depends on what you want to show, and you can always mix and match. Ne aware that each type has benefits and costs. E.g., see\n\nhttps://www.data-to-viz.com/caveats.html\nhttps://x.com/EdwardTufte/status/1092717905156993024/photo/1\nhttps://towardsdatascience.com/why-a-box-plot-should-not-be-used-alone-and-some-plots-to-use-it-with-23381f7e3cb6/\n\nFor small datasets, you can plot individual data points with a strip chart. For datasets with spatial information, a map is also helpful. Sometime tables are better than graphs (see https://www.edwardtufte.com/notebook/boxplots-data-test). For useful tips, see C. Wilke (2019) “Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures” https://clauswilke.com/dataviz/\nFor plotting math, which should be done very sparingly, see https://astrostatistics.psu.edu/su07/R/html/grDevices/html/plotmath.html and https://library.virginia.edu/data/articles/mathematical-annotation-in-r\n\n\nStatic Publishing.\n\n\nAdvanced and Optional\n\n\nYou can export figures with specific dimensions\n\n\nCode\npdf( 'Figures/plot_example.pdf', height=5, width=5)\n# plot goes here\ndev.off()\n\n\nFor exporting options, see ?pdf. For saving other types of files, see png(\"*.png\"), tiff(\"*.tiff\"), and jpeg(\"*.jpg\")\nYou can also export tables in a variety of formats, including many that other software programs can easily read\n\nCode\nlibrary(stargazer)\n# summary statistics\nstargazer(USArrests,\n    type='html', \n    summary=T,\n    title='Summary Statistics for USArrests')\n\n\n\nSummary Statistics for USArrests\n\n\n\n\n\n\n\nStatistic\n\n\nN\n\n\nMean\n\n\nSt. Dev.\n\n\nMin\n\n\nMax\n\n\n\n\n\n\n\n\nMurder\n\n\n50\n\n\n7.788\n\n\n4.356\n\n\n0.800\n\n\n17.400\n\n\n\n\nAssault\n\n\n50\n\n\n170.760\n\n\n83.338\n\n\n45\n\n\n337\n\n\n\n\nUrbanPop\n\n\n50\n\n\n65.540\n\n\n14.475\n\n\n32\n\n\n91\n\n\n\n\nRape\n\n\n50\n\n\n21.232\n\n\n9.366\n\n\n7.300\n\n\n46.000\n\n\n\n\n\n\n\nNote that many of the best plots are custom made (see https://www.r-graph-gallery.com/). Here are some ones that I have made over the years.",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "02_06_DataAnalysis.html#r-markdown-reports",
    "href": "02_06_DataAnalysis.html#r-markdown-reports",
    "title": "15  Data Analysis",
    "section": "15.3 R-Markdown Reports",
    "text": "15.3 R-Markdown Reports\nWe will use R Markdown for communicating results to each other. Note that R and R Markdown are both languages. R studio interprets R code make statistical computations and interprets R Markdown code to produce pretty documents that contain both writing and statistics. Altogether, your project will use\n\nR: does statistical computations\nR Markdown: formats statistical computations for sharing\nRstudio: graphical user interface that allows you to easily use both R and R Markdown.\n\nHomework reports are probably the smallest document you can create. They are simple reproducible reports made via R Markdown, which are almost entirely self-contained (showing both code and output). To make them, you need two additional packages\n\n\nCode\n# Packages for Rmarkdown\ninstall.packages(\"knitr\")\ninstall.packages(\"rmarkdown\")\n\n# Other packages frequently used\n#install.packages(\"plotly\") #for interactive plots\n#install.packages(\"sf\") #for spatial data\n\n\nYou may need to first install additional software on your computer\n\nPandoc\nRtools-Windows\nXcode-Mac\n\n\nExample 1: Simple Report.\nDownload the source file ReportTemplate_1Descriptive.Rmd and then then create it by following these steps\n\nOpen with Rstudio\nChange the name to your own\nThen either point-and-click “knit” or use the console to run rmarkdown::render('ReportTemplate_1Descriptive.Rmd')\nOpen the new .html file in your web browser (e.g., firefox).",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "02_06_DataAnalysis.html#further-reading",
    "href": "02_06_DataAnalysis.html#further-reading",
    "title": "15  Data Analysis",
    "section": "15.4 Further Reading",
    "text": "15.4 Further Reading\nFor more on data cleaning, see\n\nhttps://raw.githubusercontent.com/rstudio/cheatsheets/main/data-import.pdf\nhttps://cran.r-project.org/web/packages/data.table/vignettes/datatable-reshape.html\n\nFor more guidance on how to create Rmarkdown documents, see\n\nhttps://github.com/rstudio/cheatsheets/blob/main/rmarkdown.pdf\nhttps://cran.r-project.org/web/packages/rmarkdown/vignettes/rmarkdown.html\nhttp://rmarkdown.rstudio.com\nhttps://bookdown.org/yihui/rmarkdown/\nhttps://bookdown.org/yihui/rmarkdown-cookbook/\nhttps://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/rmarkdown.html\nAn Introduction to the Advanced Theory and Practice of Nonparametric Econometrics. Raccine 2019. Appendices B & D.\nhttps://rmd4sci.njtierney.com/using-rmarkdown.html\nhttps://alexd106.github.io/intro2R/Rmarkdown_intro.html\n\nIf you are still lost, try one of the many online tutorials (such as these)\n\nhttps://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf\nhttps://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\nhttps://www.neonscience.org/resources/learning-hub/tutorials/rmd-code-intro\nhttps://m-clark.github.io/Introduction-to-Rmarkdown/\nhttps://www.stat.cmu.edu/~cshalizi/rmarkdown/\nhttp://math.wsu.edu/faculty/xchen/stat412/HwWriteUp.Rmd\nhttp://math.wsu.edu/faculty/xchen/stat412/HwWriteUp.html\nhttps://holtzy.github.io/Pimp-my-rmd/\nhttps://ntaback.github.io/UofT_STA130/Rmarkdownforclassreports.html\nhttps://crd150.github.io/hw_guidelines.html\nhttps://r4ds.had.co.nz/r-markdown.html\nhttp://www.stat.cmu.edu/~cshalizi/rmarkdown\nhttp://www.ssc.wisc.edu/sscc/pubs/RFR/RFR_RMarkdown.html\nhttp://kbroman.org/knitr_knutshell/pages/Rmarkdown.html",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "02_07_MiscTopics.html",
    "href": "02_07_MiscTopics.html",
    "title": "16  Misc. Bivariate Topics",
    "section": "",
    "text": "16.1 Hypothesis Testing",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Misc. Bivariate Topics</span>"
    ]
  },
  {
    "objectID": "02_07_MiscTopics.html#hypothesis-testing",
    "href": "02_07_MiscTopics.html#hypothesis-testing",
    "title": "16  Misc. Bivariate Topics",
    "section": "",
    "text": "Gradients.\nOften, we are interested in gradients: how \\(Y\\) changes with \\(X\\). The linear model depicts this as a simple constant, \\(\\hat{b}_{1}\\), whereas other models do not. A great first way to assess gradients is to plot the predicted values over the explanatory values.\n\n\nCode\n# Adaptive-width subsamples with non-uniform weights\nxy &lt;- USArrests[,c('UrbanPop','Murder')]\nxy0 &lt;- xy[order(xy[,'UrbanPop']),]\nnames(xy0) &lt;- c('x','y')\n\n\nplot(y~x, pch=16, col=grey(0,.5), dat=xy0)\nreg_lo &lt;- loess(y~x, data=xy0, span=.6)\n\nred_col &lt;- rgb(1,0,0,.5)\nlines(xy0[,'x'], predict(reg_lo),\n    col=red_col, type='o', pch=2)\n\n\n\n\n\n\n\n\n\nMore formally, there are two ways to summarize gradients\n\nFor all methods, including regressograms, you can approximate gradients with small finite differences. For some small difference \\(d\\), we can manually compute \\[\\begin{eqnarray}\n\\hat{b}_{1}(x) &=& \\frac{ \\hat{y}(x+\\frac{d}{2}) - \\hat{y}(x-\\frac{d}{2})}{d},\n\\end{eqnarray}\\]\nWhen using split-sample regressions or local linear regressions, you can use the estimated slope coefficients \\(\\hat{b}_{1}(x)\\) as gradient estimates in each direction.\n\nAfter computing gradients, you can summarize them in various plots\n\nHistograms, Scatterplots\nPlot of gradients and their CI’s, \n\nYou may also be interested in a particular gradient or a single summary statistic. For example, a bivariate regressogram can estimate the “marginal effect at the mean”; \\(\\hat{b}_{1}( x=\\hat{M}_{X} )\\). More often you are interested in the “mean of the gradients”, sometimes said simply as “average effect”, which averages the gradients over all datapoints in the dataset: \\(1/n \\sum_{i}^{n} \\hat{b}_{1}(x=X_{i})\\). Alternatively, you may be interested in the median of the gradients, or measures of “effect heterogeneity”: the interquartile range or standard deviation of the gradients. Such statistics are single numbers that can be presented in tabular form: “mean gradient (sd gradient)”. You can alternative report standard errors: “mean gradient (estimated se), sd gradient (estimated se)”.\n\n\nCode\n## Gradients    \npred_lo &lt;- predict(reg_lo)\ngrad_x  &lt;- xy0[,'x'][-1]\ngrad_dx &lt;- diff(xy0[,'x'])\ngrad_dy &lt;- diff(pred_lo)\ngrad_lo &lt;-grad_dy/grad_dx\n\n## Visual Summary\npar(mfrow=c(1,2))\nhist(grad_lo,  breaks=20,\n    border=NA, freq=F,\n    col=red_col,\n    xlab=expression(d~hat(y)/dx),\n    main='') ## Distributional Summary\nplot(grad_x+grad_dx, grad_lo,\n    xlab='x', ylab=expression(d~hat(y)/dx),\n    col=red_col, pch=16) ## Diminishing Returns?\n\n\n\n\n\n\n\n\n\nCode\n\n## Tabular Summary\ntab_stats &lt;- c(\n    mean(grad_lo, na.rm=T),\n    sd(grad_lo, na.rm=T))\ntab_stats\n## [1] 0.01554515 0.16296598\n\n## Use bootstrapping to get SE's\nboot_stats &lt;- matrix(NA, nrow=299, ncol=2)\nfor(b in 1:nrow(boot_stats)){\n    xy_b &lt;- xy0[sample(1:nrow(xy0), replace=T),]\n    reg_lo &lt;- loess(y~x, data=xy_b, span=.6)\n    pred_lo &lt;- predict(reg_lo)\n    grad_lo &lt;- diff(pred_lo)/diff(xy_b[,'x'])\n    dydx_mean &lt;- mean(grad_lo, na.rm=T)\n    dydx_sd &lt;- sd(grad_lo, na.rm=T)\n    boot_stats[b,1] &lt;- dydx_mean\n    boot_stats[b,2] &lt;- dydx_sd\n}\napply(boot_stats, 2, sd)\n## [1] 0.05202475 0.06765710\n\n\n\n\nDiminishing Returns.\nJust as before, there are diminishing returns to larger sample sizes for bivariate statistics. For example, the slope coefficient in simple OLS varies less from sample to sample when the samples are larger. Same for the gradients in loess. This decreased variability across samples makes hypothesis testing more accurate.\n\n\nCode\nB &lt;- 300\nNseq &lt;- seq(10,50, by=1)\nSE &lt;- sapply(Nseq, function(n){\n    sample_stat &lt;- sapply(1:B, function(b){\n        x &lt;- rnorm(n)\n        e &lt;- rnorm(n)        \n        y &lt;- x*3 + x + e\n        reg_lo &lt;- loess(y~x)\n        pred_lo &lt;- predict(reg_lo)\n        grad_lo &lt;- diff(pred_lo)/diff(x)\n        dydx_mean &lt;- mean(grad_lo, na.rm=T)\n        #dydx_sd &lt;- sd(grad_lo, na.rm=T)\n        return(dydx_mean)\n    })\n    sd(sample_stat)\n})\n\nplot(Nseq, SE, pch=16, col=grey(0,.5),\n    main='Mean gradient', font.main=1,\n    ylab='standard error',\n    xlab='sample size')\n\n\n\n\n\n\n\n\n\n\n\nType II Errors.\nWhen we test a hypothesis, we start with a claim called the null hypothesis \\(H_0\\) and an alternative claim \\(H_A\\). Because we base conclusions on sample data, which has variability, mistakes are possible. There are two types of errors:\n\nType I Error: Rejecting a true null hypothesis. (False Positive).\nType II Error: Failing to reject a false null hypothesis (False Negative).\n\n\n\n\n\n\n\n\n\nTrue Situation\nDecision: Fail to Reject \\(H_0\\)\nDecision: Reject \\(H_0\\)\n\n\n\n\n\\(H_0\\) is True\nCorrect (no detection)\nType I Error (False Positive)\n\n\n\\(H_0\\) is False\nType II Error (False Negative; missed detection)\nCorrect (effect detected)\n\n\n\nHere is a Courtroom example: Someone suspected of committing a crime is at trial, and they are either guilty or not (a Bernoulli random variable). You hypothesize that the suspect is innocent, and a jury can either convict them (decide guilty) or free them (decide not-guilty). Recall that fail-to-reject a hypothesis does mean accepting it, so deciding not-guilty does not necessarily mean innocent.\n\n\n\nTrue Situation\nDecision: Free\nDecision: Convict\n\n\n\n\nSuspect Innocent\nCorrectly Freed\nFalsely Convicted\n\n\nSuspect Guilty\nFalsely Freed\nCorrectly Convicted\n\n\n\n\n\nStatistical Power.\nThe probability of Type I Error is called significance level and denoted by \\(Prob(\\text{Type I Error}) = \\alpha\\). The probability of correctly rejecting a false null is called power and denoted by \\(\\text{Power} = 1 - \\beta = 1 -  Prob(\\text{Type II Error})\\).\nSignificance is often chosen by statistical analysts to be \\(\\alpha=0.05\\). Power is less often chosen, instead following from a decision about power.\n\n\n\n\n\n\nTip\n\n\n\n\n\nThe code below runs a small simulation using a shifted, nonparametric bootstrap. Two-sided test; studentized statistic, for \\(H0: \\mu = 0\\)\n\n\nCode\n# Power for Two-sided test;\n# nonparametric bootstrap, studentized statistic\nn &lt;- 25\nmu &lt;- 0\nalpha &lt;- 0.05\nB &lt;- 299\n\nsim_reps &lt;- 100 \n\np_values &lt;- vector(length=sim_reps)\nfor (i in seq(p_values)) {\n    # Generate data\n    X &lt;- rnorm(n, mean=0.2, sd=1)\n    # Observed statistic\n    X_bar &lt;- mean(X)\n    T_obs &lt;-  (X_bar - mu) / (sd(X)/ sqrt(n)) ##studentized\n    # Bootstrap null distribution of the statistic\n    T_boot &lt;- vector(length=B)\n    X_null &lt;- X - X_bar + mu # Impose the null by recentering\n    for (b in seq(T_boot)) {\n      X_b &lt;- sample(X_null, size = n, replace = TRUE)\n      T_b &lt;- (mean(X_b) - mu) / (sd(X_b)/sqrt(n))\n      T_boot[b] &lt;- T_b\n    }\n    # Two-sided bootstrap p-value\n    pval &lt;- mean(abs(T_boot) &gt;= abs(T_obs))\n    p_values[i] &lt;- pval\n    }\npower &lt;- mean(p_values &lt; alpha)\npower\n\n\n\n\n\nThere is an important Trade-off for fixed sample sizes: Increasing significance (fewer false positive) often lowers power (more false negatives). Generally, power depends on the effect size and sample size: bigger true effects and larger \\(n\\) make it easier to detect real differences (higher power, lower \\(\\beta\\)).",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Misc. Bivariate Topics</span>"
    ]
  },
  {
    "objectID": "02_07_MiscTopics.html#predictions",
    "href": "02_07_MiscTopics.html#predictions",
    "title": "16  Misc. Bivariate Topics",
    "section": "16.2 Predictions",
    "text": "16.2 Predictions\n\nDescribe vs. Explain vs. Predict.\nUnderstanding whether we aim to describe, explain, or predict is central to empirical analysis. The three distinct purposes are to accurately describe empirical patterns, explain why the empirical patterns exist, predict what empirical patterns will exist.\n\n\n\n\n\n\n\n\n\nObjective\nCore Question\nGoal\nMethods\n\n\n\n\nDescribe\nWhat is happening?\nCharacterize patterns & facts\nSummary stats, visualizations, correlations\n\n\nExplain\nWhy is it happening?\nEstablish causal relationships & mechanisms\nTheoretical models; experimentation; counterfactual reasoning\n\n\nPredict\nWhat will happen?\nAnticipate outcomes; support decisions that affect the future\nMachine learning, treatment effect forecasting, policy simulations\n\n\n\nHere is an example for minimum wages.\n\nDescribe: Studies document that following minimum-wage increases, overall low-wage employment may look roughly stable in the short run, but disaggregated data often show larger employment declines over longer horizons, especially among youths and racial minorities.\nExplain: Studies investigate economic mechanisms such as (i) whether lower productivity populations are more vulnerable and employers adjust along hiring margins (fewer openings, higher required skills), and (ii) whether effects are larger in sectors like retail/food service where minimum wages bite hardest.\nPredict: A policy simulation that raises the wage floor incorporates subgroup-specific elasticities to forecast different employment losses (e.g., unemployment for teenage or black workers and income gains for stayers)\n\nThe distinctions matter, as they help you recognize that predictive accuracy or good data visualization does not equal causal insight. You can then better align statistical tools with questions. Try remembering this: Describe reality, Explain causes, Predict futures\n\n\nPrediction Intervals.\nIn addition to confidence intervals, we can also compute a prediction interval which estimate the variability of new data rather than a statistic\nIn this example, we consider a single variable and compute the frequency each value was covered.\n\n\nCode\nx &lt;- runif(1000)\n# Middle 90% of values\nxq0 &lt;- quantile(x, probs=c(.05,.95))\npaste0('we are 90% confident that the a future data point will be between ', \n    round(xq0[1],2), ' and ', round(xq0[2],2) )\n## [1] \"we are 90% confident that the a future data point will be between 0.04 and 0.95\"\n\nhist(x,\n    breaks=seq(0,1,by=.01), border=NA,\n    main='Prediction Interval', font.main=1)\nabline(v=xq0)\n\n\n\n\n\n\n\n\n\nIn this example, we consider a range for \\(Y_{i}\\) rather than for \\(m(X_{i})\\). These intervals also take into account the residuals, the variability of individuals rather than the variability of their mean.\n\n\nCode\n# Bivariate Data from USArrests\nxy &lt;- USArrests[,c('Murder','UrbanPop')]\ncolnames(xy) &lt;- c('y','x')\nxy0 &lt;- xy[order(xy[,'x']),]\n\n\nFor a nice overview of different types of intervals, see https://www.jstor.org/stable/2685212. For an in-depth view, see “Statistical Intervals: A Guide for Practitioners and Researchers” or “Statistical Tolerance Regions: Theory, Applications, and Computation”. See https://robjhyndman.com/hyndsight/intervals/ for constructing intervals for future observations in a time-series context. See Davison and Hinkley, chapters 5 and 6 (also Efron and Tibshirani, or Wehrens et al.)\n\n\nCode\n# From \"Basic Regression\"\nxy0 &lt;- xy[order(xy[,'x']),]\nX0 &lt;- unique(xy0[,'x'])\nreg_lo &lt;- loess(y~x, data=xy0, span=.8)\npreds_lo &lt;- predict(reg_lo, newdata=data.frame(x=X0))\n\n\n# Jackknife CI\njack_lo &lt;- sapply(1:nrow(xy), function(i){\n    xy_i &lt;- xy[-i,]\n    reg_i &lt;- loess(y~x, dat=xy_i, span=.8)\n    predict(reg_i, newdata=data.frame(x=X0))\n})\n\nboot_regs &lt;- lapply(1:399, function(b){\n    b_id &lt;- sample( nrow(xy), replace=T)\n    xy_b &lt;- xy[b_id,]\n    reg_b &lt;- lm(y~x, dat=xy_b)\n})\n\nplot(y~x, pch=16, col=grey(0,.5),\n    dat=xy0, ylim=c(0, 20))\nlines(X0, preds_lo,\n    col=hcl.colors(3,alpha=.75)[2],\n    type='o', pch=2)\n\n# Estimate Residuals CI at design points\nres_lo &lt;- sapply(1:nrow(xy), function(i){\n    y_i &lt;- xy[i,'y']\n    preds_i &lt;- jack_lo[,i]\n    resids_i &lt;- y_i - preds_i\n})\nres_cb &lt;- apply(res_lo, 1, quantile,\n    probs=c(.025,.975), na.rm=T)\n\n# Plot\nlines( X0, preds_lo +res_cb[1,],\n    col=hcl.colors(3,alpha=.75)[2], lt=2)\nlines( X0, preds_lo +res_cb[2,],\n    col=hcl.colors(3,alpha=.75)[2], lty=2)\n\n\n\n# Smooth estimates \nres_lo &lt;- lapply(1:nrow(xy), function(i){\n    y_i &lt;- xy[i,'y']\n    x_i &lt;- xy[i,'x']\n    preds_i &lt;- jack_lo[,i]\n    resids_i &lt;- y_i - preds_i\n    cbind(e=resids_i, x=x_i)\n})\nres_lo &lt;- as.data.frame(do.call(rbind, res_lo))\n\nres_fun &lt;- function(x0, h, res_lo){\n    # Assign equal weight to observations within h distance to x0\n    # 0 weight for all other observations\n    ki &lt;- dunif(res_lo[['x']], x0-h, x0+h) \n    ei &lt;- res_lo[ki!=0,'e']\n    res_i &lt;- quantile(ei, probs=c(.025,.975), na.rm=T)\n}\nres_lo2 &lt;- sapply(X0, res_fun, h=15, res_lo=res_lo)\n\nlines( X0, preds_lo + res_lo2[1,],\n    col=hcl.colors(3,alpha=.75)[2], lty=1, lwd=2)\nlines( X0, preds_lo + res_lo2[2,],\n    col=hcl.colors(3,alpha=.75)[2], lty=1, lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Bootstrap Prediction Interval\nboot_resids &lt;- lapply(boot_regs, function(reg_b){\n    e_b &lt;- resid(reg_b)\n    x_b &lt;- reg_b[['model']][['x']]\n    res_b &lt;- cbind(e_b, x_b)\n})\nboot_resids &lt;- as.data.frame(do.call(rbind, boot_resids))\n# Homoskedastic\nehat &lt;- quantile(boot_resids[,'e_b'], probs=c(.025, .975))\nx &lt;- quantile(xy[,'x'],probs=seq(0,1,by=.1))\nboot_pi &lt;- coef(reg)[1] + x*coef(reg)['x']\nboot_pi &lt;- cbind(boot_pi + ehat[1], boot_pi + ehat[2])\n\n# Plot Bootstrap PI\nplot(y~x, dat=xy, pch=16, main='Prediction Intervals',\n    ylim=c(-5,20), font.main=1)\npolygon( c(x, rev(x)), c(boot_pi[,1], rev(boot_pi[,2])),\n    col=grey(0,.2), border=NA)\n\n# Parametric PI (For Comparison)\n#pi &lt;- predict(reg, interval='prediction', newdata=data.frame(x))\n#lines( x, pi[,'lwr'], lty=2)\n#lines( x, pi[,'upr'], lty=2)\n\n\n\n\nCross Validation.\nPerhaps the most common approach to selecting a bandwidth is to minimize error. Leave-one-out Cross-validation minimizes the average “leave-one-out” mean square prediction errors: \\[\\begin{eqnarray}\n\\min_{h} \\quad \\frac{1}{n} \\sum_{i=1}^{n} \\left[ \\hat{Y}_{i} - \\hat{y_{[i]}}(X,h) \\right]^2,\n\\end{eqnarray}\\] where \\(\\hat{y}_{[i]}(X,h)\\) is the model predicted value at \\(X_{i}\\) based on a dataset that excludes \\(X_{i}\\), and \\(h\\) is the bandwidth (e.g., bin size in a regressogram).\nMinimizing out-sample prediction error is perhaps the simplest computational approach to choose bandwidths, and it also addresses an issue that plagues observational studies in the social sciences: your model explains everything and predicts nothing. Note, however, minimizing prediction error is not necessarily “best”.\n\n\nCode\n# Crossvalidated bandwidth for regression\nxy_mat &lt;- data.frame(y=CASchools[,'math'], x1=CASchools[,'income'])\nlibrary(np)\n\n## Grid Search\nBWS &lt;- seq(1,10,length.out=20)\nBWS_CV &lt;- sapply(BWS, function(bw){\n    E_bw &lt;- sapply(1:nrow(xy_mat), function(i){\n        llls &lt;- npreg(y~x1, data=xy_mat[-i,], \n            bws=bw, regtype=\"ll\",\n            ckertype='epanechnikov', bandwidth.compute=F)\n        pred_i &lt;- predict(llls, newdata=xy_mat[i,])\n        e &lt;-  (pred_i- xy_mat[i,'y'])\n        return(e)\n    })\n    return( mean(E_bw^2) )\n})\n\n## Plot MSE\npar(mfrow=c(1,2))\nplot(BWS, BWS_CV, ylab='CV', pch=16, \n    xlab='bandwidth (h)',)\n## Plot Resulting Predictions\nbw &lt;- BWS[which.min(BWS_CV)]\nllls &lt;- npreg(y~x1, data=xy_mat, \n    ckertype='epanechnikov',\n    bws=bw, regtype=\"ll\")\nplot(xy_mat[,'x'], predict(llls), pch=16, col=grey(0,.5),\n    xlab='X', ylab='Predictions')\nabline(a=0,b=1, lty=2)\n\n## Built in algorithmic Optimziation\nllls2 &lt;- npreg(y~x1, data=xy_mat, ckertype='epanechnikov', regtype=\"ll\")\npoints(xy_mat[,'x'], predict(llls2), pch=2, col=rgb(1,0,0,.25))\n\n## Add legend\nadd_legend &lt;- function(...) {\n  opar &lt;- par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), \n              mar=c(0, 0, 0, 0), new=TRUE)\n  on.exit(par(opar))\n  plot(0, 0, type='n', bty='n', xaxt='n', yaxt='n')\n  legend(...)\n}\nadd_legend('topright',\n    col=c(grey(0,.5),rgb(1,0,0,.25)), \n    pch=c(16,2),\n    bty='n', horiz=T,\n    legend=c('Grid Search', 'NP-algorithm'))\n\n\n\n\nCode\n# CV Application\n## Smoothly Estimate X & Y Density\ny &lt;- sort(xy_mat[,'y'])\nfy &lt;- npudens(y, bandwidth.compute=TRUE)\nx1 &lt;- sort(xy_mat[,'x1'])\nfx &lt;- npudens(x1, bandwidth.compute=TRUE)\n## Smoothly Estimate How Y changes with X\nllls2 &lt;- npreg(y~x1,data=xy_mat,\n    ckertype='epanechnikov',\n    regtype=\"ll\", bandwidth.compute=TRUE)\n\n\nlayout( matrix(c(2,0,1,3), ncol=2, byrow=TRUE),\n    widths=c(4/5,1/5), heights=c(1/5,4/5))\n## Joint Distribution\npar(mar=c(4,4,1,1))\nplot(y~x1, data=xy_mat,\n    pch=16, col=grey(0,.25),\n    xlab=\"District Income (1000$)\", \n    ylab=\"Test Score\")\nlines( sort(xy_mat[,'x']), predict(llls2)[order(xy_mat[,'x1'])],\n    pch=16, col=1)\n## Marginal Distribution\npar(mar=c(0,4,1,1))\nplot(x1, predict(fx),\n    col=grey(0,1), type='l', axes=F,\n    xlab='', ylab='')\nrug(x1, col=grey(0,.25))\npar(mar=c(4,0,1,1))\nplot(predict(fy), y,\n    col=grey(0,1), type='l', axes=F,\n    xlab='', ylab='')\nrug(y, col=grey(0,.25), side=2)\n\n\n\n\nBias vs. Variance",
    "crumbs": [
      "Bivariate Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Misc. Bivariate Topics</span>"
    ]
  },
  {
    "objectID": "03-00-MultivariateData.html",
    "href": "03-00-MultivariateData.html",
    "title": "Multivariate Data",
    "section": "",
    "text": "This section introduces linear regression models from the perspective that ``all models are wrong, but some are useful’’. All linear models are estimated via Ordinary Least Squares (OLS). This contrasts with many other introductory books that begin by assuming a linear data generating process.",
    "crumbs": [
      "Multivariate Data"
    ]
  },
  {
    "objectID": "03_01_IntermediateRegression.html",
    "href": "03_01_IntermediateRegression.html",
    "title": "17  Multivariate Data I",
    "section": "",
    "text": "17.1 Multiple Linear Regression\nFirst, note that you can summarize a dataset with multiple variables using the previous tools.\nYou can also use size, color, and shape to distinguish conditional relationships.\nSee also https://plotly.com/r/bubble-charts/\nWith \\(K\\) variables, the linear model is \\[\\begin{eqnarray}\n\\hat{Y}_{i}=b_0 + b_1 \\hat{X}_{i1}+ b_2 \\hat{X}_{i2}+\\ldots+b_K \\hat{X}_{iK}+e_i\n\\end{eqnarray}\\] Grouping the coefficients as a vector \\(B=(b_0 ~~ b_1 ~~... ~~ b_{K})\\). Our objective is \\[\\begin{eqnarray}\n\\min_{B} \\sum_{i=1}^{n} e_{i}^2,\n\\end{eqnarray}\\] which yields the best fitting coefficients \\(\\hat{B^{*}}\\).\nCode\n# Check\nreg &lt;- lm(Murder~Assault+UrbanPop, data=USArrests)\ncoef(reg)\n## (Intercept)     Assault    UrbanPop \n##  3.20715340  0.04390995 -0.04451047\nTo measure the ``Goodness of fit’’ of the model, we can again plot our predictions.\nCode\nplot(USArrests$Murder, predict(reg), pch=16, col=grey(0,.5))\nabline(a=0,b=1, lty=2)\nWe can also again compute sums of squared errors. Adding random data may sometimes improve the fit, however, so we adjust the \\(\\hat{R}^2\\) by the number of covariates \\(K\\). \\[\\begin{eqnarray}\n\\hat{R}_{yY}^2 = \\frac{\\hat{ESS}}{\\hat{TSS}}=1-\\frac{\\hat{RSS}}{\\hat{TSS}}\\\\\n\\hat{R}^2_{\\text{adj.}} = 1-\\frac{n-1}{n-K}(1-\\hat{R}_{yY}^2)\n\\end{eqnarray}\\]\nCode\nksims &lt;- 1:30\nfor(k in ksims){ \n    USArrests[,paste0('R',k)] &lt;- runif(nrow(USArrests),0,20)\n}\nreg_sim &lt;- lapply(ksims, function(k){\n    rvars &lt;- c('Assault','UrbanPop', paste0('R',1:k))\n    rvars2 &lt;- paste0(rvars, collapse='+')\n    reg_k &lt;- lm( paste0('Murder~',rvars2), data=USArrests)\n})\nR2_sim &lt;- sapply(reg_sim, function(reg_k){  summary(reg_k)$r.squared })\nR2adj_sim &lt;- sapply(reg_sim, function(reg_k){  summary(reg_k)$adj.r.squared })\n\nplot.new()\nplot.window(xlim=c(0,30), ylim=c(0,1))\npoints(ksims, R2_sim)\npoints(ksims, R2adj_sim, pch=16)\naxis(1)\naxis(2)\nmtext(expression(R^2),2, line=3)\nmtext('Additional Random Covariates', 1, line=3)\nlegend('topleft', horiz=T,\n    legend=c('Undjusted', 'Adjusted'), pch=c(1,16))",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Multivariate Data I</span>"
    ]
  },
  {
    "objectID": "03_01_IntermediateRegression.html#multiple-linear-regression",
    "href": "03_01_IntermediateRegression.html#multiple-linear-regression",
    "title": "17  Multivariate Data I",
    "section": "",
    "text": "Matrix Notation.\nThe linear model has a fairly simple solution for \\(\\hat{B}^{*}\\) if you know linear algebra. Denoting \\(\\hat{\\mathbf{X}}_{i} = [1~~  \\hat{X}_{i1} ~~...~~ \\hat{X}_{iK}]\\) as a row vector, we can write the model as \\(\\hat{Y}_{i} = \\hat{\\mathbf{X}}_{i}B + e_{i}\\). We can then write the model in matrix form \\[\\begin{eqnarray}\n\\hat{Y} &=& \\hat{\\textbf{X}}B + E \\\\\n\\hat{Y} &=& \\begin{pmatrix}\n\\hat{Y}_{1} \\\\ \\vdots \\\\ \\hat{Y}_{N}\n\\end{pmatrix} \\quad\n\\hat{\\textbf{X}} = \\begin{pmatrix}\n1 & \\hat{X}_{11} & ... & \\hat{X}_{1K} \\\\\n& \\vdots & & \\\\\n1 & \\hat{X}_{n1} & ... & \\hat{X}_{nK}\n\\end{pmatrix}\n\\end{eqnarray}\\] Minimizing the squared errors: \\(\\min_{B} e_{i}^2 = \\min_{B} (E' E)\\), yields coefficient estimates and predictions \\[\\begin{eqnarray}\n\\hat{B^{*}} &=& (\\hat{\\textbf{X}}'\\hat{\\textbf{X}})^{-1}\\hat{\\textbf{X}}'\\hat{Y}\\\\\n\\hat{y} &=& \\hat{\\textbf{X}} \\hat{B^{*}} \\\\\n\\hat{E} &=& \\hat{Y} - \\hat{y} \\\\\n\\end{eqnarray}\\]\n\n\nCode\n# Manually Compute\nY &lt;- USArrests[,'Murder']\nX &lt;- USArrests[,c('Assault','UrbanPop')]\nX &lt;- as.matrix(cbind(1,X))\n\nXtXi &lt;- solve(t(X)%*%X)\nBhat &lt;- XtXi %*% (t(X)%*%Y)\nc(Bhat)\n## [1]  3.20715340  0.04390995 -0.04451047\n\n\n\n\nFactor Variables.\nSo far, we have discussed cardinal data where the difference between units always means the same thing: e.g., \\(4-3=2-1\\). There are also factor variables\n\nOrdered: refers to Ordinal data. The difference between units means something, but not always the same thing. For example, \\(4th - 3rd \\neq 2nd - 1st\\).\nUnordered: refers to Categorical data. The difference between units is meaningless. For example, \\(D-C=?\\)\n\nTo analyze either factor, we often convert them into indicator variables or dummies; \\(\\hat{D}_{c}=\\mathbf{1}( \\text{Factor} = c)\\). One common case is if you have observations of individuals over time periods, then you may have two factor variables. An unordered factor that indicates who an individual is; for example \\(\\hat{D}_{i}=\\mathbf{1}( \\text{Individual} = i)\\), and an order factor that indicates the time period; for example \\(\\hat{D}_{t}=\\mathbf{1}( \\text{Time} \\in [\\text{month}~ t, \\text{month}~ t+1) )\\). There are many other cases you see factor variables, including spatial ID’s in purely cross sectional data.\nBe careful not to handle categorical data as if they were cardinal. E.g., generate city data with Leipzig=1, Lausanne=2, LosAngeles=3, … and then include city as if it were a cardinal number (that’s a big no-no). The same applied to ordinal data; PopulationLeipzig=2, PopulationLausanne=3, PopulationLosAngeles=1.\n\n\nCode\nN &lt;- 1000\nx &lt;- runif(N,3,8)\ne &lt;- rnorm(N,0,0.4)\nfo &lt;- factor(rbinom(N,4,.5), ordered=T)\nfu &lt;- factor(rep(c('A','B'),N/2), ordered=F)\ndA &lt;- 1*(fu=='A')\ny &lt;- (2^as.integer(fo)*dA )*sqrt(x)+ 2*as.integer(fo)*e\ndat_f &lt;- data.frame(y,x,fo,fu)\n\n\nWith factors, you can still include them in the design matrix of an OLS regression. For example, \\[\\begin{eqnarray}\n\\hat{Y}_{i} = b_{0} + \\hat{X}_{i} b_{1} + \\sum_{t}\\hat{D}_{t}b_{t}\n\\end{eqnarray}\\] When, as commonly done, the factors are modeled as being additively seperable, they are modeled “fixed effects”.1 Simply including the factors into the OLS regression yields a “dummy variable” fixed effects estimator. The fixed effects and dummy variable approach are algebraically equal: same coefficients and same residuals. (See Hansen Econometrics, Theorem 17.1) \n\n\nCode\nlibrary(fixest)\nfe_reg1 &lt;- feols(y~x|fo+fu, dat_f)\ncoef(fe_reg1)\n##        x \n## 1.056205\nfixef(fe_reg1)[1:2]\n## $fo\n##         0         1         2         3         4 \n##  8.531532 10.540323 15.165373 25.016644 44.309805 \n## \n## $fu\n##         A         B \n##   0.00000 -23.07247\n\n# Compare Coefficients\nfe_reg0 &lt;- lm(y~-1+x+fo+fu, dat_f)\ncoef( fe_reg0 )\n##          x        fo0        fo1        fo2        fo3        fo4        fuB \n##   1.056205   8.531532  10.540323  15.165373  25.016644  44.309805 -23.072470\n\n\nWith fixed effects, we can also compute averages for each group: \\(\\hat{M}_{Yt}=\\sum_{i}^{n_{t}} \\hat{Y}_{it}/n_{t}\\), where each period \\(t\\) has \\(n_{t}\\) observations denoted \\(\\hat{Y}_{it}\\). We can then construct a between estimator: \\(\\hat{M}_{Yt} = b_{0} + \\hat{M}_{Xt} b_{1}\\). Or we can subtract the average from each group to construct a within estimator: \\((\\hat{Y}_{it} - \\hat{M}_{Yt}) = (\\hat{X}_{it}-\\hat{M}_{Xt})b_{1}\\).",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Multivariate Data I</span>"
    ]
  },
  {
    "objectID": "03_01_IntermediateRegression.html#variability-estimates",
    "href": "03_01_IntermediateRegression.html#variability-estimates",
    "title": "17  Multivariate Data I",
    "section": "17.2 Variability Estimates",
    "text": "17.2 Variability Estimates\nAbove, we computed the coefficient \\(\\hat{B}\\) for a particular sample: \\(\\hat{X}_{1i}, \\hat{X}_{2i}, ..., \\hat{Y}_{i}\\). We now seek to know how much the best-fitting coefficients \\(B^{*}\\) varies from sample to sample. I.e., \\(\\hat{B}\\) is our estimate and we want to know the variance of our estimator: \\(\\mathbb{V}(B^{*})\\). To estimate this variability, we can use the same data-driven methods introduced previously.\nAs before, we can also conduct independent hypothesis tests using t-values. However, we can also conduct joint tests that account for interdependancies in our estimates. For example, to test whether two coefficients both equal \\(0\\), we bootstrap the joint distribution of coefficients.\n\n\nCode\n# Bootstrap SE's\nboots &lt;- 1:399\nboot_regs &lt;- lapply(boots, function(b){\n    b_id &lt;- sample( nrow(USArrests), replace=T)\n    xy_b &lt;- USArrests[b_id,]\n    reg_b &lt;- lm(Murder~Assault+UrbanPop, dat=xy_b)\n})\nboot_coefs &lt;- sapply(boot_regs, coef)\n\n# Recenter at 0 to impose the null\n#boot_means &lt;- rowMeans(boot_coefs)\n#boot_coefs0 &lt;- sweep(boot_coefs, MARGIN=1, STATS=boot_means)\n\n\n\n\nCode\nboot_coef_df &lt;- as.data.frame(cbind(ID=boots, t(boot_coefs)))\nfig &lt;- plotly::plot_ly(boot_coef_df,\n    type = 'scatter', mode = 'markers',\n    x = ~UrbanPop, y = ~Assault,\n    text = ~paste('&lt;b&gt; bootstrap dataset: ', ID, '&lt;/b&gt;',\n            '&lt;br&gt;Coef. Urban  :', round(UrbanPop,3),\n            '&lt;br&gt;Coef. Murder :', round(Assault,3),\n            '&lt;br&gt;Coef. Intercept :', round(`(Intercept)`,3)),\n    hoverinfo='text',\n    showlegend=F,\n    marker=list( color='rgba(0, 0, 0, 0.5)'))\nfig &lt;- plotly::layout(fig,\n    showlegend=F,\n    title='Joint Distribution of Coefficients (under the null)',\n    xaxis = list(title='UrbanPop Coefficient'),\n    yaxis = list(title='Assualt Coefficient'))\nfig",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Multivariate Data I</span>"
    ]
  },
  {
    "objectID": "03_01_IntermediateRegression.html#hypothesis-tests",
    "href": "03_01_IntermediateRegression.html#hypothesis-tests",
    "title": "17  Multivariate Data I",
    "section": "17.3 Hypothesis Tests",
    "text": "17.3 Hypothesis Tests\n\nF-statistic.\nWe can also use an \\(F\\) test for any \\(q\\) hypotheses. Specifically, when \\(q\\) hypotheses restrict a model, the degrees of freedom drop from \\(k_{u}\\) to \\(k_{r}\\) and the residual sum of squares \\(\\hat{RSS}=\\sum_{i}(\\hat{Y}_{i}-\\hat{y}_{i})^2\\) typically increases. We compute the statistic \\[\\begin{eqnarray}\n\\hat{F}_{q} = \\frac{(\\hat{RSS}_{r}-\\hat{RSS}_{u})/(k_{u}-k_{r})}{\\hat{RSS}_{u}/(n-k_{u})}\n\\end{eqnarray}\\]\nIf you test whether all \\(K\\) variables are significant, the restricted model is a simple intercept and \\(\\hat{RSS}_{r}=\\hat{TSS}\\), and \\(\\hat{F}_{q}\\) can be written in terms of \\(\\hat{R}^2\\): \\(\\hat{F}_{K} = \\frac{\\hat{R}^2}{1-\\hat{R}^2} \\frac{n-K}{K-1}\\). The first fraction is the relative goodness of fit, and the second fraction is an adjustment for degrees of freedom (similar to how we adjusted the \\(\\hat{R}^2\\) term before).\nTo conduct a hypothesis test, first compute a null distribution by randomly reshuffling the outcomes and recompute the F statistic, and then compare how often random data give something as extreme as your initial statistic. For some intuition on this F test, examine how the adjusted \\(\\hat{R}^2\\) statistic varies with bootstrap samples.\n\n\nCode\n# Bootstrap under the null\nboots &lt;- 1:399\nboot_regs0 &lt;- lapply(boots, function(b){\n  # Generate bootstrap sample\n  xy_b &lt;- USArrests\n  b_id &lt;- sample( nrow(USArrests), replace=T)\n  # Impose the null\n  xy_b$Murder &lt;-  xy_b$Murder[b_id]\n  # Run regression\n  reg_b &lt;- lm(Murder~Assault+UrbanPop, dat=xy_b)\n})\n# Get null distribution for adjusted R2\nR2adj_sim0 &lt;- sapply(boot_regs0, function(reg_k){\n    summary(reg_k)$adj.r.squared })\nhist(R2adj_sim0, xlim=c(-.1,1), breaks=25, border=NA,\n    main='', xlab=expression('adj.'~R[b]^2))\n\n# Compare to initial statistic\nabline(v=summary(reg)$adj.r.squared, lwd=2, col=2)\n\n\n\n\n\n\n\n\n\nNote that hypothesis testing is not to be done routinely, as additional complications arise when testing multiple hypothesis sequentially.\nUnder some additional assumptions \\(F_{q}\\) follows an F-distribution. For more about F-testing, see https://online.stat.psu.edu/stat501/lesson/6/6.2 and https://www.econometrics.blog/post/understanding-the-f-statistic/\n\n\nANOVA\nUnder some additional parametric assumptions about the data generating process, the F-statistic follows an \\(F\\) distribution. This case is well-studied historically, often under the title Analysis of Variance (ANOVA). An important case corresponds to the restricted model having no explanatory variables (i.e., our model is \\(\\hat{Y}_{i}=b_{0}\\) and our predictions are \\(\\hat{y}_{i}=\\hat{M}_{Y}\\)).\n\n\nCode\nreg_full &lt;- lm(Murder ~ Assault + UrbanPop + Rape, data = USArrests)\nreg_none &lt;- lm(Murder ~ 1, dat=USArrests)\nanova(reg_none, reg_full)\n## Analysis of Variance Table\n## \n## Model 1: Murder ~ 1\n## Model 2: Murder ~ Assault + UrbanPop + Rape\n##   Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n## 1     49 929.55                                  \n## 2     46 304.83  3    624.72 31.424 3.322e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Manual F-test\nrss0 &lt;- sum(resid(reg_none)^2) # restricted\nrss1 &lt;- sum(resid(reg_full)^2) # unrestricted\ndf0  &lt;- df.residual(reg_none)\ndf1  &lt;- df.residual(reg_full)\nF    &lt;- ((rss0 - rss1)/(df0-df1)) / (rss1/df1) # observed F stat\np    &lt;- 1-pf(F, df0-df1, df1) # where F falls in the F-distribution\ncbind(F, p)\n##             F            p\n## [1,] 31.42399 3.322431e-11\n\n\nWhether you take a parametric or nonparametric approach to hypothesis testing, you can easily test whether variables are additively separable with an F test.\n\n\nCode\n# Empirical Example\nreg1 &lt;- lm(Murder~Assault+UrbanPop, USArrests)\nreg2 &lt;- lm(Murder~Assault*UrbanPop, USArrests)\nanova(reg1, reg2)\n## Analysis of Variance Table\n## \n## Model 1: Murder ~ Assault + UrbanPop\n## Model 2: Murder ~ Assault * UrbanPop\n##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n## 1     47 312.87                           \n## 2     46 300.93  1    11.944 1.8258 0.1832\n\n\n\n\nCode\n## # Simulation Example\n## N &lt;- 1000\n## x &lt;- runif(N,3,8)\n## e &lt;- rnorm(N,0,0.4)\n## fo &lt;- factor(rbinom(N,4,.5), ordered=T)\n## fu &lt;- factor(rep(c('A','B'),N/2), ordered=F)\n## dA &lt;- 1*(fu=='A')\n## y &lt;- (2^as.integer(fo)*dA )*sqrt(x)+ 2*as.integer(fo)*e\n## dat_f &lt;- data.frame(y,x,fo,fu)\n## \n## reg0 &lt;- lm(y~-1+x+fo+fu, dat_f)\n## reg1 &lt;- lm(y~-1+x+fo*fu, dat_f)\n## reg2 &lt;- lm(y~-1+x*fo*fu, dat_f)\n## \n## anova(reg0, reg2)\n## anova(reg0, reg1, reg2)",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Multivariate Data I</span>"
    ]
  },
  {
    "objectID": "03_01_IntermediateRegression.html#further-reading",
    "href": "03_01_IntermediateRegression.html#further-reading",
    "title": "17  Multivariate Data I",
    "section": "17.4 Further Reading",
    "text": "17.4 Further Reading\nFor OLS, see\n\nhttps://bookdown.org/josiesmith/qrmbook/linear-estimation-and-minimizing-error.html\nhttps://www.econometrics-with-r.org/4-lrwor.html\nhttps://www.econometrics-with-r.org/6-rmwmr.html\nhttps://www.econometrics-with-r.org/7-htaciimr.html\nhttps://bookdown.org/ripberjt/labbook/bivariate-linear-regression.html\nhttps://bookdown.org/ripberjt/labbook/multivariable-linear-regression.html\nhttps://online.stat.psu.edu/stat462/node/137/\nhttps://book.stat420.org/\nHill, Griffiths & Lim (2007), Principles of Econometrics, 3rd ed., Wiley, S. 86f.\nVerbeek (2004), A Guide to Modern Econometrics, 2nd ed., Wiley, S. 51ff.\nAsteriou & Hall (2011), Applied Econometrics, 2nd ed., Palgrave MacMillan, S. 177ff.\nhttps://online.stat.psu.edu/stat485/lesson/11/\n\nTo derive OLS coefficients in Matrix form, see\n\nhttps://jrnold.github.io/intro-methods-notes/ols-in-matrix-form.html\nhttps://www.fsb.miamioh.edu/lij14/411_note_matrix.pdf\nhttps://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf\n\nFor fixed effects, see\n\nhttps://www.econometrics-with-r.org/10-rwpd.html\nhttps://bookdown.org/josiesmith/qrmbook/topics-in-multiple-regression.html\nhttps://bookdown.org/ripberjt/labbook/multivariable-linear-regression.html\nhttps://www.princeton.edu/~otorres/Panel101.pdf\nhttps://www.stata.com/manuals13/xtxtreg.pdf",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Multivariate Data I</span>"
    ]
  },
  {
    "objectID": "03_01_IntermediateRegression.html#footnotes",
    "href": "03_01_IntermediateRegression.html#footnotes",
    "title": "17  Multivariate Data I",
    "section": "",
    "text": "There are also random effects: the factor variable comes from a distribution that is uncorrelated with the regressors. This is rarely used in economics today, however, and are mostly included for historical reasons and special cases where fixed effects cannot be estimated due to data limitations.↩︎",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Multivariate Data I</span>"
    ]
  },
  {
    "objectID": "03_02_InterprettingRegression.html",
    "href": "03_02_InterprettingRegression.html",
    "title": "18  Multivariate Data II",
    "section": "",
    "text": "18.1 Coefficient Interpretation\nNotice that we have gotten pretty far without actually trying to meaningfully interpret regression coefficients. That is because the above procedure will always give us number, regardless as to whether the true data generating process is linear or not. So, to be cautious, we have been interpreting the regression outputs while being agnostic as to how the data are generated. We now consider a special situation where we know the data are generated according to a linear process and are only uncertain about the parameter values.\nIf the data generating process is truly linear then we have a famous result that lets us attach a simple interpretation of OLS coefficients as unbiased estimates of the effect of \\(X\\).1\nFor example, generate a simulated dataset with \\(30\\) observations and two exogenous variables. Assume the following relationship: \\(Y_{i} = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\epsilon_{i}\\) where the variables and the error term are realizations of the following data generating processes:\nCode\nN &lt;- 30\nB &lt;- c(10, 2, -1)\n\nx1 &lt;- runif(N, 0, 5)\nx2 &lt;- rbinom(N,1,.7)\nX &lt;- cbind(1,x1,x2)\ne &lt;- rnorm(N,0,3)\nY &lt;- X%*%B + e\ndat &lt;- data.frame(Y,X)\ncoef(lm(Y~x1+x2, data=dat))\n## (Intercept)          x1          x2 \n##  10.0681682   1.5709383  -0.3146205\nSimulate the distribution of coefficients under a correctly specified model. Interpret the average.\nCode\nN &lt;- 30\nB &lt;- c(10, 2, -1)\n\nCoefs &lt;- sapply(1:400, function(sim){\n    x1 &lt;- runif(N, 0, 5)\n    x2 &lt;- rbinom(N,1,.7)\n    X &lt;- cbind(1,x1,x2)\n    e &lt;- rnorm(N,0,3)\n    Y &lt;- X%*%B + e\n    dat &lt;- data.frame(Y,x1,x2)\n    coef(lm(Y~x1+x2, data=dat))\n})\n\npar(mfrow=c(1,2))\nfor(i in 2:3){\n    hist(Coefs[i,], xlab=bquote(beta[.(i)]), main='', border=NA)\n    abline(v=mean(Coefs[i,]), lwd=2)\n    abline(v=B[i], col=rgb(1,0,0))\n}\nMany economic phenomena are nonlinear, even when including potential transforms of \\(Y\\) and \\(X\\). Sometimes the linear model may still be a good or even great approximation. But sometimes not, and it is hard to know ex-ante. Examine the distribution of coefficients under this mispecified model and try to interpret the average.\nCode\nN &lt;- 30\n\nCoefs &lt;- sapply(1:600, function(sim){\n    x2 &lt;- runif(N, 0, 5)\n    x3 &lt;- rbinom(N,1,.7)\n    e &lt;- rnorm(N,0,3)\n    Y &lt;- 10*x3 + 2*log(x2)^x3 + e\n    dat &lt;- data.frame(Y,x2,x3)\n    coef(lm(Y~x2+x3, data=dat))\n})\n\npar(mfrow=c(1,2))\nfor(i in 2:3){\n    hist(Coefs[i,],  xlab=bquote(beta[.(i)]), main='', border=NA)\n    abline(v=mean(Coefs[i,]), col=1, lwd=2)\n}\nIn general, you can interpret your regression coefficients as “adjusted correlations”. There are (many) tests for whether the relationships in your dataset are actually additively separable and linear.",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Multivariate Data II</span>"
    ]
  },
  {
    "objectID": "03_02_InterprettingRegression.html#diagnostics",
    "href": "03_02_InterprettingRegression.html#diagnostics",
    "title": "18  Multivariate Data II",
    "section": "18.2 Diagnostics",
    "text": "18.2 Diagnostics\nThere’s little sense in getting great standard errors for a terrible model. Plotting your regression object a simple and easy step to help diagnose whether your model is in some way bad. We next go through what each of these figures show.\n\n\nCode\nreg &lt;- lm(Murder~Assault+UrbanPop, data=USArrests)\npar(mfrow=c(2,2))\nplot(reg, pch=16, col=grey(0,.5))\n\n\n\n\n\n\n\n\n\n\nOutliers.\nThe first diagnostic plot examines outliers in terms the observed outcome \\(\\hat{Y}_i\\) being far from its prediction \\(\\hat{y}_i\\). You may be interested in such outliers because they can (but do not have to) unduly influence your estimates.\nThe third diagnostic plot examines another type of outlier, the observed explanatory variable \\(\\hat{X}_{i}\\) is far from the others. A point has high leverage if the estimates change dramatically when you estimate the model without that data point.\n\n\nCode\nN &lt;- 40\nx &lt;- c(25, runif(N-1,3,8))\ne &lt;- rnorm(N,0,0.4)\ny &lt;- 3 + 0.6*sqrt(x) + e\nplot(y~x, pch=16, col=grey(0,.5))\npoints(x[1],y[1], pch=16, col=rgb(1,0,0,.5))\n\nabline(lm(y~x), col=2, lty=2)\nabline(lm(y[-1]~x[-1]))\n\n\n\n\n\n\n\n\n\nStandardized residuals are \\(r_i=\\frac{\\hat{e}_i}{s_{[i]}\\sqrt{1-h_i}}\\), where \\(s_{[i]}\\) is the root mean squared error of a regression with the \\(i\\)th observation removed and \\(h_i\\) is the leverage of residual \\(\\hat{e}_{i}\\).\n\n\nCode\nwhich.max(hatvalues(reg))\nwhich.max(rstandard(reg))\n\n\nThe fourth plot further assesses outliers in the explanatory variables (\\(X\\)) using Cook’s Distance, which sums of all prediction changes when observation \\(i\\) is removed and scales proportionally to the mean square error \\[\\begin{eqnarray}\nD_{i}\n&=& \\frac{\\sum_{j} \\left( \\hat{y_j} - \\hat{y_j}_{[i]} \\right)^2 }{ p \\hat{S}^2 }\n= \\frac{\\hat{e}_{i}^2}{p \\hat{S}^2 } \\frac{h_i}{(1-h_i)^2} \\\\\n\\hat{S}^2 &=& \\frac{\\sum_{i} \\hat{e}_{i}^2 }{n-K}.\n\\end{eqnarray}\\]\n\n\nCode\nwhich.max(cooks.distance(reg))\ncar::influencePlot(reg)\n\n\nSee https://www.r-bloggers.com/2016/06/leverage-and-influence-in-a-nutshell/ for a good interactive explanation, and https://online.stat.psu.edu/stat462/node/87/ for detail. See AEJ-leverage and NBER-leverage for examples of leverage in economics.\n\n\nCollinearity.\nThis is when one explanatory variable in a multiple linear regression model can be linearly predicted from the others with a substantial degree of accuracy. Coefficient estimates may change erratically in response to small changes in the model or the data. (In the extreme case, there are more variables than observations \\(K&gt;n\\) and an infinite number of solutions to a linear model.) To diagnose collinearity, we can use the Variance Inflation Factor: \\(\\hat{VIF}_{k}=\\frac{1}{1-\\hat{R}^2_k}\\), where \\(\\hat{R}^2_k\\) is the \\(\\hat{R}^2\\) for the regression of \\(\\hat{X}_k\\) on the other covariates \\(\\hat{X}_{-k}\\) (a regression that does not involve the response variable \\(\\hat{Y}\\))\n\n\nCode\ncar::vif(reg) \nsqrt(car::vif(reg)) &gt; 2 # problem?\n\n\n\n\nNormality.\nThe second plot examines whether the residuals are normally distributed. Your OLS coefficient estimates do not depend on the normality of the residuals. (Good thing, because there’s no reason the residuals of economic phenomena should be so well behaved.) Many hypothesis tests are, however, affected by the distribution of the residuals. For these reasons, you may be interested in assessing normality\n\n\nCode\npar(mfrow=c(1,2))\nhist(resid(reg),\n    main='Histogram of Residuals',\n    font.main=1, border=NA)\n\nqqnorm(resid(reg),\n    main=\"Normal Q-Q Plot of Residuals\",\n    font.main=1, col=grey(0,.5), pch=16)\nqqline(resid(reg), col=1, lty=2)\n\n#shapiro.test(resid(reg))\n\n\nHeterskedasticity may also matters for variability estimates. This is not shown in the plot, but you can conduct a simple test\n\n\nCode\nlibrary(lmtest)\nlmtest::bptest(reg)",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Multivariate Data II</span>"
    ]
  },
  {
    "objectID": "03_02_InterprettingRegression.html#transformations",
    "href": "03_02_InterprettingRegression.html#transformations",
    "title": "18  Multivariate Data II",
    "section": "18.3 Transformations",
    "text": "18.3 Transformations\nTransforming variables can often improve your model fit while still allowing it estimated via OLS. This is because OLS only requires the model to be linear in the parameters. Under the assumptions of the model is correctly specified, the following table is how we can interpret the coefficients of the transformed data. (Note for small changes, \\(\\Delta ln(x) \\approx \\Delta x / x = \\Delta x \\% \\cdot 100\\).)\n\n\n\n\n\n\n\n\n\n\nSpecification\nRegressand\nRegressor\nDerivative\nInterpretation (If True)\n\n\n\n\nlinear–linear\n\\(y\\)\n\\(x\\)\n\\(\\Delta y = \\beta_1\\cdot\\Delta x\\)\nChange \\(x\\) by one unit \\(\\rightarrow\\) change \\(y\\) by \\(\\beta_1\\) units.\n\n\nlog–linear\n\\(ln(y)\\)\n\\(x\\)\n\\(\\Delta y \\% \\cdot 100 \\approx \\beta_1 \\cdot \\Delta x\\)\nChange \\(x\\) by one unit \\(\\rightarrow\\) change \\(y\\) by \\(100 \\cdot \\beta_1\\) percent.\n\n\nlinear–log\n\\(y\\)\n\\(ln(x)\\)\n\\(\\Delta y \\approx  \\frac{\\beta_1}{100}\\cdot \\Delta x \\%\\)\nChange \\(x\\) by one percent \\(\\rightarrow\\) change \\(y\\) by \\(\\frac{\\beta_1}{100}\\) units\n\n\nlog–log\n\\(ln(y)\\)\n\\(ln(x)\\)\n\\(\\Delta y \\% \\approx \\beta_1\\cdot \\Delta x \\%\\)\nChange \\(x\\) by one percent \\(\\rightarrow\\) change \\(y\\) by \\(\\beta_1\\) percent\n\n\n\nNow recall from micro theory that an additively seperable and linear production function is referred to as ``perfect substitutes’‘. With a linear model and untranformed data, you have implicitly modelled the different regressors \\(X\\) as perfect substitutes. Further recall that the’‘perfect substitutes’’ model is a special case of the constant elasticity of substitution production function. Here, we will build on http://dx.doi.org/10.2139/ssrn.3917397, and consider box-cox transforming both \\(X\\) and \\(y\\). Specifically, apply the box-cox transform of \\(y\\) using parameter \\(\\lambda\\) and apply another box-cox transform to each \\(x\\) using the same parameter \\(\\rho\\) so that \\[\\begin{eqnarray}\nY^{(\\lambda)}_{i} &=& \\sum_{k=1}^{K}\\beta_{k} X^{(\\rho)}_{ik} + \\epsilon_{i}\\\\\nY^{(\\lambda)}_{i} &=&\n\\begin{cases}\n\\lambda^{-1}\\left[ (Y_i+1)^{\\lambda}- 1\\right] & \\lambda \\neq 0 \\\\\n\\log\\left(Y_i+1\\right) &  \\lambda=0\n\\end{cases}.\\\\\nx^{(\\rho)}_{i} &=&\n\\begin{cases}\n\\rho^{-1}\\left[ (X_i)^{\\rho}- 1\\right] & \\rho \\neq 0 \\\\\n\\log\\left(X_{i}+1\\right) &  \\rho=0\n\\end{cases}.\n\\end{eqnarray}\\]\nNotice that this nests:\n\nlinear-linear \\((\\rho=\\lambda=1)\\).\nlinear-log \\((\\rho=1, \\lambda=0)\\).\nlog-linear \\((\\rho=0, \\lambda=1)\\).\nlog-log \\((\\rho=\\lambda=0)\\).\n\nIf \\(\\rho=\\lambda\\), we get the CES production function. This nests the ‘’perfect substitutes’’ linear-linear model (\\(\\rho=\\lambda=1\\)) , the ‘’cobb-douglas’’ log-log model (\\(\\rho=\\lambda=0\\)), and many others. We can define \\(\\lambda=\\rho/\\lambda'\\) to be clear that this is indeed a CES-type transformation where\n\n\\(\\rho \\in (-\\infty,1]\\) controls the “substitutability” of explanatory variables. E.g., \\(\\rho &lt;0\\) is ‘’complementary’’.\n\\(\\lambda\\) determines ‘’returns to scale’‘. E.g., \\(\\lambda&lt;1\\) is’‘decreasing returns’’.\n\nWe compute the mean squared error in the original scale by inverting the predictions; \\[\\begin{eqnarray}\n\\hat{y}_{i} =\n\\begin{cases}\n\\left[ \\hat{y}_{i}^{(\\lambda)} \\cdot \\lambda \\right]^{1/\\lambda} -1 & \\lambda  \\neq 0 \\\\\n\\exp\\left( \\hat{y}_{i}^{(\\lambda)} \\right) -1 &  \\lambda=0\n\\end{cases}.\n\\end{eqnarray}\\]\nIt is easiest to optimize parameters in a 2-step procedure called `concentrated optimization’. We first solve for \\(\\hat{\\beta}(\\rho,\\lambda)\\) and compute the mean squared error \\(MSE(\\rho,\\lambda)\\). We then find the \\((\\rho,\\lambda)\\) which minimizes \\(MSE\\).\n\n\nCode\n# Box-Cox Transformation Function\nbxcx &lt;- function( xy, rho){\n    if (rho == 0L) {\n      log(xy+1)\n    } else if(rho == 1L){\n      xy\n    } else {\n      ((xy+1)^rho - 1)/rho\n    }\n}\nbxcx_inv &lt;- function( xy, rho){\n    if (rho == 0L) {\n      exp(xy) - 1\n    } else if(rho == 1L){\n      xy\n    } else {\n     (xy * rho + 1)^(1/rho) - 1\n    }\n}\n\n# Which Variables\nreg &lt;- lm(Murder~Assault+UrbanPop, data=USArrests)\nX &lt;- USArrests[,c('Assault','UrbanPop')]\nY &lt;- USArrests[,'Murder']\n\n# Simple Grid Search over potential (Rho,Lambda) \nrl_df &lt;- expand.grid(rho=seq(-2,2,by=.5),lambda=seq(-2,2,by=.5))\n\n# Compute Mean Squared Error\n# from OLS on Transformed Data\nerrors &lt;- apply(rl_df,1,function(rl){\n    Xr &lt;- bxcx(X,rl[[1]])\n    Yr &lt;- bxcx(Y,rl[[2]])\n    Datr &lt;- cbind(Murder=Yr,Xr)\n    Regr &lt;- lm(Murder~Assault+UrbanPop, data=Datr)\n    Predr &lt;- bxcx_inv(predict(Regr),rl[[2]])\n    Resr  &lt;- (Y - Predr)\n    return(Resr)\n})\nrl_df$mse &lt;- colMeans(errors^2)\n\n# Want Small MSE and Interpretable\nlayout(matrix(1:2,ncol=2), width=c(3,1), height=c(1,1))\npar(mar=c(4,4,2,0))\nplot(lambda~rho,rl_df, cex=8, pch=15,\n    xlab=expression(rho),\n    ylab=expression(lambda),\n    col=hcl.colors(25)[cut(1/rl_df$mse,25)])\n# Which min\nrl0 &lt;- rl_df[which.min(rl_df$mse),c('rho','lambda')]\npoints(rl0$rho, rl0$lambda, pch=0, col=1, cex=8, lwd=2)\n# Legend\nplot(c(0,2),c(0,1), type='n', axes=F,\n    xlab='',ylab='', cex.main=.8,\n    main=expression(frac(1,'Mean Square Error')))\nrasterImage(as.raster(matrix(hcl.colors(25), ncol=1)), 0, 0, 1,1)\ntext(x=1.5, y=seq(1,0,l=10), cex=.5,\n    labels=levels(cut(1/rl_df$mse,10)))\n\n\n\n\n\n\n\n\n\nThe parameters \\(-1,0,1,2\\) are easy to interpret and might be selected instead if there is only a small loss in fit. (In the above example, we might choose \\(\\lambda=0\\) instead of the \\(\\lambda\\) which minimized the mean square error). You can also plot the specific predictions to better understand the effect of data transformation beyond mean squared error.\n\n\nCode\n# Plot for Specific Comparisons\nXr &lt;- bxcx(X,rl0[[1]])\nYr &lt;- bxcx(Y,rl0[[2]])\nDatr &lt;- cbind(Murder=Yr,Xr)\nRegr &lt;- lm(Murder~Assault+UrbanPop, data=Datr)\nPredr &lt;- bxcx_inv(predict(Regr),rl0[[2]])\n\ncols &lt;- c(rgb(1,0,0,.5), col=rgb(0,0,1,.5))\nplot(Y, Predr, pch=16, col=cols[1], ylab='Prediction', \n    ylim=range(Y,Predr))\npoints(Y, predict(reg), pch=16, col=cols[2])\nlegend('topleft', pch=c(16), col=cols,\n    title=expression(rho~', '~lambda),\n    legend=c( paste0(rl0, collapse=', '),'1, 1') )\nabline(a=0,b=1, lty=2)\n\n\n\n\n\n\n\n\n\nWhen explicitly transforming data according to \\(\\lambda\\) and \\(\\rho\\), these parameters increase the degrees of freedom by two. The default hypothesis testing procedures do not account for you trying out different transformations, and should be adjusted by the increased degrees of freedom. Specification searches deflate standard errors and are a major source for false discoveries.\nNote that if you are ultimately interested in the outcome \\(Y\\), then transforming/untransforming \\(Y\\) can introduce a bias. To understand when you might be better off sticking with an untransformed outcome variable, see the literature on “smearing”.\n\nBreak Points.\nIncorporating Kinks and Discontinuities in \\(X\\) are a type of transformation that can be modeled using factor variables. As such, \\(F\\)-tests can be used to examine whether a breaks is statistically significant.\n\n\nCode\nlibrary(AER); data(CASchools)\nCASchools$score &lt;- (CASchools$read + CASchools$math) / 2\nreg &lt;- lm(score~income, data=CASchools)\n\n# F Test for Break\nreg2 &lt;- lm(score ~ income*I(income&gt;15), data=CASchools)\nanova(reg, reg2)\n\n# Chow Test for Break\ndata_splits &lt;- split(CASchools, CASchools$income &lt;= 15)\nresids &lt;- sapply(data_splits, function(dat){\n    reg &lt;- lm(score ~ income, data=dat)\n    sum( resid(reg)^2)\n})\nNs &lt;-  sapply(data_splits, function(dat){ nrow(dat)})\nRt &lt;- (sum(resid(reg)^2) - sum(resids))/sum(resids)\nRb &lt;- (sum(Ns)-2*reg$rank)/reg$rank\nFt &lt;- Rt*Rb\npf(Ft,reg$rank, sum(Ns)-2*reg$rank,lower.tail=F)\n\n# See also\n# strucchange::sctest(y~x, data=xy, type=\"Chow\", point=.5)\n# strucchange::Fstats(y~x, data=xy)\n\n# To Find Changes\n# segmented::segmented(reg)",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Multivariate Data II</span>"
    ]
  },
  {
    "objectID": "03_02_InterprettingRegression.html#regressograms",
    "href": "03_02_InterprettingRegression.html#regressograms",
    "title": "18  Multivariate Data II",
    "section": "18.4 Regressograms",
    "text": "18.4 Regressograms\nYou can estimate a nonparametric model with multiple \\(X\\) variables with a multivariate regressogram. Here, we cut the data into exclusive bins along each dimension (called dummy variables), and then run a regression on all dummy variables.\n\n\nCode\n## Simulate Data\nN &lt;- 10000\ne &lt;- rnorm(N)\nx1 &lt;- seq(.1,20,length.out=N)\nx2 &lt;- runif(N, 0,1)\ny  &lt;- 3*exp(-2*x2 + 1.5*x1 - .1*x1^2)*x1 + e\ndat &lt;- data.frame(x1, x2, y)\n\n## Create color palette (reused in later examples)\ncol_scale &lt;- seq(min(y)*1.1, max(y)*1.1, length.out=401)\nycol_pal &lt;- hcl.colors(length(col_scale),alpha=.5)\nnames(ycol_pal) &lt;- sort(col_scale)\n\n## Add legend (reused in later examples)\nadd_legend &lt;- function(col_scale,\n    yl=11,\n    colfun=function(x){ hcl.colors(x,alpha=.5) },\n    ...) {\n  opar &lt;- par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), \n              mar=c(0, 0, 0, 0), new=TRUE)\n  on.exit(par(opar))\n  h &lt;- hist(col_scale, plot=F, breaks=yl-1)$mids\n  plot(0, 0, type='n', bty='n', xaxt='n', yaxt='n')\n  legend(...,\n    legend=h,\n    fill=colfun(length(h)),\n    border=NA,\n    bty='n')\n}\n\n\n## Plot Data\npar(oma=c(0,0,0,2))\nplot(x1~x2, dat,\n    col=ycol_pal[cut(y,col_scale)],\n    pch=16, cex=.5, \n    main='Raw Data', font.main=1)\nadd_legend(x='topright', col_scale=col_scale,\n    yl=6, inset=c(0,.05), title='y')\n\n\n\n\n\n\n\n\n\n\n\nCode\n## OLS \nreg &lt;- lm(y~x1*x2, data=dat) #(with simple interaction)\nreg &lt;- lm(y~x1+x2, data=dat) #(without interaction)\n\n## Grid Points for Prediction\n# X1 bins\nl1 &lt;- 11\nbks1 &lt;- seq(0,20, length.out=l1)\nh1 &lt;- diff(bks1)[1]/2\nmids1 &lt;- bks1[-1]-h1\n# X2 bins\nl2 &lt;- 11\nbks2 &lt;- seq(0,1, length.out=l2)\nh2 &lt;- diff(bks2)[1]/2\nmids2 &lt;- bks2[-1]-h2\n# Grid\npred_x &lt;- expand.grid(x1=mids1, x2=mids2)\n\n## OLS Predictions\npred_ols &lt;- predict(reg, newdata=pred_x)\npred_df_ols  &lt;- cbind(pred_ols, pred_x)\n\n## Plot Predictions\npar(oma=c(0,0,0,2))\nplot(x1~x2, pred_df_ols,\n    col=ycol_pal[cut(pred_ols,col_scale)],\n    pch=15, cex=2,\n    main='OLS Predictions', font.main=1)\nadd_legend(x='topright', col_scale=col_scale,\n    yl=6, inset=c(0,.05),title='y')\n\n\n\n\n\n\n\n\n\n\n\nCode\n##################\n# Multivariate Regressogram\n##################\n\n## Regressogram Bins\ndat$x1c &lt;- cut(dat$x1, bks1)\n#head(dat$x1c,3)\ndat$x2c &lt;- cut(dat$x2, bks2)\n\n## Regressogram\nreg &lt;- lm(y~x1c*x2c, data=dat) #nonlinear w/ complex interactions\n\n## Predicted Values\n## For Points in Middle of Each Bin\npred_df_rgrm &lt;- expand.grid(\n    x1c=levels(dat$x1c),\n    x2c=levels(dat$x2c))\npred_df_rgrm$yhat &lt;- predict(reg, newdata=pred_df_rgrm)\npred_df_rgrm &lt;- cbind(pred_df_rgrm, pred_x)\n\n## Plot Predictions\npar(oma=c(0,0,0,2))\nplot(x1~x2, pred_df_rgrm,\n    col=ycol_pal[cut(pred_df_rgrm$yhat,col_scale)],\n    pch=15, cex=2,\n    main='Regressogram Predictions', font.main=1)\nadd_legend(x='topright', col_scale=col_scale,\n    yl=6, inset=c(0,.05),title='y')\n\n\n\n\n\n\n\n\n\nJust like with bivariate data, you can also use split-sample (or peicewise) regressions for multivariate data.\nAs such, there are two main ways to summarize gradients: how \\(Y\\) changes with \\(X\\).\n\nFor regressograms, you can approximate gradients with small finite differences. For some small \\(h_{p}\\), we can compute \\[\\begin{eqnarray}\n\\hat{\\beta}_{p}(\\mathbf{x}) &=& \\frac{ \\hat{y}(x_{1},...,x_{p}+ \\frac{h_{p}}{2}...,x_{P})-\\hat{y}(x_{1},...,x_{p}-\\frac{h_{p}}{2}...,x_{P})}{h_{p}},\n\\end{eqnarray}\\]\nWhen using split-sample regressions or local linear regressions, you can use the estimated slope coefficients \\(\\hat{\\beta}_{p}(\\mathbf{x})\\) as gradient estimates in each direction.2\n\nAfter computing gradients, you can summarize them in various plots: Histogram of \\(\\hat{\\beta}_{p}(\\mathbf{x})\\), Scatterplot of \\(\\hat{\\beta}_{p}(\\mathbf{x})\\) vs. \\(X_{p}\\), or the CI of \\(\\hat{\\beta}_{p}(\\mathbf{x})\\) vs \\(\\hat{\\beta}_{p}(\\mathbf{x})\\) after sorting the gradients \nYou may also be interested in a particular gradient or a single summary statistic. For example, a bivariate regressogram can estimate the marginal effect of \\(X_{1}\\) at the means; \\(\\hat{\\beta_{1}}(\\bar{\\mathbf{x}}=[\\bar{x_{1}}, \\bar{x_{2}}])\\). You may also be interested in the mean of the marginal effects (sometimes said simply as “average effect”), which averages the marginal effect over all datapoints in the dataset: \\(1/n \\sum_{i}^{n} \\hat{\\beta_{1}}(\\mathbf{X}_{i})\\), or the median marginal effect. Such statistics are single numbers that can be presented similar to an OLS regression table where each row corresponds a variable and each cell has two elements: “mean gradient (sd gradient)”.",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Multivariate Data II</span>"
    ]
  },
  {
    "objectID": "03_02_InterprettingRegression.html#more-literature",
    "href": "03_02_InterprettingRegression.html#more-literature",
    "title": "18  Multivariate Data II",
    "section": "18.5 More Literature",
    "text": "18.5 More Literature\nDiagnostics\n\nhttps://book.stat420.org/model-diagnostics.html#leverage\nhttps://socialsciences.mcmaster.ca/jfox/Books/RegressionDiagnostics/index.html\nhttps://bookdown.org/ripberjt/labbook/diagnosing-and-addressing-problems-in-linear-regression.html\nBelsley, D. A., Kuh, E., and Welsch, R. E. (1980). Regression Diagnostics: Identifying influential data and sources of collinearity. Wiley. https://doi.org/10.1002/0471725153\nFox, J. D. (2020). Regression diagnostics: An introduction (2nd ed.). SAGE. https://dx.doi.org/10.4135/9781071878651",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Multivariate Data II</span>"
    ]
  },
  {
    "objectID": "03_02_InterprettingRegression.html#footnotes",
    "href": "03_02_InterprettingRegression.html#footnotes",
    "title": "18  Multivariate Data II",
    "section": "",
    "text": "If you know matrix notation, then assume \\[\\begin{eqnarray}\nY_{i} = X_{i} \\beta + \\epsilon_{i} &\\quad& \\mathbb{E}[\\epsilon_{i} | X_{i} ]=0.\n\\end{eqnarray}\\] Then define the estimator and notice that it is unbiased \\[\\begin{eqnarray}\n\\beta^{*} = (X'X)^{-1}X'Y = (X'X)^{-1}X'(X\\beta + \\epsilon) = \\beta + (X'X)^{-1}X'\\epsilon\\\\\n\\mathbb{E}\\left[ \\beta^{*} \\right] = \\mathbb{E}\\left[ (X'X)^{-1}X'Y \\right] = \\beta + (X'X)^{-1}\\mathbb{E}\\left[ X'\\epsilon \\right] = \\beta\n\\end{eqnarray}\\]↩︎\nIn matrix notation, we have the estimates \\[\\begin{eqnarray}\n\\hat{\\beta}(\\mathbf{x}) &=& [\\mathbf{X}'\\mathbf{K}(\\mathbf{x})\\mathbf{X}]^{-1} \\mathbf{X}'\\mathbf{K}(\\mathbf{x})Y \\\\\n\\mathbf{K}(\\mathbf{x}) &=& \\begin{pmatrix}\nK\\left(\\frac{\\mathbf{X}_{1}-\\mathbf{x}}{h_{1}}\\right) & ... & 0\\\\\n\\vdots & & \\\\\n0 & ... & K\\left(\\frac{\\mathbf{X}_{P}-\\mathbf{x}}{h_{P}}\\right)\n\\end{pmatrix}.\n\\end{eqnarray}\\]↩︎",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Multivariate Data II</span>"
    ]
  },
  {
    "objectID": "03_03_ObservationalData.html",
    "href": "03_03_ObservationalData.html",
    "title": "19  Observational Data",
    "section": "",
    "text": "19.1 Temporal Interdependence\nMany observational datasets have temporal dependence, meaning that values at one point in time are related to past values. This violates the standard assumption of independence used in many statistical methods.\nStock prices are classic examples of temporally dependent processes. If Apple’s stock was high yesterday, it is more likely (but not guaranteed) to be high today.\nCode\n# highest price each day\nlibrary(plotly)\nstock &lt;- read.csv('https://raw.githubusercontent.com/plotly/datasets/master/finance-charts-apple.csv')\nfig &lt;- plot_ly(stock, type = 'scatter', mode = 'lines')%&gt;%\n  add_trace(x = ~Date, y = ~AAPL.High) %&gt;%\n  layout(showlegend = F)\nfig\nA random walk is the simplest mathematical model of temporal dependence. Each new value is just the previous value plus a random shock (white noise).\nCode\n# Generate Random Walk\ntN &lt;- 200\ny &lt;- numeric(tN)\ny[1] &lt;- stock$AAPL.High[1]\nfor (ti in 2:tN) {\n    y[ti] &lt;- y[ti-1] + runif(1, -10, 10)\n}\n#x &lt;- runif(tN, -1,1) White Noise\n\ny_dat &lt;- data.frame(Date=1:tN, RandomWalk=y)\nfig &lt;- plot_ly(y_dat, type = 'scatter', mode = 'lines') %&gt;%\n  add_trace(x=~Date, y=~RandomWalk) %&gt;%\n  layout(showlegend = F)\nfig\nIn both plots, we see that today’s value is not independent of past values. In contrast to cross-sectional data (e.g. individual incomes), time series often require special methods to account for memory and nonstationarity.\nIn any case, if often helps to see the marginal distribution too.\nCode\nx0 &lt;- rbinom(600, 1, 0.5)\n\n# Setup 2 Plots, side by side\nlayout(matrix(c(1, 2), nrow = 1), widths = c(4, 1))\n\n# Plot Cumulative Averages\nx0_t &lt;- seq(1, length(x0))\nx0_mt &lt;- cumsum(x0)/x0_t\npar(mar=c(4,4,1,4))\nplot(x0_t, x0_mt, type='l',\n    ylab='Cumulative Average',\n    xlab='Flip #', \n    ylim=c(0,1), \n    lwd=2)\npoints(x0_t, x0, col=grey(0,.5),\n    pch=16, cex=.2)\n\n# Plot Long run proportions\npar(mar=c(4,4,1,1))\nx_hist &lt;- hist(x0, breaks=50, plot=F)\nx_freq &lt;- x_hist$count/length(x0)\nbarplot(x_freq ,\n    axes=FALSE,\n    space=0, horiz=TRUE, border=NA)\naxis(1)\naxis(2)\nmtext('Observed Frequency', 2, line=2.5)",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Observational Data</span>"
    ]
  },
  {
    "objectID": "03_03_ObservationalData.html#temporal-interdependence",
    "href": "03_03_ObservationalData.html#temporal-interdependence",
    "title": "19  Observational Data",
    "section": "",
    "text": "Stationary.\nA stationary time series is one whose statistical properties do not change over time. Formally\n\nStationary Means: \\(E[Y_{t}]=E[Y_{t'}]\\) for all time periods \\(t, t'\\)\nStationary Variances: \\(V[Y_{t}]=V[Y_{t'}]\\) for all time periods \\(t, t'\\)\n\nFor example, consider the data generating process \\(Y_t = \\beta t + \\epsilon_t\\), with \\(\\epsilon_t \\sim \\text{N}(0, \\sigma + \\alpha t)\\), with parameters \\(\\beta\\) affecting mean stationarity and \\(\\alpha\\) affecting variance stationarity.\n\n\nCode\ntN &lt;- 200\nsimulate_series &lt;- function(beta, alpha, sigma=.2){\n    y &lt;- numeric(tN)\n    for (ti in 1:tN) {\n        mean_ti &lt;- beta*ti\n        sd_ti &lt;- (.2 + alpha*ti)\n        y[ti] &lt;- mean_ti + rnorm(1, sd=sd_ti)\n    }\n    return(y)\n}\n\n# Plotting Functions\nplot_setup &lt;- function(alpha, beta){\n    plot.new()\n    plot.window(xlim=c(1,tN), ylim=c(-5,20))\n    axis(1)\n    axis(2)\n    mtext(expression(y[t]),2, line=2.5)\n    mtext(\"Time (t)\", 1, line=2.5)\n}\nplot_title &lt;- function(alpha, beta){\n    beta_name &lt;- ifelse(beta==0, 'Mean Stationary', 'Mean Nonstationary')\n    alpha_name &lt;- ifelse(alpha==0, 'Var Stationary', 'Var Nonstationary')\n    title(paste0(beta_name,', ', alpha_name), font.main=1, adj=0)\n}\n\npar(mfrow = c(2, 2))\nfor(alpha in c(0,.015)){\nfor(beta in c(0,.05)){\n    plot_setup(alpha=alpha, beta=beta)\n    for( sim in c('red','blue')){\n        y_sim &lt;- simulate_series(beta=beta, alpha=alpha)\n        lines(y_sim, col=adjustcolor(sim ,alpha.f=0.5), lwd=2)\n    }\n    plot_title(alpha=alpha, beta=beta)\n}}\n\n\n\n\n\n\n\n\n\n\n\nMeasures of temporal association.\nTime series often exhibit serial dependence—values today are related to past values, and potentially to other processes evolving over time. We can visualize this using correlation-based diagnostics.\nThe Autocorrelation Function (AFC) measures correlation between a time series and its own lagged values: \\[\\begin{eqnarray}\nACF_{Y}(k) = \\frac{Cov(Y_{t},Y_{t-k})}{ \\sqrt{Var(Y_{t})Var(Y_{t-k})}}\n\\end{eqnarray}\\]\nThis helps detect temporal persistence (memory). For stationary processes, the ACF typically decays quickly, whereas for nonstationary processes, it typically decays slowly or persists.\n\n\nCode\npar(mfrow = c(2, 2))\nfor(alpha in c(0,.015)){\nfor(beta in c(0,.05)){\n    y_sim &lt;- simulate_series(beta=beta, alpha=alpha)\n    acf(y_sim, main='')\n    plot_title(alpha=alpha, beta=beta)\n}}\n\n\n\n\n\n\n\n\n\nThe Cross-Correlation Function (CCF) measures correlation between two time series at different lags:\n\\[\\begin{eqnarray}\nCCF_{YX}(k) = \\frac{Cov(Y_{t},X_{t-k})}{ \\sqrt{Var(Y_t)Var(X_{t-k})}}\n\\end{eqnarray}\\]\nThis is useful for detecting lagged relationships between two series, such as leading indicators or external drivers. (If \\(X\\) is white noise, any visible structure in the CCF likely reflects nonstationarity in \\(Y\\).)\n\n\nCode\nx_sim &lt;- runif(tN, -1,1) # White Noise\npar(mfrow = c(2, 2))\nfor(alpha in c(0,.015)){\nfor(beta in c(0,.05)){\n    y_sim &lt;- simulate_series(beta=beta, alpha=alpha)\n    ccf(y_sim, x_sim, main='')\n    plot_title(alpha=alpha, beta=beta)\n}}",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Observational Data</span>"
    ]
  },
  {
    "objectID": "03_03_ObservationalData.html#spatial-interdependence",
    "href": "03_03_ObservationalData.html#spatial-interdependence",
    "title": "19  Observational Data",
    "section": "19.2 Spatial Interdependence",
    "text": "19.2 Spatial Interdependence\nMany observational datasets exhibit spatial dependence, meaning that values at one location tend to be related to values at nearby locations. This violates the standard assumption of independent observations used in many classical statistical methods.\nFor example, elevation is spatially dependent: if one location is at high elevation, nearby locations are also likely (though not guaranteed) to be high. Similarly, socioeconomic outcomes like disease rates or income often cluster geographically due to shared environmental or social factors.\nJust as stock prices today depend on yesterday, spatial variables often depend on neighboring regions, creating a need for specialized statistical methods that account for spatial autocorrelation.\n\nRaster vs. Vector Data.\nSpatial data typically comes in two formats, each suited to different types of information:\n\nVector data uses geometric shapes (points, lines, polygons) to store data. E.g., a census tract map that stores data on population demographics.\nRaster data uses grid cells (typically squares, but sometimes hexagons) to store data. E.g., an image that stores data on elevation above seawater.\n\n\n\nCode\n# Vector Data\nlibrary(sf)\nnorthcarolina_vector &lt;- st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\n## Reading layer `nc' from data source `/home/Jadamso/R-Libs/sf/shape/nc.shp' using driver `ESRI Shapefile'\n## Simple feature collection with 100 features and 14 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\n## Geodetic CRS:  NAD27\nplot(northcarolina_vector['BIR74'], main='Number of Live Births in 1974')\n\n\n\n\n\n\n\n\n\nCode\n# https://r-spatial.github.io/spdep/articles/sids.html\n\n\n\n\nCode\n# Raster Data\nlibrary(terra)\nluxembourg_elevation_raster &lt;- rast(system.file(\"ex/elev.tif\", package=\"terra\"))\nplot(luxembourg_elevation_raster)\n\n\n\n\n\n\n\n\n\n\n\nStationary.\nJust as with temporal data, stationarity in spatial data means that the statistical properties (like mean, variance, or spatial correlation) are roughly the same across space.\n\nStationary Means: \\(E[Y(s)]=E[Y(s')]\\) for all spatial locations \\(s,s'\\)\nStationary Vars: \\(V[Y(s)]=V[Y(s')]\\) for all spatial locations \\(s,s'\\)\n\n\n\nCode\n# Simulated 2D spatial fields\nset.seed(1)\nn &lt;- 20\nx &lt;- y &lt;- seq(0, 1, length.out = n)\ngrid &lt;- expand.grid(x = x, y = y)\n\n# 1. Stationary: Gaussian with constant mean and var\nz_stationary &lt;- matrix(rnorm(n^2, 0, 1), n, n)\n\n# 2. Nonstationary: Mean increases with x and y\nz_nonstationary &lt;- outer(x, y, function(x, y) 3*x*y) + rnorm(n^2, 0, 1)\n\npar(mfrow = c(1, 2))\n# Stationary field\nimage(x, y, z_stationary,\n      main = \"Stationary Field\",\n      col = terrain.colors(100),\n      xlab = \"x\", ylab = \"y\")\n# Nonstationary field\nimage(x, y, z_nonstationary,\n      main = \"Nonstationary Field\",\n      col = terrain.colors(100),\n      xlab = \"x\", ylab = \"y\")\n\n\n\n\n\n\n\n\n\n\n\nMeasures of spatial association.\nJust like temporal data may exhibit autocorrelation, spatial data may show spatial autocorrelation or spatial cross-correlation—meaning that observations located near each other are more (or less) similar than we would expect under spatial independence.\nAutocorrelation. We can measure spatial autocorrelation using Moran’s I, a standard index of spatial dependence. Global Moran’s I summarizes overall spatial association (just like the ACF)\n\n\nCode\n# Raster Data Example\nautocor(luxembourg_elevation_raster, method='moran', global=T)\n## elevation \n## 0.8917057\n\n\nCross-Correlation. We can also assesses the relationship between two variables at varying distances.\n\n\nCode\n# Vector Data Example\ndat &lt;- as.data.frame(northcarolina_vector)[, c('BIR74', 'SID74')]\nmu &lt;- colMeans(dat)\n\n# Format Distances\ndmat &lt;- st_distance( st_centroid(northcarolina_vector) )\ndmat &lt;- units::set_units(dmat, 'km')\n\n# At Which Distances to Compute CCF\n# summary(dmat[,1])\nrdists &lt;- c(-1,seq(0,100,by=25)) # includes 0\nrdists &lt;- units::set_units(rdists , 'km')\n\n# Compute Cross-Covariances\nvarXY &lt;- prod( apply(dat, 2, sd) )\nCCF &lt;- lapply( seq(2, length(rdists)), function(ri){\n    # Which Observations are within (rmin, rmax] distance\n    dmat_r &lt;- dmat\n    d_id &lt;- (dmat_r &gt; rdists[ri-1] & dmat_r &lt;= rdists[ri]) \n    dmat_r[!d_id]  &lt;- NA\n    # Compute All Covariances (Stationary)\n    covs_r &lt;- lapply(1:nrow(dmat_r), function(i){\n        pairsi &lt;- which(!is.na(dmat_r[i,]))        \n        covXiYj &lt;- sapply(pairsi, function(j) {\n            dXi &lt;- dat[i,1] - mu[1]\n            dYj &lt;- dat[j,2] - mu[2]\n            return(dXi*dYj)\n        })\n        return(covXiYj)\n    })\n    corXY &lt;- unlist(covs_r)/varXY\n    return(corXY)\n} )\n\n\n\n\nCode\n# Plot Cross-Covariance Function\nx &lt;- as.numeric(rdists[-1])\n\npar(mfrow=c(1,2))\n\n# Distributional Summary\nboxplot(CCF,\n    outline=F, whisklty=0, staplelty=0,\n    ylim=c(-1,1), #quantile(unlist(CCF), probs=c(.05,.95)),\n    names=x, \n    main='',\n    font.main=1,\n    xlab='Distance [km]',\n    ylab='Cross-Correlation of BIR74 and SID74')\ntitle('Binned Medians and IQRs', font.main=1, adj=0)\nabline(h=0, lty=2)\n\n# Inferential Summary\nCCF_means &lt;- sapply(CCF, mean)\nplot(x, CCF_means,\n    ylim=c(-1,1),\n    type='o', pch=16,\n    main='',\n    xlab='Distance [km]',\n    ylab='Cross-Correlation of BIR74 and SID74')\ntitle('Binned Means + 95% Confidence Band', font.main=1, adj=0)\nabline(h=0, lty=2)    \n# Quick and Dirty Subsampling CI\nCCF_meanCI &lt;- sapply(CCF, function(corXY){\n    ss_size &lt;- floor(length(corXY)*3/4)\n    corXY_boot &lt;- sapply(1:200, function(b){\n        corXY_b &lt;- sample(corXY, ss_size, replace=F)\n        mean(corXY_b, na.rm=T)\n    })\n    quantile(corXY_boot,  probs=c(.025,.975), na.rm=T)\n})\npolygon( c(x, rev(x)), \n    c(CCF_meanCI[1,], rev(CCF_meanCI[2,])), \n    col=grey(0,.25), border=NA)",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Observational Data</span>"
    ]
  },
  {
    "objectID": "03_03_ObservationalData.html#economic-interdependence",
    "href": "03_03_ObservationalData.html#economic-interdependence",
    "title": "19  Observational Data",
    "section": "19.3 Economic Interdependence",
    "text": "19.3 Economic Interdependence\nIn addition to spatial and temporal dependence, many observational datasets exhibit interdependence between variables for economic reasons. In the minds of economists, many variables are endogenous: meaning that they are an economic outcome determined (or caused: \\(\\to\\)) by some other variable.\n\nIf \\(Y \\to X\\), then we have reverse causality\nIf \\(Y \\to X\\) and \\(X \\to Y\\), then we have simultaneity\nIf \\(Z\\to Y\\) and either \\(Z\\to X\\) or \\(X \\to Z\\), then we have omitted a potentially important variable\n\nNow recall the linear model is \\(Y=X\\beta + \\epsilon\\). The endogeneity issues imply \\(X\\) and \\(\\epsilon\\) are correlated, which is a barrier to interpreting OLS estimates as reflecting a causal relationship.1\n\n\nCode\n# Simulate data with an endogeneity issue\nn &lt;- 300\nz &lt;- rbinom(n,1,.5)\nxy &lt;- sapply(z, function(zi){\n    y &lt;- rnorm(1,zi,1)\n    x &lt;- rnorm(1,zi*2,1)\n    c(x,y)\n})\nxy &lt;- data.frame(x=xy[1,],y=xy[2,])\nplot(y~x, data=xy, pch=16, col=grey(0,.5))\nabline(lm(y~x,data=xy))\n\n\n\n\n\n\n\n\n\nI will focus on the seminal economic example to provide some intuition.\n\nCompetitive Market Equilibrium.\nThis model has three structural relationships: (1) market supply is the sum of quantities supplied by individual firms at a given price, (2) market demand is the sum of quantities demanded by individual people at a given price, and (3) market supply equals market demand in equilibrium. Assuming market supply and demand are linear, we can write these three relationships as \\[\\begin{eqnarray}\n\\label{eqn:market_supply}\nQ_{S}(P) &=& \\alpha_{S} + \\beta_{S} P + E_{S},\\\\\n\\label{eqn:market_demand}\nQ_{D}(P) &=& \\alpha_{D} - \\beta_{D} P + E_{D},\\\\\n\\label{eqn:market_eq}\nQ_{D} &=& Q_{S} = Q.\n%%  $Q_{D}(P) = \\sum_{i} q_{D}_{i}(P)$,\n\\end{eqnarray}\\] This last equation implies a simultaneous “reduced form” relationship where both the price and the quantity are outcomes.\n\n\nCode\n# Demand Curve Simulator\nqd_fun &lt;- function(p, Ad=8, Bd=-.8, Ed_sigma=.25){\n    Qd &lt;- Ad + Bd*p + rnorm(1,0,Ed_sigma)\n    return(Qd)\n}\n\n# Supply Curve Simulator\nqs_fun &lt;- function(p, As=-8, Bs=1, Es_sigma=.25){\n    Qs &lt;- As + Bs*p + rnorm(1,0,Es_sigma)\n    return(Qs)\n}\n\n# Quantity Supplied and Demanded at 3 Prices\ncbind(P=8:10, D=qd_fun(8:10), S=qs_fun(8:10))\n##       P          D          S\n## [1,]  8  1.1925652 0.01120111\n## [2,]  9  0.3925652 1.01120111\n## [3,] 10 -0.4074348 2.01120111\n\n# Market Equilibrium Finder\neq_fun &lt;- function(demand, supply, P){\n    # Compute EQ (what we observe)\n    eq_id &lt;- which.min( abs(demand-supply) )\n    eq &lt;- c(P=P[eq_id], Q=demand[eq_id]) \n    return(eq)\n}\n\n\n\n\nCode\n# Simulations Parameters\nN &lt;- 300 # Number of Market Interactions\nP &lt;- seq(5,10,by=.01) # Price Range to Consider\n\n# Generate Data from Competitive Market  \n# Plot Underlying Process\nplot.new()\nplot.window(xlim=c(0,2), ylim=range(P))\nEQ1 &lt;- sapply(1:N, function(n){\n    # Market Data Generating Process\n    demand &lt;- qd_fun(P)\n    supply &lt;- qs_fun(P)\n    eq &lt;- eq_fun(demand, supply, P)    \n    # Plot Theoretical Supply and Demand\n    lines(demand, P, col=grey(0,.01))\n    lines(supply, P, col=grey(0,.01))\n    points(eq[2], eq[1], col=grey(0,.05), pch=16)\n    # Save Data\n    return(eq)\n})\naxis(1)\naxis(2)\nmtext('Quantity',1, line=2)\nmtext('Price',2, line=2)\n\n\n\n\n\n\n\n\n\nSuppose we ask “what is the effect of price on quantity?” You can simply run a regression of quantity, \\(Y\\), on price, \\(X\\), and get the slope coefficient \\(\\hat{b}_{1} = \\hat{C}_{Q^{*} P^{*}} / \\hat{V}_{P^{*}}\\). You always get a number back, but it is hard to interpret meaningfully.\n\n\nCode\n# Analyze Market Data\ndat1 &lt;- data.frame(t(EQ1), cost='1', T=1:N)\nreg1 &lt;- lm(Q~P, data=dat1)\nsummary(reg1)\n## \n## Call:\n## lm(formula = Q ~ P, data = dat1)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.57279 -0.11977 -0.00272  0.11959  0.45525 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)  \n## (Intercept) -0.21323    0.43212  -0.493   0.6221  \n## P            0.12355    0.04864   2.540   0.0116 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1674 on 298 degrees of freedom\n## Multiple R-squared:  0.02119,    Adjusted R-squared:  0.0179 \n## F-statistic: 6.451 on 1 and 298 DF,  p-value: 0.0116\n\n\nThis simple example has a profound insight: price-quantity data does not generally tell you how price affects quantity (or vice-versa). The reason is simultaneity: price and quantity mutually cause one another in markets.2\nMoreover, this example also clarifies that our initial question “what is the effect of price on quantity?” is misguided. We could more sensibly ask “what is the effect of price on quantity supplied?” or “what is the effect of price on quantity demanded?”\n\n\nContamination.\nWith multiple linear regression, endogeneity biases are not just a problem for your main variable of interest. Suppose you are interested in how \\(X_{1}\\) affects \\(Y\\), conditioning on \\(X_{2}\\), and that the data generating process is linear: \\(Y=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\epsilon\\). You paid special attention in your research design to find a case where \\(X_{1}\\) is truly exogenous. Unfortunately, if \\(X_{2}\\) is correlated with the error term, then there is also a bias for \\(X_{1}\\). The magnitude of the bias for \\(X_{1}\\) depends on the correlations between \\(X_{1}\\) and \\(X_{2}\\) as well as \\(X_{2}\\) and \\(\\epsilon\\).3",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Observational Data</span>"
    ]
  },
  {
    "objectID": "03_03_ObservationalData.html#further-reading",
    "href": "03_03_ObservationalData.html#further-reading",
    "title": "19  Observational Data",
    "section": "19.4 Further Reading",
    "text": "19.4 Further Reading",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Observational Data</span>"
    ]
  },
  {
    "objectID": "03_03_ObservationalData.html#footnotes",
    "href": "03_03_ObservationalData.html#footnotes",
    "title": "19  Observational Data",
    "section": "",
    "text": "Note that \\(X\\) and \\(\\epsilon\\) may be correlated for other reasons too, such as when \\(X\\) is measured with error.↩︎\nAlthough there are many ways this simultaneity can happen, economic theorists have made great strides in analyzing the simultaneity problem as it arises from equilibrium market relationships. In fact, 2SLS arose to understand agricultural markets. With a linear structure to supply and demand, we can even use algebra to solve for the equilibrium price and quantity analytically as \\[\\begin{eqnarray}\nP^{*} &=& \\frac{\\alpha_{D}-\\alpha_{S}}{\\beta_{D}+\\beta_{S}} + \\frac{E_{D} - E_{S}}{\\beta_{D}+\\beta_{S}}, \\\\\nQ^{*} &=& \\frac{\\alpha_{S}\\beta_{D}+ \\alpha_{D}\\beta_{S}}{\\beta_{D}+\\beta_{S}} + \\frac{E_{S}\\beta_{D}+ E_{D}\\beta_{S}}{\\beta_{D}+\\beta_{S}}.\n\\end{eqnarray}\\]↩︎\nDenoting \\(X=[X_{1}, X_{2}]\\), you estimate the OLS coefficients \\(\\hat{B}^{*}\\). After algebraic work, you can find the bias \\(\\mathbb{E}[ \\hat{B}^{*} - \\beta]\\) suffers from a contamination effect. \\[\\begin{eqnarray}\n\\hat{B^{*}} &=& [\\hat{X}'\\hat{X}]^{-1}X'y \\\\\n\\mathbb{E}[X'\\epsilon] &=&\n\\begin{bmatrix}\n0 \\\\ \\rho\n\\end{bmatrix}\\\\\n\\mathbb{E}[ \\hat{B}^{*} - \\beta] &=& [\\hat{X}'\\hat{X}]^{-1} \\begin{bmatrix}\n0 \\\\ \\rho\n\\end{bmatrix} \\neq\n\\begin{bmatrix}\n0 \\\\ \\rho\n\\end{bmatrix}\n\\end{eqnarray}\\]↩︎",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Observational Data</span>"
    ]
  },
  {
    "objectID": "03_04_ExperimentalData.html",
    "href": "03_04_ExperimentalData.html",
    "title": "20  Experimental Data",
    "section": "",
    "text": "20.1 Design Basics",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Experimental Data</span>"
    ]
  },
  {
    "objectID": "03_04_ExperimentalData.html#design-basics",
    "href": "03_04_ExperimentalData.html#design-basics",
    "title": "20  Experimental Data",
    "section": "",
    "text": "Control and Randomize.\nPerhaps the main feature that distinguishes experimental data from observational data is “control”. You can manipulate one of the explanatory variables, and also ensure that other factors are not changing at the same time.\nAnother distinguishing feature of experiments is “randomization”. You can ensure that a manipulated variable is not systematically associated with other variables.\nTo be concrete, we will continue with our supply and demand example from the last chapter, and this time introduce a cost shock.\n\n\nCompetitive Equilibrium Example.\n\n\nCode\n# Demand Curve Simulator\nqd_fun &lt;- function(p, Ad=8, Bd=-.8, Ed_sigma=.25){\n    Qd &lt;- Ad + Bd*p + rnorm(1,0,Ed_sigma)\n    return(Qd)\n}\n\n# Supply Curve Simulator\nqs_fun &lt;- function(p, As=-8, Bs=1, Es_sigma=.25){\n    Qs &lt;- As + Bs*p + rnorm(1,0,Es_sigma)\n    return(Qs)\n}\n\n# Quantity Supplied and Demanded at 3 Prices\ncbind(P=8:10, D=qd_fun(8:10), S=qs_fun(8:10))\n##       P          D          S\n## [1,]  8 1.68508047 -0.2082764\n## [2,]  9 0.88508047  0.7917236\n## [3,] 10 0.08508047  1.7917236\n\n# Market Equilibrium Finder\neq_fun &lt;- function(demand, supply, P){\n    # Compute EQ (what we observe)\n    eq_id &lt;- which.min( abs(demand-supply) )\n    eq &lt;- c(P=P[eq_id], Q=demand[eq_id]) \n    return(eq)\n}\n\n\n\n\nCode\nN &lt;- 300 # Number of Market Interactions\nP &lt;- seq(5,10,by=.01) # Price Range to Consider\nEQ1 &lt;- sapply(1:N, function(n){\n    # Market Data Generating Process\n    demand &lt;- qd_fun(P)\n    supply &lt;- qs_fun(P)\n    eq &lt;- eq_fun(demand, supply, P)    \n    return(eq)\n})\ndat1 &lt;- data.frame(t(EQ1), cost='1', T=1:N)\n\n\nIf you have exogenous variation on one side of the market, you can get information on the other. For example, lower costs shift out supply (more is produced at given price), allowing you to trace out part of a demand curve.\nTo see this, consider an experiment where student subjects are recruited to a classroom and randomly assigned to be either buyers or sellers in a market for little red balls. In this case, the classroom environment allows the experimenter to control for various factors (e.g., the temperature of the room is constant for all subjects) and the explicit randomization of subjects means that there are not typically systematic differences in different groups of students.\nIn the experiment, sellers are given linear “cost functions” that theoretically yield individual supplies like \\(\\eqref{eqn:market_supply}\\) and are paid “price - cost”. Buyers are given linear “benefit functions” that theoretically yield individual demands like \\(\\eqref{eqn:market_demand}\\), and are paid “benefit - price”. The theoretical predictions are theorefore given in \\(\\eqref{eqn:market_supply}\\). Moreover, experimental manipulation of \\(\\alpha_{S}\\) leads to \\[\\begin{eqnarray}\n\\label{eqn:comp_market_statics}\n\\frac{d P^{*}}{d \\alpha_{S}} = \\frac{-1}{\\beta_{D}+\\beta_{S}}, \\\\\n\\frac{d Q^{*}}{d \\alpha_{S}} = \\frac{\\beta_{D}}{\\beta_{D}+\\beta_{S}}.\n\\end{eqnarray}\\] In this case, the supply shock has identified the demand slope: \\(-\\beta_{D}=d Q^{*}/d P^{*}\\).\n\n\nCode\n# New Observations After Cost Change\nEQ2 &lt;- sapply(1:N, function(n){\n    demand &lt;- qd_fun(P)\n    supply2 &lt;- qs_fun(P, As=-6.5) # More Supplied at Given Price\n    eq &lt;- eq_fun(demand, supply2, P)\n    return(eq)\n    # lines(supply2, P, col=rgb(0,0,1,.01))\n    #points(eq[2], eq[1], col=rgb(0,0,1,.05), pch=16)\n})\ndat2 &lt;- data.frame(t(EQ2), cost='2', T=(1:N) + N)\ndat2 &lt;- rbind(dat1, dat2)\n\n# Plot Simulated Market Data\ncols &lt;- ifelse(as.numeric(dat2$cost)==2, rgb(0,0,1,.2), rgb(0,0,0,.2))\nplot.new()\nplot.window(xlim=c(0,2), ylim=range(P))\npoints(dat2$Q, dat2$P, col=cols, pch=16)\naxis(1)\naxis(2)\nmtext('Quantity',1, line=2)\nmtext('Price',2, line=2)\n\n\n\n\n\n\n\n\n\nIf the function forms for supply and demand are different from what we predicted, we can still measure how much the experimental manipulation of production costs affects the equilibrium quantity sold (and compare that to what was predicted).1",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Experimental Data</span>"
    ]
  },
  {
    "objectID": "03_04_ExperimentalData.html#comparisons-over-time",
    "href": "03_04_ExperimentalData.html#comparisons-over-time",
    "title": "20  Experimental Data",
    "section": "20.2 Comparisons Over Time",
    "text": "20.2 Comparisons Over Time\n\nRegression Discontinuities/Kinks.\nThe basic idea of RDD/RKD is to examine how a variable changes just before and just after a treatment. RDD estimates the difference in the levels of an outcome variable, whereas RKD estimates the difference in the slope. Turning to our canonical competitive market example, the RDD estimate is the difference between the lines at \\(T=300\\).\n\n\nCode\n# Locally Linear Regression \n# (Compare means near break)\n\ncols &lt;- ifelse(as.numeric(dat2$cost)==2, rgb(0,0,1,.5), rgb(0,0,0,.5))\nplot(P~T, dat2, main='Effect of Cost Shock on Price', \n    font.main=1, pch=16, col=cols)\nregP1 &lt;- loess(P~T, dat2[dat2$cost==1,]) \nx1 &lt;- regP1$x\n#lm(): x1 &lt;- regP1$model$T \nlines(x1, predict(regP1), col=rgb(0,0,0), lwd=2)\nregP2 &lt;- loess(P~T, dat2[dat2$cost==2,])\nx2 &lt;- regP2$x #regP1$model$T\nlines(x2, predict(regP2), col=rgb(0,0,1), lwd=2)\n\n\n\n\n\n\n\n\n\nCode\n\nplot(Q~T, dat2, main='Effect of Cost Shock on Quantity',\n    font.main=1, pch=16, col=cols)\nregQ1 &lt;- loess(Q~T, dat2[dat2$cost==1,]) \nlines(x1, predict(regQ1), col=rgb(0,0,0), lwd=2)\nregQ2 &lt;- loess(Q~T, dat2[dat2$cost==2,])\nx2 &lt;- regP2$x #regP1$model$T\nlines(x2, predict(regQ2), col=rgb(0,0,1), lwd=2)\n\n\n\n\n\n\n\n\n\n\nCode\n# Linear Regression Alternative\nsub_id &lt;- (dat2$cost==1 & dat2$T &gt; 250) | (dat2$cost==2 & dat2$T &lt; 300)\ndat2W &lt;- dat2[sub_id,  ]\nregP &lt;- lm(P~T*cost, dat2)\nregQ &lt;- lm(Q~T*cost, dat2)\nstargazer::stargazer(regP, regQ, \n    type='html',\n    title='Recipe RDD',\n    header=F)\n\n\n\nRecipe RDD\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nP\n\n\nQ\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nT\n\n\n-0.0001\n\n\n0.0001\n\n\n\n\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n\n\n\n\n\n\n\n\n\n\ncost2\n\n\n-0.706***\n\n\n0.628***\n\n\n\n\n\n\n(0.065)\n\n\n(0.054)\n\n\n\n\n\n\n\n\n\n\n\n\nT:cost2\n\n\n-0.0002\n\n\n0.00003\n\n\n\n\n\n\n(0.0002)\n\n\n(0.0002)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n8.892***\n\n\n0.879***\n\n\n\n\n\n\n(0.023)\n\n\n(0.019)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n600\n\n\n600\n\n\n\n\nR2\n\n\n0.805\n\n\n0.803\n\n\n\n\nAdjusted R2\n\n\n0.804\n\n\n0.802\n\n\n\n\nResidual Std. Error (df = 596)\n\n\n0.199\n\n\n0.165\n\n\n\n\nF Statistic (df = 3; 596)\n\n\n819.647***\n\n\n808.340***\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\nRemember that this is effect is local: different magnitudes of the cost shock or different demand curves generally yield different estimates.\nMoreover, note that more than just costs have changed over time: subjects in the later periods have history experience behind them while they do not in earlier periods. So hidden variables like “beliefs” are implicitly treated as well. This is one concrete reason to have an explicit control group.\n\n\nDifference in Differences.\nThe basic idea of DID is to examine how a variable changes in response to an exogenous shock, compared to a control group.\n\n\nCode\nEQ3 &lt;- sapply(1:(2*N), function(n){\n\n    # Market Mechanisms\n    demand &lt;- qd_fun(P)\n    supply &lt;- qs_fun(P)\n\n    # Compute EQ (what we observe)\n    eq_id &lt;- which.min( abs(demand-supply) )\n    eq &lt;- c(P=P[eq_id], Q=demand[eq_id]) \n\n    # Return Equilibrium Observations\n    return(eq)\n})\ndat3 &lt;- data.frame(t(EQ3), cost='1', T=1:ncol(EQ3))\ndat3_pre  &lt;- dat3[dat3$T &lt;= N ,]\ndat3_post &lt;- dat3[dat3$T &gt; N ,]\n\n# Plot Price Data\npar(mfrow=c(1,2))\nplot(P~T, dat2, main='Effect of Cost Shock on Price', \n    font.main=1, pch=16, col=cols, cex=.5)\nlines(x1, predict(regP1), col=rgb(0,0,0), lwd=2)\nlines(x2, predict(regP2), col=rgb(0,0,1), lwd=2)\n# W/ Control group\npoints(P~T, dat3, pch=16, col=rgb(1,0,0,.5), cex=.5)\nregP3a &lt;- loess(P~T, dat3_pre)\nx3a &lt;- regP3a$x\nlines(x3a, predict(regP3a), col=rgb(1,0,0), lwd=2)\nregP3b &lt;- loess(P~T, dat3_post)\nx3b &lt;- regP3b$x\nlines(x3b, predict(regP3b), col=rgb(1,0,0), lwd=2)\n\n\n# Plot Quantity Data\nplot(Q~T, dat2, main='Effect of Cost Shock on Quantity',\n    font.main=1, pch=17, col=cols, cex=.5)\nlines(x1, predict(regQ1), col=rgb(0,0,0), lwd=2)\nlines(x2, predict(regQ2), col=rgb(0,0,1), lwd=2)\n# W/ Control group\npoints(Q~T, dat3, pch=16, col=rgb(1,0,0,.5), cex=.5)\nregQ3a &lt;- loess(Q~T, dat3_pre) \nlines(x3a, predict(regQ3a), col=rgb(1,0,0), lwd=2)\nregQ3b &lt;- loess(Q~T, dat3_post) \nlines(x3b, predict(regQ3b), col=rgb(1,0,0), lwd=2)\n\n\n\n\n\n\n\n\n\nLinear Regression Estimates\n\nCode\n# Pool Data\ndat_pooled &lt;- rbind(\n    cbind(dat2, EverTreated=1, PostPeriod=(dat2$T &gt; N)),\n    cbind(dat3, EverTreated=0, PostPeriod=(dat3$T &gt; N)))\ndat_pooled$EverTreated &lt;- as.factor(dat_pooled$EverTreated)\ndat_pooled$PostPeriod &lt;- as.factor(dat_pooled$PostPeriod)\n\n# Estimate Level Shift for Different Groups after T=300\nregP &lt;- lm(P~PostPeriod*EverTreated, dat_pooled)\nregQ &lt;- lm(Q~PostPeriod*EverTreated, dat_pooled)\nstargazer::stargazer(regP, regQ, \n    type='html',\n    title='Recipe DiD',\n    header=F)\n\n\n\nRecipe DiD\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nP\n\n\nQ\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nPostPeriod\n\n\n-0.007\n\n\n0.001\n\n\n\n\n\n\n(0.016)\n\n\n(0.014)\n\n\n\n\n\n\n\n\n\n\n\n\nEverTreated1\n\n\n-0.004\n\n\n0.001\n\n\n\n\n\n\n(0.016)\n\n\n(0.014)\n\n\n\n\n\n\n\n\n\n\n\n\nPostPeriodTRUE:EverTreated1\n\n\n-0.799***\n\n\n0.662***\n\n\n\n\n\n\n(0.022)\n\n\n(0.020)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n8.888***\n\n\n0.889***\n\n\n\n\n\n\n(0.011)\n\n\n(0.010)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n1,200\n\n\n1,200\n\n\n\n\nR2\n\n\n0.765\n\n\n0.732\n\n\n\n\nAdjusted R2\n\n\n0.765\n\n\n0.731\n\n\n\n\nResidual Std. Error (df = 1196)\n\n\n0.194\n\n\n0.174\n\n\n\n\nF Statistic (df = 3; 1196)\n\n\n1,300.757***\n\n\n1,089.820***\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\nBlocking and Clustering .\nRandomization is the process of randomly manipulating an explanatory variable of (assigning treatments to units).\nA completely randomized design is useful when the experimental units are homogeneous. If the experimental units are heterogeneous, blocking is often used to form homogeneous groups.\nContinuing with supply and demand example, we might manipulate costs (randomize high or low treatments) for companies in different industries (computer services, lumber harvesting).",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Experimental Data</span>"
    ]
  },
  {
    "objectID": "03_04_ExperimentalData.html#quasi-experiments",
    "href": "03_04_ExperimentalData.html#quasi-experiments",
    "title": "20  Experimental Data",
    "section": "20.3 Quasi Experiments",
    "text": "20.3 Quasi Experiments\nQuasi or “Natural” experiments are historical case studies have the second distinguishing feature of experiments: randomization, but not the first: control. This helps remedy the endogeneity issues in observational data. “Intrumental Variables”, “RDD”, “DID” methods discussed above are used in historical event studies. The elementary versions use linear regression, so I can cover them here using our competitive equilibrium example from before.\n\nTwo Stage Least Squares (2SLS).\nConsider the market equilibrium example, which contains a cost shock. We can simply run another regression, but there will still be a problem.\n\n\nCode\n# Not exactly right, but at least right sign\nreg2 &lt;- lm(Q~P, data=dat2)\nsummary(reg2)\n## \n## Call:\n## lm(formula = Q ~ P, data = dat2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.71544 -0.13923 -0.01042  0.15863  0.60378 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  6.73463    0.17518   38.44   &lt;2e-16 ***\n## P           -0.65011    0.02063  -31.52   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.227 on 598 degrees of freedom\n## Multiple R-squared:  0.6242, Adjusted R-squared:  0.6236 \n## F-statistic: 993.3 on 1 and 598 DF,  p-value: &lt; 2.2e-16\n\n\nIt turns out that the noisiness of the process within each group affects our OLS estimate: \\(\\hat{B^{*}}=\\hat{C}_{Q^{*}P^{*}} / \\hat{V}_{P^{*}}\\). For details, see\n\n\nWithin Group Variance\n\nYou can experiment with the effect of different variances on both OLS and IV in the code below. And note that if we had multiple supply shifts and recorded their magnitudes, then we could recover more information about demand, perhaps tracing it out entirely.\n\n\nCode\n# Examine\nEgrid &lt;- expand.grid(Ed_sigma=c(.001, .25, 1), Es_sigma=c(.001, .25, 1))\n\nEgrid_regs &lt;- lapply(1:nrow(Egrid), function(i){\n    Ed_sigma &lt;- Egrid[i,1]\n    Es_sigma &lt;- Egrid[i,2]    \n    EQ1 &lt;- sapply(1:N, function(n){\n        demand &lt;- qd_fun(P, Ed_sigma=Ed_sigma)\n        supply &lt;- qs_fun(P, Es_sigma=Es_sigma)\n        return(eq_fun(demand, supply, P))\n    })\n    EQ2 &lt;- sapply(1:N, function(n){\n        demand &lt;- qd_fun(P,Ed_sigma=Ed_sigma)\n        supply2 &lt;- qs_fun(P, As=-6.5,Es_sigma=Es_sigma)\n        return(eq_fun(demand, supply2, P))\n    })\n    dat &lt;- rbind(\n        data.frame(t(EQ1), cost='1'),\n        data.frame(t(EQ2), cost='2'))\n    return(dat)\n})\nEgrid_OLS &lt;- sapply(Egrid_regs, function(dat) coef( lm(Q~P, data=dat)))\nEgrid_IV &lt;- sapply(Egrid_regs, function(dat) coef( feols(Q~1|P~cost, data=dat)))\n\n#cbind(Egrid, coef_OLS=t(Egrid_OLS)[,2], coef_IV=t(Egrid_IV)[,2])\nlapply( list(Egrid_OLS, Egrid_IV), function(ei){\n    Emat &lt;- matrix(ei[2,],3,3)\n    rownames(Emat) &lt;- paste0('Ed_sigma.',c(.001, .25, 1))\n    colnames(Emat) &lt;- paste0('Es_sigma.',c(.001, .25, 1))\n    return( round(Emat,2))\n})\n\n\n\nTo overcome this issue, we can compute the change in the expected values \\(d \\mathbb{E}[Q^{*}] / d \\mathbb{E}[P^{*}] =-\\beta_{D}\\). Empirically, this is estimated via the change in average value.\n\n\nCode\n# Wald (1940) Estimate\ndat_mean &lt;- rbind(\n    colMeans(dat2[dat2$cost==1,1:2]),\n    colMeans(dat2[dat2$cost==2,1:2]))\ndat_mean\n##             P         Q\n## [1,] 8.883867 0.8898249\n## [2,] 8.078167 1.5521779\nB_est &lt;- diff(dat_mean[,2])/diff(dat_mean[,1])\nround(B_est, 2)\n## [1] -0.82\n\n\nWe can also separately recover \\(d \\mathbb{E}[Q^{*}] / d \\mathbb{E}[\\alpha_{S}]\\) and \\(d \\mathbb{E}[P^{*}] / d \\mathbb{E}[\\alpha_{S}]\\) from separate regressions.2\n\n\nCode\n# Heckman (2000, p.58) Estimate\nols_1 &lt;- lm(P~cost, data=dat2)\nols_2 &lt;- lm(Q~cost, data=dat2)\nB_est2 &lt;- coef(ols_2)/coef(ols_1)\nround(B_est2[[2]],2)\n## [1] -0.82\n\n\nAlternatively, we can recover the same estimate using an 2SLS regression with two equations: \\[\\begin{eqnarray}\n\\hat{P} &=& b_{0p} + b_{1p} \\alpha_{S} + e_{p} \\\\\n\\hat{Q} &=& b_{0q} + b_{1q} \\hat{p}  + e_{q}.\n\\end{eqnarray}\\] where \\(\\hat{p}\\), the predicted value of \\(\\hat{P}\\) from the first equation, is used to explain quantity in the second equation. In the first regression, we estimate how the cost shock affects prices: \\(\\hat{b}_{1p}\\) and then predict prices \\(\\hat{p}\\). In the second equation, we estimate how the average effect of predicted prices (which are exogenous to demand) affect quantity demanded.\nTo understand this theoretically, first substitute the equilibrium condition into the supply equation: \\(Q_{D}=Q_{S}=\\alpha_{S}+ \\beta_{S} P + E_{S}\\), lets us rewrite \\(P\\) as a function of \\(Q_{D}\\). This yields two theoretical equations. We are using the cost shock to understand the theoretical demand curve. \\[\\begin{eqnarray}\n\\label{eqn:linear_supply_iv}\nP &=& -\\frac{\\alpha_{S}}{{\\beta_{S}}} + \\frac{Q_{D}}{\\beta_{S}} - \\frac{E_{S}}{\\beta_{S}} \\\\\n\\label{eqn:linear_demand_iv}\nQ_{D} &=& \\alpha_{D} + \\beta_{D} P  + E_{D}.\n\\end{eqnarray}\\]\n\n\nCode\n# Two Stage Least Squares Estimate\nols_1 &lt;- lm(P~cost, data=dat2)\ndat2_new  &lt;- cbind(dat2, Phat=predict(ols_1))\nreg_2sls &lt;- lm(Q~Phat, data=dat2_new)\nsummary(reg_2sls)\n## \n## Call:\n## lm(formula = Q ~ Phat, data = dat2_new)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.43947 -0.10944  0.00566  0.11676  0.53412 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  8.19311    0.14170   57.82   &lt;2e-16 ***\n## Phat        -0.82208    0.01669  -49.26   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1647 on 598 degrees of freedom\n## Multiple R-squared:  0.8023, Adjusted R-squared:  0.8019 \n## F-statistic:  2426 on 1 and 598 DF,  p-value: &lt; 2.2e-16\n\n# One Stage Instrumental Variables Estimate\nlibrary(fixest)\nreg2_iv &lt;- feols(Q~1|P~cost, data=dat2)\nsummary(reg2_iv)\n## TSLS estimation - Dep. Var.: Q\n##                   Endo.    : P\n##                   Instr.   : cost\n## Second stage: Dep. Var.: Q\n## Observations: 600\n## Standard-errors: IID \n##              Estimate Std. Error  t value  Pr(&gt;|t|)    \n## (Intercept)  8.193109   0.206388  39.6975 &lt; 2.2e-16 ***\n## fit_P       -0.822084   0.024308 -33.8196 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## RMSE: 0.239465   Adj. R2: 0.801948\n## F-test (1st stage), P: stat = 2,449.56, p &lt; 2.2e-16, on 1 and 598 DoF.\n##            Wu-Hausman: stat =   542.56, p &lt; 2.2e-16, on 1 and 597 DoF.\n\n\n\n\nCaveats.\n2SLS regression analysis can be very insightful, but I also want to stress some caveats about their practical application. Most of which stem directly from the absence of control that true experiments have.\n\nInstrument exogeneity (Exclusion Restriction): The instrument must affect outcomes only through the treatment variable (e.g., only supply is affected directly, not demand).\nInstrument relevance: The instrument must be strongly correlated with the endogenous regressor, implying the shock creates meaningful variation.\nFunctional form correctness: Supply and demand are assumed linear and additively separable.\nMultiple hypothesis testing risks: We were not repeatedly testing different instruments, which can artificially produce significant findings by chance.\nExclusion restriction violations: Spatial or temporal spillovers may cause instruments to affect the outcome through unintended channels, undermining instrument exogeneity.\nWeak instruments: Spatial clustering, serial correlation, or network interdependencies can reduce instrument variation, causing weak instruments.\nInference and standard errors: Spatial or temporal interdependence reduces the effective sample size, making conventional standard errors misleadingly small.\n\nWe always get coefficients back when running feols, and sometimes the computed p-values are very small. The interpretation of those numbers rests on many assumptions, and we are rarely sure that all of these assumptions hold. Researchers often also report their OLS results, but that is insufficient.",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Experimental Data</span>"
    ]
  },
  {
    "objectID": "03_04_ExperimentalData.html#further-reading",
    "href": "03_04_ExperimentalData.html#further-reading",
    "title": "20  Experimental Data",
    "section": "20.4 Further Reading",
    "text": "20.4 Further Reading\nYou are directed to the following resources which discusses endogeneity in more detail and how it applies generally.\n\nCausal Inference for Statistics, Social, and Biomedical Sciences: An Introduction\nhttps://www.mostlyharmlesseconometrics.com/\nhttps://www.econometrics-with-r.org\nhttps://bookdown.org/paul/applied-causal-analysis/\nhttps://mixtape.scunning.com/\nhttps://theeffectbook.net/\nhttps://www.r-causal.org/\nhttps://matheusfacure.github.io/python-causality-handbook/landing-page.html\n\nFor RDD and DID methods in natural experiments, see\n\nhttps://bookdown.org/paul/applied-causal-analysis/rdd-regression-discontinuity-design.html\nhttps://mixtape.scunning.com/06-regression_discontinuity\nhttps://theeffectbook.net/ch-RegressionDiscontinuity.html\nhttps://mixtape.scunning.com/09-difference_in_differences\nhttps://theeffectbook.net/ch-DifferenceinDifference.html\nhttp://www.urfie.net/read/index.html#page/226\n\nFor IV methods in natural experiments, see\n\nhttps://cameron.econ.ucdavis.edu/e240a/ch04iv.pdf\nhttps://mru.org/courses/mastering-econometrics/introduction-instrumental-variables-part-one\nhttps://www.econometrics-with-r.org/12-ivr.html\nhttps://bookdown.org/paul/applied-causal-analysis/estimation-2.html\nhttps://mixtape.scunning.com/07-instrumental_variables\nhttps://theeffectbook.net/ch-InstrumentalVariables.html\nhttp://www.urfie.net/read/index.html#page/247",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Experimental Data</span>"
    ]
  },
  {
    "objectID": "03_04_ExperimentalData.html#footnotes",
    "href": "03_04_ExperimentalData.html#footnotes",
    "title": "20  Experimental Data",
    "section": "",
    "text": "Notice that even in this linear model, however, all effects are conditional: The effect of a cost change on quantity or price depends on the demand curve. A change in costs affects quantity supplied but not quantity demanded (which then affects equilibrium price) but the demand side of the market still matters! The change in price from a change in costs depends on the elasticity of demand.↩︎\nMathematically, we can also do this in a single step by exploiting linear algebra: \\(\\frac{\\frac{ Cov(Q^{*},\\alpha_{S})}{ V(\\alpha_{S}) } }{\\frac{ Cov(P^{*},\\alpha_{S})}{ V(\\alpha_{S}) }} = \\frac{Cov(Q^{*},\\alpha_{S} )}{ Cov(P^{*},\\alpha_{S})}.\\)↩︎",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Experimental Data</span>"
    ]
  },
  {
    "objectID": "03_05_DataScientism.html",
    "href": "03_05_DataScientism.html",
    "title": "21  Data Scientism",
    "section": "",
    "text": "21.1 False Positives\nIn practice, it is hard to find a good natural experiment. For example, suppose we asked “what is the effect of wages on police demanded?” and examined a policy which lowered the educational requirements from 4 years to 2 to become an officer. This increases the labour supply, but it also affects the demand curve through “general equilibrium”: as some of the new officers were potentially criminals and, with fewer criminals, the demand for police shifts down.\nIn practice, it is also easy to find a bad instrument. Paradoxically, natural experiments are something you are supposed to find but never search for. As you search for good instruments, for example, sometimes random noise will appear like a good instrument (spurious instruments). In this age of big data, we are getting increasingly more data and, perhaps surprisingly, this makes it easier to make false discoveries.\nWe will consider three classical ways for false discoveries to arise. After that, there are examples with the latest and greatest empirical recipes—we don’t have so many theoretical results yet but I think you can understand the issue with the numerical example. Although it is difficult to express numerically, you must also know that if you search for a good natural experiment for too long, you can also be led astray from important questions. There are good reasons to be excited about empirical social science, but we would be wise to recall some earlier wisdom from economists on the matter.",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Data Scientism</span>"
    ]
  },
  {
    "objectID": "03_05_DataScientism.html#false-positives",
    "href": "03_05_DataScientism.html#false-positives",
    "title": "21  Data Scientism",
    "section": "",
    "text": "Data Errors.\nA huge amount of data normally means a huge amount of data cleaning/merging/aggregating. This avoids many copy-paste errors, which are a recipe for disaster, but may also introduce other types of errors. Some spurious results are driven by honest errors in data cleaning. According to one estimate, this is responsible for around one fifth of all medical science retractions (there is even a whole book about this!). Although there are not similar meta-analysis in economics, there are some high-profile examples. This includes papers that are highly influential, like Lott, Levitt and Reinhart and Rogoff as well as others the top economics journals, like the RESTUD and AER. There are some reasons to think such errors are more widespread across the social sciences; e.g., in Census data and Aid data. So be careful!\nNote: one reason to plot your data is to help spot such errors.\n\n\nP-Hacking.\nAnother class of errors pertains to P-hacking (and it’s various synonyms: data drudging, star mining,….). While there are cases of fraudulent data manipulation (which can be considered as a dishonest data error), P-hacking need not even be intentional. You can simply be trying different variable transformations to uncover patterns in the data, for example, without accounting for how easy it is to find patterns when transforming completely random data. P-hacking is pernicious and widespread.\n\n\nCode\n# P-hacking OSLS with different explanatory vars\nset.seed(123)\nn &lt;- 50\nX1 &lt;- runif(n)\n\n# Regression Machine:\n# repeatedly finds covariate, runs regression\n# stops when statistically significant at .1%\np &lt;- 1\ni &lt;- 0\nwhile(p &gt;= .001){ \n    # Get Random Covariate\n    X2 &lt;-  runif(n)\n    # Merge and `Analyze'\n    dat_i &lt;- data.frame(X1,X2)\n    reg_i &lt;- lm(X1~X2, data=dat_i)\n    # update results in global environment\n    p &lt;- summary(reg_i)$coefficients[2,4]\n    i &lt;- i+1\n}\n#summary(reg_i)\n\nplot(X1~X2, data=dat_i,\n    pch=16, col=grey(0,.5), font.main=1,\n    main=paste0('Random Dataset ', i,\":   p=\",\n        formatC(p,digits=2, format='fg')))\nabline(reg_i)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# P-hacking 2SLS with different explanatory vars\n# and different instrumental vars\nlibrary(fixest)\np &lt;- 1\nii &lt;- 0\nset.seed(123)\nwhile(p &gt;= .05){\n    # Get Random Covariates\n    X2 &lt;-  runif(n)    \n    X3 &lt;-  runif(n)\n    # Create Treatment Variable based on Cutoff\n    cutoffs &lt;- seq(0,1,length.out=11)[-c(1,11)]\n    for(tau in cutoffs){\n        T3 &lt;- 1*(X3 &gt; tau)\n        # Merge and `Analyze'\n        dat_i &lt;- data.frame(X1,X2,T3)\n        ivreg_i &lt;- feols(X1~1|X2~T3, data=dat_i)\n        # Update results in global environment\n        ptab &lt;- summary(ivreg_i)$coeftable\n        if( nrow(ptab)==2){\n            p &lt;- ptab[2,4]\n            ii &lt;- ii+1\n        }\n    }\n}\nsummary(ivreg_i)\n## TSLS estimation - Dep. Var.: X1\n##                   Endo.    : X2\n##                   Instr.   : T3\n## Second stage: Dep. Var.: X1\n## Observations: 50\n## Standard-errors: IID \n##              Estimate Std. Error   t value  Pr(&gt;|t|)    \n## (Intercept) -9.95e-14      1e-06 -9.95e-08         1    \n## fit_X2       1.00e+00      1e-06  1.00e+06 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## RMSE: 5.81e-14   Adj. R2: -0.006886\n## F-test (1st stage), X2: stat = 0.66488, p = 0.418869, on 1 and 48 DoF.\n##             Wu-Hausman: stat = 0.23218, p = 0.632145, on 1 and 47 DoF.",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Data Scientism</span>"
    ]
  },
  {
    "objectID": "03_05_DataScientism.html#spurious-regression",
    "href": "03_05_DataScientism.html#spurious-regression",
    "title": "21  Data Scientism",
    "section": "21.2 Spurious Regression",
    "text": "21.2 Spurious Regression\nEven without any coding errors or p-hacking, you can sometimes make a false discovery. We begin with a motivating empirical example of “US Gov’t Spending on Science”.\nFirst, get and inspect some data from https://tylervigen.com/spurious-correlations\n\n\nCode\n# Your data is not made up in the computer (hopefully!)\nvigen_csv &lt;- read.csv( paste0(\n'https://raw.githubusercontent.com/the-mad-statter/',\n'whysospurious/master/data-raw/tylervigen.csv') ) \nclass(vigen_csv)\n## [1] \"data.frame\"\nnames(vigen_csv)\n##  [1] \"year\"                         \"science_spending\"            \n##  [3] \"hanging_suicides\"             \"pool_fall_drownings\"         \n##  [5] \"cage_films\"                   \"cheese_percap\"               \n##  [7] \"bed_deaths\"                   \"maine_divorce_rate\"          \n##  [9] \"margarine_percap\"             \"miss_usa_age\"                \n## [11] \"steam_murders\"                \"arcade_revenue\"              \n## [13] \"computer_science_doctorates\"  \"noncom_space_launches\"       \n## [15] \"sociology_doctorates\"         \"mozzarella_percap\"           \n## [17] \"civil_engineering_doctorates\" \"fishing_drownings\"           \n## [19] \"kentucky_marriage_rate\"       \"oil_imports_norway\"          \n## [21] \"chicken_percap\"               \"train_collision_deaths\"      \n## [23] \"oil_imports_total\"            \"pool_drownings\"              \n## [25] \"nuclear_power\"                \"japanese_cars_sold\"          \n## [27] \"motor_vehicle_suicides\"       \"spelling_bee_word_length\"    \n## [29] \"spider_deaths\"                \"math_doctorates\"             \n## [31] \"uranium\"\nvigen_csv[1:5,1:5]\n##   year science_spending hanging_suicides pool_fall_drownings cage_films\n## 1 1996               NA               NA                  NA         NA\n## 2 1997               NA               NA                  NA         NA\n## 3 1998               NA               NA                  NA         NA\n## 4 1999            18079             5427                 109          2\n## 5 2000            18594             5688                 102          2\n\n\nExamine some data\n\n\nCode\npar(mfrow=c(1,2), mar=c(2,2,2,1))\nplot.new()\nplot.window(xlim=c(1999, 2009), ylim=c(5,9)*1000)\nlines(science_spending/3~year, data=vigen_csv, lty=1, col=2, pch=16)\ntext(2003, 8200, 'US spending on science, space, technology (USD/3)', col=2, cex=.6, srt=30)\nlines(hanging_suicides~year, data=vigen_csv, lty=1, col=4, pch=16)\ntext(2004, 6500, 'US Suicides by hanging, strangulation, suffocation (Deaths)', col=4, cex=.6, srt=30)\naxis(1)\naxis(2)\n\n\nplot.new()\nplot.window(xlim=c(2002, 2009), ylim=c(0,5))\nlines(cage_films~year, data=vigen_csv[vigen_csv$year&gt;=2002,], lty=1, col=2, pch=16)\ntext(2006, 0.5, 'Number of films with Nicolas Cage (Films)', col=2, cex=.6, srt=0)\nlines(pool_fall_drownings/25~year, data=vigen_csv[vigen_csv$year&gt;=2002,], lty=1, col=4, pch=16)\ntext(2006, 4.5, 'Number of drownings by falling into pool (US Deaths/25)', col=4, cex=.6, srt=0)\naxis(1)\naxis(2)\n\n\n\n\n\n\n\n\n\n\nCode\n# Include an intercept to regression 1\n#reg2 &lt;-  lm(cage_films ~ science_spending, data=vigen_csv)\n#suppressMessages(library(stargazer))\n#stargazer(reg1, reg2, type='html')\n\n\nAnother Example.\nThe US government spending on science is ruining cinema (p&lt;.001)!?\n\n\nCode\n# Drop Data before 1999\nvigen_csv &lt;- vigen_csv[vigen_csv$year &gt;= 1999,] \n\n# Run OLS Regression\nreg1 &lt;-  lm(cage_films ~ -1 + science_spending, data=vigen_csv)\nsummary(reg1)\n## \n## Call:\n## lm(formula = cage_films ~ -1 + science_spending, data = vigen_csv)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.7670 -0.7165  0.1447  0.7890  1.4531 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(&gt;|t|)    \n## science_spending 9.978e-05  1.350e-05    7.39 2.34e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.033 on 10 degrees of freedom\n##   (1 observation deleted due to missingness)\n## Multiple R-squared:  0.8452, Adjusted R-squared:  0.8297 \n## F-statistic: 54.61 on 1 and 10 DF,  p-value: 2.343e-05\n\n\nIt’s not all bad, because people in Maine stay married longer?\n\n\nCode\nplot.new()\nplot.window(xlim=c(1999, 2009), ylim=c(7,9))\nlines(log(maine_divorce_rate*1000)~year, data=vigen_csv)\nlines(log(science_spending/10)~year, data=vigen_csv, lty=2)\naxis(1)\naxis(2)\nlegend('topright', lty=c(1,2), legend=c(\n    'log(maine_divorce_rate*1000)',\n    'log(science_spending/10)'))\n\n\n\n\n\n\n\n\n\nFor more intuition on spurious correlations, try http://shiny.calpoly.sh/Corr_Reg_Game/ The same principles apply to more sophisticated methods.",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Data Scientism</span>"
    ]
  },
  {
    "objectID": "03_05_DataScientism.html#spurious-causal-impacts",
    "href": "03_05_DataScientism.html#spurious-causal-impacts",
    "title": "21  Data Scientism",
    "section": "21.3 Spurious Causal Impacts",
    "text": "21.3 Spurious Causal Impacts\nIn practice, it is hard to find “good” natural experiments. For example, suppose we asked “what is the effect of wages on police demanded?” and examined a policy which lowered the educational requirements from 4 years to 2 to become an officer. This increases the labour supply, but it also affects the demand curve through “general equilibrium”: as some of the new officers were potentially criminals. With fewer criminals, the demand for likely police shifts down.\nIn practice, it is also surprisingly easy to find “bad” natural experiments. Paradoxically, natural experiments are something you are supposed to find but never search for. As you search for good instruments, for example, sometimes random noise will appear like a good instrument (Spurious instruments). Worse, if you search for a good instrument for too long, you can also be led astray from important questions.\n\nExample: Vigen IV’s.\nWe now run IV regressions for different variable combinations in the dataset of spurious relationships\n\n\nCode\nknames &lt;- names(vigen_csv)[2:11] # First 10 Variables\n#knames &lt;- names(vigen_csv)[-1] # Try All Variables\np &lt;- 1\nii &lt;- 1\nivreg_list &lt;- vector(\"list\", factorial(length(knames))/factorial(length(knames)-3))\n\n# Choose 3 variable\nfor( k1 in knames){\nfor( k2 in setdiff(knames,k1)){\nfor( k3 in setdiff(knames,c(k1,k2)) ){   \n    X1 &lt;- vigen_csv[,k1]\n    X2 &lt;- vigen_csv[,k2]\n    X3 &lt;- vigen_csv[,k3]\n    # Merge and `Analyze'        \n    dat_i &lt;- na.omit(data.frame(X1,X2,X3))\n    ivreg_i &lt;- feols(X1~1|X2~X3, data=dat_i)\n    ivreg_list[[ii]] &lt;- list(ivreg_i, c(k1,k2,k3))\n    ii &lt;- ii+1\n}}}\npvals &lt;- sapply(ivreg_list, function(ivreg_i){ivreg_i[[1]]$coeftable[2,4]})\n\nplot(ecdf(pvals), xlab='p-value', ylab='CDF', font.main=1,\n    main='Frequency IV is Statistically Significant')\nabline(v=c(.01,.05), col=c(2,4))\n\n\n\n\n\n\n\n\n\nCode\n\n# Most Significant Spurious Combinations\npvars &lt;- sapply(ivreg_list, function(ivreg_i){ivreg_i[[2]]})\npdat &lt;- data.frame(t(pvars), pvals)\npdat &lt;- pdat[order(pdat$pvals),]\nhead(pdat)\n##                     X1                 X2            X3        pvals\n## 4     science_spending   hanging_suicides    bed_deaths 3.049883e-08\n## 76    hanging_suicides   science_spending    bed_deaths 3.049883e-08\n## 3     science_spending   hanging_suicides cheese_percap 3.344890e-08\n## 75    hanging_suicides   science_spending cheese_percap 3.344890e-08\n## 485 maine_divorce_rate   margarine_percap cheese_percap 3.997738e-08\n## 557   margarine_percap maine_divorce_rate cheese_percap 3.997738e-08\n\n\n\n\nSimulation Study.\nWe apply the three major credible methods (IV, RDD, DID) to random walks. Each time, we find a result that fits mold and add various extensions that make it appear robust. One could tell a story about how \\(X_{2}\\) affects \\(X_{1}\\) but \\(X_{1}\\) might also affect \\(X_{2}\\), and how they discovered an instrument \\(X_{3}\\) to provide the first causal estimate of \\(X_{2}\\) on \\(X_{1}\\). The analysis looks scientific and the story sounds plausible, so you could probably be convinced if it were not just random noise.\n\n\nCode\nn &lt;- 1000\nn_index &lt;- seq(n)\n\nset.seed(1)\nrandom_walk1 &lt;- cumsum(runif(n,-1,1))\n\nset.seed(2)\nrandom_walk2 &lt;- cumsum(runif(n,-1,1))\n\npar(mfrow=c(1,2))\nplot(random_walk1, pch=16, col=rgb(1,0,0,.25),\n    xlab='Time', ylab='Random Value')\nplot(random_walk2, pch=16, col=rgb(0,0,1,.25),\n    xlab='Time', ylab='Random Value')\n\n\n\n\n\n\n\n\n\nIV. First, find an instrument that satisfy various statistical criterion to provide a causal estimate of \\(X_{2}\\) on \\(X_{1}\\).\n\n\nCode\n# \"Find\" \"valid\" ingredients\nlibrary(fixest)\nrandom_walk3 &lt;- cumsum(runif(n,-1,1))\ndat_i &lt;- data.frame(\n    X1=random_walk1,\n    X2=random_walk2,\n    X3=random_walk3)\nivreg_i &lt;- feols(X1~1|X2~X3, data=dat_i)\nsummary(ivreg_i)\n## TSLS estimation - Dep. Var.: X1\n##                   Endo.    : X2\n##                   Instr.   : X3\n## Second stage: Dep. Var.: X1\n## Observations: 1,000\n## Standard-errors: IID \n##             Estimate Std. Error t value   Pr(&gt;|t|)    \n## (Intercept)  8.53309   1.644285 5.18954 2.5533e-07 ***\n## fit_X2       1.79901   0.472285 3.80916 1.4796e-04 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## RMSE: 6.25733   Adj. R2: 0.032314\n## F-test (1st stage), X2: stat = 10.804, p = 0.001048, on 1 and 998 DoF.\n##             Wu-Hausman: stat = 23.407, p = 1.518e-6, on 1 and 997 DoF.\n\n# After experimenting with different instruments\n# you can find even stronger results!\n\n\nRDD. Second, find a large discrete change in the data that you can associate with a policy. You can use this as an instrument too, also providing a causal estimate of \\(X_{2}\\) on \\(X_{1}\\).\n\n\nCode\n# Let the data take shape\n# (around the large differences before and after)\nn1 &lt;- 290\nwind1 &lt;- c(n1-300,n1+300)\ndat1 &lt;- data.frame(t=n_index, y=random_walk1, d=1*(n_index &gt; n1))\ndat1_sub &lt;- dat1[ n_index&gt;wind1[1] & n_index &lt; wind1[2],]\n\n# Then find your big break\nreg0 &lt;- lm(y~t, data=dat1_sub[dat1_sub$d==0,])\nreg1 &lt;- lm(y~t, data=dat1_sub[dat1_sub$d==1,])\n\n# The evidence should show openly (it's just science)\nplot(random_walk1, pch=16, col=rgb(0,0,1,.25),\n    xlim=wind1, xlab='Time', ylab='Random Value')\nabline(v=n1, lty=2)\nlines(reg0$model$t, reg0$fitted.values, col=1)\nlines(reg1$model$t, reg1$fitted.values, col=1)\n\n\n\n\n\n\n\n\n\n\nCode\n# Dress with some statistics for added credibility\nrdd_sub &lt;- lm(y~d+t+d*t, data=dat1_sub)\nrdd_full &lt;- lm(y~d+t+d*t, data=dat1)\nstargazer::stargazer(rdd_sub, rdd_full, \n    type='html',\n    title='Recipe RDD',\n    header=F,\n    omit=c('Constant'),\n    notes=c('First column uses a dataset around the discontinuity.',\n    'Smaller windows are more causal, and where the effect is bigger.'))\n\n\n\nRecipe RDD\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ny\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nd\n\n\n-13.169***\n\n\n-9.639***\n\n\n\n\n\n\n(0.569)\n\n\n(0.527)\n\n\n\n\n\n\n\n\n\n\n\n\nt\n\n\n0.011***\n\n\n0.011***\n\n\n\n\n\n\n(0.001)\n\n\n(0.002)\n\n\n\n\n\n\n\n\n\n\n\n\nd:t\n\n\n0.009***\n\n\n0.004*\n\n\n\n\n\n\n(0.002)\n\n\n(0.002)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n589\n\n\n1,000\n\n\n\n\nR2\n\n\n0.771\n\n\n0.447\n\n\n\n\nAdjusted R2\n\n\n0.770\n\n\n0.446\n\n\n\n\nResidual Std. Error\n\n\n1.764 (df = 585)\n\n\n3.081 (df = 996)\n\n\n\n\nF Statistic\n\n\n658.281*** (df = 3; 585)\n\n\n268.763*** (df = 3; 996)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\n\nFirst column uses a dataset around the discontinuity.\n\n\n\n\n\n\nSmaller windows are more causal, and where the effect is bigger.\n\n\n\nDID. Third, find a change in the data that you can associate with a policy where the control group has parallel trends. This also provides a causal estimate of \\(X_{2}\\) on \\(X_{1}\\).\n\n\nCode\n# Find a reversal of fortune\n# (A good story always goes well with a nice pre-trend)\nn2 &lt;- 318\nwind2 &lt;- c(n2-20,n2+20)\nplot(random_walk2, pch=16, col=rgb(0,0,1,.5),\n    xlim=wind2, ylim=c(-15,15), xlab='Time', ylab='Random Value')\npoints(random_walk1, pch=16, col=rgb(1,0,0,.5))\nabline(v=n2, lty=2)\n\n\n\n\n\n\n\n\n\n\nCode\n# Knead out any effects that are non-causal (aka correlation)\ndat2A &lt;- data.frame(t=n_index, y=random_walk1, d=1*(n_index &gt; n2), RWid=1)\ndat2B &lt;- data.frame(t=n_index, y=random_walk2, d=0, RWid=2)\ndat2  &lt;- rbind(dat2A, dat2B)\ndat2$RWid &lt;- as.factor(dat2$RWid)\ndat2$tid &lt;- as.factor(dat2$t)\ndat2_sub &lt;- dat2[ dat2$t&gt;wind2[1] & dat2$t &lt; wind2[2],]\n\n# Report the stars for all to enjoy\n# (what about the intercept?)\n# (stable coefficients are the good ones?)\ndid_fe1 &lt;- lm(y~d+tid, data=dat2_sub)\ndid_fe2 &lt;- lm(y~d+RWid, data=dat2_sub)\ndid_fe3 &lt;- lm(y~d*RWid+tid, data=dat2_sub)\nstargazer::stargazer(did_fe1, did_fe2, did_fe3,\n    type='html',\n    title='Recipe DID',\n    header=F,\n    omit=c('tid','RWid', 'Constant'),\n    notes=c(\n     'Fixed effects for time in column 1, for id in column 2, and both in column 3.',\n     'Fixed effects control for most of your concerns.',\n     'Anything else creates a bias in the opposite direction.'))\n\n\n\nRecipe DID\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ny\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n\n\n\n\n\n\nd\n\n\n1.804*\n\n\n1.847***\n\n\n5.851***\n\n\n\n\n\n\n(0.892)\n\n\n(0.652)\n\n\n(0.828)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n78\n\n\n78\n\n\n78\n\n\n\n\nR2\n\n\n0.227\n\n\n0.164\n\n\n0.668\n\n\n\n\nAdjusted R2\n\n\n-0.566\n\n\n0.142\n\n\n0.309\n\n\n\n\nResidual Std. Error\n\n\n2.750 (df = 38)\n\n\n2.035 (df = 75)\n\n\n1.827 (df = 37)\n\n\n\n\nF Statistic\n\n\n0.287 (df = 39; 38)\n\n\n7.379*** (df = 2; 75)\n\n\n1.860** (df = 40; 37)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\n\nFixed effects for time in column 1, for id in column 2, and both in column 3.\n\n\n\n\n\n\nFixed effects control for most of your concerns.\n\n\n\n\n\n\nAnything else creates a bias in the opposite direction.",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Data Scientism</span>"
    ]
  },
  {
    "objectID": "03_10_MiscTopics.html",
    "href": "03_10_MiscTopics.html",
    "title": "22  Misc. Multivariate Topics",
    "section": "",
    "text": "Filtering.\nIn some cases, we may want to smooth time series data instead of predict into the future. We can distinguish two types of smoothing, incorporating future observations or not. When only weighting two other observations, the differences can be expressed as trying to estimate the average with different available data:\n\nFiltering: \\(\\mathbb{E}[Y_{t} | X_{t-1}, X_{t-2}]\\)\nSmoothing: \\(\\mathbb{E}[Y_{t} | X_{t-1}, X_{t+1}]\\)\n\nOne example of filtering is Exponential Filtering (sometimes confusingly referred to as “Exponential Smoothing”) which weights only previous observations using an exponential kernel.\n\n\nCode\n##################\n# Time series data\n##################\n\nset.seed(1)\n## Underlying Trend\nx0 &lt;- cumsum(rnorm(500,0,1))\n## Observed Datapoints\nx &lt;- x0 + runif(length(x0),-10,10)\ndat &lt;- data.frame(t=seq(x), x0=x0, x=x)\n\n## Asymmetric Kernel\n#bw &lt;- c(2/3,1/3)\n#s1 &lt;- filter(x, bw/sum(bw), sides=1)\n\n## Symmetric Kernel\n#bw &lt;- c(1/6,2/3,1/6)\n#s2 &lt;- filter(x,  bw/sum(bw), sides=2)\n\n\nThere are several cross-validation procedures for filtering time series data . One is called time series cross-validation (TSCV), which is useful for temporally dependent data .\n\n\nCode\n## Plot Simulated Data\nx &lt;- dat$x\nx0 &lt;- dat$x0\npar(fig = c(0,1,0,1), new=F)\nplot(x, pch=16, col=grey(0,.25))\nlines(x0, col=1, lty=1, lwd=2)\n\n## Work with differenced data?\n#n       &lt;- length(Yt)\n#plot(Yt[1:(n-1)], Yt[2:n],\n#    xlab='d Y (t-1)', ylab='d Y (t)', \n#    col=grey(0,.5), pch=16)\n#Yt &lt;- diff(Yt)\n\n## TSCV One Sided Moving Average\nfilter_bws &lt;- seq(1,20,by=1)\nfilter_mape_bws &lt;- sapply(filter_bws, function(h){\n    bw &lt;- c(0,rep(1/h,h)) ## Leave current one out\n    s2 &lt;- filter(x, bw, sides=1)\n    pe &lt;- s2 - x\n    mape &lt;- mean( abs(pe)^2, na.rm=T)\n})\nfilter_mape_star &lt;- filter_mape_bws[which.min(filter_mape_bws)]\nfilter_h_star &lt;- filter_bws[which.min(filter_mape_bws)]\nfilter_tscv &lt;- filter(x,  c(rep(1/filter_h_star,filter_h_star)), sides=1)\n# Plot Optimization Results\n#par(fig = c(0.07, 0.35, 0.07, 0.35), new=T) \n#plot(filter_bws, filter_mape_bws, type='o', ylab='mape', pch=16)\n#points(filter_h_star, filter_mape_star, pch=19, col=2, cex=1.5)\n\n## TSCV for LLLS\nlibrary(np)\nllls_bws &lt;- seq(8,28,by=1)\nllls_burnin &lt;- 10\nllls_mape_bws &lt;- sapply(llls_bws, function(h){ # cat(h,'\\n')\n    pe &lt;- sapply(llls_burnin:nrow(dat), function(t_end){\n        dat_t &lt;- dat[dat$t&lt;t_end, ]\n        reg &lt;- npreg(x~t, data=dat_t, bws=h,\n            ckertype='epanechnikov',\n            bandwidth.compute=F, regtype='ll')\n        edat &lt;- dat[dat$t==t_end,]\n        pred &lt;- predict(reg, newdata=edat)\n        pe &lt;- edat$x - pred\n        return(pe)    \n    })\n    mape &lt;- mean( abs(pe)^2, na.rm=T)\n})\nllls_mape_star &lt;- llls_mape_bws[which.min(llls_mape_bws)]\nllls_h_star &lt;- llls_bws[which.min(llls_mape_bws)]\n#llls_tscv &lt;- predict( npreg(x~t, data=dat, bws=llls_h_star,\n#    bandwidth.compute=F, regtype='ll', ckertype='epanechnikov'))\nllls_tscv &lt;- sapply(llls_burnin:nrow(dat), function(t_end, h=llls_h_star){\n    dat_t &lt;- dat[dat$t&lt;t_end, ]\n    reg &lt;- npreg(x~t, data=dat_t, bws=h,\n        ckertype='epanechnikov',\n        bandwidth.compute=F, regtype='ll')\n    edat &lt;- dat[dat$t==t_end,]\n    pred &lt;- predict(reg, newdata=edat)\n    return(pred)    \n})\n\n## Compare Fits Qualitatively\nlines(filter_tscv, col=2, lty=1, lwd=1)\nlines(llls_burnin:nrow(dat), llls_tscv, col=4, lty=1, lwd=1)\nlegend('topleft', lty=1, col=c(1,2,4), bty='n',\n    c('Underlying Trend', 'MA-1sided + TSCV', 'LLLS-1sided + TSCV'))\n\n## Compare Fits Quantitatively\ncbind(\n    bandwidth=c(LLLS=llls_h_star, MA=filter_h_star),\n    mape=round(c(LLLS=llls_mape_star, MA=filter_mape_star),2) )\n\n## See also https://cran.r-project.org/web/packages/smoots/smoots.pdf\n#https://otexts.com/fpp3/tscv.html\n#https://robjhyndman.com/hyndsight/tscvexample/\n\n\n\n\nCross Validation.\nPerhaps the most common approach to selecting a bandwidth is to minimize error. Leave-one-out Cross-validation minimizes the average “leave-one-out” mean square prediction errors: \\[\\begin{eqnarray}\n\\min_{\\mathbf{H}} \\quad \\frac{1}{n} \\sum_{i=1}^{n} \\left[ \\hat{Y}_{i} - \\hat{y_{[i]}}(\\mathbf{X},\\mathbf{H}) \\right]^2,\n% \\hat{Y_{[i]}}(\\mathbf{X},\\mathbf{H}) &=& \\sum_{j\\neq i} k(\\mathbf{X}_{j},\\mathbf{X}_{i},\\mathbf{H}) \\left[ \\hat{\\alpha}(\\mathbf{X}_{j}) + \\hat{\\beta}(\\mathbf{X}_{j}) \\mathbf{X}_{i} \\right]\n\\end{eqnarray}\\] where \\(\\hat{y}_{[i]}(\\mathbf{X},\\mathbf{H})\\) is the model predicted value at \\(\\mathbf{X}_{i}\\) based on a dataset that excludes \\(\\mathbf{X}_{i}\\), and \\(\\mathbf{H}\\) is matrix of bandwidths. With a weighted least squares regression on three explanatory variables, for example, the matrix is \\[\\begin{eqnarray}\n\\mathbf{H}=\\begin{pmatrix}\nh_{1} & 0 & 0  \\\\\n0    & h_{2} & 0 \\\\\n0    & 0 & h_{3} \\\\  \n\\end{pmatrix},\n\\end{eqnarray}\\] where each \\(h_{k}\\) is the bandwidth for variable \\(X_{k}\\).\nThere are many types of cross-validation . For example, one extension is k-fold cross validation, which splits \\(N\\) datapoints into \\(k=1...K\\) groups, each sized \\(B\\), and predicts values for the left-out group. adjusts for the degrees of freedom, whereas the function in R uses by default. You can refer to extensions on a case by case basis.",
    "crumbs": [
      "Multivariate Data",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Misc. Multivariate Topics</span>"
    ]
  }
]