<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 8 OLS (multiple linear regression) | index</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.38 and GitBook 2.6.7" />

  <meta property="og:title" content=" 8 OLS (multiple linear regression) | index" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 8 OLS (multiple linear regression) | index" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="simple-linear-regression.html"/>
<link rel="next" href="endogeneity-issues.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<script src="libs/d3-3.5.6/d3.min.js"></script>
<link href="libs/profvis-0.3.6.9000/profvis.css" rel="stylesheet" />
<script src="libs/profvis-0.3.6.9000/profvis.js"></script>
<script src="libs/profvis-0.3.6.9000/scroll.js"></script>
<link href="libs/highlight-6.2.0/textmate.css" rel="stylesheet" />
<script src="libs/highlight-6.2.0/highlight.js"></script>
<script src="libs/profvis-binding-0.3.8/profvis.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Rbooks</a></li>
<li class="toc-logo"><a href="./"><img src="Figures_Manual/Logo.png" width="140" height="100"></a></li>        

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="part"><span><b>I Programming in R</b></span></li>
<li class="chapter" data-level="2" data-path="first-steps.html"><a href="first-steps.html"><i class="fa fa-check"></i><b>2</b> First Steps</a>
<ul>
<li class="chapter" data-level="2.1" data-path="first-steps.html"><a href="first-steps.html#why-r"><i class="fa fa-check"></i><b>2.1</b> Why R</a></li>
<li class="chapter" data-level="2.2" data-path="first-steps.html"><a href="first-steps.html#install-r"><i class="fa fa-check"></i><b>2.2</b> Install R</a></li>
<li class="chapter" data-level="2.3" data-path="first-steps.html"><a href="first-steps.html#interfacing-with-r"><i class="fa fa-check"></i><b>2.3</b> Interfacing with R</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mathematics.html"><a href="mathematics.html"><i class="fa fa-check"></i><b>3</b> Mathematics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="mathematics.html"><a href="mathematics.html#scalars"><i class="fa fa-check"></i><b>3.1</b> Scalars</a></li>
<li class="chapter" data-level="3.2" data-path="mathematics.html"><a href="mathematics.html#vectors"><i class="fa fa-check"></i><b>3.2</b> Vectors</a></li>
<li class="chapter" data-level="3.3" data-path="mathematics.html"><a href="mathematics.html#functions"><i class="fa fa-check"></i><b>3.3</b> Functions</a></li>
<li class="chapter" data-level="3.4" data-path="mathematics.html"><a href="mathematics.html#matrices"><i class="fa fa-check"></i><b>3.4</b> Matrices</a></li>
<li class="chapter" data-level="3.5" data-path="mathematics.html"><a href="mathematics.html#arrays"><i class="fa fa-check"></i><b>3.5</b> Arrays</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>4</b> Statistics</a>
<ul>
<li class="chapter" data-level="4.1" data-path="statistics.html"><a href="statistics.html#data-types"><i class="fa fa-check"></i><b>4.1</b> Data Types</a></li>
<li class="chapter" data-level="4.2" data-path="statistics.html"><a href="statistics.html#random-variables"><i class="fa fa-check"></i><b>4.2</b> Random Variables</a></li>
<li class="chapter" data-level="4.3" data-path="statistics.html"><a href="statistics.html#functions-of-data"><i class="fa fa-check"></i><b>4.3</b> Functions of Data</a></li>
<li class="chapter" data-level="4.4" data-path="statistics.html"><a href="statistics.html#value-of-more-data"><i class="fa fa-check"></i><b>4.4</b> Value of More Data</a></li>
<li class="chapter" data-level="4.5" data-path="statistics.html"><a href="statistics.html#further-reading"><i class="fa fa-check"></i><b>4.5</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-analysis.html"><a href="data-analysis.html"><i class="fa fa-check"></i><b>5</b> Data Analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="data-analysis.html"><a href="data-analysis.html#cleaning-data"><i class="fa fa-check"></i><b>5.1</b> Cleaning Data</a></li>
<li class="chapter" data-level="5.2" data-path="data-analysis.html"><a href="data-analysis.html#static-plots"><i class="fa fa-check"></i><b>5.2</b> Static Plots</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="data-analysis.html"><a href="data-analysis.html#histograms"><i class="fa fa-check"></i><b>5.2.1</b> Histograms</a></li>
<li class="chapter" data-level="5.2.2" data-path="data-analysis.html"><a href="data-analysis.html#boxplots"><i class="fa fa-check"></i><b>5.2.2</b> Boxplots</a></li>
<li class="chapter" data-level="5.2.3" data-path="data-analysis.html"><a href="data-analysis.html#scatterplots"><i class="fa fa-check"></i><b>5.2.3</b> Scatterplots</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="data-analysis.html"><a href="data-analysis.html#interactive-plots"><i class="fa fa-check"></i><b>5.3</b> Interactive Plots</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="beyond-basics.html"><a href="beyond-basics.html"><i class="fa fa-check"></i><b>6</b> Beyond Basics</a>
<ul>
<li class="chapter" data-level="6.1" data-path="beyond-basics.html"><a href="beyond-basics.html#the-r-ecosystem"><i class="fa fa-check"></i><b>6.1</b> The R Ecosystem</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="beyond-basics.html"><a href="beyond-basics.html#packages"><i class="fa fa-check"></i><b>6.1.1</b> Packages</a></li>
<li class="chapter" data-level="6.1.2" data-path="beyond-basics.html"><a href="beyond-basics.html#task-views"><i class="fa fa-check"></i><b>6.1.2</b> Task Views</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="beyond-basics.html"><a href="beyond-basics.html#introductions-to-r"><i class="fa fa-check"></i><b>6.2</b> Introductions to R</a></li>
<li class="chapter" data-level="6.3" data-path="beyond-basics.html"><a href="beyond-basics.html#custom-figures"><i class="fa fa-check"></i><b>6.3</b> Custom Figures</a></li>
</ul></li>
<li class="part"><span><b>II Linear Regression in R</b></span></li>
<li class="chapter" data-level="7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>7</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#variability-estimates"><i class="fa fa-check"></i><b>7.1</b> Variability Estimates</a></li>
<li class="chapter" data-level="7.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#hypothesis-tests"><i class="fa fa-check"></i><b>7.2</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="7.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#prediction-intervals"><i class="fa fa-check"></i><b>7.3</b> Prediction Intervals</a></li>
<li class="chapter" data-level="7.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#locally-linear"><i class="fa fa-check"></i><b>7.4</b> Locally Linear</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ols-multiple-linear-regression.html"><a href="ols-multiple-linear-regression.html"><i class="fa fa-check"></i><b>8</b> OLS (multiple linear regression)</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ols-multiple-linear-regression.html"><a href="ols-multiple-linear-regression.html#variability-estimates-and-hypothesis-tests"><i class="fa fa-check"></i><b>8.1</b> Variability Estimates and Hypothesis Tests</a></li>
<li class="chapter" data-level="8.2" data-path="ols-multiple-linear-regression.html"><a href="ols-multiple-linear-regression.html#factor-variables"><i class="fa fa-check"></i><b>8.2</b> Factor Variables</a></li>
<li class="chapter" data-level="8.3" data-path="ols-multiple-linear-regression.html"><a href="ols-multiple-linear-regression.html#coefficient-interpretation"><i class="fa fa-check"></i><b>8.3</b> Coefficient Interpretation</a></li>
<li class="chapter" data-level="8.4" data-path="ols-multiple-linear-regression.html"><a href="ols-multiple-linear-regression.html#diagnostics"><i class="fa fa-check"></i><b>8.4</b> Diagnostics</a></li>
<li class="chapter" data-level="8.5" data-path="ols-multiple-linear-regression.html"><a href="ols-multiple-linear-regression.html#linear-in-parameters"><i class="fa fa-check"></i><b>8.5</b> Linear in Parameters</a></li>
<li class="chapter" data-level="8.6" data-path="ols-multiple-linear-regression.html"><a href="ols-multiple-linear-regression.html#more-literature"><i class="fa fa-check"></i><b>8.6</b> More Literature</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="endogeneity-issues.html"><a href="endogeneity-issues.html"><i class="fa fa-check"></i><b>9</b> Endogeneity Issues</a>
<ul>
<li class="chapter" data-level="9.1" data-path="endogeneity-issues.html"><a href="endogeneity-issues.html#two-stage-least-squares-2sls"><i class="fa fa-check"></i><b>9.1</b> Two Stage Least Squares (2SLS)</a></li>
<li class="chapter" data-level="9.2" data-path="endogeneity-issues.html"><a href="endogeneity-issues.html#regression-discontinuitieskink-rdrk"><i class="fa fa-check"></i><b>9.2</b> Regression Discontinuities/Kink (RD/RK)</a></li>
<li class="chapter" data-level="9.3" data-path="endogeneity-issues.html"><a href="endogeneity-issues.html#difference-in-differences-did"><i class="fa fa-check"></i><b>9.3</b> Difference in Differences (DID)</a></li>
<li class="chapter" data-level="9.4" data-path="endogeneity-issues.html"><a href="endogeneity-issues.html#more-literature-1"><i class="fa fa-check"></i><b>9.4</b> More Literature</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="data-scientism.html"><a href="data-scientism.html"><i class="fa fa-check"></i><b>10</b> Data Scientism</a>
<ul>
<li class="chapter" data-level="10.1" data-path="data-scientism.html"><a href="data-scientism.html#data-errors"><i class="fa fa-check"></i><b>10.1</b> Data Errors</a></li>
<li class="chapter" data-level="10.2" data-path="data-scientism.html"><a href="data-scientism.html#p-hacking"><i class="fa fa-check"></i><b>10.2</b> P-Hacking</a></li>
<li class="chapter" data-level="10.3" data-path="data-scientism.html"><a href="data-scientism.html#spurious-regression"><i class="fa fa-check"></i><b>10.3</b> Spurious Regression</a></li>
<li class="chapter" data-level="10.4" data-path="data-scientism.html"><a href="data-scientism.html#spurious-causal-impacts"><i class="fa fa-check"></i><b>10.4</b> Spurious Causal Impacts</a></li>
</ul></li>
<li class="part"><span><b>III Reproducible Research in R</b></span></li>
<li class="chapter" data-level="11" data-path="why.html"><a href="why.html"><i class="fa fa-check"></i><b>11</b> Why?</a>
<ul>
<li class="chapter" data-level="11.1" data-path="why.html"><a href="why.html#an-example-workflow"><i class="fa fa-check"></i><b>11.1</b> An example workflow</a></li>
<li class="chapter" data-level="11.2" data-path="why.html"><a href="why.html#an-alternative-workflow"><i class="fa fa-check"></i><b>11.2</b> An alternative workflow</a></li>
<li class="chapter" data-level="11.3" data-path="why.html"><a href="why.html#replicable"><i class="fa fa-check"></i><b>11.3</b> Replicable</a></li>
<li class="chapter" data-level="11.4" data-path="why.html"><a href="why.html#r-and-r-markdown"><i class="fa fa-check"></i><b>11.4</b> R and R-Markdown</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="small-projects.html"><a href="small-projects.html"><i class="fa fa-check"></i><b>12</b> Small Projects</a>
<ul>
<li class="chapter" data-level="12.1" data-path="small-projects.html"><a href="small-projects.html#code-chunks"><i class="fa fa-check"></i><b>12.1</b> Code Chunks</a></li>
<li class="chapter" data-level="12.2" data-path="small-projects.html"><a href="small-projects.html#reports"><i class="fa fa-check"></i><b>12.2</b> Reports</a></li>
<li class="chapter" data-level="12.3" data-path="small-projects.html"><a href="small-projects.html#posters-and-slides"><i class="fa fa-check"></i><b>12.3</b> Posters and Slides</a></li>
<li class="chapter" data-level="12.4" data-path="small-projects.html"><a href="small-projects.html#more-literature-2"><i class="fa fa-check"></i><b>12.4</b> More Literature</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="large-projects.html"><a href="large-projects.html"><i class="fa fa-check"></i><b>13</b> Large Projects</a>
<ul>
<li class="chapter" data-level="13.1" data-path="large-projects.html"><a href="large-projects.html#loggingsinking"><i class="fa fa-check"></i><b>13.1</b> Logging/Sinking</a></li>
<li class="chapter" data-level="13.2" data-path="large-projects.html"><a href="large-projects.html#class-projects"><i class="fa fa-check"></i><b>13.2</b> Class Projects</a></li>
<li class="chapter" data-level="13.3" data-path="large-projects.html"><a href="large-projects.html#debugging"><i class="fa fa-check"></i><b>13.3</b> Debugging</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="large-projects.html"><a href="large-projects.html#tracing"><i class="fa fa-check"></i><b>13.3.1</b> Tracing</a></li>
<li class="chapter" data-level="13.3.2" data-path="large-projects.html"><a href="large-projects.html#isolating"><i class="fa fa-check"></i><b>13.3.2</b> Isolating</a></li>
<li class="chapter" data-level="13.3.3" data-path="large-projects.html"><a href="large-projects.html#handling"><i class="fa fa-check"></i><b>13.3.3</b> Handling</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="large-projects.html"><a href="large-projects.html#optimizing"><i class="fa fa-check"></i><b>13.4</b> Optimizing</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="large-projects.html"><a href="large-projects.html#benchmarking"><i class="fa fa-check"></i><b>13.4.1</b> Benchmarking</a></li>
<li class="chapter" data-level="13.4.2" data-path="large-projects.html"><a href="large-projects.html#speed-ups"><i class="fa fa-check"></i><b>13.4.2</b> Speed-Ups</a></li>
<li class="chapter" data-level="13.4.3" data-path="large-projects.html"><a href="large-projects.html#bottlenecks"><i class="fa fa-check"></i><b>13.4.3</b> Bottlenecks</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="large-projects.html"><a href="large-projects.html#more-literature-3"><i class="fa fa-check"></i><b>13.5</b> More Literature</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="web-applications.html"><a href="web-applications.html"><i class="fa fa-check"></i><b>14</b> Web Applications</a>
<ul>
<li class="chapter" data-level="14.1" data-path="web-applications.html"><a href="web-applications.html#more-literature-4"><i class="fa fa-check"></i><b>14.1</b> More Literature</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="software.html"><a href="software.html"><i class="fa fa-check"></i><b>15</b> Software</a>
<ul>
<li class="chapter" data-level="15.1" data-path="software.html"><a href="software.html#latest-versions"><i class="fa fa-check"></i><b>15.1</b> Latest versions</a></li>
<li class="chapter" data-level="15.2" data-path="software.html"><a href="software.html#general-workflow"><i class="fa fa-check"></i><b>15.2</b> General Workflow</a></li>
<li class="chapter" data-level="15.3" data-path="software.html"><a href="software.html#sweave"><i class="fa fa-check"></i><b>15.3</b> Sweave</a></li>
<li class="chapter" data-level="15.4" data-path="software.html"><a href="software.html#stata"><i class="fa fa-check"></i><b>15.4</b> Stata</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://sites.google.com/view/jordan-adamson" target="blank">Jordan Adamson</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ols-multiple-linear-regression" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number"> 8</span> OLS (multiple linear regression)<a href="ols-multiple-linear-regression.html#ols-multiple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<hr />
<p>Model and objective
<span class="math display">\[
y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_kx_{ik}+\epsilon_i = X_{i}\beta +\epsilon_i \\
min_{\beta} \sum_{i=1}^{n} (\epsilon_i)^2
\]</span>
Can also be written in matrix form
<span class="math display">\[
y=\textbf{X}\beta+\epsilon\\
min_{\beta} (\epsilon&#39; \epsilon)
\]</span></p>
<p>Point Estimates
<span class="math display">\[
\hat{\beta}=(\textbf{X}&#39;\textbf{X})^{-1}\textbf{X}&#39;y
\]</span></p>
<p><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>Before fitting the model, summarize your data (as in Part I)</p>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb257-1"><a href="ols-multiple-linear-regression.html#cb257-1" tabindex="-1"></a><span class="do">## Inspect Dataset on police arrests for the USA in 1973</span></span>
<span id="cb257-2"><a href="ols-multiple-linear-regression.html#cb257-2" tabindex="-1"></a><span class="fu">head</span>(USArrests)</span></code></pre></div>
<pre><code>##            Murder Assault UrbanPop Rape         ID
## Alabama      13.2     236       58 21.2    Alabama
## Alaska       10.0     263       48 44.5     Alaska
## Arizona       8.1     294       80 31.0    Arizona
## Arkansas      8.8     190       50 19.5   Arkansas
## California    9.0     276       91 40.6 California
## Colorado      7.9     204       78 38.7   Colorado</code></pre>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb259-1"><a href="ols-multiple-linear-regression.html#cb259-1" tabindex="-1"></a><span class="fu">library</span>(psych)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;psych&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:car&#39;:
## 
##     logit</code></pre>
<pre><code>## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     %+%, alpha</code></pre>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb263-1"><a href="ols-multiple-linear-regression.html#cb263-1" tabindex="-1"></a><span class="fu">pairs.panels</span>( USArrests[,<span class="fu">c</span>(<span class="st">&#39;Murder&#39;</span>,<span class="st">&#39;Assault&#39;</span>,<span class="st">&#39;UrbanPop&#39;</span>)],</span>
<span id="cb263-2"><a href="ols-multiple-linear-regression.html#cb263-2" tabindex="-1"></a>    <span class="at">hist.col=</span><span class="fu">grey</span>(<span class="dv">0</span>,.<span class="dv">25</span>), <span class="at">breaks=</span><span class="dv">30</span>, <span class="at">density=</span>F, <span class="do">## Diagonal</span></span>
<span id="cb263-3"><a href="ols-multiple-linear-regression.html#cb263-3" tabindex="-1"></a>    <span class="at">ellipses=</span>F, <span class="at">rug=</span>F, <span class="at">smoother=</span>F, <span class="at">pch=</span><span class="dv">16</span>, <span class="at">col=</span><span class="fu">grey</span>(<span class="dv">0</span>,.<span class="dv">5</span>) <span class="do">## Lower Triangle</span></span>
<span id="cb263-4"><a href="ols-multiple-linear-regression.html#cb263-4" tabindex="-1"></a>    )</span></code></pre></div>
<p><img src="03-ROLS_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Now we fit the model to the data</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="ols-multiple-linear-regression.html#cb264-1" tabindex="-1"></a><span class="do">## Manually Compute</span></span>
<span id="cb264-2"><a href="ols-multiple-linear-regression.html#cb264-2" tabindex="-1"></a>Y <span class="ot">&lt;-</span> USArrests[,<span class="st">&#39;Murder&#39;</span>]</span>
<span id="cb264-3"><a href="ols-multiple-linear-regression.html#cb264-3" tabindex="-1"></a>X <span class="ot">&lt;-</span> USArrests[,<span class="fu">c</span>(<span class="st">&#39;Assault&#39;</span>,<span class="st">&#39;UrbanPop&#39;</span>)]</span>
<span id="cb264-4"><a href="ols-multiple-linear-regression.html#cb264-4" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">cbind</span>(<span class="dv">1</span>,X))</span>
<span id="cb264-5"><a href="ols-multiple-linear-regression.html#cb264-5" tabindex="-1"></a></span>
<span id="cb264-6"><a href="ols-multiple-linear-regression.html#cb264-6" tabindex="-1"></a>XtXi <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)</span>
<span id="cb264-7"><a href="ols-multiple-linear-regression.html#cb264-7" tabindex="-1"></a>Bhat <span class="ot">&lt;-</span> XtXi <span class="sc">%*%</span> (<span class="fu">t</span>(X)<span class="sc">%*%</span>Y)</span>
<span id="cb264-8"><a href="ols-multiple-linear-regression.html#cb264-8" tabindex="-1"></a><span class="fu">c</span>(Bhat)</span></code></pre></div>
<pre><code>## [1]  3.20715340  0.04390995 -0.04451047</code></pre>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb266-1"><a href="ols-multiple-linear-regression.html#cb266-1" tabindex="-1"></a><span class="do">## Check</span></span>
<span id="cb266-2"><a href="ols-multiple-linear-regression.html#cb266-2" tabindex="-1"></a>reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(Murder<span class="sc">~</span>Assault<span class="sc">+</span>UrbanPop, <span class="at">data=</span>USArrests)</span>
<span id="cb266-3"><a href="ols-multiple-linear-regression.html#cb266-3" tabindex="-1"></a><span class="fu">coef</span>(reg)</span></code></pre></div>
<pre><code>## (Intercept)     Assault    UrbanPop 
##  3.20715340  0.04390995 -0.04451047</code></pre>
<p>To measure the ``Goodness of fit’’ of the model, we can again compute sums of squared srrors. Adding random data may sometimes improve the fit, however, so we adjust the <span class="math inline">\(R^2\)</span> by the number of covariates <span class="math inline">\(K\)</span>.
<span class="math display">\[
R^2 = \frac{ESS}{TSS}=1-\frac{RSS}{TSS}\\
R^2_{\text{adj.}} = 1-\frac{n-1}{n-K}(1-R^2)
\]</span></p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="ols-multiple-linear-regression.html#cb268-1" tabindex="-1"></a>ksims <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">30</span></span>
<span id="cb268-2"><a href="ols-multiple-linear-regression.html#cb268-2" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> ksims){ </span>
<span id="cb268-3"><a href="ols-multiple-linear-regression.html#cb268-3" tabindex="-1"></a>    USArrests[,<span class="fu">paste0</span>(<span class="st">&#39;R&#39;</span>,k)] <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="fu">nrow</span>(USArrests),<span class="dv">0</span>,<span class="dv">20</span>)</span>
<span id="cb268-4"><a href="ols-multiple-linear-regression.html#cb268-4" tabindex="-1"></a>}</span>
<span id="cb268-5"><a href="ols-multiple-linear-regression.html#cb268-5" tabindex="-1"></a>reg_sim <span class="ot">&lt;-</span> <span class="fu">lapply</span>(ksims, <span class="cf">function</span>(k){</span>
<span id="cb268-6"><a href="ols-multiple-linear-regression.html#cb268-6" tabindex="-1"></a>    rvars <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&#39;Assault&#39;</span>,<span class="st">&#39;UrbanPop&#39;</span>, <span class="fu">paste0</span>(<span class="st">&#39;R&#39;</span>,<span class="dv">1</span><span class="sc">:</span>k))</span>
<span id="cb268-7"><a href="ols-multiple-linear-regression.html#cb268-7" tabindex="-1"></a>    rvars2 <span class="ot">&lt;-</span> <span class="fu">paste0</span>(rvars, <span class="at">collapse=</span><span class="st">&#39;+&#39;</span>)</span>
<span id="cb268-8"><a href="ols-multiple-linear-regression.html#cb268-8" tabindex="-1"></a>    reg_k <span class="ot">&lt;-</span> <span class="fu">lm</span>( <span class="fu">paste0</span>(<span class="st">&#39;Murder~&#39;</span>,rvars2), <span class="at">data=</span>USArrests)</span>
<span id="cb268-9"><a href="ols-multiple-linear-regression.html#cb268-9" tabindex="-1"></a>})</span>
<span id="cb268-10"><a href="ols-multiple-linear-regression.html#cb268-10" tabindex="-1"></a>R2_sim <span class="ot">&lt;-</span> <span class="fu">sapply</span>(reg_sim, <span class="cf">function</span>(reg_k){  <span class="fu">summary</span>(reg_k)<span class="sc">$</span>r.squared })</span>
<span id="cb268-11"><a href="ols-multiple-linear-regression.html#cb268-11" tabindex="-1"></a>R2adj_sim <span class="ot">&lt;-</span> <span class="fu">sapply</span>(reg_sim, <span class="cf">function</span>(reg_k){  <span class="fu">summary</span>(reg_k)<span class="sc">$</span>adj.r.squared })</span>
<span id="cb268-12"><a href="ols-multiple-linear-regression.html#cb268-12" tabindex="-1"></a></span>
<span id="cb268-13"><a href="ols-multiple-linear-regression.html#cb268-13" tabindex="-1"></a><span class="fu">plot.new</span>()</span>
<span id="cb268-14"><a href="ols-multiple-linear-regression.html#cb268-14" tabindex="-1"></a><span class="fu">plot.window</span>(<span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">30</span>), <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</span>
<span id="cb268-15"><a href="ols-multiple-linear-regression.html#cb268-15" tabindex="-1"></a><span class="fu">points</span>(ksims, R2_sim)</span>
<span id="cb268-16"><a href="ols-multiple-linear-regression.html#cb268-16" tabindex="-1"></a><span class="fu">points</span>(ksims, R2adj_sim, <span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb268-17"><a href="ols-multiple-linear-regression.html#cb268-17" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>)</span>
<span id="cb268-18"><a href="ols-multiple-linear-regression.html#cb268-18" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">2</span>)</span>
<span id="cb268-19"><a href="ols-multiple-linear-regression.html#cb268-19" tabindex="-1"></a><span class="fu">mtext</span>(<span class="fu">expression</span>(R<span class="sc">^</span><span class="dv">2</span>),<span class="dv">2</span>, <span class="at">line=</span><span class="dv">3</span>)</span>
<span id="cb268-20"><a href="ols-multiple-linear-regression.html#cb268-20" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&#39;Additional Random Covariates&#39;</span>, <span class="dv">1</span>, <span class="at">line=</span><span class="dv">3</span>)</span>
<span id="cb268-21"><a href="ols-multiple-linear-regression.html#cb268-21" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&#39;topleft&#39;</span>, <span class="at">horiz=</span>T,</span>
<span id="cb268-22"><a href="ols-multiple-linear-regression.html#cb268-22" tabindex="-1"></a>    <span class="at">legend=</span><span class="fu">c</span>(<span class="st">&#39;Undjusted&#39;</span>, <span class="st">&#39;Adjusted&#39;</span>), <span class="at">pch=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">16</span>))</span></code></pre></div>
<p><img src="03-ROLS_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<div id="variability-estimates-and-hypothesis-tests" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Variability Estimates and Hypothesis Tests<a href="ols-multiple-linear-regression.html#variability-estimates-and-hypothesis-tests" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To estimate the variability of our estimates, we can use the same <em>data-driven</em> methods introduced with simple OLS.</p>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb269-1"><a href="ols-multiple-linear-regression.html#cb269-1" tabindex="-1"></a><span class="do">## Bootstrap SE&#39;s</span></span>
<span id="cb269-2"><a href="ols-multiple-linear-regression.html#cb269-2" tabindex="-1"></a>boots <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">399</span></span>
<span id="cb269-3"><a href="ols-multiple-linear-regression.html#cb269-3" tabindex="-1"></a>boot_regs <span class="ot">&lt;-</span> <span class="fu">lapply</span>(boots, <span class="cf">function</span>(b){</span>
<span id="cb269-4"><a href="ols-multiple-linear-regression.html#cb269-4" tabindex="-1"></a>    b_id <span class="ot">&lt;-</span> <span class="fu">sample</span>( <span class="fu">nrow</span>(USArrests), <span class="at">replace=</span>T)</span>
<span id="cb269-5"><a href="ols-multiple-linear-regression.html#cb269-5" tabindex="-1"></a>    xy_b <span class="ot">&lt;-</span> USArrests[b_id,]</span>
<span id="cb269-6"><a href="ols-multiple-linear-regression.html#cb269-6" tabindex="-1"></a>    reg_b <span class="ot">&lt;-</span> <span class="fu">lm</span>(Murder<span class="sc">~</span>Assault<span class="sc">+</span>UrbanPop, <span class="at">dat=</span>xy_b)</span>
<span id="cb269-7"><a href="ols-multiple-linear-regression.html#cb269-7" tabindex="-1"></a>})</span>
<span id="cb269-8"><a href="ols-multiple-linear-regression.html#cb269-8" tabindex="-1"></a>boot_coefs <span class="ot">&lt;-</span> <span class="fu">sapply</span>(boot_regs, coef)</span>
<span id="cb269-9"><a href="ols-multiple-linear-regression.html#cb269-9" tabindex="-1"></a>boot_mean <span class="ot">&lt;-</span> <span class="fu">apply</span>(boot_coefs,<span class="dv">1</span>, mean)</span>
<span id="cb269-10"><a href="ols-multiple-linear-regression.html#cb269-10" tabindex="-1"></a>boot_se <span class="ot">&lt;-</span> <span class="fu">apply</span>(boot_coefs,<span class="dv">1</span>, sd)</span></code></pre></div>
<p>Also as before, we can conduct independant hypothesis tests.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> We can conduct joint tests, such as whether two coefficients are equal, by looking at the their joint distribution.</p>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="ols-multiple-linear-regression.html#cb270-1" tabindex="-1"></a>boot_coef_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(<span class="at">ID=</span>boots, <span class="fu">t</span>(boot_coefs)))</span>
<span id="cb270-2"><a href="ols-multiple-linear-regression.html#cb270-2" tabindex="-1"></a>fig <span class="ot">&lt;-</span> plotly<span class="sc">::</span><span class="fu">plot_ly</span>(boot_coef_df,</span>
<span id="cb270-3"><a href="ols-multiple-linear-regression.html#cb270-3" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">&#39;scatter&#39;</span>, <span class="at">mode =</span> <span class="st">&#39;markers&#39;</span>,</span>
<span id="cb270-4"><a href="ols-multiple-linear-regression.html#cb270-4" tabindex="-1"></a>    <span class="at">x =</span> <span class="sc">~</span>UrbanPop, <span class="at">y =</span> <span class="sc">~</span>Assault,</span>
<span id="cb270-5"><a href="ols-multiple-linear-regression.html#cb270-5" tabindex="-1"></a>    <span class="at">text =</span> <span class="sc">~</span><span class="fu">paste</span>(<span class="st">&#39;&lt;b&gt; boot: &#39;</span>, ID, <span class="st">&#39;&lt;/b&gt;&#39;</span>),</span>
<span id="cb270-6"><a href="ols-multiple-linear-regression.html#cb270-6" tabindex="-1"></a>    <span class="at">hoverinfo=</span><span class="st">&#39;text&#39;</span>,</span>
<span id="cb270-7"><a href="ols-multiple-linear-regression.html#cb270-7" tabindex="-1"></a>    <span class="at">showlegend=</span>F,</span>
<span id="cb270-8"><a href="ols-multiple-linear-regression.html#cb270-8" tabindex="-1"></a>    <span class="at">marker=</span><span class="fu">list</span>( <span class="at">color=</span><span class="st">&#39;rgba(0, 0, 0, 0.5)&#39;</span>))</span>
<span id="cb270-9"><a href="ols-multiple-linear-regression.html#cb270-9" tabindex="-1"></a>fig</span></code></pre></div>
<div class="plotly html-widget html-fill-item" id="htmlwidget-872c212d3cf42794581a" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-872c212d3cf42794581a">{"x":{"visdat":{"339c647d3047":["function () ","plotlyVisDat"]},"cur_data":"339c647d3047","attrs":{"339c647d3047":{"mode":"markers","x":{},"y":{},"text":{},"hoverinfo":"text","showlegend":false,"marker":{"color":"rgba(0, 0, 0, 0.5)"},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter"}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"xaxis":{"domain":[0,1],"automargin":true,"title":"UrbanPop"},"yaxis":{"domain":[0,1],"automargin":true,"title":"Assault"},"hovermode":"closest","showlegend":false},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"mode":"markers","x":[-0.046149897839731065,-0.069613689320827121,-0.064159022419445627,-0.049726866669937968,-0.029091735470056116,-0.04906437264462464,-0.087725705402838638,-0.034077771467839212,-0.047609703045654234,-0.05579248813725831,-0.013041211246450543,-0.054766636656658603,-0.05056889586869439,-0.027809766398210027,-0.015762180892620995,-0.026952408584689579,-0.040775864467593417,-0.039790819714417912,-0.040202461112826486,-0.038764254753899427,-0.052895872865027706,-0.031847928639126319,-0.090404726255510687,-0.042875910510706786,-0.039179294804171372,-0.067837117701684019,-0.042200260424273141,-0.07678216387183652,-0.048876590251145088,-0.064163727465702716,-0.096696569475255859,-0.069943927194700523,-0.033093164630068284,-0.019977951077339878,-0.037172943159954654,-0.0891736730218574,-0.023281423740990303,-0.021747121439174937,-0.032990001972098226,-0.0098878051165795372,-0.051367804245784569,-0.038779559559115453,-0.021619863972052088,0.0043399021589856611,-0.045242371004800337,-0.019105877721055979,-0.04993560421261755,-0.020412762919302482,-0.04485100112009259,-0.063981624252517869,-0.10486353662530595,-0.081929760864382611,-0.029874271148756525,-0.039853188957207124,-0.083498434643221875,-0.059054513823635212,-0.032926594921559631,-0.045105695554690094,-0.052248160248088413,-0.05977743485479349,-0.033389233500128845,-0.026352750655007466,0.015808886656979076,-0.094457976197113491,0.0062237006812326705,-0.075670059924679081,-0.0012921634101337239,-0.041883970119801481,-0.059870614615938143,-0.052591657707695215,-0.070780589826599488,-0.024339546378833326,-0.024292594204721005,-0.0283317798129092,-0.060331336073367117,-0.043645154623117105,-0.031100109204596538,-0.0010964598507184984,-0.10140884221737716,-0.069096599280194834,-0.054098001258507174,-0.056372797820305492,-0.03032228957584426,-0.064554546811543642,8.783741765243635e-05,-0.031324381084071831,-0.038088271825318459,-0.067746719350779167,-0.028805781519798818,-0.084802301783259759,-0.074278261848833418,-0.10690119578857298,-0.068303514879835783,-0.068098183728414308,-0.021328585065271764,-0.0098883114839333798,-0.062745245577057532,-0.029195588406451187,-0.039775907298297691,-0.052013205689491381,-0.085120645185959645,-0.030347097046796726,-0.078432793701650613,-0.043872023338983025,-0.021211185202309336,-0.036217802021725636,-0.042178243048861232,-0.066420385153447781,-0.051657447146428191,-0.049232698149455703,-0.043404574675618632,-0.071607177987857235,0.00017247332528420539,-0.11455175224318485,-0.087380478452224422,-0.033944894723938419,-0.019475903988443816,-0.031625194441861092,-0.017188795872742291,-0.048235836910825093,-0.0098104244197436616,-0.039664297334442433,-0.032844422919688185,-0.053872983993751233,-0.068173633355875868,-0.07382247708600044,-0.093084876166852196,-0.024380274133557062,-0.031699612477629327,-0.079409935920695188,-0.070094297797743968,-0.026507315654276321,-0.046320897403217973,-0.030989999264495503,-0.094814756852198437,-0.066064276973075695,-0.088968784651594704,-0.044877357939919198,-0.027887705125270196,-0.035895408184010553,-0.0088542328205952255,-0.075263471429601977,-0.012760756872379693,-0.023300783268386201,-0.071083308667297285,-0.047421315861824166,-0.036916094736056566,-0.068312328559079774,-0.037382885178573452,-0.034980797854802355,-0.058324304022912762,-0.055448506843965739,-0.054401137465706968,-0.01654003201526278,-0.015008619690717264,-0.044670756239050778,-0.06688516240311547,-0.007181690732777489,-0.059217557606576522,-0.04216164751292284,-0.024047226563899782,0.0037620197131638063,-0.042730527729674686,-0.04380990727208918,-0.061879619810912834,-0.022356004092453297,-0.032089756417960504,-0.0051725522275466755,-0.10772711428146452,-0.04074168658034355,-0.050292282565263552,-0.084716953316427951,-0.051216640283335751,-0.082685649716597656,-0.044942623972458962,-0.026206377548201341,-0.041934232753052761,-0.046218917572795858,-0.0075547724010744319,-0.044441909599160445,-0.088542203893580446,-0.08464703841208239,-0.063160282568465648,-0.0889875982421683,-0.030246823279286798,-0.031700109026532787,-0.050214042745578578,-0.069719345796297183,-0.047635987283735491,-0.029124493788627394,-0.055492912414628386,-0.0027309610048657429,-0.12141477790474357,-0.12487085384706621,-0.042257769451036632,-0.041945274547101175,-0.047138425813747366,-0.071299062245188297,-0.031678546895920909,-0.042208479170042223,-0.023263211056199138,-0.066677734802184169,-0.027552096868102283,-0.019813098777715785,-0.059168783066863594,-0.00099747995426268678,-0.045880233663626473,-0.070255821719465553,-0.0315017260755064,-0.020084175712989982,-0.057656577044267264,-0.082944869936480828,-0.020948115082715275,-0.074774678259770008,-0.052140507059083109,-0.060370535635254186,-0.037337702991416158,-0.06010810913482343,-0.057116531138377689,-0.071893790532108953,-0.044006360828658928,-0.013578721395018749,-0.067868621745740737,-0.01729632208024514,-0.053606765796864318,-0.028041406118194036,-0.050069170820535629,-0.031468366255398471,-0.05663268795052763,-0.03515231555099374,-0.071222857069822509,-0.075998330778114992,-0.04763292340809272,-0.060527103243397565,-0.066158052225500044,-0.047798508805971321,-0.046282490941415226,-0.016227338551364011,-0.072097175224792726,-0.090047705291513139,-0.048952709904975023,-0.023640413677423651,-0.040120419161342361,-0.033738539128814582,-0.086077703557759885,-0.055689147539036775,-0.084572596092006763,-0.10677894387835991,-0.026733879558414185,-0.005189829242566093,-0.051467734029050617,-0.034414204908154576,-0.003271734340912835,-0.068719230792409311,-0.04103529757940523,-0.070836620067751155,-0.052640218633320988,-0.0020515567110363036,-0.02896174827269693,-0.097827461569667165,-0.081855774286122798,-0.073858815040793252,-0.062984486416233845,-0.058348253580467573,-0.051865422765904307,-0.037835008023676737,-0.080151734837749761,-0.064960015404634017,-0.095103804513468893,-0.050533623006684424,0.012224168784532499,-0.052158463901841499,-0.047452172472624225,-0.10827994936722861,-0.055800492092058729,-0.056642022776563036,-0.063993137732524338,-0.055316718377340979,-0.0063471876012538963,-0.077235630243322265,-0.056282987478766977,-0.10898297046834152,-0.016394143406827291,-0.023045272405932807,-0.0074663745076584713,-0.056942463730954011,-0.055466299347526817,-0.036742060499811803,-0.042882609903777534,-0.021903883478909376,0.011544130554717003,-0.060999225627628842,-0.074831993344846304,-0.04507009909617498,-0.053212756914397162,-0.020550561455948968,-0.046554801673534166,-0.064807024590913281,-0.037346517288996434,-0.095404061954452113,-0.044273738719236962,-0.020653810769174972,-0.057104819476236697,-0.080008696752247824,-0.02390881859401842,-0.073288932042836361,-0.057791367808563511,-0.034140205974400518,-0.073108014462707788,-0.0021740026376846164,-0.10498697030561825,-0.05845564916094783,-0.027131147394362586,-0.0018728425151035887,-0.030783271826992933,-0.041338947180948007,-0.013475982380245348,-0.046254938154018189,-0.087183449538694127,-0.034335284796922383,-0.057455739251009962,-0.059948417933395517,-0.014797869124989848,-0.068257877704889766,-0.040600066451890057,-0.029595345701040911,-0.083534511813023418,0.021370283166325766,-0.028700773272802793,-0.092560034587102782,-0.044617462392624596,-0.040271106814356257,0.010450750102442456,-0.013890566727919653,-0.025460828553970225,-0.078501905788747717,-0.020331306440865692,-0.06477131953949393,-0.044311471708971713,-0.04447568903161369,-0.049993587758223428,-0.033618095656955185,-0.030381738299044985,-0.068604415348972977,-0.034747045928631627,-0.015489009254461698,-0.042631433921883809,-0.060309042705207781,-0.05381026239383082,-0.053799381513285748,-0.054564260234064438,-0.035153915722510899,-0.032489968659858114,-0.073836245135468714,-0.095364952945581435,-0.082677873757021583,-0.052442465192517322,-0.060460962217492392,-0.032952981349962086,-0.090020066966599352,-0.083326053331441122,-0.10879274715696111,-0.062203569190982549,-0.018481669698767852,-0.042047519376622065,-0.012245098444219983,-0.018683532096804973,-0.011657830225903273,-0.038629354803445669,-0.040198938078101543,-0.033356226511352179,-0.040029708286763992,-0.087357462970968608,-0.05191678510283855,-0.085732175656586973,-0.032148141473641263,-0.0020762019982253992,-0.038210622322541118,-0.038091979471253215,-0.017468922520414883,-0.015077807248250473,-0.060132107167578989,-0.08038942721386097,-0.056730161257652165,-0.050393889444133998,-0.0054115901961654285,-0.044483172748449207,-0.028859704709594293,-0.023267194256602553,-0.046401553547941392,-0.016347237870967982,-0.032621297075884376,0.0074132883351964423,-0.030809937526438267,-0.045687138352264518,-0.083052539890433805,-0.082408753011754016,-0.078514226983523769,-0.043643854400989848],"y":[0.04213893915540623,0.043374449099063905,0.04412107850034077,0.045193898513671976,0.046914493110317257,0.045310073354757105,0.050346171289876812,0.046873453343016257,0.042671371323333949,0.046646668305157062,0.04251986780584751,0.043543477191526445,0.052161934549626712,0.0371473828332477,0.04155274845848006,0.038469215850007593,0.048230273234209801,0.040770813149037147,0.040863689079423934,0.038664151358689516,0.040035568795072474,0.043260905561960286,0.046898564115884327,0.040955305862054772,0.045497500230647366,0.048686688358933361,0.049597774746605719,0.048652399724733029,0.057029042117239172,0.049934311810520784,0.040598766186372842,0.049396724407570397,0.049558592712733773,0.037503881308678197,0.043866439854905528,0.047352891365677663,0.03920761143799923,0.044198701193896739,0.045461283136587136,0.046184917811645694,0.039487916045385318,0.040788292334514149,0.03781232228418472,0.03567641667695165,0.044284473456938235,0.03846671025294586,0.048158440455982665,0.044305087137733248,0.044122936512685597,0.046581904201393534,0.051806077217050835,0.04751262900305641,0.044609187553471458,0.047829228956748973,0.046760422392080808,0.043790224743804909,0.035977479358132891,0.055051806999267733,0.046593823266450833,0.043441518634021445,0.039076596183410174,0.038883663741925388,0.035871890484941917,0.04643419536829687,0.041356460481775811,0.050103813126250576,0.042050744400759843,0.043989745515180902,0.04393306936968118,0.039602710841525224,0.05047564146612829,0.047272811791609019,0.037714116308143836,0.04100559171259828,0.041519490537358587,0.035233072274567248,0.045525497446948747,0.04067444986376377,0.040350496052739979,0.042836048767827456,0.046601825529552464,0.045146857883590921,0.039904643884416002,0.047041689966580917,0.040794074997118443,0.04312404368559241,0.041445287953522224,0.039981406153742592,0.049150337970374693,0.045172418620919953,0.043986266802257062,0.049799955258305668,0.043087588983191073,0.042219567231814845,0.043265161023316144,0.047983887620334474,0.049107590381374064,0.037999706371142195,0.038672696874188409,0.041825877857415095,0.048638215875146579,0.039259566904542173,0.047363270661214159,0.047410660752009504,0.046707694396512317,0.043287409161597727,0.045893147796424595,0.048479210317743014,0.044425968318068929,0.048511733747221751,0.041185343581832547,0.046758494836774321,0.041331964252417738,0.05035976995455816,0.048540108176414139,0.043153866328655539,0.044338560584195656,0.043252753200514277,0.043737912067546104,0.052439127985815272,0.039887086809876571,0.046633368989152435,0.041014746240964718,0.04360907518616064,0.04404828590273939,0.0434978022578658,0.054177428947436011,0.047920084138801666,0.039817489917242474,0.037390576235777455,0.050287241013451639,0.039959673505976318,0.038062355736616919,0.045888941516276989,0.056585361184401155,0.046831186304142215,0.047154761932439809,0.046526402911356429,0.04357074883026394,0.037468202850250112,0.042527930293682605,0.038059965366753545,0.043606997575319759,0.040018447291499934,0.048215705105961008,0.047894554314149095,0.043319074634743292,0.042551178096492059,0.042240783649641712,0.042805405884553047,0.043153577272439562,0.049989196105504952,0.034196343509795564,0.046155855862069183,0.044934592914218571,0.046419836276147035,0.050032450797721097,0.043181519289002858,0.049001625760892099,0.042230947271542502,0.034816855752549017,0.038806833396773925,0.036524922698391568,0.046505544798981044,0.050731502358017738,0.047377568313354129,0.04422964129688918,0.039491612990214191,0.044735180423413463,0.043010120470790593,0.041650901217415402,0.049225020826541857,0.048400814550071301,0.049155681262240271,0.050781500105377776,0.031935641917699216,0.04202529786187148,0.043483613742704853,0.038610279235304315,0.046275756266112932,0.040167046169036734,0.043072870050081861,0.048007602317671254,0.045213672590163151,0.043001503481700405,0.050130854294846289,0.043564785097940283,0.047267147510446153,0.044682506649211681,0.040544155146206205,0.045236981836280082,0.039351568253499961,0.046936119775302584,0.053806419353671968,0.037315974903050796,0.04783036860533401,0.046168572905063696,0.040312946184245767,0.045474047500126999,0.041380903248230336,0.035921755008282243,0.047540884107838924,0.049193384613848548,0.037373124407475117,0.045847551131111906,0.045955270508452647,0.04127429676944383,0.055128030378193402,0.042558242799944156,0.042937700752212045,0.042528558551952367,0.047928687236438623,0.043541722892856925,0.050075430034906256,0.043221072062376763,0.048716501773582326,0.045045078654064055,0.047495071968670997,0.046533045478798461,0.046752487053131071,0.049524658882341525,0.040255840997066993,0.052221215422810517,0.047559845161557743,0.040390115527343201,0.049502907694273617,0.049502862306514586,0.043128407502582483,0.046158072234227396,0.040608713120489061,0.05419403324440699,0.040789799109985023,0.047247470034190373,0.033721880677525851,0.046976127224036754,0.040756234652352508,0.044877687174891462,0.040249727470927067,0.046202351726724995,0.053892560219797055,0.040123495987462836,0.046942334372417321,0.042258936925723412,0.046545828886732463,0.046718092225694348,0.048231327436847632,0.053298177541879196,0.047994581983263632,0.04126656385647328,0.043190512696691628,0.046783654400890252,0.047667490396060476,0.039414144672522834,0.043525995895194125,0.04897428367044987,0.043287721066824311,0.043534636575722103,0.038576878795482453,0.03864591103340978,0.050726754095613738,0.046779483591507014,0.052281139343056129,0.046864495235472294,0.045174786756705686,0.046621634891436094,0.045731305077928781,0.046968855109883162,0.043214069965526354,0.044602046402076156,0.044773160695284697,0.041485906053363428,0.049047455432774736,0.046970066530222107,0.053658970822478266,0.043660611989553531,0.045127244658550413,0.043576080685245253,0.037155105672013848,0.034363927617033843,0.047620385146043998,0.04345107659558655,0.055882246836469446,0.037282752335371164,0.036930139586732004,0.037586162726272306,0.048732087908091505,0.045040759316786555,0.044234560109849851,0.037484960923361718,0.042253398227728903,0.040879873567625936,0.047846180389195393,0.044693388504314424,0.039777576749769505,0.040186715079164916,0.041064218993268072,0.039843081715032695,0.045661944030217061,0.043507766514557226,0.048028872385370132,0.041992648423784758,0.047454916923681402,0.044728639044822473,0.048689642790643139,0.035912796963616704,0.048723893308379534,0.034632004896873125,0.033979388318697887,0.039257957702884684,0.038422437299898811,0.041861262852385485,0.040577052178051687,0.041378602746244136,0.038722462217051015,0.047604406873540443,0.041839918051239294,0.034530184576011286,0.04446114852035904,0.041318514526774618,0.048551799949218731,0.044569594965225202,0.050030571820823787,0.046894329174937965,0.047205410133144866,0.045595116960341055,0.049537830558763006,0.047413883395617092,0.039514690348305417,0.038526332180374268,0.047248050652481118,0.040153209739818174,0.047875532192709368,0.043661411237029372,0.043095177827785908,0.043053616701296191,0.045944456410163523,0.045333159397562287,0.042466351610763398,0.043894271413479409,0.038557107847026796,0.044938018379664192,0.043610055581000493,0.041210044587646794,0.043171203974012168,0.04136445252622329,0.038935036162391939,0.043866133145858119,0.047979015734134857,0.047121930054947629,0.045444967617930807,0.048324072951081876,0.041035542669559132,0.04601893249564748,0.045868391124006591,0.051096356393212361,0.045451224928001917,0.040059823857162592,0.047427607489277011,0.04837346856410453,0.048666227570375691,0.049423946618148626,0.049038523213341539,0.046681681156196517,0.046805142804338368,0.041895056487198219,0.037535659502452488,0.040832402452636841,0.042435217994030695,0.042045387714913736,0.040631446565297977,0.044005144302134293,0.040890189720366552,0.046343511562334301,0.043997769629865797,0.038239510919387834,0.043135893316863731,0.036191270119286953,0.044467930978881715,0.043170188715366216,0.04769429345733861,0.045136118934146326,0.04901851673898712,0.042667555002217541,0.043614634271409311,0.053760615005914399,0.04209189420284945,0.044841678811361141,0.040904074913585879,0.041168695383584107,0.039971346100652755,0.039060099822877073,0.040932532343924362,0.040937049459500795,0.043367697601741917,0.044793667678047024,0.052080250712243099,0.050494747459390407,0.04998257041726132,0.046356160235683501],"text":["<b> boot:  1 <\/b>","<b> boot:  2 <\/b>","<b> boot:  3 <\/b>","<b> boot:  4 <\/b>","<b> boot:  5 <\/b>","<b> boot:  6 <\/b>","<b> boot:  7 <\/b>","<b> boot:  8 <\/b>","<b> boot:  9 <\/b>","<b> boot:  10 <\/b>","<b> boot:  11 <\/b>","<b> boot:  12 <\/b>","<b> boot:  13 <\/b>","<b> boot:  14 <\/b>","<b> boot:  15 <\/b>","<b> boot:  16 <\/b>","<b> boot:  17 <\/b>","<b> boot:  18 <\/b>","<b> boot:  19 <\/b>","<b> boot:  20 <\/b>","<b> boot:  21 <\/b>","<b> boot:  22 <\/b>","<b> boot:  23 <\/b>","<b> boot:  24 <\/b>","<b> boot:  25 <\/b>","<b> boot:  26 <\/b>","<b> boot:  27 <\/b>","<b> boot:  28 <\/b>","<b> boot:  29 <\/b>","<b> boot:  30 <\/b>","<b> boot:  31 <\/b>","<b> boot:  32 <\/b>","<b> boot:  33 <\/b>","<b> boot:  34 <\/b>","<b> boot:  35 <\/b>","<b> boot:  36 <\/b>","<b> boot:  37 <\/b>","<b> boot:  38 <\/b>","<b> boot:  39 <\/b>","<b> boot:  40 <\/b>","<b> boot:  41 <\/b>","<b> boot:  42 <\/b>","<b> boot:  43 <\/b>","<b> boot:  44 <\/b>","<b> boot:  45 <\/b>","<b> boot:  46 <\/b>","<b> boot:  47 <\/b>","<b> boot:  48 <\/b>","<b> boot:  49 <\/b>","<b> boot:  50 <\/b>","<b> boot:  51 <\/b>","<b> boot:  52 <\/b>","<b> boot:  53 <\/b>","<b> boot:  54 <\/b>","<b> boot:  55 <\/b>","<b> boot:  56 <\/b>","<b> boot:  57 <\/b>","<b> boot:  58 <\/b>","<b> boot:  59 <\/b>","<b> boot:  60 <\/b>","<b> boot:  61 <\/b>","<b> boot:  62 <\/b>","<b> boot:  63 <\/b>","<b> boot:  64 <\/b>","<b> boot:  65 <\/b>","<b> boot:  66 <\/b>","<b> boot:  67 <\/b>","<b> boot:  68 <\/b>","<b> boot:  69 <\/b>","<b> boot:  70 <\/b>","<b> boot:  71 <\/b>","<b> boot:  72 <\/b>","<b> boot:  73 <\/b>","<b> boot:  74 <\/b>","<b> boot:  75 <\/b>","<b> boot:  76 <\/b>","<b> boot:  77 <\/b>","<b> boot:  78 <\/b>","<b> boot:  79 <\/b>","<b> boot:  80 <\/b>","<b> boot:  81 <\/b>","<b> boot:  82 <\/b>","<b> boot:  83 <\/b>","<b> boot:  84 <\/b>","<b> boot:  85 <\/b>","<b> boot:  86 <\/b>","<b> boot:  87 <\/b>","<b> boot:  88 <\/b>","<b> boot:  89 <\/b>","<b> boot:  90 <\/b>","<b> boot:  91 <\/b>","<b> boot:  92 <\/b>","<b> boot:  93 <\/b>","<b> boot:  94 <\/b>","<b> boot:  95 <\/b>","<b> boot:  96 <\/b>","<b> boot:  97 <\/b>","<b> boot:  98 <\/b>","<b> boot:  99 <\/b>","<b> boot:  100 <\/b>","<b> boot:  101 <\/b>","<b> boot:  102 <\/b>","<b> boot:  103 <\/b>","<b> boot:  104 <\/b>","<b> boot:  105 <\/b>","<b> boot:  106 <\/b>","<b> boot:  107 <\/b>","<b> boot:  108 <\/b>","<b> boot:  109 <\/b>","<b> boot:  110 <\/b>","<b> boot:  111 <\/b>","<b> boot:  112 <\/b>","<b> boot:  113 <\/b>","<b> boot:  114 <\/b>","<b> boot:  115 <\/b>","<b> boot:  116 <\/b>","<b> boot:  117 <\/b>","<b> boot:  118 <\/b>","<b> boot:  119 <\/b>","<b> boot:  120 <\/b>","<b> boot:  121 <\/b>","<b> boot:  122 <\/b>","<b> boot:  123 <\/b>","<b> boot:  124 <\/b>","<b> boot:  125 <\/b>","<b> boot:  126 <\/b>","<b> boot:  127 <\/b>","<b> boot:  128 <\/b>","<b> boot:  129 <\/b>","<b> boot:  130 <\/b>","<b> boot:  131 <\/b>","<b> boot:  132 <\/b>","<b> boot:  133 <\/b>","<b> boot:  134 <\/b>","<b> boot:  135 <\/b>","<b> boot:  136 <\/b>","<b> boot:  137 <\/b>","<b> boot:  138 <\/b>","<b> boot:  139 <\/b>","<b> boot:  140 <\/b>","<b> boot:  141 <\/b>","<b> boot:  142 <\/b>","<b> boot:  143 <\/b>","<b> boot:  144 <\/b>","<b> boot:  145 <\/b>","<b> boot:  146 <\/b>","<b> boot:  147 <\/b>","<b> boot:  148 <\/b>","<b> boot:  149 <\/b>","<b> boot:  150 <\/b>","<b> boot:  151 <\/b>","<b> boot:  152 <\/b>","<b> boot:  153 <\/b>","<b> boot:  154 <\/b>","<b> boot:  155 <\/b>","<b> boot:  156 <\/b>","<b> boot:  157 <\/b>","<b> boot:  158 <\/b>","<b> boot:  159 <\/b>","<b> boot:  160 <\/b>","<b> boot:  161 <\/b>","<b> boot:  162 <\/b>","<b> boot:  163 <\/b>","<b> boot:  164 <\/b>","<b> boot:  165 <\/b>","<b> boot:  166 <\/b>","<b> boot:  167 <\/b>","<b> boot:  168 <\/b>","<b> boot:  169 <\/b>","<b> boot:  170 <\/b>","<b> boot:  171 <\/b>","<b> boot:  172 <\/b>","<b> boot:  173 <\/b>","<b> boot:  174 <\/b>","<b> boot:  175 <\/b>","<b> boot:  176 <\/b>","<b> boot:  177 <\/b>","<b> boot:  178 <\/b>","<b> boot:  179 <\/b>","<b> boot:  180 <\/b>","<b> boot:  181 <\/b>","<b> boot:  182 <\/b>","<b> boot:  183 <\/b>","<b> boot:  184 <\/b>","<b> boot:  185 <\/b>","<b> boot:  186 <\/b>","<b> boot:  187 <\/b>","<b> boot:  188 <\/b>","<b> boot:  189 <\/b>","<b> boot:  190 <\/b>","<b> boot:  191 <\/b>","<b> boot:  192 <\/b>","<b> boot:  193 <\/b>","<b> boot:  194 <\/b>","<b> boot:  195 <\/b>","<b> boot:  196 <\/b>","<b> boot:  197 <\/b>","<b> boot:  198 <\/b>","<b> boot:  199 <\/b>","<b> boot:  200 <\/b>","<b> boot:  201 <\/b>","<b> boot:  202 <\/b>","<b> boot:  203 <\/b>","<b> boot:  204 <\/b>","<b> boot:  205 <\/b>","<b> boot:  206 <\/b>","<b> boot:  207 <\/b>","<b> boot:  208 <\/b>","<b> boot:  209 <\/b>","<b> boot:  210 <\/b>","<b> boot:  211 <\/b>","<b> boot:  212 <\/b>","<b> boot:  213 <\/b>","<b> boot:  214 <\/b>","<b> boot:  215 <\/b>","<b> boot:  216 <\/b>","<b> boot:  217 <\/b>","<b> boot:  218 <\/b>","<b> boot:  219 <\/b>","<b> boot:  220 <\/b>","<b> boot:  221 <\/b>","<b> boot:  222 <\/b>","<b> boot:  223 <\/b>","<b> boot:  224 <\/b>","<b> boot:  225 <\/b>","<b> boot:  226 <\/b>","<b> boot:  227 <\/b>","<b> boot:  228 <\/b>","<b> boot:  229 <\/b>","<b> boot:  230 <\/b>","<b> boot:  231 <\/b>","<b> boot:  232 <\/b>","<b> boot:  233 <\/b>","<b> boot:  234 <\/b>","<b> boot:  235 <\/b>","<b> boot:  236 <\/b>","<b> boot:  237 <\/b>","<b> boot:  238 <\/b>","<b> boot:  239 <\/b>","<b> boot:  240 <\/b>","<b> boot:  241 <\/b>","<b> boot:  242 <\/b>","<b> boot:  243 <\/b>","<b> boot:  244 <\/b>","<b> boot:  245 <\/b>","<b> boot:  246 <\/b>","<b> boot:  247 <\/b>","<b> boot:  248 <\/b>","<b> boot:  249 <\/b>","<b> boot:  250 <\/b>","<b> boot:  251 <\/b>","<b> boot:  252 <\/b>","<b> boot:  253 <\/b>","<b> boot:  254 <\/b>","<b> boot:  255 <\/b>","<b> boot:  256 <\/b>","<b> boot:  257 <\/b>","<b> boot:  258 <\/b>","<b> boot:  259 <\/b>","<b> boot:  260 <\/b>","<b> boot:  261 <\/b>","<b> boot:  262 <\/b>","<b> boot:  263 <\/b>","<b> boot:  264 <\/b>","<b> boot:  265 <\/b>","<b> boot:  266 <\/b>","<b> boot:  267 <\/b>","<b> boot:  268 <\/b>","<b> boot:  269 <\/b>","<b> boot:  270 <\/b>","<b> boot:  271 <\/b>","<b> boot:  272 <\/b>","<b> boot:  273 <\/b>","<b> boot:  274 <\/b>","<b> boot:  275 <\/b>","<b> boot:  276 <\/b>","<b> boot:  277 <\/b>","<b> boot:  278 <\/b>","<b> boot:  279 <\/b>","<b> boot:  280 <\/b>","<b> boot:  281 <\/b>","<b> boot:  282 <\/b>","<b> boot:  283 <\/b>","<b> boot:  284 <\/b>","<b> boot:  285 <\/b>","<b> boot:  286 <\/b>","<b> boot:  287 <\/b>","<b> boot:  288 <\/b>","<b> boot:  289 <\/b>","<b> boot:  290 <\/b>","<b> boot:  291 <\/b>","<b> boot:  292 <\/b>","<b> boot:  293 <\/b>","<b> boot:  294 <\/b>","<b> boot:  295 <\/b>","<b> boot:  296 <\/b>","<b> boot:  297 <\/b>","<b> boot:  298 <\/b>","<b> boot:  299 <\/b>","<b> boot:  300 <\/b>","<b> boot:  301 <\/b>","<b> boot:  302 <\/b>","<b> boot:  303 <\/b>","<b> boot:  304 <\/b>","<b> boot:  305 <\/b>","<b> boot:  306 <\/b>","<b> boot:  307 <\/b>","<b> boot:  308 <\/b>","<b> boot:  309 <\/b>","<b> boot:  310 <\/b>","<b> boot:  311 <\/b>","<b> boot:  312 <\/b>","<b> boot:  313 <\/b>","<b> boot:  314 <\/b>","<b> boot:  315 <\/b>","<b> boot:  316 <\/b>","<b> boot:  317 <\/b>","<b> boot:  318 <\/b>","<b> boot:  319 <\/b>","<b> boot:  320 <\/b>","<b> boot:  321 <\/b>","<b> boot:  322 <\/b>","<b> boot:  323 <\/b>","<b> boot:  324 <\/b>","<b> boot:  325 <\/b>","<b> boot:  326 <\/b>","<b> boot:  327 <\/b>","<b> boot:  328 <\/b>","<b> boot:  329 <\/b>","<b> boot:  330 <\/b>","<b> boot:  331 <\/b>","<b> boot:  332 <\/b>","<b> boot:  333 <\/b>","<b> boot:  334 <\/b>","<b> boot:  335 <\/b>","<b> boot:  336 <\/b>","<b> boot:  337 <\/b>","<b> boot:  338 <\/b>","<b> boot:  339 <\/b>","<b> boot:  340 <\/b>","<b> boot:  341 <\/b>","<b> boot:  342 <\/b>","<b> boot:  343 <\/b>","<b> boot:  344 <\/b>","<b> boot:  345 <\/b>","<b> boot:  346 <\/b>","<b> boot:  347 <\/b>","<b> boot:  348 <\/b>","<b> boot:  349 <\/b>","<b> boot:  350 <\/b>","<b> boot:  351 <\/b>","<b> boot:  352 <\/b>","<b> boot:  353 <\/b>","<b> boot:  354 <\/b>","<b> boot:  355 <\/b>","<b> boot:  356 <\/b>","<b> boot:  357 <\/b>","<b> boot:  358 <\/b>","<b> boot:  359 <\/b>","<b> boot:  360 <\/b>","<b> boot:  361 <\/b>","<b> boot:  362 <\/b>","<b> boot:  363 <\/b>","<b> boot:  364 <\/b>","<b> boot:  365 <\/b>","<b> boot:  366 <\/b>","<b> boot:  367 <\/b>","<b> boot:  368 <\/b>","<b> boot:  369 <\/b>","<b> boot:  370 <\/b>","<b> boot:  371 <\/b>","<b> boot:  372 <\/b>","<b> boot:  373 <\/b>","<b> boot:  374 <\/b>","<b> boot:  375 <\/b>","<b> boot:  376 <\/b>","<b> boot:  377 <\/b>","<b> boot:  378 <\/b>","<b> boot:  379 <\/b>","<b> boot:  380 <\/b>","<b> boot:  381 <\/b>","<b> boot:  382 <\/b>","<b> boot:  383 <\/b>","<b> boot:  384 <\/b>","<b> boot:  385 <\/b>","<b> boot:  386 <\/b>","<b> boot:  387 <\/b>","<b> boot:  388 <\/b>","<b> boot:  389 <\/b>","<b> boot:  390 <\/b>","<b> boot:  391 <\/b>","<b> boot:  392 <\/b>","<b> boot:  393 <\/b>","<b> boot:  394 <\/b>","<b> boot:  395 <\/b>","<b> boot:  396 <\/b>","<b> boot:  397 <\/b>","<b> boot:  398 <\/b>","<b> boot:  399 <\/b>"],"hoverinfo":["text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text"],"showlegend":false,"marker":{"color":"rgba(0, 0, 0, 0.5)","line":{"color":"rgba(31,119,180,1)"}},"type":"scatter","error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"xaxis":"x","yaxis":"y","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb271-1"><a href="ols-multiple-linear-regression.html#cb271-1" tabindex="-1"></a><span class="do">## Show Histogram of Coefficients</span></span>
<span id="cb271-2"><a href="ols-multiple-linear-regression.html#cb271-2" tabindex="-1"></a><span class="do">## plotly::add_histogram2d(fig, nbinsx=20, nbinsy=20)</span></span>
<span id="cb271-3"><a href="ols-multiple-linear-regression.html#cb271-3" tabindex="-1"></a></span>
<span id="cb271-4"><a href="ols-multiple-linear-regression.html#cb271-4" tabindex="-1"></a><span class="do">## Show 95% Contour</span></span>
<span id="cb271-5"><a href="ols-multiple-linear-regression.html#cb271-5" tabindex="-1"></a><span class="do">## plotly::add_histogram2dcontour(fig)</span></span>
<span id="cb271-6"><a href="ols-multiple-linear-regression.html#cb271-6" tabindex="-1"></a><span class="do">## fig &lt;- layout(fig,</span></span>
<span id="cb271-7"><a href="ols-multiple-linear-regression.html#cb271-7" tabindex="-1"></a><span class="do">##    yaxis = list(title=expression(beta[3])),</span></span>
<span id="cb271-8"><a href="ols-multiple-linear-regression.html#cb271-8" tabindex="-1"></a><span class="do">##    xaxis = list(title=expression(beta[2])))</span></span></code></pre></div>
<p>We can also use an <span class="math inline">\(F\)</span> test for <span class="math inline">\(q\)</span> hypotheses;
<span class="math display">\[
\hat{F}_{q} = \frac{(ESS_{restricted}-ESS_{unrestricted})/q}{ESS_{unrestricted}/(n-K)},
\]</span>
and <span class="math inline">\(\hat{F}\)</span> can be written in terms of unrestricted and restricted <span class="math inline">\(R^2\)</span>. Under some additional assumptions <span class="math inline">\(\hat{F}_{q} \sim F_{q,n-K}\)</span>. For some inuition, we will examine how the <span class="math inline">\(R^2\)</span> statistic varies with bootstrap samples. Specifically, compute a null <span class="math inline">\(R^2\)</span> distribution by randomly reshuffling the outcomes and compare it to the observed <span class="math inline">\(R^2\)</span>.</p>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb272-1"><a href="ols-multiple-linear-regression.html#cb272-1" tabindex="-1"></a><span class="do">## Bootstrap NULL</span></span>
<span id="cb272-2"><a href="ols-multiple-linear-regression.html#cb272-2" tabindex="-1"></a>boots <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">399</span></span>
<span id="cb272-3"><a href="ols-multiple-linear-regression.html#cb272-3" tabindex="-1"></a>boot_regs0 <span class="ot">&lt;-</span> <span class="fu">lapply</span>(boots, <span class="cf">function</span>(b){</span>
<span id="cb272-4"><a href="ols-multiple-linear-regression.html#cb272-4" tabindex="-1"></a>  xy_b <span class="ot">&lt;-</span> USArrests</span>
<span id="cb272-5"><a href="ols-multiple-linear-regression.html#cb272-5" tabindex="-1"></a>  b_id <span class="ot">&lt;-</span> <span class="fu">sample</span>( <span class="fu">nrow</span>(USArrests), <span class="at">replace=</span>T)</span>
<span id="cb272-6"><a href="ols-multiple-linear-regression.html#cb272-6" tabindex="-1"></a>  xy_b<span class="sc">$</span>Murder <span class="ot">&lt;-</span>  xy_b<span class="sc">$</span>Murder[b_id]</span>
<span id="cb272-7"><a href="ols-multiple-linear-regression.html#cb272-7" tabindex="-1"></a>  reg_b <span class="ot">&lt;-</span> <span class="fu">lm</span>(Murder<span class="sc">~</span>Assault<span class="sc">+</span>UrbanPop, <span class="at">dat=</span>xy_b)</span>
<span id="cb272-8"><a href="ols-multiple-linear-regression.html#cb272-8" tabindex="-1"></a>})</span>
<span id="cb272-9"><a href="ols-multiple-linear-regression.html#cb272-9" tabindex="-1"></a></span>
<span id="cb272-10"><a href="ols-multiple-linear-regression.html#cb272-10" tabindex="-1"></a>boot_coefs0 <span class="ot">&lt;-</span> <span class="fu">sapply</span>(boot_regs0, <span class="cf">function</span>(reg_k){</span>
<span id="cb272-11"><a href="ols-multiple-linear-regression.html#cb272-11" tabindex="-1"></a>    <span class="fu">coef</span>(reg_k) })</span>
<span id="cb272-12"><a href="ols-multiple-linear-regression.html#cb272-12" tabindex="-1"></a>R2_sim0 <span class="ot">&lt;-</span> <span class="fu">sapply</span>(boot_regs0, <span class="cf">function</span>(reg_k){</span>
<span id="cb272-13"><a href="ols-multiple-linear-regression.html#cb272-13" tabindex="-1"></a>    <span class="fu">summary</span>(reg_k)<span class="sc">$</span>r.squared })</span>
<span id="cb272-14"><a href="ols-multiple-linear-regression.html#cb272-14" tabindex="-1"></a>R2adj_sim0 <span class="ot">&lt;-</span> <span class="fu">sapply</span>(boot_regs0, <span class="cf">function</span>(reg_k){</span>
<span id="cb272-15"><a href="ols-multiple-linear-regression.html#cb272-15" tabindex="-1"></a>    <span class="fu">summary</span>(reg_k)<span class="sc">$</span>adj.r.squared })</span>
<span id="cb272-16"><a href="ols-multiple-linear-regression.html#cb272-16" tabindex="-1"></a></span>
<span id="cb272-17"><a href="ols-multiple-linear-regression.html#cb272-17" tabindex="-1"></a><span class="fu">hist</span>(R2adj_sim0, <span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">breaks=</span><span class="dv">25</span>,</span>
<span id="cb272-18"><a href="ols-multiple-linear-regression.html#cb272-18" tabindex="-1"></a>    <span class="at">main=</span><span class="st">&#39;&#39;</span>, <span class="at">xlab=</span><span class="fu">expression</span>(<span class="st">&#39;adj.&#39;</span><span class="sc">~</span>R[b]<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb272-19"><a href="ols-multiple-linear-regression.html#cb272-19" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fu">summary</span>(reg)<span class="sc">$</span>adj.r.squared, <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="03-ROLS_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p><em>Hypothesis Testing is not to be done routinely</em> and additional complications arise when testing multiple hypothesis.</p>
</div>
<div id="factor-variables" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Factor Variables<a href="ols-multiple-linear-regression.html#factor-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far, we have discussed cardinal data where the difference between units always means the same thing: e.g., <span class="math inline">\(4-3=2-1\)</span>. There are also factor variables</p>
<ul>
<li>Ordered: refers to Ordinal data. The difference between units means something, but not always the same thing. For example, <span class="math inline">\(4th - 3rd \neq 2nd - 1st\)</span>.</li>
<li>Unordered: refers to Categorical data. The difference between units is meaningless. For example, <span class="math inline">\(B-A=?\)</span></li>
</ul>
<p>To analyze either factor, we often convert them into indicator variables or dummies; <span class="math inline">\(D_{c}=\mathbf{1}( Factor = c)\)</span>. One common case is if you have observations of individuals over time periods, then you may have two factor variables. An unordered factor that indicates who an individual is; for example <span class="math inline">\(D_{i}=\mathbf{1}( Individual = i)\)</span>, and an order factor that indicates the time period; for example <span class="math inline">\(D_{t}=\mathbf{1}( Time \in [month~ t, month~ t+1) )\)</span>. There are many other cases you see factor variables, including spatial ID’s in purely cross sectional data.</p>
<p>Be careful not to handle categorical data as if they were cardinal. E.g., generate city data with Leipzig=1, Lausanne=2, LosAngeles=3, … and then include city as if it were a cardinal number (that’s a big no-no). The same applied to ordinal data; PopulationLeipzig=2, PopulationLausanne=3, PopulationLosAngeles=1.</p>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="ols-multiple-linear-regression.html#cb273-1" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb273-2"><a href="ols-multiple-linear-regression.html#cb273-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(N,<span class="dv">3</span>,<span class="dv">8</span>)</span>
<span id="cb273-3"><a href="ols-multiple-linear-regression.html#cb273-3" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N,<span class="dv">0</span>,<span class="fl">0.4</span>)</span>
<span id="cb273-4"><a href="ols-multiple-linear-regression.html#cb273-4" tabindex="-1"></a>fo <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">rbinom</span>(N,<span class="dv">4</span>,.<span class="dv">5</span>), <span class="at">ordered=</span>T)</span>
<span id="cb273-5"><a href="ols-multiple-linear-regression.html#cb273-5" tabindex="-1"></a>fu <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&#39;A&#39;</span>,<span class="st">&#39;B&#39;</span>),N<span class="sc">/</span><span class="dv">2</span>), <span class="at">ordered=</span>F)</span>
<span id="cb273-6"><a href="ols-multiple-linear-regression.html#cb273-6" tabindex="-1"></a>dA <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">*</span>(fu<span class="sc">==</span><span class="st">&#39;A&#39;</span>)</span>
<span id="cb273-7"><a href="ols-multiple-linear-regression.html#cb273-7" tabindex="-1"></a>y <span class="ot">&lt;-</span> (<span class="dv">2</span><span class="sc">^</span><span class="fu">as.integer</span>(fo)<span class="sc">*</span>dA )<span class="sc">*</span><span class="fu">sqrt</span>(x)<span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">as.integer</span>(fo)<span class="sc">*</span>e</span>
<span id="cb273-8"><a href="ols-multiple-linear-regression.html#cb273-8" tabindex="-1"></a>dat_f <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y,x,fo,fu)</span></code></pre></div>
<p>With factors, you can still include them in the design matrix of an OLS regression
<span class="math display">\[
y_{it} = x_{it} \beta_{x} + d_{c}\beta_{c}
\]</span>
When, as commonly done, the factors are modeled as being additively seperable, they are modelled as either “fixed” or “random” effects.</p>
<p>Simply including the factors into the OLS regression yields a “dummy variable” fixed effects estimator.</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="ols-multiple-linear-regression.html#cb274-1" tabindex="-1"></a>fe_reg0 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>x<span class="sc">+</span>fo<span class="sc">+</span>fu, dat_f)</span>
<span id="cb274-2"><a href="ols-multiple-linear-regression.html#cb274-2" tabindex="-1"></a><span class="fu">summary</span>(fe_reg0)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x + fo + fu, data = dat_f)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -34.488  -5.832   0.246   5.823  41.124 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  20.8011     1.2240  16.995  &lt; 2e-16 ***
## x             1.1051     0.2031   5.441 6.66e-08 ***
## fo.L         26.2435     1.0686  24.559  &lt; 2e-16 ***
## fo.Q         11.4777     0.9407  12.201  &lt; 2e-16 ***
## fo.C          2.1208     0.7254   2.923  0.00354 ** 
## fo^4          0.4491     0.5566   0.807  0.41998    
## fuB         -23.5002     0.5816 -40.409  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 9.176 on 993 degrees of freedom
## Multiple R-squared:  0.7251, Adjusted R-squared:  0.7234 
## F-statistic: 436.5 on 6 and 993 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We can also compute averages for each group and construct a “between estimator”
<span class="math display">\[
\overline{y}_i = \alpha + \overline{x}_i \beta
\]</span>
Or we can subtract the average from each group to construct a “within estimator”,
<span class="math display">\[
(y_{it} - \overline{y}_i) = (x_{it}-\overline{x}_i)\beta\\
\]</span>
that tends to be more computationally efficient, has corrections for standard errors, and has additional summary statistics.</p>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="ols-multiple-linear-regression.html#cb276-1" tabindex="-1"></a><span class="fu">library</span>(fixest)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;fixest&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:np&#39;:
## 
##     se</code></pre>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb279-1"><a href="ols-multiple-linear-regression.html#cb279-1" tabindex="-1"></a>fe_reg1 <span class="ot">&lt;-</span> <span class="fu">feols</span>(y<span class="sc">~</span>x<span class="sc">|</span>fo<span class="sc">+</span>fu, dat_f)</span>
<span id="cb279-2"><a href="ols-multiple-linear-regression.html#cb279-2" tabindex="-1"></a><span class="fu">summary</span>(fe_reg1)</span></code></pre></div>
<pre><code>## OLS estimation, Dep. Var.: y
## Observations: 1,000 
## Fixed-effects: fo: 5,  fu: 2
## Standard-errors: Clustered (fo) 
##   Estimate Std. Error t value Pr(&gt;|t|)    
## x  1.10509   0.343178 3.22017 0.032275 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## RMSE: 9.14424     Adj. R2: 0.723405
##                 Within R2: 0.028953</code></pre>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb281-1"><a href="ols-multiple-linear-regression.html#cb281-1" tabindex="-1"></a><span class="do">## Compare Coefficients</span></span>
<span id="cb281-2"><a href="ols-multiple-linear-regression.html#cb281-2" tabindex="-1"></a><span class="fu">coef</span>( <span class="fu">lm</span>(y<span class="sc">~-</span><span class="dv">1</span><span class="sc">+</span>x<span class="sc">+</span>fo<span class="sc">+</span>fu, dat_f) )</span></code></pre></div>
<pre><code>##          x        fo0        fo1        fo2        fo3        fo4        fuB 
##   1.105091   9.721436  10.561255  14.988054  24.476528  44.258404 -23.500243</code></pre>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb283-1"><a href="ols-multiple-linear-regression.html#cb283-1" tabindex="-1"></a><span class="fu">fixef</span>(fe_reg1)</span></code></pre></div>
<pre><code>## $fo
##         0         1         2         3         4 
##  9.721436 10.561255 14.988054 24.476528 44.258404 
## 
## $fu
##         A         B 
##   0.00000 -23.50024 
## 
## attr(,&quot;class&quot;)
## [1] &quot;fixest.fixef&quot; &quot;list&quot;        
## attr(,&quot;references&quot;)
## fo fu 
##  0  1 
## attr(,&quot;exponential&quot;)
## [1] FALSE</code></pre>
<p><strong>Hansen Econometrics, Theorem 17.1:</strong> The fixed effects estimator of <span class="math inline">\(\beta\)</span> algebraically equals the dummy
variable estimator of <span class="math inline">\(\beta\)</span>. The two estimators have the same residuals.
<!--
In fact, if the fixed effect is ``fully unstructured then the only way to consistently estimate the coefficient $\beta$ is by an estimator which is invariant'' (Hansen Econometrics, p). 
--></p>
<p>Consistency is a great property, but only if the data generating process does in fact match the model. Many factor variables have effects that are not additively seperable.</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="ols-multiple-linear-regression.html#cb285-1" tabindex="-1"></a>reg1 <span class="ot">&lt;-</span> <span class="fu">feols</span>(y<span class="sc">~</span>x<span class="sc">|</span>fo<span class="sc">^</span>fu, dat_f)</span>
<span id="cb285-2"><a href="ols-multiple-linear-regression.html#cb285-2" tabindex="-1"></a><span class="fu">summary</span>(reg1)</span></code></pre></div>
<pre><code>## OLS estimation, Dep. Var.: y
## Observations: 1,000 
## Fixed-effects: fo^fu: 10
## Standard-errors: Clustered (fo^fu) 
##   Estimate Std. Error t value Pr(&gt;|t|)    
## x  1.07645   0.495719  2.1715 0.057972 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## RMSE: 3.28568     Adj. R2: 0.964145
##                 Within R2: 0.179712</code></pre>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="ols-multiple-linear-regression.html#cb287-1" tabindex="-1"></a>reg2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>x<span class="sc">*</span>fo<span class="sc">*</span>fu, dat_f)</span>
<span id="cb287-2"><a href="ols-multiple-linear-regression.html#cb287-2" tabindex="-1"></a><span class="fu">summary</span>(reg2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x * fo * fu, data = dat_f)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.0659 -1.4297 -0.0037  1.4415  9.1035 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  14.8533     0.6264  23.712  &lt; 2e-16 ***
## x             2.5657     0.1085  23.642  &lt; 2e-16 ***
## fo.L         27.2151     1.8003  15.117  &lt; 2e-16 ***
## fo.Q         11.1637     1.5756   7.085 2.65e-12 ***
## fo.C          2.3654     1.1707   2.020   0.0436 *  
## fo^4         -0.2224     0.8679  -0.256   0.7978    
## fuB         -14.8514     0.8317 -17.856  &lt; 2e-16 ***
## x:fo.L        4.7132     0.3110  15.155  &lt; 2e-16 ***
## x:fo.Q        1.6710     0.2724   6.134 1.24e-09 ***
## x:fo.C        0.4229     0.2038   2.075   0.0382 *  
## x:fo^4        0.1602     0.1520   1.054   0.2922    
## x:fuB        -2.5411     0.1442 -17.621  &lt; 2e-16 ***
## fo.L:fuB    -27.8179     2.3452 -11.862  &lt; 2e-16 ***
## fo.Q:fuB    -11.7010     2.0654  -5.665 1.93e-08 ***
## fo.C:fuB     -2.3772     1.5990  -1.487   0.1374    
## fo^4:fuB     -0.3064     1.2298  -0.249   0.8033    
## x:fo.L:fuB   -4.5425     0.4051 -11.213  &lt; 2e-16 ***
## x:fo.Q:fuB   -1.4899     0.3572  -4.171 3.30e-05 ***
## x:fo.C:fuB   -0.2991     0.2788  -1.073   0.2836    
## x:fo^4:fuB   -0.0339     0.2156  -0.157   0.8751    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.53 on 980 degrees of freedom
## Multiple R-squared:  0.9794, Adjusted R-squared:  0.979 
## F-statistic:  2449 on 19 and 980 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="ols-multiple-linear-regression.html#cb289-1" tabindex="-1"></a><span class="co">#reg2 &lt;- feols(y~x*fo*fu|fo^fu, dat_f)</span></span></code></pre></div>
<p>With <em>Random Effects</em>, the factor variable is modelled as coming from a distribution that is uncorrelated with the regressors. This is rarely used in economics today, and mostly included for historical reasons and a few cases where fixed effects cannot be estimates.</p>
<!-- 
> The labels "random effects" and "fixed effects" are misleading. These are labels which arose in the early literature and we are stuck with these labels today. In a previous era regressors were viewed as "fixed". Viewing the individual effect as an unobserved regressor leads to the label of the individual effect as "fixed". Today, we rarely refer to regressors as "fixed" when dealing with observational data. We view all variables as random. Consequently describing u i as "fixed" does not make much sense and it is hardly a contrast with the "random effect" label since under either assumption u i is treated as random. Once again, the labels are unfortunate but the key difference is whether u i is correlated with the regressors.
-->
</div>
<div id="coefficient-interpretation" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Coefficient Interpretation<a href="ols-multiple-linear-regression.html#coefficient-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Notice that we have gotten pretty far without actually trying to meaningfully interpret regression coefficients. That is because the above procedure will always give us number, regardless as to whether the true data generating process is linear or not. So, to be cautious, we have been interpretting the regression outputs while being agnostic as to how the data are generated. We now consider a special situation where we know the data are generated according to a linear process and are only uncertain about the parameter values.</p>
<p><em>If</em> the data generating process is
<span class="math display">\[
y=X\beta + \epsilon\\
\mathbb{E}[\epsilon | X]=0,
\]</span>
then we have a famous result that lets us attach a simple interpretation of OLS coefficients as unbiased estimates of the effect of X:
<span class="math display">\[
\hat{\beta} = (X&#39;X)^{-1}X&#39;y = (X&#39;X)^{-1}X&#39;(X\beta + \epsilon) = \beta + (X&#39;X)^{-1}X&#39;\epsilon\\
\mathbb{E}\left[ \hat{\beta} \right] = \mathbb{E}\left[ (X&#39;X)^{-1}X&#39;y \right] = \beta + (X&#39;X)^{-1}\mathbb{E}\left[ X&#39;\epsilon \right] = \beta
\]</span></p>
<p>Generate a simulated dataset with 30 observations and two exogenous variables. Assume the following relationship: <span class="math inline">\(y_{i} = \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \epsilon_i\)</span> where the variables and the error term are realizations of the following data generating processes (DGP):</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="ols-multiple-linear-regression.html#cb290-1" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">30</span></span>
<span id="cb290-2"><a href="ols-multiple-linear-regression.html#cb290-2" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">2</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb290-3"><a href="ols-multiple-linear-regression.html#cb290-3" tabindex="-1"></a></span>
<span id="cb290-4"><a href="ols-multiple-linear-regression.html#cb290-4" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(N, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb290-5"><a href="ols-multiple-linear-regression.html#cb290-5" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(N,<span class="dv">1</span>,.<span class="dv">7</span>)</span>
<span id="cb290-6"><a href="ols-multiple-linear-regression.html#cb290-6" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>,x1,x2)</span>
<span id="cb290-7"><a href="ols-multiple-linear-regression.html#cb290-7" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N,<span class="dv">0</span>,<span class="dv">3</span>)</span>
<span id="cb290-8"><a href="ols-multiple-linear-regression.html#cb290-8" tabindex="-1"></a>Y <span class="ot">&lt;-</span> X<span class="sc">%*%</span>B <span class="sc">+</span> e</span>
<span id="cb290-9"><a href="ols-multiple-linear-regression.html#cb290-9" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(Y,X)</span>
<span id="cb290-10"><a href="ols-multiple-linear-regression.html#cb290-10" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>x1<span class="sc">+</span>x2, <span class="at">data=</span>dat))</span></code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##  12.4728811   1.0597567  -0.7851197</code></pre>
<p>Simulate the distribution of coefficients under a correctly specified model. Interpret the average.</p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb292-1"><a href="ols-multiple-linear-regression.html#cb292-1" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">30</span></span>
<span id="cb292-2"><a href="ols-multiple-linear-regression.html#cb292-2" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">2</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb292-3"><a href="ols-multiple-linear-regression.html#cb292-3" tabindex="-1"></a></span>
<span id="cb292-4"><a href="ols-multiple-linear-regression.html#cb292-4" tabindex="-1"></a>Coefs <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">400</span>, <span class="cf">function</span>(sim){</span>
<span id="cb292-5"><a href="ols-multiple-linear-regression.html#cb292-5" tabindex="-1"></a>    x1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(N, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb292-6"><a href="ols-multiple-linear-regression.html#cb292-6" tabindex="-1"></a>    x2 <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(N,<span class="dv">1</span>,.<span class="dv">7</span>)</span>
<span id="cb292-7"><a href="ols-multiple-linear-regression.html#cb292-7" tabindex="-1"></a>    X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>,x1,x2)</span>
<span id="cb292-8"><a href="ols-multiple-linear-regression.html#cb292-8" tabindex="-1"></a>    e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N,<span class="dv">0</span>,<span class="dv">3</span>)</span>
<span id="cb292-9"><a href="ols-multiple-linear-regression.html#cb292-9" tabindex="-1"></a>    Y <span class="ot">&lt;-</span> X<span class="sc">%*%</span>B <span class="sc">+</span> e</span>
<span id="cb292-10"><a href="ols-multiple-linear-regression.html#cb292-10" tabindex="-1"></a>    dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(Y,x1,x2)</span>
<span id="cb292-11"><a href="ols-multiple-linear-regression.html#cb292-11" tabindex="-1"></a>    <span class="fu">coef</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>x1<span class="sc">+</span>x2, <span class="at">data=</span>dat))</span>
<span id="cb292-12"><a href="ols-multiple-linear-regression.html#cb292-12" tabindex="-1"></a>})</span>
<span id="cb292-13"><a href="ols-multiple-linear-regression.html#cb292-13" tabindex="-1"></a></span>
<span id="cb292-14"><a href="ols-multiple-linear-regression.html#cb292-14" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb292-15"><a href="ols-multiple-linear-regression.html#cb292-15" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>){</span>
<span id="cb292-16"><a href="ols-multiple-linear-regression.html#cb292-16" tabindex="-1"></a>    <span class="fu">hist</span>(Coefs[i,], <span class="at">xlab=</span><span class="fu">bquote</span>(beta[.(i)]), <span class="at">main=</span><span class="st">&#39;&#39;</span>)</span>
<span id="cb292-17"><a href="ols-multiple-linear-regression.html#cb292-17" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">v=</span><span class="fu">mean</span>(Coefs[i,]), <span class="at">col=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb292-18"><a href="ols-multiple-linear-regression.html#cb292-18" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">v=</span>B[i], <span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb292-19"><a href="ols-multiple-linear-regression.html#cb292-19" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="03-ROLS_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>Many economic phenomena are nonlinear, even when including potential transforms of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. Sometimes the linear model may still be a good or even great approximation (how good depends on the research question). In any case, you are safe to interpret your OLS coefficients as “conditional correlations”. For example, examine the distribution of coefficients under this mispecified model. Interpret the average.</p>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="ols-multiple-linear-regression.html#cb293-1" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">30</span></span>
<span id="cb293-2"><a href="ols-multiple-linear-regression.html#cb293-2" tabindex="-1"></a></span>
<span id="cb293-3"><a href="ols-multiple-linear-regression.html#cb293-3" tabindex="-1"></a>Coefs <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">600</span>, <span class="cf">function</span>(sim){</span>
<span id="cb293-4"><a href="ols-multiple-linear-regression.html#cb293-4" tabindex="-1"></a>    x1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(N, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb293-5"><a href="ols-multiple-linear-regression.html#cb293-5" tabindex="-1"></a>    x2 <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(N,<span class="dv">1</span>,.<span class="dv">7</span>)</span>
<span id="cb293-6"><a href="ols-multiple-linear-regression.html#cb293-6" tabindex="-1"></a>    e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N,<span class="dv">0</span>,<span class="dv">3</span>)</span>
<span id="cb293-7"><a href="ols-multiple-linear-regression.html#cb293-7" tabindex="-1"></a>    Y <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">*</span>x2 <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">log</span>(x1)<span class="sc">^</span>x2 <span class="sc">+</span> e</span>
<span id="cb293-8"><a href="ols-multiple-linear-regression.html#cb293-8" tabindex="-1"></a>    dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(Y,x1,x2)</span>
<span id="cb293-9"><a href="ols-multiple-linear-regression.html#cb293-9" tabindex="-1"></a>    <span class="fu">coef</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>x1<span class="sc">+</span>x2, <span class="at">data=</span>dat))</span>
<span id="cb293-10"><a href="ols-multiple-linear-regression.html#cb293-10" tabindex="-1"></a>})</span>
<span id="cb293-11"><a href="ols-multiple-linear-regression.html#cb293-11" tabindex="-1"></a></span>
<span id="cb293-12"><a href="ols-multiple-linear-regression.html#cb293-12" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb293-13"><a href="ols-multiple-linear-regression.html#cb293-13" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>){</span>
<span id="cb293-14"><a href="ols-multiple-linear-regression.html#cb293-14" tabindex="-1"></a>    <span class="fu">hist</span>(Coefs[i,],  <span class="at">xlab=</span><span class="fu">bquote</span>(beta[.(i)]), <span class="at">main=</span><span class="st">&#39;&#39;</span>)</span>
<span id="cb293-15"><a href="ols-multiple-linear-regression.html#cb293-15" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">v=</span><span class="fu">mean</span>(Coefs[i,]), <span class="at">col=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb293-16"><a href="ols-multiple-linear-regression.html#cb293-16" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="03-ROLS_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
</div>
<div id="diagnostics" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> Diagnostics<a href="ols-multiple-linear-regression.html#diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There’s little sense in getting great standard errors for a terrible model. Plotting your regression object a simple and easy step to help diagnose whether your model is in some way bad.</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="ols-multiple-linear-regression.html#cb294-1" tabindex="-1"></a>reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(Murder<span class="sc">~</span>Assault<span class="sc">+</span>UrbanPop, <span class="at">data=</span>USArrests)</span>
<span id="cb294-2"><a href="ols-multiple-linear-regression.html#cb294-2" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb294-3"><a href="ols-multiple-linear-regression.html#cb294-3" tabindex="-1"></a><span class="fu">plot</span>(reg)</span></code></pre></div>
<p><img src="03-ROLS_files/figure-html/unnamed-chunk-31-1.png" width="672" />
We now go through what these figures show, and then some additional</p>
<p><strong>Outliers</strong> The first plot examines outlier <span class="math inline">\(Y\)</span> and <span class="math inline">\(\hat{Y}\)</span>.</p>
<blockquote>
<p>``In our <span class="math inline">\(y_i = a + b x_i + e_i\)</span> regression, the residuals are, of course, <span class="math inline">\(e_i\)</span> – they reveal how much our fitted value <span class="math inline">\(\hat{y}_i = a + b x_i\)</span> differs from the observed <span class="math inline">\(y_i\)</span>. A point <span class="math inline">\((x_i ,y_i)\)</span> with a corresponding large residual is called an <em>outlier</em>. Say that you are interested in outliers because you somehow think that such points will exert undue <em>influence</em> on your estimates. Your feelings are generally right, but there are exceptions. A point might have a huge residual and yet not affect the estimated <span class="math inline">\(b\)</span> at all’’
Stata Press (2015) Base Reference Manual, Release 14, p. 2138.</p>
</blockquote>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="ols-multiple-linear-regression.html#cb295-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">fitted</span>(reg), <span class="fu">resid</span>(reg),<span class="at">col =</span> <span class="st">&quot;grey&quot;</span>, <span class="at">pch =</span> <span class="dv">20</span>,</span>
<span id="cb295-2"><a href="ols-multiple-linear-regression.html#cb295-2" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Fitted&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Residual&quot;</span>,</span>
<span id="cb295-3"><a href="ols-multiple-linear-regression.html#cb295-3" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Fitted versus Residuals&quot;</span>)</span>
<span id="cb295-4"><a href="ols-multiple-linear-regression.html#cb295-4" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="03-ROLS_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="ols-multiple-linear-regression.html#cb296-1" tabindex="-1"></a><span class="co"># car::outlierTest(reg)</span></span></code></pre></div>
<p>The third plot examines outlier <span class="math inline">\(X\)</span> via ``leverage’’</p>
<blockquote>
<p>“<span class="math inline">\((x_i ,y_i)\)</span> can be an outlier in another way – just as <span class="math inline">\(y_i\)</span> can be far from <span class="math inline">\(\hat{y}_i\)</span>, <span class="math inline">\(x_i\)</span> can be far from the center of mass of the other <span class="math inline">\(x\)</span>’s. Such an `outlier’ should interest you just as much as the more traditional outliers. Picture a scatterplot of <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span> with thousands of points in some sort of mass at the lower left of the graph and one point at the upper right of the graph. Now run a regression line through the points—the regression line will come close to the point at the upper right of the graph and may in fact, go through it. That is, this isolated point will not appear as an outlier as measured by residuals because its residual will be small. Yet this point might have a dramatic effect on our resulting estimates in the sense that, were you to delete the point, the estimates would change markedly. Such a point is said to have high <em>leverage</em>’’
Stata Press (2015) Base Reference Manual, Release 14, pp. 2138-39.</p>
</blockquote>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="ols-multiple-linear-regression.html#cb297-1" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">40</span></span>
<span id="cb297-2"><a href="ols-multiple-linear-regression.html#cb297-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">25</span>, <span class="fu">runif</span>(N<span class="dv">-1</span>,<span class="dv">3</span>,<span class="dv">8</span>))</span>
<span id="cb297-3"><a href="ols-multiple-linear-regression.html#cb297-3" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N,<span class="dv">0</span>,<span class="fl">0.4</span>)</span>
<span id="cb297-4"><a href="ols-multiple-linear-regression.html#cb297-4" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">3</span> <span class="sc">+</span> <span class="fl">0.6</span><span class="sc">*</span><span class="fu">sqrt</span>(x) <span class="sc">+</span> e</span>
<span id="cb297-5"><a href="ols-multiple-linear-regression.html#cb297-5" tabindex="-1"></a><span class="fu">plot</span>(y<span class="sc">~</span>x, <span class="at">pch=</span><span class="dv">16</span>, <span class="at">col=</span><span class="fu">grey</span>(.<span class="dv">5</span>,.<span class="dv">5</span>))</span>
<span id="cb297-6"><a href="ols-multiple-linear-regression.html#cb297-6" tabindex="-1"></a><span class="fu">points</span>(x[<span class="dv">1</span>],y[<span class="dv">1</span>], <span class="at">pch=</span><span class="dv">16</span>, <span class="at">col=</span><span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,.<span class="dv">5</span>))</span>
<span id="cb297-7"><a href="ols-multiple-linear-regression.html#cb297-7" tabindex="-1"></a></span>
<span id="cb297-8"><a href="ols-multiple-linear-regression.html#cb297-8" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(y<span class="sc">~</span>x), <span class="at">col=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb297-9"><a href="ols-multiple-linear-regression.html#cb297-9" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(y[<span class="sc">-</span><span class="dv">1</span>]<span class="sc">~</span>x[<span class="sc">-</span><span class="dv">1</span>]))</span></code></pre></div>
<p><img src="03-ROLS_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>See <a href="https://www.r-bloggers.com/2016/06/leverage-and-influence-in-a-nutshell/" class="uri">https://www.r-bloggers.com/2016/06/leverage-and-influence-in-a-nutshell/</a> for a good interactive explaination.</p>
<p>Leverage Vector: Distance within explanatory variables
<span class="math display">\[
H = [h_{1}, h_{2}, ...., h_{N}]
\]</span>
<span class="math inline">\(h_i\)</span> is the leverage of residual <span class="math inline">\(\hat{\epsilon_i}\)</span>.</p>
<p>Studentized residuals
<span class="math display">\[
r_i=\frac{\hat{\epsilon}_i}{s_{[i]}\sqrt{1-h_i}}
\]</span>
and <span class="math inline">\(s_{(i)}\)</span> the root mean squared error of a regression with the <span class="math inline">\(i\)</span>th observation removed.</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="ols-multiple-linear-regression.html#cb298-1" tabindex="-1"></a>reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>x)</span>
<span id="cb298-2"><a href="ols-multiple-linear-regression.html#cb298-2" tabindex="-1"></a><span class="fu">which.max</span>(<span class="fu">hatvalues</span>(reg))</span></code></pre></div>
<pre><code>## 1 
## 1</code></pre>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="ols-multiple-linear-regression.html#cb300-1" tabindex="-1"></a><span class="fu">which.max</span>(<span class="fu">rstandard</span>(reg))</span></code></pre></div>
<pre><code>## 34 
## 34</code></pre>
<p>The fourth plot further assesses outlier <span class="math inline">\(X\)</span> using “Cook’s Distance”. Cook’s Distance is defined as the sum of all the changes in the regression model when observation i is removed from.
<span class="math display">\[
D_{i} = \frac{\sum_{j} \left( \hat{y_j} - \hat{y_j}_{[i]} \right)^2 }{ p s^2 }
= \frac{[e_{i}]^2}{p s^2 } \frac{h_i}{(1-h_i)^2}\\
s^2 = \frac{\sum_{i} (e_{i})^2 }{n-K}
\]</span></p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="ols-multiple-linear-regression.html#cb302-1" tabindex="-1"></a><span class="fu">which.max</span>(<span class="fu">cooks.distance</span>(reg))</span></code></pre></div>
<pre><code>## 1 
## 1</code></pre>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="ols-multiple-linear-regression.html#cb304-1" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">influencePlot</span>(reg)</span></code></pre></div>
<p><img src="03-ROLS_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<pre><code>##       StudRes        Hat        CookD
## 1  -0.9955984 0.84676378 2.7393002666
## 2  -1.8010188 0.02955880 0.0466455398
## 20  0.1222142 0.04331284 0.0003471093
## 34  2.0728082 0.02661388 0.0540482030</code></pre>
<p>Note that we can also calculate <span class="math inline">\(H\)</span> directly from our OLS projection matrix <span class="math inline">\(\hat{P}\)</span>, since <span class="math inline">\(H=diag(\hat{P})\)</span> and
<span class="math display">\[
\hat{P}=X(X&#39;X)^{-1}X&#39;\\
\hat{\epsilon}=y-X\hat{\beta}=y-X(X&#39;X)^{-1}X&#39;y=y-\hat{P}y\\
\hat{P}y=X(X&#39;X)^{-1}X&#39;y=y-(y-X(X&#39;X)^{-1}X&#39;y)=y-\hat{\epsilon}=\hat{y}\\
\]</span></p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="ols-multiple-linear-regression.html#cb306-1" tabindex="-1"></a>Ehat <span class="ot">&lt;-</span> Y <span class="sc">-</span> X<span class="sc">%*%</span> Bhat</span>
<span id="cb306-2"><a href="ols-multiple-linear-regression.html#cb306-2" tabindex="-1"></a><span class="do">## Ehat</span></span>
<span id="cb306-3"><a href="ols-multiple-linear-regression.html#cb306-3" tabindex="-1"></a><span class="do">## resid(reg)</span></span>
<span id="cb306-4"><a href="ols-multiple-linear-regression.html#cb306-4" tabindex="-1"></a></span>
<span id="cb306-5"><a href="ols-multiple-linear-regression.html#cb306-5" tabindex="-1"></a>Pmat <span class="ot">&lt;-</span> X<span class="sc">%*%</span>XtXi<span class="sc">%*%</span><span class="fu">t</span>(X)</span>
<span id="cb306-6"><a href="ols-multiple-linear-regression.html#cb306-6" tabindex="-1"></a>Yhat <span class="ot">&lt;-</span> Pmat<span class="sc">%*%</span>Y</span>
<span id="cb306-7"><a href="ols-multiple-linear-regression.html#cb306-7" tabindex="-1"></a><span class="do">## Yhat</span></span>
<span id="cb306-8"><a href="ols-multiple-linear-regression.html#cb306-8" tabindex="-1"></a><span class="do">## predict(reg)</span></span></code></pre></div>
<p>There are many other diagnostics (which can often be written in terms of Cooks Distance or Vice Versa).</p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="ols-multiple-linear-regression.html#cb307-1" tabindex="-1"></a><span class="co"># Sall, J. (1990) Leverage plots for general linear hypotheses. American Statistician *44*, 308-315.</span></span>
<span id="cb307-2"><a href="ols-multiple-linear-regression.html#cb307-2" tabindex="-1"></a><span class="co"># car::leveragePlots(reg)</span></span></code></pre></div>
<p>(Welsch and Kuh. 1977; Belsley, Kuh, and Welsch. 1980) attempt to summarize the information in the leverage versus residual-squared plot into one DFITS statistic where <span class="math inline">\(DFITS &gt; 2\sqrt{{k}/{n}}\)</span> should be examined.
<span class="math display">\[
\text{DFITS}_i=r_i\sqrt{\frac{h_i}{1-h_i}}\\
\]</span></p>
<p>See also “dfbetas” and “covratio”</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="ols-multiple-linear-regression.html#cb308-1" tabindex="-1"></a><span class="co">#dfbetas(reg)</span></span>
<span id="cb308-2"><a href="ols-multiple-linear-regression.html#cb308-2" tabindex="-1"></a><span class="co">#dffits(reg)</span></span>
<span id="cb308-3"><a href="ols-multiple-linear-regression.html#cb308-3" tabindex="-1"></a><span class="co">#covratio(reg)</span></span>
<span id="cb308-4"><a href="ols-multiple-linear-regression.html#cb308-4" tabindex="-1"></a><span class="co">#hatvalues(reg)</span></span>
<span id="cb308-5"><a href="ols-multiple-linear-regression.html#cb308-5" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">influence.measures</span>(reg)<span class="sc">$</span>infmat)</span></code></pre></div>
<pre><code>##        dfb.1_       dfb.x      dffit     cov.r     cook.d        hat
## 1  1.81933871 -2.30556292 -2.3403705 6.5288900 2.73930027 0.84676378
## 2 -0.03264632 -0.12344088 -0.3143236 0.9187616 0.04664554 0.02955880
## 3  0.06071050  0.05065504  0.2218149 0.9843486 0.02408360 0.02637551
## 4 -0.11816436  0.05975574 -0.1482338 1.0464551 0.01106991 0.02985090
## 5 -0.21022355  0.12493916 -0.2425127 0.9997062 0.02889724 0.03403290
## 6  0.05465909  0.03214241  0.1732324 1.0196426 0.01495395 0.02589136</code></pre>
<p><strong>Normality</strong></p>
<p>The second plot examines whether the residuals are normally distributed. OLS point estimates do not depend on the normality of the residuals. (Good thing, because there’s no reason the residuals of economic phenomena should be so well behaved.) Many hypothesis tests of the regression estimates are, however, affected by the distribution of the residuals. For these reasons, you may be interested in assessing normality</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="ols-multiple-linear-regression.html#cb310-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb310-2"><a href="ols-multiple-linear-regression.html#cb310-2" tabindex="-1"></a><span class="fu">hist</span>(<span class="fu">resid</span>(reg), <span class="at">main=</span><span class="st">&#39;Histogram of Residuals&#39;</span>, <span class="at">border=</span><span class="cn">NA</span>)</span>
<span id="cb310-3"><a href="ols-multiple-linear-regression.html#cb310-3" tabindex="-1"></a></span>
<span id="cb310-4"><a href="ols-multiple-linear-regression.html#cb310-4" tabindex="-1"></a><span class="fu">qqnorm</span>(<span class="fu">resid</span>(reg), <span class="at">main=</span><span class="st">&quot;Normal Q-Q Plot of Residuals&quot;</span>, <span class="at">col=</span><span class="st">&quot;darkgrey&quot;</span>)</span>
<span id="cb310-5"><a href="ols-multiple-linear-regression.html#cb310-5" tabindex="-1"></a><span class="fu">qqline</span>(<span class="fu">resid</span>(reg), <span class="at">col=</span><span class="st">&quot;dodgerblue&quot;</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="03-ROLS_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="ols-multiple-linear-regression.html#cb311-1" tabindex="-1"></a><span class="fu">shapiro.test</span>(<span class="fu">resid</span>(reg))</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  resid(reg)
## W = 0.9588, p-value = 0.1524</code></pre>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb313-1"><a href="ols-multiple-linear-regression.html#cb313-1" tabindex="-1"></a><span class="co"># car::qqPlot(reg)</span></span></code></pre></div>
<p>Heterskedasticity may also matters for variance estimates. This is not shown in the plot, but you can run a simple test</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="ols-multiple-linear-regression.html#cb314-1" tabindex="-1"></a><span class="fu">library</span>(lmtest)</span>
<span id="cb314-2"><a href="ols-multiple-linear-regression.html#cb314-2" tabindex="-1"></a>lmtest<span class="sc">::</span><span class="fu">bptest</span>(reg)</span></code></pre></div>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  reg
## BP = 0.02105, df = 1, p-value = 0.8846</code></pre>
<p><strong>Collinearity</strong>
This is when one explanatory variable in a multiple linear regression model can be linearly predicted from the others with a substantial degree of accuracy. Coefficient estimates may change erratically in response to small changes in the model or the data. (In the extreme case where there are more variables than observations <span class="math inline">\(K&gt;\geq N\)</span>, <span class="math inline">\(X&#39;X\)</span> has an infinite number of solutions and is not invertible.)</p>
<p>To diagnose this, we can use the Variance Inflation Factor
<span class="math display">\[
VIF_{k}=\frac{1}{1-R^2_k},
\]</span>
where <span class="math inline">\(R^2_k\)</span> is the <span class="math inline">\(R^2\)</span> for the regression of <span class="math inline">\(X_k\)</span> on the other covariates <span class="math inline">\(X_{-k}\)</span> (a regression that does not involve the response variable Y)</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="ols-multiple-linear-regression.html#cb316-1" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">vif</span>(reg) </span>
<span id="cb316-2"><a href="ols-multiple-linear-regression.html#cb316-2" tabindex="-1"></a><span class="fu">sqrt</span>(car<span class="sc">::</span><span class="fu">vif</span>(reg)) <span class="sc">&gt;</span> <span class="dv">2</span> <span class="co"># problem?</span></span></code></pre></div>
</div>
<div id="linear-in-parameters" class="section level2 hasAnchor" number="8.5">
<h2><span class="header-section-number">8.5</span> Linear in Parameters<a href="ols-multiple-linear-regression.html#linear-in-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Data transformations can often improve model fit and still be estimated via OLS. This is because OLS only requires the model to be linear in the parameters. Under the assumptions of the model is correctly specified, the following table is how we can interpret the coefficients of the transformed data. (Note for small changes, <span class="math inline">\(\Delta ln(x) \approx \Delta x / x = \Delta x \% \cdot 100\)</span>.)</p>
<table>
<colgroup>
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
</colgroup>
<thead>
<tr class="header">
<th><em>Specification</em></th>
<th><em>Regressand</em></th>
<th><em>Regressor</em></th>
<th><em>Derivative</em></th>
<th><em>Interpretation (If True)</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>linear–linear</td>
<td><span class="math inline">\(y\)</span></td>
<td><span class="math inline">\(x\)</span></td>
<td><span class="math inline">\(\Delta y = \beta_1\cdot\Delta x\)</span></td>
<td>Change <span class="math inline">\(x\)</span> by one unit <span class="math inline">\(\rightarrow\)</span> change <span class="math inline">\(y\)</span> by <span class="math inline">\(\beta_1\)</span> units.</td>
</tr>
<tr class="even">
<td>log–linear</td>
<td><span class="math inline">\(ln(y)\)</span></td>
<td><span class="math inline">\(x\)</span></td>
<td><span class="math inline">\(\Delta y \% \cdot 100 \approx \beta_1 \cdot \Delta x\)</span></td>
<td>Change <span class="math inline">\(x\)</span> by one unit <span class="math inline">\(\rightarrow\)</span> change <span class="math inline">\(y\)</span> by <span class="math inline">\(100 \cdot \beta_1\)</span> percent.</td>
</tr>
<tr class="odd">
<td>linear–log</td>
<td><span class="math inline">\(y\)</span></td>
<td><span class="math inline">\(ln(x)\)</span></td>
<td><span class="math inline">\(\Delta y \approx \frac{\beta_1}{100}\cdot \Delta x \%\)</span></td>
<td>Change <span class="math inline">\(x\)</span> by one percent <span class="math inline">\(\rightarrow\)</span> change <span class="math inline">\(y\)</span> by <span class="math inline">\(\frac{\beta_1}{100}\)</span> units</td>
</tr>
<tr class="even">
<td>log–log</td>
<td><span class="math inline">\(ln(y)\)</span></td>
<td><span class="math inline">\(ln(x)\)</span></td>
<td><span class="math inline">\(\Delta y \% \approx \beta_1\cdot \Delta x \%\)</span></td>
<td>Change <span class="math inline">\(x\)</span> by one percent <span class="math inline">\(\rightarrow\)</span> change <span class="math inline">\(y\)</span> by <span class="math inline">\(\beta_1\)</span> percent</td>
</tr>
</tbody>
</table>
<p>Now recall from micro theory that an additively seperable and linear production function is referred to as ``perfect substitutes’‘. With a linear model and untranformed data, you have implicitly modelled the different regressors <span class="math inline">\(X\)</span> as perfect substitutes. Further recall that the’‘perfect substitutes’’ model is a special case of the constant elasticity of substitution production function. Here, we will build on <a href="http://dx.doi.org/10.2139/ssrn.3917397" class="uri">http://dx.doi.org/10.2139/ssrn.3917397</a>, and consider box-cox transforming both <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span>. Specifically, apply the box-cox transform of <span class="math inline">\(y\)</span> using parameter <span class="math inline">\(\lambda\)</span> and apply another box-cox transform to each <span class="math inline">\(x\)</span> using the same parameter <span class="math inline">\(\rho\)</span> so that
<span class="math display">\[
y^{(\lambda)}_{i} = \sum_{k}\beta_{k} x^{(\rho)}_{k,i} + \epsilon_{i}\\
y^{(\lambda)}_{i} =
\begin{cases}
\lambda^{-1}[ (y_i+1)^{\lambda}- 1] &amp; \lambda \neq 0 \\
log(y_i+1) &amp;  \lambda=0
\end{cases}.\\
x^{(\rho)}_{i} =
\begin{cases}
\rho^{-1}[ (x_i)^{\rho}- 1] &amp; \rho \neq 0 \\
log(x_{i}+1) &amp;  \rho=0
\end{cases}.
\]</span></p>
<p>Notice that this nests:</p>
<ul>
<li>linear-linear <span class="math inline">\((\rho=\lambda=1)\)</span>.</li>
<li>linear-log <span class="math inline">\((\rho=1, \lambda=0)\)</span>.</li>
<li>log-linear <span class="math inline">\((\rho=0, \lambda=1)\)</span>.</li>
<li>log-log <span class="math inline">\((\rho=\lambda=0)\)</span>.</li>
</ul>
<p>If <span class="math inline">\(\rho=\lambda\)</span>, we get the CES production function. This nests the ‘’perfect substitutes’’ linear-linear model (<span class="math inline">\(\rho=\lambda=1\)</span>) , the ‘’cobb-douglas’’ log-log model (<span class="math inline">\(\rho=\lambda=0\)</span>), and many others. We can define <span class="math inline">\(\lambda=\rho/\lambda&#39;\)</span> to be clear that this is indeed a CES-type transformation where</p>
<ul>
<li><span class="math inline">\(\rho \in (-\infty,1]\)</span> controls the “substitutability” of explanatory variables. E.g., <span class="math inline">\(\rho &lt;0\)</span> is ‘’complementary’’.</li>
<li><span class="math inline">\(\lambda\)</span> determines ‘’returns to scale’‘. E.g., <span class="math inline">\(\lambda&lt;1\)</span> is’‘decreasing returns’’.</li>
</ul>
<p>We compute the mean squared error in the original scale by inverting the predictions;
<span class="math display">\[
\widehat{y}_{i} =
\begin{cases}
[ \widehat{y^{(\lambda)}}_{i} \cdot \lambda ]^{1/\lambda} -1 &amp; \lambda  \neq 0 \\
exp( \widehat{y^{(\lambda)}}_{i}) -1 &amp;  \lambda=0
\end{cases}.
\]</span></p>
<p>It is easiest to optimize parameters in a 2-step procedure called `concentrated optimization’. We first solve for <span class="math inline">\(\widehat{\beta}(\rho,\lambda)\)</span> and compute the mean squared error <span class="math inline">\(MSE(\rho,\lambda)\)</span>. We then find the <span class="math inline">\((\rho,\lambda)\)</span> which minimizes <span class="math inline">\(MSE\)</span>.</p>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb317-1"><a href="ols-multiple-linear-regression.html#cb317-1" tabindex="-1"></a><span class="do">## Box-Cox Transformation Function</span></span>
<span id="cb317-2"><a href="ols-multiple-linear-regression.html#cb317-2" tabindex="-1"></a>bxcx <span class="ot">&lt;-</span> <span class="cf">function</span>( xy, rho){</span>
<span id="cb317-3"><a href="ols-multiple-linear-regression.html#cb317-3" tabindex="-1"></a>    <span class="cf">if</span> (rho <span class="sc">==</span> <span class="dv">0</span><span class="dt">L</span>) {</span>
<span id="cb317-4"><a href="ols-multiple-linear-regression.html#cb317-4" tabindex="-1"></a>      <span class="fu">log</span>(xy<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb317-5"><a href="ols-multiple-linear-regression.html#cb317-5" tabindex="-1"></a>    } <span class="cf">else</span> <span class="cf">if</span>(rho <span class="sc">==</span> <span class="dv">1</span><span class="dt">L</span>){</span>
<span id="cb317-6"><a href="ols-multiple-linear-regression.html#cb317-6" tabindex="-1"></a>      xy</span>
<span id="cb317-7"><a href="ols-multiple-linear-regression.html#cb317-7" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb317-8"><a href="ols-multiple-linear-regression.html#cb317-8" tabindex="-1"></a>      ((xy<span class="sc">+</span><span class="dv">1</span>)<span class="sc">^</span>rho <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">/</span>rho</span>
<span id="cb317-9"><a href="ols-multiple-linear-regression.html#cb317-9" tabindex="-1"></a>    }</span>
<span id="cb317-10"><a href="ols-multiple-linear-regression.html#cb317-10" tabindex="-1"></a>}</span>
<span id="cb317-11"><a href="ols-multiple-linear-regression.html#cb317-11" tabindex="-1"></a>bxcx_inv <span class="ot">&lt;-</span> <span class="cf">function</span>( xy, rho){</span>
<span id="cb317-12"><a href="ols-multiple-linear-regression.html#cb317-12" tabindex="-1"></a>    <span class="cf">if</span> (rho <span class="sc">==</span> <span class="dv">0</span><span class="dt">L</span>) {</span>
<span id="cb317-13"><a href="ols-multiple-linear-regression.html#cb317-13" tabindex="-1"></a>      <span class="fu">exp</span>(xy) <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb317-14"><a href="ols-multiple-linear-regression.html#cb317-14" tabindex="-1"></a>    } <span class="cf">else</span> <span class="cf">if</span>(rho <span class="sc">==</span> <span class="dv">1</span><span class="dt">L</span>){</span>
<span id="cb317-15"><a href="ols-multiple-linear-regression.html#cb317-15" tabindex="-1"></a>      xy</span>
<span id="cb317-16"><a href="ols-multiple-linear-regression.html#cb317-16" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb317-17"><a href="ols-multiple-linear-regression.html#cb317-17" tabindex="-1"></a>     (xy <span class="sc">*</span> rho <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">^</span>(<span class="dv">1</span><span class="sc">/</span>rho) <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb317-18"><a href="ols-multiple-linear-regression.html#cb317-18" tabindex="-1"></a>    }</span>
<span id="cb317-19"><a href="ols-multiple-linear-regression.html#cb317-19" tabindex="-1"></a>}</span>
<span id="cb317-20"><a href="ols-multiple-linear-regression.html#cb317-20" tabindex="-1"></a></span>
<span id="cb317-21"><a href="ols-multiple-linear-regression.html#cb317-21" tabindex="-1"></a><span class="do">## Which Variables</span></span>
<span id="cb317-22"><a href="ols-multiple-linear-regression.html#cb317-22" tabindex="-1"></a>reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(Murder<span class="sc">~</span>Assault<span class="sc">+</span>UrbanPop, <span class="at">data=</span>USArrests)</span>
<span id="cb317-23"><a href="ols-multiple-linear-regression.html#cb317-23" tabindex="-1"></a>X <span class="ot">&lt;-</span> USArrests[,<span class="fu">c</span>(<span class="st">&#39;Assault&#39;</span>,<span class="st">&#39;UrbanPop&#39;</span>)]</span>
<span id="cb317-24"><a href="ols-multiple-linear-regression.html#cb317-24" tabindex="-1"></a>Y <span class="ot">&lt;-</span> USArrests[,<span class="st">&#39;Murder&#39;</span>]</span>
<span id="cb317-25"><a href="ols-multiple-linear-regression.html#cb317-25" tabindex="-1"></a></span>
<span id="cb317-26"><a href="ols-multiple-linear-regression.html#cb317-26" tabindex="-1"></a><span class="do">## Simple Grid Search</span></span>
<span id="cb317-27"><a href="ols-multiple-linear-regression.html#cb317-27" tabindex="-1"></a><span class="do">## Which potential (Rho,Lambda) </span></span>
<span id="cb317-28"><a href="ols-multiple-linear-regression.html#cb317-28" tabindex="-1"></a>rl_df <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">rho=</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">2</span>,<span class="at">by=</span>.<span class="dv">5</span>),<span class="at">lambda=</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">2</span>,<span class="at">by=</span>.<span class="dv">5</span>))</span>
<span id="cb317-29"><a href="ols-multiple-linear-regression.html#cb317-29" tabindex="-1"></a></span>
<span id="cb317-30"><a href="ols-multiple-linear-regression.html#cb317-30" tabindex="-1"></a><span class="do">## Compute Mean Squared Error</span></span>
<span id="cb317-31"><a href="ols-multiple-linear-regression.html#cb317-31" tabindex="-1"></a><span class="do">## from OLS on Transformed Data</span></span>
<span id="cb317-32"><a href="ols-multiple-linear-regression.html#cb317-32" tabindex="-1"></a>errors <span class="ot">&lt;-</span> <span class="fu">apply</span>(rl_df,<span class="dv">1</span>,<span class="cf">function</span>(rl){</span>
<span id="cb317-33"><a href="ols-multiple-linear-regression.html#cb317-33" tabindex="-1"></a>    Xr <span class="ot">&lt;-</span> <span class="fu">bxcx</span>(X,rl[[<span class="dv">1</span>]])</span>
<span id="cb317-34"><a href="ols-multiple-linear-regression.html#cb317-34" tabindex="-1"></a>    Yr <span class="ot">&lt;-</span> <span class="fu">bxcx</span>(Y,rl[[<span class="dv">2</span>]])</span>
<span id="cb317-35"><a href="ols-multiple-linear-regression.html#cb317-35" tabindex="-1"></a>    Datr <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="at">Murder=</span>Yr,Xr)</span>
<span id="cb317-36"><a href="ols-multiple-linear-regression.html#cb317-36" tabindex="-1"></a>    Regr <span class="ot">&lt;-</span> <span class="fu">lm</span>(Murder<span class="sc">~</span>Assault<span class="sc">+</span>UrbanPop, <span class="at">data=</span>Datr)</span>
<span id="cb317-37"><a href="ols-multiple-linear-regression.html#cb317-37" tabindex="-1"></a>    Predr <span class="ot">&lt;-</span> <span class="fu">bxcx_inv</span>(<span class="fu">predict</span>(Regr),rl[[<span class="dv">2</span>]])</span>
<span id="cb317-38"><a href="ols-multiple-linear-regression.html#cb317-38" tabindex="-1"></a>    Resr  <span class="ot">&lt;-</span> (Y <span class="sc">-</span> Predr)</span>
<span id="cb317-39"><a href="ols-multiple-linear-regression.html#cb317-39" tabindex="-1"></a>    <span class="fu">return</span>(Resr)</span>
<span id="cb317-40"><a href="ols-multiple-linear-regression.html#cb317-40" tabindex="-1"></a>})</span>
<span id="cb317-41"><a href="ols-multiple-linear-regression.html#cb317-41" tabindex="-1"></a>rl_df<span class="sc">$</span>mse <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(errors<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb317-42"><a href="ols-multiple-linear-regression.html#cb317-42" tabindex="-1"></a></span>
<span id="cb317-43"><a href="ols-multiple-linear-regression.html#cb317-43" tabindex="-1"></a><span class="do">## Want Small MSE and Interpretable</span></span>
<span id="cb317-44"><a href="ols-multiple-linear-regression.html#cb317-44" tabindex="-1"></a><span class="do">## (-1,0,1,2 are Easy to interpretable)</span></span>
<span id="cb317-45"><a href="ols-multiple-linear-regression.html#cb317-45" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb317-46"><a href="ols-multiple-linear-regression.html#cb317-46" tabindex="-1"></a><span class="fu">ggplot</span>(rl_df, <span class="fu">aes</span>(rho, lambda, <span class="at">fill=</span><span class="fu">log</span>(mse) )) <span class="sc">+</span></span>
<span id="cb317-47"><a href="ols-multiple-linear-regression.html#cb317-47" tabindex="-1"></a>    <span class="fu">geom_tile</span>() <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">&#39;Mean Squared Error&#39;</span>) </span></code></pre></div>
<p><img src="03-ROLS_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="ols-multiple-linear-regression.html#cb318-1" tabindex="-1"></a><span class="do">## Which min</span></span>
<span id="cb318-2"><a href="ols-multiple-linear-regression.html#cb318-2" tabindex="-1"></a>rl0 <span class="ot">&lt;-</span> rl_df[<span class="fu">which.min</span>(rl_df<span class="sc">$</span>mse),<span class="fu">c</span>(<span class="st">&#39;rho&#39;</span>,<span class="st">&#39;lambda&#39;</span>)]</span>
<span id="cb318-3"><a href="ols-multiple-linear-regression.html#cb318-3" tabindex="-1"></a></span>
<span id="cb318-4"><a href="ols-multiple-linear-regression.html#cb318-4" tabindex="-1"></a><span class="do">## Which give NA?</span></span>
<span id="cb318-5"><a href="ols-multiple-linear-regression.html#cb318-5" tabindex="-1"></a><span class="do">## which(is.na(errors), arr.ind=T)</span></span>
<span id="cb318-6"><a href="ols-multiple-linear-regression.html#cb318-6" tabindex="-1"></a></span>
<span id="cb318-7"><a href="ols-multiple-linear-regression.html#cb318-7" tabindex="-1"></a><span class="do">## Plot</span></span>
<span id="cb318-8"><a href="ols-multiple-linear-regression.html#cb318-8" tabindex="-1"></a>Xr <span class="ot">&lt;-</span> <span class="fu">bxcx</span>(X,rl0[[<span class="dv">1</span>]])</span>
<span id="cb318-9"><a href="ols-multiple-linear-regression.html#cb318-9" tabindex="-1"></a>Yr <span class="ot">&lt;-</span> <span class="fu">bxcx</span>(Y,rl0[[<span class="dv">2</span>]])</span>
<span id="cb318-10"><a href="ols-multiple-linear-regression.html#cb318-10" tabindex="-1"></a>Datr <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="at">Murder=</span>Yr,Xr)</span>
<span id="cb318-11"><a href="ols-multiple-linear-regression.html#cb318-11" tabindex="-1"></a>Regr <span class="ot">&lt;-</span> <span class="fu">lm</span>(Murder<span class="sc">~</span>Assault<span class="sc">+</span>UrbanPop, <span class="at">data=</span>Datr)</span>
<span id="cb318-12"><a href="ols-multiple-linear-regression.html#cb318-12" tabindex="-1"></a>Predr <span class="ot">&lt;-</span> <span class="fu">bxcx_inv</span>(<span class="fu">predict</span>(Regr),rl0[[<span class="dv">2</span>]])</span>
<span id="cb318-13"><a href="ols-multiple-linear-regression.html#cb318-13" tabindex="-1"></a></span>
<span id="cb318-14"><a href="ols-multiple-linear-regression.html#cb318-14" tabindex="-1"></a>cols <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,.<span class="dv">5</span>), <span class="at">col=</span><span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,.<span class="dv">5</span>))</span>
<span id="cb318-15"><a href="ols-multiple-linear-regression.html#cb318-15" tabindex="-1"></a><span class="fu">plot</span>(Y, Predr, <span class="at">pch=</span><span class="dv">16</span>, <span class="at">col=</span>cols[<span class="dv">1</span>], <span class="at">ylab=</span><span class="st">&#39;Prediction&#39;</span>)</span>
<span id="cb318-16"><a href="ols-multiple-linear-regression.html#cb318-16" tabindex="-1"></a><span class="fu">points</span>(Y, <span class="fu">predict</span>(reg), <span class="at">pch=</span><span class="dv">16</span>, <span class="at">col=</span>cols[<span class="dv">2</span>])</span>
<span id="cb318-17"><a href="ols-multiple-linear-regression.html#cb318-17" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&#39;topleft&#39;</span>, <span class="at">pch=</span><span class="fu">c</span>(<span class="dv">16</span>), <span class="at">col=</span>cols, <span class="at">title=</span><span class="st">&#39;Rho,Lambda&#39;</span>,</span>
<span id="cb318-18"><a href="ols-multiple-linear-regression.html#cb318-18" tabindex="-1"></a>    <span class="at">legend=</span><span class="fu">c</span>(  <span class="fu">paste0</span>(rl0, <span class="at">collapse=</span><span class="st">&#39;,&#39;</span>),<span class="st">&#39;1,1&#39;</span>) )</span></code></pre></div>
<p><img src="03-ROLS_files/figure-html/unnamed-chunk-42-2.png" width="672" /></p>
<p>Note that the default hypothesis testing procedures do not account for you trying out different transformations. Specification searches deflate standard errors and are a major source for false discoveries.</p>
</div>
<div id="more-literature" class="section level2 hasAnchor" number="8.6">
<h2><span class="header-section-number">8.6</span> More Literature<a href="ols-multiple-linear-regression.html#more-literature" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For OLS, see</p>
<ul>
<li><a href="https://bookdown.org/josiesmith/qrmbook/linear-estimation-and-minimizing-error.html" class="uri">https://bookdown.org/josiesmith/qrmbook/linear-estimation-and-minimizing-error.html</a></li>
<li><a href="https://www.econometrics-with-r.org/4-lrwor.html" class="uri">https://www.econometrics-with-r.org/4-lrwor.html</a></li>
<li><a href="https://www.econometrics-with-r.org/6-rmwmr.html" class="uri">https://www.econometrics-with-r.org/6-rmwmr.html</a></li>
<li><a href="https://www.econometrics-with-r.org/7-htaciimr.html" class="uri">https://www.econometrics-with-r.org/7-htaciimr.html</a></li>
<li><a href="https://bookdown.org/ripberjt/labbook/bivariate-linear-regression.html" class="uri">https://bookdown.org/ripberjt/labbook/bivariate-linear-regression.html</a></li>
<li><a href="https://bookdown.org/ripberjt/labbook/multivariable-linear-regression.html" class="uri">https://bookdown.org/ripberjt/labbook/multivariable-linear-regression.html</a></li>
<li><a href="https://online.stat.psu.edu/stat462/node/137/" class="uri">https://online.stat.psu.edu/stat462/node/137/</a></li>
<li><a href="https://book.stat420.org/" class="uri">https://book.stat420.org/</a></li>
<li>Hill, Griffiths &amp; Lim (2007), Principles of Econometrics, 3rd ed., Wiley, S. 86f.</li>
<li>Verbeek (2004), A Guide to Modern Econometrics, 2nd ed., Wiley, S. 51ff.</li>
<li>Asteriou &amp; Hall (2011), Applied Econometrics, 2nd ed., Palgrave MacMillan, S. 177ff.</li>
<li><a href="https://online.stat.psu.edu/stat485/lesson/11/" class="uri">https://online.stat.psu.edu/stat485/lesson/11/</a></li>
</ul>
<p>For fixed effects, see</p>
<ul>
<li><a href="https://www.econometrics-with-r.org/10-rwpd.html" class="uri">https://www.econometrics-with-r.org/10-rwpd.html</a></li>
<li><a href="https://bookdown.org/josiesmith/qrmbook/topics-in-multiple-regression.html" class="uri">https://bookdown.org/josiesmith/qrmbook/topics-in-multiple-regression.html</a></li>
<li><a href="https://bookdown.org/ripberjt/labbook/multivariable-linear-regression.html" class="uri">https://bookdown.org/ripberjt/labbook/multivariable-linear-regression.html</a></li>
<li><a href="https://www.princeton.edu/~otorres/Panel101.pdf" class="uri">https://www.princeton.edu/~otorres/Panel101.pdf</a></li>
<li><a href="https://www.stata.com/manuals13/xtxtreg.pdf" class="uri">https://www.stata.com/manuals13/xtxtreg.pdf</a></li>
</ul>
<p>Diagnostics</p>
<ul>
<li><a href="https://book.stat420.org/model-diagnostics.html#leverage" class="uri">https://book.stat420.org/model-diagnostics.html#leverage</a></li>
<li><a href="https://socialsciences.mcmaster.ca/jfox/Books/RegressionDiagnostics/index.html" class="uri">https://socialsciences.mcmaster.ca/jfox/Books/RegressionDiagnostics/index.html</a></li>
<li><a href="https://bookdown.org/ripberjt/labbook/diagnosing-and-addressing-problems-in-linear-regression.html" class="uri">https://bookdown.org/ripberjt/labbook/diagnosing-and-addressing-problems-in-linear-regression.html</a></li>
<li>Belsley, D. A., Kuh, E., and Welsch, R. E. (1980). Regression Diagnostics: Identifying influential data and sources of collinearity. Wiley. <a href="https://doi.org/10.1002/0471725153" class="uri">https://doi.org/10.1002/0471725153</a></li>
<li>Fox, J. D. (2020). Regression diagnostics: An introduction (2nd ed.). SAGE. <a href="https://dx.doi.org/10.4135/9781071878651" class="uri">https://dx.doi.org/10.4135/9781071878651</a></li>
</ul>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>
To derive OLS coefficients in Matrix form, see
* <a href="https://jrnold.github.io/intro-methods-notes/ols-in-matrix-form.html" class="uri">https://jrnold.github.io/intro-methods-notes/ols-in-matrix-form.html</a>
* <a href="https://www.fsb.miamioh.edu/lij14/411_note_matrix.pdf" class="uri">https://www.fsb.miamioh.edu/lij14/411_note_matrix.pdf</a>
* <a href="https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf" class="uri">https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf</a><a href="ols-multiple-linear-regression.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>This is done using t-values <span class="math display">\[\hat{t}_{j} = \frac{\hat{\beta}_j - \beta_{0} }{\hat{\sigma}_{\hat{\beta}_j}}\]</span>. Under some additional assumptions <span class="math inline">\(\hat{t}_{j} \sim t_{n-K}\)</span>.<a href="ols-multiple-linear-regression.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="endogeneity-issues.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["index.pdf", "index.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
