{
  "hash": "aeabaa08e913a8f47ec53bf17eae6198",
  "result": {
    "engine": "knitr",
    "markdown": "# Introduction to Data Analysis\n\n# First Steps\n***\n\n## Why Program in R?\n\nYou should program your statistical analysis, and we will cover some of the basics of how to do this in R. You also want your work to be replicable\n\n* *Replicable*: someone collecting new data comes to the same results.\n* *Reproducibile*: someone reusing your data comes to the same results.\n\nYou can read more about the distinction in many places, including\n\n* https://www.annualreviews.org/doi/10.1146/annurev-psych-020821-114157\n* https://nceas.github.io/sasap-training/materials/reproducible_research_in_r_fairbanks/\n\nWe focus on R because it is good for complex stats, concise figures, and coherent organization. It is built and developed by applied statisticians for statistics, and used by many in academia and industry. For students, think about labor demand and what may be good for getting a job. Do some of your own research to best understand how much to invest.\n\n\nMy main sell to you is that being reproducible is in your own self-interest.\n\n#### **An example workflow**. {-}\n\n**First Steps...**\n\n*Step 1:* Some ideas and data about how variable $X_{1}$ affects $Y_{1}$\n\n* You copy some data into a spreadsheet, manually aggregate\n* do some calculations and tables the same spreadsheet\n* some other analysis from here and there, using this software and that.\n\n*Step 2:* Pursuing the lead for a week or two\n\n* you extend your dataset with more observations\n* copy in a spreadsheet data, manually aggregate\n* do some more calculations and tables, same as before\n\n**A Little Way Down the Road ...**\n\n*1 month later*, someone asks about another factor: $X_{2}$\n\n* you download some other type of data\n* You repeat <u>Step 2</u> with some data on $X_{2}$.\n* The details from your \"point and click\" method are a bit fuzzy.\n* It takes a little time, but you successfully redo the analysis.\n\n*4 months later*, someone asks about another factor: $X_{3}\\to Y_{1}$\n\n* You again repeat <u>Step 2</u> with some data on $X_{3}$.\n* You're pretty sure none of tables your tried messed up the order of the rows or columns.\n* It takes more time and effort. The data processing was not transparent, but you eventually redo the analysis.\n\n*6 months later*, you want to explore: $X_{2} \\to Y_{2}$.\n\n* You found out Excel had some bugs in it's statistical calculations (see e.g., https://biostat.app.vumc.org/wiki/pub/Main/TheresaScott/StatsInExcel.TAScot.handout.pdf). You now use a new version of the spreadsheet\n* You're not sure you merged everything correctly. After much time and effort, most (but not all) of the numbers match exactly.\n\n*2 years later*, your boss wants you to replicate: $\\{ X_{1}, X_{2}, X_{3} \\} \\to Y_{1}$\n\n* A rival has proposed something new. Their idea doesn't actually make any sense, but their figures and statistics look better.\n* You don't even use that computer anymore and a collaborator who handled the data on $X_{2}$ has moved on.\n\n\n#### **An alternative workflow**. {-}\n\nSuppose you decided to code what you did beginning with Step 2.\n\n*It does not take much time to update or replicate your results*.\n\n* Your computer runs for 2 hours and reproduces the figures and tables.\n* You also rewrote your big calculations to use multiple cores, this took two hours to do but saved 6 hours *each time* you rerun your code.\n* You add some more data. It adds almost no time to see whether much has changed.\n\n*Your results are transparent and easier to build on*.\n\n* You see the exact steps you took and found an error\n  * glad you found it before sending it out! See https://retractionwatch.com/ and https://econjwatch.org/\n  * Google \"worst excell errors\" and note the frequency they arise from copy/paste via the \"point-and-click\" approach. Future economists should also read https://core.ac.uk/download/pdf/300464894.pdf.\n* You try out a new plot you found in *The Visual Display of Quantitative Information*, by Edward Tufte.\n  * It's not a standard plot, but google answers most of your questions.\n  * Tutorials help avoid bad practices, such as plotting 2D data as a 3D object (see e.g., https://clauswilke.com/dataviz/no-3d.html).\n* You try out an obscure statistical approach that's hot in your field.\n  * it doesn't make the report, but you have some confidence that candidate issue isn't a big problem\n\n\n## First Steps\n\n#### Install R {-}\n\nFirst Install [R](https://cloud.r-project.org/).\nThen Install [Rstudio](https://www.rstudio.com/products/rstudio/download/).\n\nFor help setting up, see any of the following links\n\n* https://learnr-examples.shinyapps.io/ex-setup-r/\n* https://rstudio-education.github.io/hopr/starting.html\n* https://a-little-book-of-r-for-bioinformatics.readthedocs.io/en/latest/src/installr.html\n* https://cran.r-project.org/doc/manuals/R-admin.html\n* https://courses.edx.org/courses/UTAustinX/UT.7.01x/3T2014/56c5437b88fa43cf828bff5371c6a924/\n* https://owi.usgs.gov/R/training-curriculum/installr/\n* https://www.earthdatascience.org/courses/earth-analytics/document-your-science/setup-r-rstudio/\n\nFor Fedora users, note that you need to first enable the repo and then install\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nsudo dnf install 'dnf-command(copr)'\nsudo dnf copr enable iucar/rstudio\nsudo dnf install rstudio-desktop\n```\n:::\n\n\n\n*Make sure you have the latest version of R and Rstudio for class.* If not, then reinstall. \n\n#### Interfacing with R {-}\n\nRstudio is perhaps the easiest to get going with. (There are other GUI's.)\n\nIn Rstudio, there are 4 panes. (If you do not see 4, click \"file > new file > R script\" on the top left of the toolbar.)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](./Figures_Manual/Rstudio.svg)\n:::\n:::\n\n\nThe top left pane is where you write your code. For example, type\n\n::: {.cell}\n\n```{.r .cell-code}\n1+1\n```\n:::\n\n\nThe pane below is where your code is executed. \nKeep you mouse on the same line as your code, and then click \"Run\". \nYou should see\n```\n> 1+1\n[1] 2\n```\nIf you click \"Run\" again, you should see that same output printed again.\n\n\n**Multiple Lines**\n\nYou should add comments to your codes, and you do this with hashtags. For example\n\n::: {.cell}\n\n```{.r .cell-code}\n# This is my first comment!\n1+1 # The simplest calculation I could think of\n```\n:::\n\nYou can execute each line one-at-a-time. \n\nOr you can highlight them both, to take advantage of how R executes commands line by line.\n\n\n**Assignment**\n\nYou can create \"variables\" that store values. For example,\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 1 # Make your first variable\nx + 1 # The simplest calculation I could think of\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 23 #Another example\nx + 1\n```\n:::\n\n\nYour variables must be defined in order to use them. Otherwise you get an error. For example,\n\n::: {.cell}\n\n```{.r .cell-code}\ny + 1\n```\n:::\n\n\n**Scripting**\n\n* Save your `R Script` file as *My_First_Script.R*\n* re-run your entire script \n\nAs we proceed, you can see both my source scripts and output like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 1 # Make your first variable\nx + 1 # The simplest calculation I could think of\n## [1] 2\n```\n:::\n\n\nAs you work through the material, make sure to both execute and save your scripts (with comments).\n\n\n## Further Reading\n\nThere are many good and free programming materials online.\n\nThe most common tasks can be found https://github.com/rstudio/cheatsheets/blob/main/rstudio-ide.pdf\n\nSome of my programming examples originally come from https://r4ds.had.co.nz/ and I recommend https://intro2r.com. I have also used online material from many places over the years, including\n\n* https://cran.r-project.org/doc/manuals/R-intro.html\n* R Graphics Cookbook, 2nd edition. Winston Chang. 2021. https://r-graphics.org/\n* R for Data Science. H. Wickham and G. Grolemund. 2017. https://r4ds.had.co.nz/index.html\n* An Introduction to R. W. N. Venables, D. M. Smith, R Core Team. 2017. https://colinfay.me/intro-to-r/\n* Introduction to R for Econometrics. Kieran Marray. https://bookdown.org/kieranmarray/intro_to_r_for_econometrics/\n* Wollschläger, D. (2020). Grundlagen der Datenanalyse mit R: eine anwendungsorientierte Einführung. http://www.dwoll.de/rexrepos/\n* Spatial Data Science with R: Introduction to R. Robert J. Hijmans. 2021. https://rspatial.org/intr/index.html\n\nWhat we cover in this primer should be enough to get you going. But there are also many good yet free-online tutorials and courses. \n\n* https://www.econometrics-with-r.org/1.2-a-very-short-introduction-to-r-and-rstudio.html\n* https://rafalab.github.io/dsbook/\n* https://moderndive.com/foreword.html\n* https://rstudio.cloud/learn/primers/1.2\n* https://cran.r-project.org/manuals.html\n* https://stats.idre.ucla.edu/stat/data/intro_r/intro_r_interactive_flat.html\n* https://cswr.nrhstat.org/app-r\n\n\nFor more on why to program in R, see\n\n* http://www.r-bloggers.com/the-reproducibility-crisis-in-science-and-prospects-for-r/\n* http://fmwww.bc.edu/GStat/docs/pointclick.html\n* https://github.com/qinwf/awesome-R\\#reproducible-research\n* A Guide to Reproducible Code in Ecology and Evolution\n* https://biostat.app.vumc.org/wiki/pub/Main/TheresaScott/ReproducibleResearch.TAScott.handout.pdf\n\n\n\n# Mathematics\n***\n\n\n\n## Objects\n\nIn R: scalars, vectors, and matrices are different kinds of \"objects\". \n\nThese objects are used extensively in data analysis\n\n* scalars: summary statistics (average household income).\n* vectors: single variables in data sets (the household income of each family in Vancouver).\n* matrices: multiple variables in data sets (the age and education level of every person in class).\n\n\nVectors are probably your most common object in R, but we will start with scalars (which are treated as a special case in R).\n\n\n#### **Scalars**. {-}\n\nMake your first scalar\n\n::: {.cell}\n\n```{.r .cell-code}\nxs <- 2 # Make your first scalar\nxs  # Print the scalar\n## [1] 2\n```\n:::\n\n\nPerform simple calculations and see how R is doing the math for you\n\n::: {.cell}\n\n```{.r .cell-code}\nxs + 2\n## [1] 4\nxs*2 # Perform and print a simple calculation\n## [1] 4\n(xs+1)^2 # Perform and print a simple calculation\n## [1] 9\nxs + NA # often used for missing values\n## [1] NA\n```\n:::\n\n\nNow change `xs`, predict what will happen, then re-run the code.\n\n#### **Vectors**. {-}\n\nMake Your First Vector\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(0,1,3,10,6) # Your First Vector\nx # Print the vector\n## [1]  0  1  3 10  6\nx[2] # Print the 2nd Element; 1\n## [1] 1\nx+2 # Print simple calculation; 2,3,5,8,12\n## [1]  2  3  5 12  8\nx*2\n## [1]  0  2  6 20 12\nx^2\n## [1]   0   1   9 100  36\n```\n:::\n\n\nApply Mathematical calculations \"elementwise\"\n\n::: {.cell}\n\n```{.r .cell-code}\nx+x\n## [1]  0  2  6 20 12\nx*x\n## [1]   0   1   9 100  36\nx^x\n## [1] 1.0000e+00 1.0000e+00 2.7000e+01 1.0000e+10 4.6656e+04\n```\n:::\n\n\nSee that scalars are vectors\n\n::: {.cell}\n\n```{.r .cell-code}\nc(1)\n## [1] 1\n\n1:7\n## [1] 1 2 3 4 5 6 7\nseq(0,1,by=.1)\n##  [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n```\n:::\n\n\n\n#### **Matrices**. {-}\n\nMatrices are also common objects\n\n::: {.cell}\n\n```{.r .cell-code}\nx1 <- c(1,4,9)\nx2 <- c(3,0,2)\nx_mat <- rbind(x1, x2)\n\nx_mat       # Print full matrix\n##    [,1] [,2] [,3]\n## x1    1    4    9\n## x2    3    0    2\nx_mat[2,]   # Print Second Row\n## [1] 3 0 2\nx_mat[,2]   # Print Second Column\n## x1 x2 \n##  4  0\nx_mat[2,2]  # Print Element in Second Column and Second Row\n## x2 \n##  0\n```\n:::\n\n\nThere are elementwise calculations\n\n::: {.cell}\n\n```{.r .cell-code}\nx_mat+2\n##    [,1] [,2] [,3]\n## x1    3    6   11\n## x2    5    2    4\nx_mat*2\n##    [,1] [,2] [,3]\n## x1    2    8   18\n## x2    6    0    4\nx_mat^2\n##    [,1] [,2] [,3]\n## x1    1   16   81\n## x2    9    0    4\n\nx_mat + x_mat\n##    [,1] [,2] [,3]\n## x1    2    8   18\n## x2    6    0    4\nx_mat*x_mat\n##    [,1] [,2] [,3]\n## x1    1   16   81\n## x2    9    0    4\nx_mat^x_mat\n##    [,1] [,2]      [,3]\n## x1    1  256 387420489\n## x2   27    1         4\n```\n:::\n\n\nAnd you can also use matrix algebra\n\n::: {.cell}\n\n```{.r .cell-code}\nx_mat1 <- matrix(2:7,2,3)\nx_mat1\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n\nx_mat2 <- matrix(4:-1,2,3)\nx_mat2\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\ntcrossprod(x_mat1, x_mat2) #x_mat1 %*% t(x_mat2)\n##      [,1] [,2]\n## [1,]   16    4\n## [2,]   22    7\n\ncrossprod(x_mat1, x_mat2)\n##      [,1] [,2] [,3]\n## [1,]   17    7   -3\n## [2,]   31   13   -5\n## [3,]   45   19   -7\n# x_mat1 * x_mat2\n```\n:::\n\n\n\n##  Functions\n\nFunctions are applied to objects\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define a function that adds two to any vector\nadd_2 <- function(input_vector) {\n    output_vector <- input_vector + 2 # new object defined locally \n    return(output_vector) # return new object \n}\n# Apply that function to a vector\nx <- c(0,1,3,10,6)\nadd_2(x)\n## [1]  2  3  5 12  8\n\n# notice 'output_vector' is not available here\n```\n:::\n\n\nThere are many many generalizations\n\n::: {.cell}\n\n```{.r .cell-code}\nadd_vec <- function(input_vector1, input_vector2) {\n    output_vector <- input_vector1 + input_vector2\n    return(output_vector)\n}\nadd_vec(x,3)\n## [1]  3  4  6 13  9\nadd_vec(x,x)\n## [1]  0  2  6 20 12\n\nsum_squared <- function(x1, x2) {\n    y <- (x1 + x2)^2\n    return(y)\n}\n\nsum_squared(1, 3)\n## [1] 16\nsum_squared(x, 2)\n## [1]   4   9  25 144  64\nsum_squared(x, NA) \n## [1] NA NA NA NA NA\nsum_squared(x, x)\n## [1]   0   4  36 400 144\nsum_squared(x, 2*x)\n## [1]   0   9  81 900 324\n```\n:::\n\n\nFunctions can take functions as arguments\n\n::: {.cell}\n\n```{.r .cell-code}\nfun_of_seq <- function(f){\n    x <- seq(1,3, length.out=12)\n    y <- f(x)\n    return(y)\n}\n\nfun_of_seq(mean)\n## [1] 2\n\nfun_of_seq(sd)\n## [1] 0.6555548\n```\n:::\n\n\nYou can apply functions to matrices\n\n::: {.cell}\n\n```{.r .cell-code}\nsum_squared(x_mat, x_mat)\n##    [,1] [,2] [,3]\n## x1    4   64  324\n## x2   36    0   16\n\n# Apply function to each matrix row\ny <- apply(x_mat, 1, sum)^2 \n# ?apply  #checks the function details\ny - sum_squared(x, x) # tests if there are any differences\n## [1]  196   21  160 -375   52\n```\n:::\n\n\nThere are many possible functions you can apply\n\n::: {.cell}\n\n```{.r .cell-code}\n# Return Y-value with minimum absolute difference from 3\nabs_diff_y <- abs( y - 3 ) \nabs_diff_y # is this the luckiest number?\n##  x1  x2 \n## 193  22\n\n#min(abs_diff_y)\n#which.min(abs_diff_y)\ny[ which.min(abs_diff_y) ]\n## x2 \n## 25\n```\n:::\n\n\nThere are also some useful built in functions\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- matrix(c(1:3,2*(1:3)),byrow=TRUE,ncol=3)\nm\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    2    4    6\n\n# normalize rows\nm/rowSums(m)\n##           [,1]      [,2] [,3]\n## [1,] 0.1666667 0.3333333  0.5\n## [2,] 0.1666667 0.3333333  0.5\n\n# normalize columns\nt(t(m)/colSums(m))\n##           [,1]      [,2]      [,3]\n## [1,] 0.3333333 0.3333333 0.3333333\n## [2,] 0.6666667 0.6666667 0.6666667\n\n# de-mean rows\nsweep(m,1,rowMeans(m), '-')\n##      [,1] [,2] [,3]\n## [1,]   -1    0    1\n## [2,]   -2    0    2\n\n# de-mean columns\nsweep(m,2,colMeans(m), '-')\n##      [,1] [,2] [,3]\n## [1,] -0.5   -1 -1.5\n## [2,]  0.5    1  1.5\n```\n:::\n\n\n\n#### **Loops**. {-}\n\nApplying the same function over and over again\n\n::: {.cell}\n\n```{.r .cell-code}\nexp_vector <- vector(length=3)\nfor(i in 1:3){\n    exp_vector[i] <- exp(i)\n}\n\n# Compare\nexp_vector\n## [1]  2.718282  7.389056 20.085537\nc( exp(1), exp(2), exp(3))\n## [1]  2.718282  7.389056 20.085537\n```\n:::\n\n\nstore complicated example\n\n::: {.cell}\n\n```{.r .cell-code}\ncomplicate_fun <- function(i, j=0){\n    x <- i^(i-1)\n    y <- x + mean( j:i )\n    z <- log(y)/i\n    return(z)\n}\ncomplicated_vector <- vector(length=10)\nfor(i in 1:10){\n    complicated_vector[i] <- complicate_fun(i)\n}\n```\n:::\n\n\nrecursive loop\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- vector(length=4)\nx[1] <- 1\nfor(i in 2:4){\n    x[i] <- (x[i-1]+1)^2\n}\nx\n## [1]   1   4  25 676\n```\n:::\n\n\n<!---\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# for loop in a function\nr_fun <- function(n){\n    x <- rep(1,n)\n    for(i in 2:length(x) ){\n        x[i] <- (x[i-1]+1)^2\n    }\n    return(x)\n}\nr_fun(5)\n## [1]      1      4     25    676 458329\n```\n:::\n\n--->\n\n\n#### **Logic**. {-}\n\nBasic Logic\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(1,2,3,NA)\nx > 2\n## [1] FALSE FALSE  TRUE    NA\nx==2\n## [1] FALSE  TRUE FALSE    NA\n\nany(x==2)\n## [1] TRUE\nall(x==2)\n## [1] FALSE\n2 %in% x\n## [1] TRUE\n\nis.numeric(x)\n## [1] TRUE\nis.na(x)\n## [1] FALSE FALSE FALSE  TRUE\n```\n:::\n\n\nThe \"&\" and \"|\" commands are logical calculations that compare vectors to the left and right.\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 1:3\nis.numeric(x) & (x < 2)\n## [1]  TRUE FALSE FALSE\nis.numeric(x) | (x < 2)\n## [1] TRUE TRUE TRUE\n\nif(length(x) >= 5 & x[5] > 12) print(\"ok\")\n```\n:::\n\n\nAdvanced Logic.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 1:10\ncut(x, 4)\n##  [1] (0.991,3.25] (0.991,3.25] (0.991,3.25] (3.25,5.5]   (3.25,5.5]  \n##  [6] (5.5,7.75]   (5.5,7.75]   (7.75,10]    (7.75,10]    (7.75,10]   \n## Levels: (0.991,3.25] (3.25,5.5] (5.5,7.75] (7.75,10]\nsplit(x, cut(x, 4))\n## $`(0.991,3.25]`\n## [1] 1 2 3\n## \n## $`(3.25,5.5]`\n## [1] 4 5\n## \n## $`(5.5,7.75]`\n## [1] 6 7\n## \n## $`(7.75,10]`\n## [1]  8  9 10\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxs <- split(x, cut(x, 4))\nsapply(xs, mean)\n## (0.991,3.25]   (3.25,5.5]   (5.5,7.75]    (7.75,10] \n##          2.0          4.5          6.5          9.0\n\n# shortcut\naggregate(x, list(cut(x,4)), mean)\n##        Group.1   x\n## 1 (0.991,3.25] 2.0\n## 2   (3.25,5.5] 4.5\n## 3   (5.5,7.75] 6.5\n## 4    (7.75,10] 9.0\n```\n:::\n\n\nsee https://bookdown.org/rwnahhas/IntroToR/logical.html\n\n\n## Arrays\n\nArrays are generalization of matrices. They are often used in spatial econometrics, and are a very efficient way to store numeric data with the same dimensions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- array(data = 1:24, dim = c(2, 3, 4))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\na\n\na[1, , , drop = FALSE]  # Row 1\n#a[, 1, , drop = FALSE]  # Column 1\n#a[, , 1, drop = FALSE]  # Layer 1\n\na[ 1, 1,  ]  # Row 1, column 1\n#a[ 1,  , 1]  # Row 1, \"layer\" 1\n#a[  , 1, 1]  # Column 1, \"layer\" 1\na[1 , 1, 1]  # Row 1, column 1, \"layer\" 1\n```\n:::\n\n\nApply extends to arrays\n\n::: {.cell}\n\n```{.r .cell-code}\napply(a, 1, mean)    # Row means\n## [1] 12 13\napply(a, 2, mean)    # Column means\n## [1] 10.5 12.5 14.5\napply(a, 3, mean)    # \"Layer\" means\n## [1]  3.5  9.5 15.5 21.5\napply(a, 1:2, mean)  # Row/Column combination \n##      [,1] [,2] [,3]\n## [1,]   10   12   14\n## [2,]   11   13   15\n```\n:::\n\n\nOuter products yield arrays\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(1,2,3)\nx_mat1 <- outer(x, x) # x %o% x\nx_mat1\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    2    4    6\n## [3,]    3    6    9\nis.array(x_mat) # Matrices are arrays\n## [1] TRUE\n\nx_mat2 <- matrix(6:1,2,3)\nouter(x_mat2, x)\n## , , 1\n## \n##      [,1] [,2] [,3]\n## [1,]    6    4    2\n## [2,]    5    3    1\n## \n## , , 2\n## \n##      [,1] [,2] [,3]\n## [1,]   12    8    4\n## [2,]   10    6    2\n## \n## , , 3\n## \n##      [,1] [,2] [,3]\n## [1,]   18   12    6\n## [2,]   15    9    3\n# outer(x_mat2, matrix(x))\n# outer(x_mat2, t(x))\n# outer(x_mat1, x_mat2)\n```\n:::\n\n\n# Data\n***\n\n## Types\n\n#### **Basic Types**. {-}\nThe two basic types of data are *cardinal* and *factor* data. We can further distinguish between whether cardinal data are discrete or continuous. We can also further distinguish between whether factor data are ordered or not\n\n* *cardinal*: the difference between elements always mean the same thing. \n    * discrete: E.g., 2-1=3-2.\n    * continuous: E.g., 2.11-1.4444=3.11-2.4444\n* *factor*: the difference between elements does not always mean the same thing.\n    * ordered: E.g., First place - Second place ?? Second place - Third place.\n    * unordered (categorical): E.g., A - B ????\n\n\nHere are some examples\n\n::: {.cell}\n\n```{.r .cell-code}\nd1d <- 1:3 # Cardinal data (Discrete)\nd1d\n## [1] 1 2 3\n#class(d1d)\n\nd1c <- c(1.1, 2/3, 3) # Cardinal data (Continuous)\nd1c\n## [1] 1.1000000 0.6666667 3.0000000\n#class(d1c)\n\nd2o <- factor(c('A','B','C'), ordered=T) # Factor data (Ordinal)\nd2o\n## [1] A B C\n## Levels: A < B < C\n#class(d2o)\n\nd2c <- factor(c('Leipzig','Los Angeles','Logan'), ordered=F) # Factor data (Categorical)\nd2c\n## [1] Leipzig     Los Angeles Logan      \n## Levels: Leipzig Logan Los Angeles\n#class(d2c)\n```\n:::\n\n\nNote that for theoretical analysis, the types are sometimes grouped differently as\n\n* continuous (continuous cardinal data)\n* discrete (discrete cardinal, ordered factor, and unordered factor data)\n\n#### **Other Types**. {-} \nR also allows for more unstructured data types, such as *strings* and *lists*. You often combine all of the different data types into a single dataset called a *data.frame*\n\n::: {.cell}\n\n```{.r .cell-code}\nc('hello world', 'hi mom')  # character strings\n## [1] \"hello world\" \"hi mom\"\n\nlist(d1c, d2c)  # lists\n## [[1]]\n## [1] 1.1000000 0.6666667 3.0000000\n## \n## [[2]]\n## [1] Leipzig     Los Angeles Logan      \n## Levels: Leipzig Logan Los Angeles\n\n# data.frames: your most common data type\n    # matrix of different data-types\n    # well-ordered lists\nd0 <- data.frame(y=d1c, x=d2c)\nd0\n##           y           x\n## 1 1.1000000     Leipzig\n## 2 0.6666667 Los Angeles\n## 3 3.0000000       Logan\n```\n:::\n\n\nNote that strings are encounter in a variety of settings, and you often have to format them after reading them into R.^[We will not cover the statistical analysis of text in this course, but strings are amenable to statistical analysis.]\n\n::: {.cell}\n\n```{.r .cell-code}\n# Strings\npaste( 'hi', 'mom')\n## [1] \"hi mom\"\npaste( c('hi', 'mom'), collapse='--')\n## [1] \"hi--mom\"\n\nlist(d1c, c('hello world'),\n    list(d1d, list('...inception...'))) # lists\n## [[1]]\n## [1] 1.1000000 0.6666667 3.0000000\n## \n## [[2]]\n## [1] \"hello world\"\n## \n## [[3]]\n## [[3]][[1]]\n## [1] 1 2 3\n## \n## [[3]][[2]]\n## [[3]][[2]][[1]]\n## [1] \"...inception...\"\n\nkingText <- \"The king infringes the law on playing curling.\"\ngsub(pattern=\"ing\", replacement=\"\", kingText)\n## [1] \"The k infres the law on play curl.\"\n# advanced usage\n#gsub(\"[aeiouy]\", \"_\", kingText)\n#gsub(\"([[:alpha:]]{3})ing\\\\b\", \"\\\\1\", kingText) \n```\n:::\n\nSee \n\n* https://meek-parfait-60672c.netlify.app/docs/M1_R-intro_03_text.html\n* https://raw.githubusercontent.com/rstudio/cheatsheets/main/regex.pdf\n\n\n\n\n## Densities and Distributions\n\n#### **Initial Data Inspection**. {-}\nRegardless of the data types you have, you typically begin by inspecting your data by examining the first few observations.\n \nConsider, for example, historical data on crime in the US.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(USArrests)\n##            Murder Assault UrbanPop Rape\n## Alabama      13.2     236       58 21.2\n## Alaska       10.0     263       48 44.5\n## Arizona       8.1     294       80 31.0\n## Arkansas      8.8     190       50 19.5\n## California    9.0     276       91 40.6\n## Colorado      7.9     204       78 38.7\n\n# Check NA values\nsum(is.na(x))\n## [1] 0\n```\n:::\n\n\nTo further examine a particular variable, we look at its distribution. In what follows, we will denote the data for a single variable as $\\{X_{i}\\}_{i=1}^{N}$, where there are $N$ observations and $X_{i}$ is the value of the $i$th one.\n\n#### **Histogram Density Estimate**. {-}\nThe histogram divides the range of $\\{X_{i}\\}_{i=1}^{N}$ into $L$ exclusive bins of equal-width $h=[\\text{max}(X_{i}) - \\text{min}(X_{i})]/L$, and counts the number of observations within each bin. We often scale the counts to interpret the numbers as a density. Mathematically, for an exclusive bin with midpoint $x$, we compute\n\\begin{eqnarray}\n\\widehat{f}_{HIST}(x) &=& \\frac{  \\sum_{i}^{N} \\mathbf{1}\\left( X_{i} \\in \\left[x-\\frac{h}{2}, x+\\frac{h}{2} \\right) \\right) }{N h}.\n\\end{eqnarray}\nWe compute $\\widehat{f}_{HIST}(x)$ for each $x \\in \\left\\{ \\frac{\\ell h}{2} + \\text{min}(X) \\right\\}_{\\ell=1}^{L}$.\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(USArrests$Murder, freq=F,\n    border=NA, main='', xlab='Murder Arrests')\n# Raw Observations\nrug(USArrests$Murder, col=grey(0,.5))\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\n\nNote that if you your data are factor data, or discrete cardinal data, you can directly plot the counts.\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- floor(USArrests$Murder) #Discretized\nplot(table(x), xlab='Murder Rate (Discrete)', ylab='Count')\n```\n:::\n\n\n#### **Empirical *Cumulative* Distribution Function**. {-}\nThe ECDF counts the proportion of observations whose values $X_{i}$ are less than $x$; \n\\begin{eqnarray}\n\\widehat{F}_{ECDF}(x) = \\frac{1}{N} \\sum_{i}^{N} \\mathbf{1}(X_{i} \\leq x) \n\\end{eqnarray}\nfor each unique value of $x$ in the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nF_murder <- ecdf(USArrests$Murder)\n# proportion of murders < 10\nF_murder(10)\n## [1] 0.7\n# proportion of murders < x, for all x\nplot(F_murder, main='', xlab='Murder Arrests',\n    pch=16, col=grey(0,.5))\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\n#### **Boxplots**. {-}\nBoxplots summarize the distribution of data using *quantiles*: the $q$th quantile is the value where $q$ percent of the data are below and ($1-q$) percent are above.\n\n* The \"median\" is the point where half of the data has lower values and the other half has higher values.\n* The \"lower quartile\" is the point where 25% of the data has lower values and the other 75% has higher values.\n* The \"min\" is the smallest value (or largest negative value if there are any) where 0% of the data has lower values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- USArrests$Murder\n\n# quantiles\nmedian(x)\n## [1] 7.25\nrange(x)\n## [1]  0.8 17.4\nquantile(x, probs=c(0,.25,.5))\n##    0%   25%   50% \n## 0.800 4.075 7.250\n\n# deciles are quantiles\nquantile(x, probs=seq(0,1, by=.1))\n##    0%   10%   20%   30%   40%   50%   60%   70%   80%   90%  100% \n##  0.80  2.56  3.38  4.75  6.00  7.25  8.62 10.12 12.12 13.32 17.40\n```\n:::\n\n\nTo compute quantiles, we sort the observations from smallest to largest as $X_{(1)}, X_{(2)},... X_{(N)}$, and then compute quantiles as $X_{ (q*N) }$. Note that $(q*N)$ is rounded and there are different ways to break ties.\n\n::: {.cell}\n\n```{.r .cell-code}\nxo <- sort(x)\nxo\n##  [1]  0.8  2.1  2.1  2.2  2.2  2.6  2.6  2.7  3.2  3.3  3.4  3.8  4.0  4.3  4.4\n## [16]  4.9  5.3  5.7  5.9  6.0  6.0  6.3  6.6  6.8  7.2  7.3  7.4  7.9  8.1  8.5\n## [31]  8.8  9.0  9.0  9.7 10.0 10.4 11.1 11.3 11.4 12.1 12.2 12.7 13.0 13.2 13.2\n## [46] 14.4 15.4 15.4 16.1 17.4\n\n# median\nxo[length(xo)*.5]\n## [1] 7.2\nquantile(x, probs=.5, type=4)\n## 50% \n## 7.2\n\n# min\nxo[1]\n## [1] 0.8\nmin(xo)\n## [1] 0.8\nquantile(xo,probs=0)\n##  0% \n## 0.8\n```\n:::\n\n\nThe boxplot shows the median (solid black line) and interquartile range ($IQR=$ upper quartile $-$ lower quartile; filled box),^[Technically, the upper and lower ``hinges'' use two different versions of the first and third quartile. See https://stackoverflow.com/questions/40634693/lower-and-upper-quartiles-in-boxplot-in-r] as well extreme values as outliers beyond the $1.5\\times IQR$ (points beyond whiskers).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(USArrests$Murder, main='', ylab='Murder Arrests')\n# Raw Observations\nstripchart(USArrests$Murder,\n    pch='-', col=grey(0,.5), cex=2,\n    vert=T, add=T)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-45-1.png){width=672}\n:::\n:::\n\n\n## Joint Distributions\n\nScatterplots are used frequently to summarize the joint relationship between two variables. They can be enhanced in several ways. As a default, use semi-transparent points so as not to hide any points (and perhaps see if your observations are concentrated anywhere).\n\nYou can also add regression lines (and confidence intervals), although I will defer this until later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(Murder~UrbanPop, USArrests, pch=16, col=grey(0.,.5))\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-46-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Add the line of best fit for pooled data\n#reg <- lm(Murder~UrbanPop, data=USArrests)\n#abline(reg, lty=2)\n```\n:::\n\n\n#### **Marginal Distributions**.{-}\nYou can also show the distributions of each variable along each axis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setup Plot\nlayout( matrix(c(2,0,1,3), ncol=2, byrow=TRUE),\n    widths=c(9/10,1/10), heights=c(1/10,9/10))\n\n# Scatterplot\npar(mar=c(4,4,1,1))\nplot(Murder~UrbanPop, USArrests, pch=16, col=rgb(0,0,0,.5))\n\n# Add Marginals\npar(mar=c(0,4,1,1))\nxhist <- hist(USArrests$UrbanPop, plot=FALSE)\nbarplot(xhist$counts, axes=FALSE, space=0, border=NA)\n\npar(mar=c(4,0,1,1))\nyhist <- hist(USArrests$Murder, plot=FALSE)\nbarplot(yhist$counts, axes=FALSE, space=0, horiz=TRUE, border=NA)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-47-1.png){width=672}\n:::\n:::\n\n\n\n## Conditional Distributions\n\nIt is easy to show how distributions change according to a third variable using data splits. E.g., \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tailored Histogram \nylim <- c(0,8)\nxbks <-  seq(min(USArrests$Murder)-1, max(USArrests$Murder)+1, by=1)\n\n# Also show more information\n# Split Data by Urban Population above/below mean\npop_mean <- mean(USArrests$UrbanPop)\nmurder_lowpop <- USArrests[USArrests$UrbanPop< pop_mean,'Murder']\nmurder_highpop <- USArrests[USArrests$UrbanPop>= pop_mean,'Murder']\ncols <- c(low=rgb(0,0,1,.75), high=rgb(1,0,0,.75))\n\npar(mfrow=c(1,2))\nhist(murder_lowpop,\n    breaks=xbks, col=cols[1],\n    main='Urban Pop >= Mean', font.main=1,\n    xlab='Murder Arrests',\n    border=NA, ylim=ylim)\n\nhist(murder_highpop,\n    breaks=xbks, col=cols[2],\n    main='Urban Pop < Mean', font.main=1,\n    xlab='Murder Arrests',\n    border=NA, ylim=ylim)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-48-1.png){width=672}\n:::\n:::\n\n\nIt is sometimes it is preferable to show the ECDF instead. And you can glue various combinations together to convey more information all at once\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\n# Full Sample Density\nhist(USArrests$Murder, \n    main='Density Function Estimate', font.main=1,\n    xlab='Murder Arrests',\n    breaks=xbks, freq=F, border=NA)\n\n# Split Sample Distribution Comparison\nF_lowpop <- ecdf(murder_lowpop)\nplot(F_lowpop, col=cols[1],\n    pch=16, xlab='Murder Arrests',\n    main='Distribution Function Estimates',\n    font.main=1, bty='n')\nF_highpop <- ecdf(murder_highpop)\nplot(F_highpop, add=T, col=cols[2], pch=16)\n\nlegend('bottomright', col=cols,\n    pch=16, bty='n', inset=c(0,.1),\n    title='% Urban Pop.',\n    legend=c('Low (<= Mean)','High (>= Mean)'))\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-49-1.png){width=672}\n:::\n:::\n\n\n\nYou can also split data into grouped boxplots in the same way\n\n::: {.cell}\n\n```{.r .cell-code}\nlayout( t(c(1,2,2)))\nboxplot(USArrests$Murder, main='',\n    xlab='All Data', ylab='Murder Arrests')\n\n# K Groups with even spacing\nK <- 3\nUSArrests$UrbanPop_Kcut <- cut(USArrests$UrbanPop,K)\nKcols <- hcl.colors(K,alpha=.5)\nboxplot(Murder~UrbanPop_Kcut, USArrests,\n    main='', col=Kcols,\n    xlab='Urban Population', ylab='')\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-50-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# 4 Groups with equal numbers of observations\n#Qcuts <- c(\n#    '0%'=min(USArrests$UrbanPop)-10*.Machine$double.eps,\n#    quantile(USArrests$UrbanPop, probs=c(.25,.5,.75,1)))\n#USArrests$UrbanPop_cut <- cut(USArrests$UrbanPop, Qcuts)\n#boxplot(Murder~UrbanPop_cut, USArrests, col=hcl.colors(4,alpha=.5))\n```\n:::\n\n\nYou can also use size, color, and shape to further distinguish different conditional relationships.\n\n::: {.cell}\n\n```{.r .cell-code}\n# High Assault Areas\nassault_high <- USArrests$Assault > median(USArrests$Assault)\ncols <- ifelse(assault_high, rgb(1,0,0,.5), rgb(0,0,1,.5))\n\n# Scatterplot\n# Show High Assault Areas via 'cex=' or 'pch='\n# Could further add regression lines for each data split\nplot(Murder~UrbanPop, USArrests, pch=16, col=cols)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n:::\n\n\n\n\n## Further Reading \n\nFor plotting histograms and marginals, see \n\n* https://www.r-bloggers.com/2011/06/example-8-41-scatterplot-with-marginal-histograms/\n* https://r-graph-gallery.com/histogram.html\n* https://r-graph-gallery.com/74-margin-and-oma-cheatsheet.html \n* https://jtr13.github.io/cc21fall2/tutorial-for-scatter-plot-with-marginal-distribution.html.\n\n\n\n# Random Variables\n\nIn the last section we computed a distribution given the data, whereas now we generate data given the distribution.\n\nRandom variables are vectors that are generated from a known Cumulative Distribution Function. They are a sample from a potentially infinite population with\n\n* A *sample space* which refers to the set of all possible outcomes.\n* A *probability* for each particular set of outcomes, which is the proportion that those outcomes occur in the long run.\n\n\nThere are many [probability distributions](https://en.wikipedia.org/wiki/List_of_probability_distributions), and the most common ones are [easily accessible](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Distributions.html). But there are only two basic types of sample spaces: discrete (encompassing cardinal-discrete, factor-ordered, and factor-unordered data) and continuous, which lead to two types of random variables.\n\n\n## Discrete\nThe random variable can take one of several values in a set. E.g., any number in $\\{1,2,3,...\\}$ or any letter in $\\{A,B,C,...\\}$.\n\n#### **Bernoulli.** {-}\nThink of a Coin Flip: Heads=1 or Tails=0, with Prob. Heads = 1/2. In general, the probability can vary.\n\n$$X \\in \\{0,1\\} \\\\\nProb(X=0) = p \\\\\nProb(X=1) = 1-p.\n$$\n\n**Example**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrbinom(1, 1, 0.5) # 1 Flip\n## [1] 0\nrbinom(4, 1, 0.5) # 4 Flips\n## [1] 1 0 0 1\nx0 <- rbinom(600, 1, 0.5)\n\n# Setup Plot\nlayout(matrix(c(1, 2), nrow = 1), widths = c(4, 1))\n\n# Plot Cumulative Averages\nx0_t <- seq_len(length(x0))\nx0_mt <- cumsum(x0)/x0_t\npar(mar=c(4,4,1,4))\nplot(x0_t, x0_mt, type='l',\n    ylab='Cumulative Average',\n    xlab='Flip #', \n    ylim=c(0,1), \n    lwd=2)\npoints(x0_t, x0, col=grey(0,.5),\n    pch=16, cex=.2)\n\n# Plot Long run proportions\npar(mar=c(4,4,1,1))\nx_hist <- hist(x0, breaks=50, plot=F)\nbarplot(x_hist$count, axes=FALSE,\n    space=0, horiz=TRUE, border=NA)\naxis(1)\naxis(2)\nmtext('Overall Count', 2, line=2.5)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-52-1.png){width=672}\n:::\n:::\n\n\n#### **Discrete Uniform.** {-}\nNumbers with equal probability\n$$X \\in \\{1,...N\\} \\\\\nProb(X=1) = Prob(X=2) = ... = Prob(X=N) = 1/N.\n$$\n\n**Example**.\nSuppose $N=4$. \n\nThe probability of a value smaller that $3$ is $P(X\\leq 3)=1/4 + 1/4 + 1/4 = 3/4$.\n\nThe probability of a value larger than $3$ is $1-P(X<3)=1/4$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# sample(1:4, 1, replace=T, prob=rep(1/4,4) ) # sample of 1\nx1 <- sample(1:4, 2000, replace=T, prob=rep(1/4,4))\nhist(x1, breaks=50, border=NA, main=NA, freq=T)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-53-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Alternative Plot\n#props <- table(x1)\n#barplot(props, ylim = c(0, 0.35), ylab = \"Proportion\", xlab = \"Value\")\n#abline(h = 1/4, lty = 2)\n```\n:::\n\n\n#### **Multinoulli (aka Categorical).** {-}\nNumbers 1,...4 (or letters A,..D) with unequal probabilities.\n\n**Example**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx1 <- sample(1:4, 2000, replace=T, prob=c(3,4,1,2)/10) # sample of 2000,\nhist(x1, breaks=50, border=NA, main=NA, freq=T)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n:::\n\n\n## Continuous\nThe random variable can take one value out of an uncountably infinite number. The probability of any individual point is zero.\n\n\n#### **Continuous Uniform.** {-}\nAny number between $[0,1]$, allowing for any number of decimal points, with every number having the same probability.\n$$F(x) = Prob(X \\leq x) = \\begin{cases} \n0 & x < 0\nx & x \\in [0,1]\n1 & x > 1.\n\\end{cases}\n$$\n\nThe probability of a value smaller that $0.25$ is $F(0.25)=0.25$.\n\nThe probability of a value larger than $0.25$ is $1-F(0.25)=0.75$.\n\nThe probability of a value being exactly $0.25$ is $Prob(X=0.25)=0$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrunif(3) # 3 draws\n## [1] 0.7399838 0.9390029 0.8052767\nx2 <- runif(2000)\nhist(x2, breaks=20, border=NA, main=NA, freq=F)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-55-1.png){width=672}\n:::\n:::\n\n\n\n\n#### **Beta.** {-}\nAny number between $[0,1]$ with unequal probabilities. \n\n::: {.cell}\n\n```{.r .cell-code}\nx3 <- rbeta(2000,2,2) ## two shape parameters\nhist(x3, breaks=20, border=NA, main=NA, freq=F)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-56-1.png){width=672}\n:::\n:::\n\n\nSee the underlying probabilities\n\n::: {.cell}\n\n```{.r .cell-code}\nf_25 <- dbeta(.25, 2, 2)\n\nx <- seq(0,1,by=.1)\nfx <- dbeta(x, 2, 2)\nrbind(x, fx)\n##    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]\n## x     0 0.10 0.20 0.30 0.40  0.5 0.60 0.70 0.80  0.90     1\n## fx    0 0.54 0.96 1.26 1.44  1.5 1.44 1.26 0.96  0.54     0\n```\n:::\n\n\nAlso see that the [Beta](https://en.wikipedia.org/wiki/Beta_distribution) distribution can take many different shapes.\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(0,1,by=.01)\npars <- expand.grid( c(.5,1,2), c(.5,1,2) )\npar(mfrow=c(3,3))\napply(pars, 1, function(p){\n    fx <- dbeta( x,p[1], p[2])\n    plot(x, fx, type='l', xlim=c(0,1), ylim=c(0,4), lwd=2)\n    #hist(rbeta(2000, p[1], p[2]), breaks=50, border=NA, main=NA, freq=F)\n})\ntitle('Beta densities', outer=T, line=-1)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-58-1.png){width=672}\n:::\n:::\n\n\n\n#### **Normal (Gaussian).** {-}\nAny number between $(\\infty,\\infty)$, with a bell shaped probabilities. \nThe distribution is complex, and not written here, but we will encounter it again and again.\n\n::: {.cell}\n\n```{.r .cell-code}\nrnorm(3) # 3 draws\n## [1] 0.7709328 0.8335778 1.0314164\n\nx3 <- rnorm(2000) \nhist(x3, breaks=20, border=NA, main=NA, freq=F)\n\nx <- seq(-10,10,by=.025)\nfx <- dnorm(x)\nlines(x, fx)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-59-1.png){width=672}\n:::\n:::\n\n\nWe might further distinguish types of random variables based on whether their maximum value is theoretically finite or infinite. We will return to the theory behind probability distributions in a later chapter. \n\n\n\n## Drawing Samples\n\n#### **Using Computers**. {-}\nThere are several ways to computationally generate random variables from a probability distribution. Perhaps the most common one is ``inverse sampling'' for continuous random variables. \n\nContinuous random variables have an associated quantile function: $Q_{X}(p)$, which is the inverse of the CDF: the $x$ value where $p$ percent of the data fall below it. (Recall that the median is the value $x$ where $50\\%$ of the data fall below $x$, for example.) To generate a random variable, first sample $p$ from a uniform distribution and then find the associated quantile.\n\nHere is an in-depth example of drawing random variables from the [Dagum distribution](https://en.wikipedia.org/wiki/Dagum_distribution)\n\n::: {.cell}\n\n```{.r .cell-code}\n# Quantile Function (VGAM::qdagum)\nqdagum <- function(p, scale=1, shape1.a, shape2.p) {\n  # Quantile function (theoretically derived from the CDF)\n  ans <- scale * (expm1(-log(p) / shape2.p))^(-1 / shape1.a)\n  # Special known cases\n  ans[p == 0] <- 0\n  ans[p == 1] <- Inf\n  # Checks\n  ans[p < 0] <- NaN\n  ans[p > 1] <- NaN\n  if(scale <= 0 | shape1.a <= 0 | shape2.p <= 0){ ans <- ans*NaN }\n  # Return\n  return(ans)\n}\n\n# Generate Random Variables (VGAM::rdagum)\nrdagum <-function(n, scale=1, shape1.a, shape2.p){\n    p <- runif(n) # generate random quantile probabilities\n    x <- qdagum(p, scale=scale, shape1.a=shape1.a, shape2.p=shape2.p) #find the inverses\n    return(x)\n}\n\n# Example\nset.seed(123)\nx <- rdagum(3000,1,3,1)\n\n# Empirical Distribution\nFx_hat <- ecdf(x)\nplot(Fx_hat, lwd=2, xlim=c(0,5), main='')\n\n# Two Examples of generating a random variable\np <- c(.25, .9)\ncols <- c(2,4)\nQx_hat <- quantile(x, p)\nsegments(Qx_hat,p,-10,p, col=cols)\nsegments(Qx_hat,p,Qx_hat,0, col=cols)\nmtext( round(Qx_hat,2), 1, at=Qx_hat, col=cols)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-60-1.png){width=672}\n:::\n:::\n\n\n#### **In the Real World**. {-}\n\n\n# Statistics\n***\n\nWe often summarize distributions with *statistics*: functions of data. The most basic way to do this is with `summary`, whose values can all be calculated individually. (E.g., the \"mean\" computes the [sum of all values] divided by [number of values].) There are *many* other statistics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary( runif(1000))\n##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n## 6.534e-05 2.434e-01 5.054e-01 5.040e-01 7.685e-01 9.995e-01\nsummary( rnorm(1000) )\n##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n## -2.84855 -0.65619 -0.05057 -0.02011  0.64258  3.42109\n```\n:::\n\n\n## Mean and Variance\n\nThe most basic statistics summarize the center of a distribution and how far apart the values are spread.\n\n#### **Mean**. {-}\nPerhaps the most common statistic is the mean;\n$$\\overline{X}=\\frac{\\sum_{i=1}^{N}X_{i}}{N},$$ where $X_{i}$ denotes the value of the $i$th observation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute the mean of a random sample\nx <- runif(100)\nhist(x, border=NA, main=NA)\nm <- mean(x)  #sum(x)/length(x)\nabline(v=m, col=2, lwd=2)\ntitle(paste0('mean= ', round(m,2)), font.main=1)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-62-1.png){width=672}\n:::\n:::\n\n\n#### **Variance**.{-}\nPerhaps the second most common statistic is the variance: the average squared deviation from the mean\n$$V_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\overline{X}]^2}{N}.$$\nThe standard deviation is simply $s_{X} = \\sqrt{V_{X}}$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns <- sd(x) # sqrt(var(x))\nhist(x, border=NA, main=NA, freq=F)\ns_lh <- c(m - s,  m + s)\nabline(v=s_lh, col=4)\ntext(s_lh, -.02,\n    c( expression(bar(X)-s[X]), expression(bar(X)+s[X])),\n    col=4, adj=0)\ntitle(paste0('sd= ', round(s,2)), font.main=1)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-63-1.png){width=672}\n:::\n:::\n\n\nNote that a \"corrected version\" is used by R and many statisticians: $V_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\overline{X}]^2}{N-1}$.\n\n::: {.cell}\n\n```{.r .cell-code}\nvar(x)\n## [1] 0.08561227\nmean( (x - mean(x))^2 )\n## [1] 0.08475614\n```\n:::\n\n\nTogether, these statistics summarize the central tendency and dispersion of a distribution. In some special cases, such as with the normal distribution, they completely describe the distribution. Other distributions are easier to describe with other statistics.\n\n\n## Other Center/Spread Statistics\n\n#### **Absolute Deviations**. {-}\nWe can use the *Median* as a \"robust alternative\" to means. Recall that the $q$th quantile is the value where $q$ percent of the data are below and ($1-q$) percent are above. The median ($q=.5$) is the point where half of the data is lower values and the other half is higher.\n\n\nWe can also use the *Interquartile Range* or *Median Absolute Deviation* as an alternative to variance. The first and third quartiles ($q=.25$ and $q=.75$) together measure is the middle 50 percent of the data. The size of that range (interquartile range: the difference between the quartiles) represents \"spread\" or \"dispersion\" of the data. The median absolute deviation also measures spread\n$$\n\\tilde{X} = Med(X_{i}) \\\\\nMAD_{X} = Med\\left( | X_{i} - \\tilde{X} | \\right).\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rgeom(50, .4)\nx\n##  [1]  2  0  5  0  2  0  0  2  4  3  3  0  0  1  0  3  4  1  2  0  0  6  1  0  1\n## [26]  0  1  0  0  0  0  0  3  0  0 11  1  7  2  1  0  8  4  0  2  5  2  0  0  0\n\nplot(table(x))\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-65-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n#mean(x)\nmedian(x)\n## [1] 1\n\n#sd(x)\n#IQR(x) # diff( quantile(x, probs=c(.25,.75)))\nmad(x, constant=1) # median( abs(x - median(x)) )\n## [1] 1\n```\n:::\n\n\nNote that there other absolute deviations:\n\n::: {.cell}\n\n```{.r .cell-code}\nmean( abs(x - mean(x)) )\nmean( abs(x - median(x)) )\nmedian( abs(x - mean(x)) )\n```\n:::\n\n\n#### **Mode and Share Concentration**. {-}\nSometimes, none of the above work well. With categorical data, for example, distributions are easier to describe with other statistics. The mode is the most common observation: the value with the highest observed frequency. We can also measure the spread/dispersion of the frequencies, or compare the highest frequency to the average frequency to measure concentration at the mode.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Draw 3 Random Letters\nK <- length(LETTERS)\nx_id <- rmultinom(3, 1, prob=rep(1/K,K))\nx_id\n##       [,1] [,2] [,3]\n##  [1,]    0    0    0\n##  [2,]    0    0    0\n##  [3,]    0    0    0\n##  [4,]    0    0    0\n##  [5,]    0    0    0\n##  [6,]    0    0    0\n##  [7,]    0    0    0\n##  [8,]    0    0    0\n##  [9,]    0    0    1\n## [10,]    0    0    0\n## [11,]    0    0    0\n## [12,]    0    0    0\n## [13,]    0    0    0\n## [14,]    0    0    0\n## [15,]    0    0    0\n## [16,]    0    0    0\n## [17,]    0    0    0\n## [18,]    0    0    0\n## [19,]    0    0    0\n## [20,]    0    1    0\n## [21,]    0    0    0\n## [22,]    0    0    0\n## [23,]    0    0    0\n## [24,]    0    0    0\n## [25,]    0    0    0\n## [26,]    1    0    0\n\n# Draw Random Letters 100 Times\nx_id <- rowSums(rmultinom(100, 1, prob=rep(1/K,K)))\nx <- lapply(1:K, function(k){\n    rep(LETTERS[k], x_id[k])\n})\nx <- factor(unlist(x), levels=LETTERS)\n\nplot(x)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-67-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\ntx <- table(x)\n# mode(s)\nnames(tx)[tx==max(tx)]\n## [1] \"Z\"\n\n# freq. spread\nsx <- tx/sum(tx)\nsd(sx) # mad(sx)\n## [1] 0.01953301\n\n# freq. concentration \nmax(tx)/mean(tx)\n## [1] 2.08\n```\n:::\n\n\n## Shape Statistics\n\nCentral tendency and dispersion are often insufficient to describe a distribution. To further describe shape, we can compute the \"standard moments\" skew and kurtosis, as well as other statistics.\n\n\n#### **Skewness**. {-}\nThis captures how symmetric the distribution is.\n$$Skew_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\overline{X}]^3 / N}{ [s_{X}]^3 }$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rweibull(1000, shape=1)\nhist(x, border=NA, main=NA, freq=F, breaks=20)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-68-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\nskewness <-  function(x) {\n x_bar <- mean(x)\n m3 <- mean((x - x_bar)^3)\n skew <- m3/(sd(x)^3)\n return(skew)\n}\n\nskewness( rweibull(1000, shape=1))\n## [1] 2.271089\nskewness( rweibull(1000, shape=10) )\n## [1] -0.5094236\n```\n:::\n\n\n#### **Kurtosis**. {-}\nThis captures how many \"outliers\" there are.\n$$Kurt_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\overline{X}]^4 / N}{ [s_{X}]^4 }.$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rweibull(1000, shape=1)\nboxplot(x, main=NA)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-69-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\nkurtosis <- function(x) {  \n x_bar <- mean(x)\n m4 <- mean((x - x_bar)^4) \n kurt <- m4/(sd(x)^4) - 3  \n return(kurt)\n}\n\nkurtosis( rweibull(1000, shape=1))\n## [1] 5.471682\nkurtosis( rweibull(1000, shape=10) )\n## [1] 0.5462298\n```\n:::\n\n\n#### **Clusters/Gaps**. {-}\nYou can also describe distributions in terms of how clustered the values are. Remember: *a picture is worth a thousand words*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Number of Modes\nx <- rbeta(1000, .6, .6)\nhist(x, border=NA, main=NA, freq=F, breaks=20)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-70-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Random Number Generator \nr_ugly1 <- function(n, theta1=c(-8,-1), theta2=c(-2,2), rho=.25){\n    omega   <- rbinom(n, size=1, rho)\n    epsilon <- omega * runif(n, theta1[1], theta2[1]) +\n        (1-omega) * rnorm(n, theta1[2], theta2[2])\n    return(epsilon)\n}\n# Large Sample\npar(mfrow=c(1,1))\nX <- seq(-12,6,by=.001)\nrx <- r_ugly1(1000000)\nhist(rx, breaks=1000,  freq=F, border=NA,\n    xlab=\"x\", main='')\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-70-2.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Show True Density\nd_ugly1 <- function(x, theta1=c(-8,-1), theta2=c(-2,2), rho=.25){\n    rho     * dunif(x, theta1[1], theta2[1]) +\n    (1-rho) * dnorm(x, theta1[2], theta2[2]) }\ndx <- d_ugly1(X)\nlines(X, dx, col=1)\n```\n:::\n\n\n## Probability Theory\n\nYou were already introduced to this with [https://jadamso.github.io/Rbooks/random-variables.html](random variables) and probability distributions. In this section, we will dig a little deeper theoretically into the statistics we are most likely to use in practice.\n\nThe mean and variance are probably the two most basic statistics we might compute, and are often used. To understand them theoretically, we separately analyze how they are computed for discrete and continuous random variables.\n\n#### **Discrete**. {-} \nIf the sample space is discrete, we can compute the theoretical mean (or expected value) as\n$$\n\\mu = \\sum_{i} x_{i} Prob(X=x_{i}),\n$$\nwhere $Prob(X=x_{i})$ is the probability the random variable $X$ takes the particular value $x_{i}$. Similarly, we can compute the theoretical variance as\n$$\n\\sigma^2 = \\sum_{i} [x_{i} - \\mu]^2 Prob(X=x_{i}),\n$$\n\n**Example**. Consider an unfair coin with a $.75$ probability of heads ($x_{i}=1$) and a $.25$ probability of tails ($x_{i}=0$) has a theoretical mean of \n$$\n\\mu = 1\\times.75 + 0 \\times .25 = .75\n$$\nand a theoretical variance of \n$$\n\\sigma^2 = [1 - .75]^2 \\times.75 + [0 - .75]^2 \\times.25 = 0.1875\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rbinom(10000, size=1, prob=.75)\n\nround( mean(x), 4)\n## [1] 0.7554\n\nround( var(x), 4)\n## [1] 0.1848\n```\n:::\n\n\n\n**Weighted Data**.\nSometimes, you may have a dataset of values and probability weights. Othertimes, you can calculate them yourself. In either case, you can explicitly do the computations \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute probability weights for unique values\nh  <- table(x) #table of counts\nwt <- c(h)/length(x) #probabilities (must sum to 1)\nxt <- as.numeric(names(h)) #values\n# Weighted Mean\nxm <- sum(wt*xt)\nxm\n## [1] 0.7554\n```\n:::\n\n\nTry computing the mean both ways for another random sample\n\n::: {.cell}\n\n```{.r .cell-code}\nx  <-  sample(c(0,1,2), 1000, replace=T)\n```\n:::\n\nTry also computing a weighted variance\n\n::: {.cell}\n\n```{.r .cell-code}\n# xv <- sum(wt * (x - xm)^2)/sum(wt)\n```\n:::\n\n\n#### **Continuous**. {-}\nIf the sample space is continuous, we can compute the theoretical mean (or expected value) as\n$$\n\\mu = \\int x f(x) d x,\n$$\nwhere $f(x)$ is the probability the random variable takes the particular value $x$. Similarly, we can compute the theoretical variance as\n$$\n\\sigma^2 = \\int [x - \\mu]^2 f(x) d x,\n$$\n\n**Example**. Consider a random variable with a continuous uniform distribution over [-1, 1]. In this case, $f(x)=1/[1 - (-1)]=1/2$ for each $x$ in  [-1, 1] and \n$$\n\\mu = \\int_{-1}^{1} \\frac{x}{2} d x = \\int_{-1}^{0} \\frac{x}{2} d x + \\int_{0}^{1} \\frac{x}{2} d x = 0\n$$\nand \n$$\n\\sigma^2 = \\int_{-1}^{1} x^2 \\frac{1}{2} d x = \\frac{1}{2} \\frac{x^3}{3}|_{-1}^{1} = \\frac{1}{6}[1 - (-1)] = 2/6 =1/3\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- runif(10000, -1,1)\nround( mean(x), 4)\n## [1] 0.0041\nround( var(x), 4)\n## [1] 0.3297\n```\n:::\n\n\n**Weighted Data**.\nYou can again explicitly do the computations with weighted data, but here we have an additional approximation error\n\n::: {.cell}\n\n```{.r .cell-code}\n# values and probabilities\nh  <- hist(x, plot=F)\nwt <- h$counts/length(x) \nxt <- h$mids\n# Weighted mean\nxm <- sum(wt*xt)\nxm\n## [1] 0.00364\n\n# Compare to \"mean(x)\"\n```\n:::\n\n\n\n## Further Reading\n\nProbability Theory\n\n* [Refresher] https://www.khanacademy.org/math/statistics-probability/probability-library/basic-theoretical-probability/a/probability-the-basics\n* https://book.stat420.org/probability-and-statistics-in-r.html\n* https://bookdown.org/speegled/foundations-of-statistics/\n* https://math.dartmouth.edu/~prob/prob/prob.pdf\n* https://bookdown.org/probability/beta/discrete-random-variables.html\n* https://www.econometrics-with-r.org/2.1-random-variables-and-probability-distributions.html\n* https://probability4datascience.com/ch02.html\n* https://statsthinking21.github.io/statsthinking21-R-site/probability-in-r-with-lucy-king.html\n* https://bookdown.org/probability/statistics/\n* https://www.atmos.albany.edu/facstaff/timm/ATM315spring14/R/IPSUR.pdf\n* https://rc2e.com/probability\n* https://bookdown.org/probability/beta/\n* https://bookdown.org/a_shaker/STM1001_Topic_3/\n* https://bookdown.org/fsancier/bookdown-demo/\n* https://bookdown.org/kevin_davisross/probsim-book/\n* https://bookdown.org/machar1991/ITER/2-pt.html\n* https://www.atmos.albany.edu/facstaff/timm/ATM315spring14/R/IPSUR.pdf\n* https://math.dartmouth.edu/~prob/prob/prob.pdf\n\nFor weighted statistics, see\n\n* https://seismo.berkeley.edu/~kirchner/Toolkits/Toolkit_12.pdf\n* https://www.bookdown.org/rwnahhas/RMPH/survey-desc.html\n\nNote that many random variables are related to each other\n\n* https://en.wikipedia.org/wiki/Relationships_among_probability_distributions\n* https://www.math.wm.edu/~leemis/chart/UDR/UDR.html\n* https://qiangbo-workspace.oss-cn-shanghai.aliyuncs.com/2018-11-11-common-probability-distributions/distab.pdf\n\nAlso note that numbers randomly generated on your computer cannot be truly random, they are \"Pseudorandom\".\n\n\n\n\n# (Re)Sampling \n***\n\n## Sample Distributions\n\nThe *sampling distribution* of a statistic shows us how much a statistic varies from sample to sample.\n\nFor example, see how the mean varies from sample to sample to sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Three Sample Example\npar(mfrow=c(1,3))\nfor(i in 1:3){\n    x <- runif(100) \n    m <-  mean(x)\n    hist(x,\n        breaks=seq(0,1,by=.1), #for comparability\n        main=NA, border=NA)\n    abline(v=m, col=2, lwd=2)\n    title(paste0('mean= ', round(m,2)),  font.main=1)\n}\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-78-1.png){width=672}\n:::\n:::\n\n\nExamine the sampling distribution of the mean\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_means <- sapply(1:1000, function(i){\n    m <- mean(runif(100))\n    return(m)\n})\nhist(sample_means, breaks=50, border=NA,\n    col=2, font.main=1,\n    main='Sampling Distribution of the mean')\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-79-1.png){width=672}\n:::\n:::\n\n\nIn this figure, you see two the most profound results known in statistics\n\n* *Law of Large Numbers*: the sample mean is centered around the true mean.\n* *Central Limit Theorem*: the sampling distribution of the mean is approximately standard normal.\n\n#### **Central Limit Theorem**. {-}\nThere are actually many different variants of the central limit theorem, as it applies more generally: the sampling distribution of many statistics are standard normal. For example, examine the sampling distribution of the standard deviation.\n\n::: {.cell}\n\n```{.r .cell-code}\nthree_sds <- c(  sd(runif(100)),  sd(runif(100)),  sd(runif(100))  )\nthree_sds\n## [1] 0.2914321 0.2884851 0.2791886\n\nsample_sds <- sapply(1:1000, function(i){\n    s <- sd(runif(100))\n    return(s)\n})\nhist(sample_sds, breaks=50, border=NA,\n    col=4, font.main=1,\n    main='Sampling Distribution of the sd')\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-80-1.png){width=672}\n:::\n:::\n\n\nIt is beyond this class to prove this result, but you should know that not all sampling distributions are standard normal. For example, examine the sampling distribution of the three main \"order statistics\"\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 300 samples, each with 1000 random uniform variables\nx <- sapply(1:300, function(i) runif(1000) )\n# Each row is a new sample\nlength(x[1,])\n## [1] 300\n\n# Median looks normal, Maximum and Minumum do not!\nxmin <- apply(x,1,quantile, probs=0)\nxmed <- apply(x,1,quantile, probs=.5)\nxmax <- apply(x,1,quantile, probs=1)\npar(mfrow=c(1,3))\nhist(xmin, breaks=100, border=NA, main='Min', font.main=1)\nhist(xmed, breaks=100, border=NA, main='Med', font.main=1)\nhist(xmax, breaks=100, border=NA, main='Max', font.main=1)\ntitle('Sampling Distributions', outer=T, line=-1)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-81-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# To explore, try any function!\nfun_of_rv <- function(f, n=100){\n  x <- runif(n)\n  y <- f(x)\n  return(y)\n}\n\nfun_of_rv( f=mean )\n## [1] 0.4476911\n\nfun_of_rv( f=function(i){ diff(range(exp(i))) } )\n## [1] 1.685819\n```\n:::\n\n\n\n## Intervals\n\nUsing either the bootstrap or jackknife distribution, we can calculate \n\n* *confidence interval:* range your statistic varies across different samples.\n* *standard error*: variance of your statistic across different samples.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_means <- apply(x,1,mean)\n# standard error\nsd(sample_means)\n## [1] 0.01673167\n```\n:::\n\n\nNote that in some cases, you can estimate the standard error to get a confidence interval.\n\n::: {.cell}\n\n```{.r .cell-code}\nx00 <- x[1,]\n# standard error\ns00 <- sd(x00)/sqrt(length(x00))\nci <- mean(x00) + c(1.96, -1.96)*s00\n```\n:::\n\n\n#### **Confidence Interval**.  {-}\nCompute the upper and lower quantiles of the sampling distribution.\n\n*Sample Mean*. We simulate the sampling distribution of the sample mean and construct a 90% confidence interval by taking the 5th and 95th percentiles of the simulated means. This gives an empirical estimate of the interval within which the true mean is expected to lie with 90% confidence, assuming repeated sampling.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Middle 90%\nmq <- quantile(sample_means, probs=c(.05,.95))\npaste0('we are 90% confident that the mean is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\n## [1] \"we are 90% confident that the mean is between 0.47 and 0.53\"\n\nbks <- seq(.4,.6,by=.001)\nhist(sample_means, breaks=bks, border=NA,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='Confidence Interval for the mean')\nabline(v=mq)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-85-1.png){width=672}\n:::\n:::\n\n\n*Sample Percentile*. We repeat the process to estimate the 99th percentile for each sample. We then construct a 95% confidence interval for the 99th percentile estimator, using the 2.5th and 97.5th quantiles of these estimates.\n\n::: {.cell}\n\n```{.r .cell-code}\n## Upper Percentile\nsample_quants <- apply(x,1,quantile, probs=.99)\n\n# Middle 95% of estimates\nmq <- quantile(sample_quants, probs=c(.025,.975))\npaste0('we are 95% confident that the upper percentile is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\n## [1] \"we are 95% confident that the upper percentile is between 0.97 and 1\"\n\nbks <- seq(.92,1,by=.001)\nhist(sample_quants, breaks=bks, border=NA,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='95% Confidence Interval for the 99% percentile')\nabline(v=mq)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-86-1.png){width=672}\n:::\n:::\n\n\nNote that X% confidence intervals do not generally cover X% of the data. Those intervals are a type of prediction interval that is covered later. See also https://online.stat.psu.edu/stat200/lesson/4/4.4/4.4.2\n\n#### **Advanced Intervals**. {-}\nIn many cases, we want a X% interval to mean that X% of the intervals we generate will contain the true mean. E.g., in repeated sampling, 50% of constructed confidence intervals are expected to contain the true population mean.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Theoretically: [-1 sd, +1 sd] has 2/3 coverage\n\n# Confidence Interval for each sample\nxq <- apply(x,1, function(r){ #theoretical se's \n    mean(r) + c(-1,1)*sd(r)/sqrt(length(r))\n})\n# First 4 interval estimates\nxq[,1:4]\n##           [,1]      [,2]      [,3]     [,4]\n## [1,] 0.4636191 0.4807875 0.5048761 0.489175\n## [2,] 0.4958163 0.5140642 0.5378481 0.522204\n\n# Explicit calculation\nmu_true <- 0.5\n# Logical vector: whether the true mean is in each CI\ncovered <- mu_true >= xq[1, ] & mu_true <= xq[2, ]\n# Empirical coverage rate\ncoverage_rate <- mean(covered)\ncat(sprintf(\"Estimated coverage probability: %.2f%%\\n\", 100 * coverage_rate))\n## Estimated coverage probability: 67.30%\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize first N confidence intervals\nN <- 100\nplot.new()\nplot.window(xlim = range(xq), ylim = c(0, N))\nfor (i in 1:N) {\n  col_i <- if (covered[i]) rgb(0, 0, 0, 0.3) else rgb(1, 0, 0, 0.5)\n  segments(xq[1, i], i, xq[2, i], i, col = col_i, lwd = 2)\n}\nabline(v = mu_true, col = \"blue\", lwd = 2)\naxis(1)\ntitle(\"Visualizing CI Coverage (Red = Missed)\")\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-88-1.png){width=672}\n:::\n:::\n\n\nThis differs from a **pointwise inclusion frequency interval**\n\n::: {.cell}\n\n```{.r .cell-code}\n# Frequency each point was in an interval\nbks <- seq(0,1,by=.01)\nxcovr <- sapply(bks, function(b){\n    bl <- b >= xq[1,]\n    bu <- b <= xq[2,]\n    mean( bl & bu )\n})\n# 50\\% Coverage\nc_ul <- range(bks[xcovr>=.5])\nc_ul # 50% confidence interval\n## [1] 0.49 0.51\n\nplot.new()\nplot.window(xlim=c(0,1), ylim=c(0,1))\npolygon( c(bks, rev(bks)), c(xcovr, xcovr*0), col=grey(.5,.5), border=NA)\nmtext('Frequency each value was in an interval',2, line=2.5)\naxis(1)\naxis(2)\nabline(h=.5, lwd=2)\nsegments(c_ul,0,c_ul,.5, lty=2)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-89-1.png){width=672}\n:::\n:::\n\n\n\n## Resampling\n\nOften, we only have one sample. How then can we estimate the sampling distribution of a statistic? \n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests$Murder\nmean(sample_dat)\n## [1] 7.788\n```\n:::\n\n\nWe can \"resample\" our data. *Hesterberg (2015)* provides a nice illustration of the idea. The two most basic versions are the jackknife and the bootstrap, which are discussed below.\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-92-1.png){width=672}\n:::\n:::\n\n\n#### **Jackknife Distribution**. {-}\nHere, we compute all \"leave-one-out\" estimates. Specifically, for a dataset with $n$ observations, the jackknife uses $n-1$ observations other than $i$ for each unique subsample. Taking the mean, for example, we have \n\\begin{itemize}\n\\item jackknifed estimates: $\\overline{x}^{Jack}_{i}=\\frac{1}{n-1} \\sum_{j \\neq i}^{n-1} X_{j}$\n\\item mean of the jackknife: $\\overline{x}^{Jack}=\\frac{1}{n} \\sum_{i}^{n} \\overline{x}^{Jack}_{i}$.\n\\item standard error of the jackknife: $\\widehat{\\sigma}^{Jack}= \\sqrt{ \\frac{1}{n} \\sum_{i}^{n} \\left[\\overline{x}^{Jack}_{i} - \\overline{x}^{Jack} \\right]^2 }$.\n\\end{itemize}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests$Murder\nsample_mean <- mean(sample_dat)\n\n# Jackknife Estimates\nn <- length(sample_dat)\nJmeans <- sapply(1:n, function(i){\n    dati <- sample_dat[-i]\n    mean(dati)\n})\nhist(Jmeans, breaks=25, border=NA,\n    main='', xlab=expression(bar(X)[-i]))\nabline(v=sample_mean, col='red', lty=2)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-93-1.png){width=672}\n:::\n:::\n\n\n#### **Bootstrap Distribution**. {-}\nHere, we draw $n$ observations with replacement from the original data to create a bootstrap sample and calculate a statistic. Each bootstrap sample $b=1...B$ uses a random set of observations (denoted $N_{b}$) to compute a statistic. We repeat that many times, say $B=9999$, to estimate the sampling distribution. Consider the sample mean as an example;\n\\begin{itemize}\n\\item bootstrap estimate: $\\overline{x}^{Boot}_{b}= \\frac{1}{n} \\sum_{i \\in N_b} X_{i} $\n\\item mean of the bootstrap: $\\overline{x}^{Boot}= \\frac{1}{B} \\sum_{b} \\overline{x}^{Boot}_{b}$.\n\\item standard error of the bootstrap: $\\widehat{\\sigma}^{Boot}= \\sqrt{ \\frac{1}{B} \\sum_{b=1}^{B} \\left[\\overline{x}^{Boot}_{b} - \\overline{x}^{Boot} \\right]^2 }$.\n\\end{itemize}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap estimates\nset.seed(2)\nBmeans <- sapply(1:10^4, function(i) {\n    dat_b <- sample(sample_dat, replace=T) # c.f. jackknife\n    mean(dat_b)\n})\n\nhist(Bmeans, breaks=25, border=NA,\n    main='', xlab=expression(bar(X)[b]))\nabline(v=sample_mean, col='red', lty=2)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-94-1.png){width=672}\n:::\n:::\n\n\n**Caveat**. Note that we do not use the mean of the bootstrap or jackknife statistics as a replacement for the original estimate. This is because the bootstrap and jackknife distributions are centered at the observed statistic, not the population parameter. (The bootstrapped mean is centered at the sample mean, not the population mean.) This means that we cannot use the bootstrap to improve on $\\overline{x}$; no matter how many bootstrap samples we take. We can, however, use the jackknife and bootstrap to estimate sampling variability.\n\n\n#### **Intervals**. {-}\nNote that both methods provide imperfect estimates, and can give different numbers. Percentiles of jackknife resamples are systematically less variable than they should be. Until you know more, a conservative approach is to take the larger estimate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Boot CI\nboot_ci <- quantile(Bmeans, probs=c(.025, .975))\nboot_ci\n##    2.5%   97.5% \n## 6.58200 8.97005\n\n# Jack CI\njack_ci <- quantile(Jmeans, probs=c(.025, .975))\njack_ci\n##     2.5%    97.5% \n## 7.621582 7.904082\n\n# more conservative estimate\nci_est <- boot_ci\n```\n:::\n\n\nAlso note that the *standard deviation* refers to variance within a single sample, and is hence different from the standard error. Nonetheless, they can both be used to estimate the variability of a statistic.\n\n::: {.cell}\n\n```{.r .cell-code}\nboot_se <- sd(Bmeans)\n\nsample_sd <- sd(sample_dat)\n\nc(boot_se, sample_sd/sqrt(n))\n## [1] 0.6056902 0.6159621\n```\n:::\n\n\n#### **Value of More Data**.{-}\nEach additional data point you have provides more information, which ultimately decreases the standard error of your estimates. However, it does so at a decreasing rate (known in economics as diminishing returns).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNseq <- seq(1,100, by=1) # Sample sizes\nB <- 1000 # Number of draws per sample\n\nSE <- sapply(Nseq, function(n){\n    sample_statistics <- sapply(1:B, function(b){\n        x <- rnorm(n) # Sample of size N\n        quantile(x,probs=.4) # Statistic\n    })\n    sd(sample_statistics)\n})\n\npar(mfrow=c(1,2))\nplot(Nseq, SE, pch=16, col=grey(0,.5),\n    main='Absolute Gain', font.main=1,\n    ylab='standard error', xlab='sample size')\nplot(Nseq[-1], abs(diff(SE)), pch=16, col=grey(0,.5),\n    main='Marginal Gain', font.main=1,\n    ylab='decrease in standard error', xlab='sample size')\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-97-1.png){width=672}\n:::\n:::\n\n\n\n## Further Reading\n\nSee \n\n* https://www.r-bloggers.com/2025/02/bootstrap-vs-standard-error-confidence-intervals/\n\n\n# Hypothesis Tests\n***\n\n## Basic Ideas\n\nIn this section, we test hypotheses using *data-driven* methods that assume much less about the data generating process. There are two main ways to conduct a hypothesis test to do so: inverting a confidence interval and imposing the null.\n\n#### **Invert a CI**.{-}\nOne main way to conduct hypothesis tests is to examine whether a confidence interval contains a hypothesized value. We then use this decision rule\n\n* reject the null if value falls outside of the interval\n* fail to reject the null if value falls inside of the interval\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests$Murder\nsample_mean <- mean(sample_dat)\n\nn <- length(sample_dat)\nJmeans <- sapply(1:n, function(i){\n    dati <- sample_dat[-i]\n    mean(dati)\n})\nhist(Jmeans, breaks=25,\n    border=NA, xlim=c(7.5,8.1),\n    main='', xlab=expression( bar(X)[-i]))\n# CI\nci_95 <- quantile(Jmeans, probs=c(.025, .975))\nabline(v=ci_95, lwd=2)\n# H0: mean=8\nabline(v=8, col=2, lwd=2)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-98-1.png){width=672}\n:::\n:::\n\n\n#### **Impose the Null**. {-}\nWe can also compute a *null distribution*: the sampling distribution of the statistic under the null hypothesis (assuming your null hypothesis was true). We use the bootstrap to loop through a large number of \"resamples\". In each iteration of the loop, we impose the null hypothesis and re-estimate the statistic of interest. We then calculate the range of the statistic across all resamples and compare how extreme the original value we observed is. We use a 95% confidence interval of the null distribution to create a *rejection region*.\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests$Murder\nsample_mean <- mean(sample_dat)\n\n# Bootstrap NULL: mean=8\nset.seed(1)\nBmeans0 <- sapply(1:10^4, function(i) {\n    dat_b <- sample(sample_dat, replace=T) \n    mean_b <- mean(dat_b) + (8 - sample_mean) # impose the null by recentering\n    return(mean_b)\n})\nhist(Bmeans0, breaks=25, border=NA,\n    main='', xlab=expression( bar(X)[b]) )\nci_95 <- quantile(Bmeans0, probs=c(.025, .975)) # critical region\nabline(v=ci_95, lwd=2)\nabline(v=sample_mean, lwd=2, col=2)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-99-1.png){width=672}\n:::\n:::\n\n\n\n\n## Default Statistics\n\n#### **p-values**.{-}\nThis is the frequency you would see something as extreme as your statistic when sampling from the null distribution.\n\nThere are three associated tests: the two-sided test (observed statistic is extremely high or low) or one of the one-sided tests (observed statistic is extremely low, observed statistic is extremely high). E.g.\n\n* $HA​: \\bar{X} > 8$ implies a right tail test\n* $HA: \\bar{X} < 8$ implies a left tail test\n* $HA​: \\bar{X} \\neq 8$ implies a two tail test\n\nIn any case, typically \"p<.05: statistically significant\" and \"p>.05: not statistically significant\".\n\n\nOne sided example\n\n::: {.cell}\n\n```{.r .cell-code}\n# One-Sided Test, ALTERNATIVE: mean > 8\n# Prob( boot0_means > sample_mean) \nFhat0 <- ecdf(Bmeans0) # Right tail\nplot(Fhat0,\n    xlab=expression( beta[b] ),\n    main='Null Bootstrap Distribution for means', font.main=1)\nabline(v=sample_mean, col='red')\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-100-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\np <- 1- Fhat0(sample_mean) #Right Tail\nif(p >.05){\n    message('fail to reject the null that sample_mean=8, at the 5% level')\n} else {\n    message('reject the null that sample_mean=8 in favor of >8, at the 5% level')\n}\n```\n:::\n\n\nTwo sided example\n\n::: {.cell}\n\n```{.r .cell-code}\n# Two-Sided Test, ALTERNATIVE: mean < 8 or mean >8\n# Prob(boot0_means > sample_mean or -boot0_means < sample_mean)\n\nFhat0 <- ecdf(Bmeans0)\np_left <- Fhat0(sample_mean) #Left Tail\np_right <- 1 - Fhat0(sample_mean) #Right Tail\np <- 2*min(p_left, p_right)\n\nif(p >.05){\n    message('fail to reject the null that sample_mean=8 at the 5% level')\n} else {\n    message('reject the null that sample_mean=8 in favor of either <8 or >8 at the 5% level')\n}\n```\n:::\n\n\n\n#### **t-values**. {-}\nA t-value standardizes the statistic you are using for hypothesis testing:\n$$ t = (\\hat{\\mu} - \\mu_{0}) / \\hat{s_{\\mu}} $$\n\n::: {.cell}\n\n```{.r .cell-code}\njack_se <- sd(Jmeans)\nmean0 <- 8\njack_t <- (sample_mean - mean0)/jack_se\n\n# Note that you can also use a corrected se\n# jack_se <- sqrt((n-1)/n) * sd(Jmeans)\n```\n:::\n\nThere are several benefits to this:\n\n* makes the statistic comparable across different studies\n* makes the null distribution not depend on theoretical parameters ($\\sigma$)\n* makes the null distribution theoretically known asymptotically (approximately)\n\nThe last point implies we are dealing with a symmetric distributions: $Prob( t_{boot} > t ~\\text{or}~ t_{boot} < -t) = Prob( |t| < |t_{boot}| )$.^[In another statistics class, you will learn the math behind the null t-distribution. In this class, we skip this because we can simply bootstrap the t-statistic too.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nboot_t0 <- sapply(1:10^3, function(i) {\n    dat_b <- sample(sample_dat, replace=T) \n    mean_b <- mean(dat_b) + (8 - sample_mean) # impose the null by recentering\n    # jack ses\n    jack_se_b <- sd( sapply(1:length(dat_b), function(i){\n        mean(dat_b[-i])\n    }) )\n    jack_t <- (mean_b - mean0)/jack_se_b\n})\n\n# Two Sided Test\nFhat0 <- ecdf(abs(boot_t0))\nplot(Fhat0, xlim=range(boot_t0, jack_t),\n    xlab=expression( abs(hat(t)[b]) ),\n    main='Null Bootstrap Distribution for t', font.main=1)\nabline(v=abs(jack_t), col='red')\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-103-1.png){width=672}\n:::\n\n```{.r .cell-code}\np <- 1 - Fhat0( abs(jack_t) ) \np\n## [1] 0.727\n\nif(p >.05){\n    message('fail to reject the null that sample_mean=8, at the 5% level')\n} else {\n    message('reject the null that sample_mean=8 in favor of either <8 or >8, at the 5% level')\n}\n```\n:::\n\n\n\n## Two-Sample Differences\n\nSuppose we have 2 samples of data. \n\nEach $X_{is}$ is an individual observation $i$ from the sample $s=1,2$. (For example, the wages for men and women in Canada. For another example, homicide rates in two different American states.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wooldridge)\nx1 <- wage1[wage1$educ == 15, 'wage']\nx2 <- wage1[wage1$educ == 16, 'wage']\n```\n:::\n\n\nFor simplicity, we will assume that each observation is an independent observation. We will further assume the data from each group are normally distributed, but the mean and variance can be different across groups.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample 1 (e.g., males)\nn1 <- 100\nx1 <- rnorm(n1, 0, 2)\n# Sample 2 (e.g., females)\nn2 <- 80\nx2 <- rnorm(n2, 1, 1)\n\npar(mfrow=c(1,2))\nbks <- seq(-7,7, by=.5)\nhist(x1, border=NA, breaks=bks,\n    main='Sample 1', font.main=1)\n\nhist(x2, border=NA, breaks=bks, \n    main='Sample 2', font.main=1)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-105-1.png){width=672}\n:::\n:::\n\nThere may be several differences between these samples. Often, the first summary statistic we investigate is the difference in means. \n\n#### **Equal Means**. {-}\nWe often want to know if the means of different sample are different in . To test this hypothesis, we compute the sample mean $\\overline{X}_{s}$ over all observations in each sample and then examine the differences term\n\\begin{eqnarray} \nD = \\overline{X}_{1} - \\overline{X}_{2},\n\\end{eqnarray}\nwith a null hypothesis of $D=0$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Differences between means\nm1 <- mean(x1)\nm2 <- mean(x2)\nd <- m1-m2\n    \n# Bootstrap Distribution\nboot_d <- sapply(1:10^4, function(b){\n    x1_b <- sample(x1, replace=T)\n    x2_b <- sample(x2, replace=T)\n    m1_b <- mean(x1_b)\n    m2_b <- mean(x2_b)\n    d_b <- m1_b - m2_b\n    return(d_b)\n})\nhist(boot_d, border=NA, font.main=1,\n    main='Difference in Means')\n\n# 2-Sided Test\nboot_ci <- quantile(boot_d, probs=c(.025, .975))\nabline(v=boot_ci, lwd=2)\nabline(v=0, lwd=2, col=2)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-106-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# p-value\n1 - ecdf(boot_d)(0)\n## [1] 0\n```\n:::\n\n\n<details>\n<summary>Standardized Differences</summary>\nJust as with one sample tests, we can standardize $D$ into a $t$ statistic. In which case, we can easily compute one or two sided hypothesis tests. Note, however, that we have to compute the standard error for the difference statistic, which is a bit more complicated. \n\n::: {.cell}\n\n```{.r .cell-code}\nse_hat <- sqrt(var(x1)/n1 + var(x2)/n2);\nt_obs <- d/se_hat\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n </details>\n\n#### **Other Differences**. {-}\nThe above procedure generalized from differences in \"means\" to other statistics like \"quantiles\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap Distribution Function\nboot_fun <- function( fun, B=10^4, ...){\n    boot_d <- sapply(1:B, function(b){\n        x1_b <- sample(x1, replace=T)\n        x2_b <- sample(x2, replace=T)\n        f1_b <- fun(x1_b, ...)\n        f2_b <- fun(x2_b, ...)\n        d_b <- f1_b - f2_b\n        return(d_b)\n    })\n    return(boot_d)\n}\n\n# 2-Sided Test for Median Differences\n# d <- median(x2) - median(x1)\nboot_d <- boot_fun(median)\nhist(boot_d, border=NA, font.main=1,\n    main='Difference in Medians')\nabline(v=quantile(boot_d, probs=c(.025, .975)), lwd=2)\nabline(v=0, lwd=2, col=2)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-109-1.png){width=672}\n:::\n\n```{.r .cell-code}\n1 - ecdf(boot_d)(0)\n## [1] 0\n```\n:::\n\n\nNote that these estimates suffer from a finite-sample bias, which we can correct for. Also note that bootstrap tests can perform poorly with highly unequal variances or skewed data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 2-Sided Test for SD Differences\n#d <- sd(x2) - sd(x1)\nboot_d <- boot_fun(sd)\nhist(boot_d, border=NA, font.main=1,\n    main='Difference in Standard Deviations')\nabline(v=quantile(boot_d, probs=c(.025, .975)), lwd=2)\nabline(v=0, lwd=2, col=2)\n1 - ecdf(boot_d)(0)\n\n\n# Try any function!\n# boot_fun( function(xs) { IQR(xs)/median(xs) } )\n```\n:::\n\n\n## Further Reading\n\n* https://learningstatisticswithr.com/book/hypothesistesting.html\n* https://okanbulut.github.io/rbook/part5.html\n\n\n# Data Analysis\n***\n\n## Beyond Basics\n\nUse expansion \"packages\" for less common procedures and more functionality\n\n#### **CRAN**. {-}\nMost packages can be found on CRAN and can be easily installed\n\n::: {.cell}\n\n```{.r .cell-code}\n# commonly used packages\ninstall.packages(\"stargazer\")\ninstall.packages(\"data.table\")\ninstall.packages(\"plotly\")\n# other statistical packages\ninstall.packages(\"extraDistr\")\ninstall.packages(\"twosamples\")\n# install.packages(\"purrr\")\n# install.packages(\"reshape2\")\n```\n:::\n\n\nThe most common tasks also have [cheatsheets](https://www.rstudio.com/resources/cheatsheets/) you can use. \n\nFor example, to generate 'exotic' probability distributions\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(extraDistr)\n\npar(mfrow=c(1,2))\nfor(p in c(-.5,0)){\n    x <- rgev(2000, mu=0, sigma=1, xi=p)\n    hist(x, breaks=50, border=NA, main=NA, freq=F)\n}\ntitle('GEV densities', outer=T, line=-1)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-112-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(extraDistr)\n\npar(mfrow=c(1,3))\nfor(p in c(-1, 0,2)){\n    x <- rtlambda(2000, p)\n    hist(x, breaks=100, border=NA, main=NA, freq=F)\n}\ntitle('Tukey-Lambda densities', outer=T, line=-1)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-113-1.png){width=672}\n:::\n:::\n\n\n#### **Github**.  {-}\nSometimes you will want to install a package from GitHub. For this, you can use [devtools](https://devtools.r-lib.org/) or its light-weight version [remotes](https://remotes.r-lib.org/)\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"devtools\")\ninstall.packages(\"remotes\")\n```\n:::\n\n\nNote that to install `devtools`, you also need to have developer tools installed on your computer.\n\n* Windows: [Rtools](https://cran.r-project.org/bin/windows/Rtools/rtools42/rtools.html)\n* Mac: [Xcode](https://apps.apple.com/us/app/xcode/id497799835?mt=12)\n\nTo color terminal output on Linux systems, you can use the colorout package\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(remotes)\n# Install https://github.com/jalvesaq/colorout\n# to .libPaths()[1]\ninstall_github('jalvesaq/colorout')\nlibrary(colorout)\n```\n:::\n\n\n#### **Base**. {-}\nWhile additional packages can make your code faster, they also create dependancies that can lead to problems. So learn base R well before becoming dependant on other packages\n\n* https://bitsofanalytics.org/posts/base-vs-tidy/\n* https://jtr13.github.io/cc21fall2/comparison-among-base-r-tidyverse-and-datatable.html\n\n\n#### Updating. {-}\n\nMake sure R and your packages are up to date. The current version of R and any packages used can be found (and recorded) with \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n:::\n\n\nTo update your R packages, use \n\n::: {.cell}\n\n```{.r .cell-code}\nupdate.packages()\n```\n:::\n\n\n\n**Rarely Used Tricks:**\nNote that after updating R, you can update *all* packages stored in *all* `.libPaths()` with the following command\n\n::: {.cell}\n\n```{.r .cell-code}\nupdate.packages(checkBuilt=T, ask=F)\n# install.packages(old.packages(checkBuilt=T)[,\"Package\"])\n```\n:::\n\n\nSometimes there is a problem. To find specific broken packages after an update\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(purrr)\n\nset_names(.libPaths()) %>%\n  map(function(lib) {\n    .packages(all.available = TRUE, lib.loc = lib) %>%\n        keep(function(pkg) {\n            f <- system.file('Meta', 'package.rds', package = pkg, lib.loc = lib)\n            tryCatch({readRDS(f); FALSE}, error = function(e) TRUE)\n        })\n  })\n# https://stackoverflow.com/questions/31935516/installing-r-packages-error-in-readrdsfile-error-reading-from-connection/55997765\n```\n:::\n\n\nTo remove packages duplicated in multiple libraries\n\n::: {.cell}\n\n```{.r .cell-code}\n# Libraries\ni <- installed.packages()\nlibs <- .libPaths()\n# Find Duplicated Packages\ni1 <- i[ i[,'LibPath']==libs[1], ]\ni2 <- i[ i[,'LibPath']==libs[2], ]\ndups <- i2[,'Package'] %in% i1[,'Package']\nall( dups )\n# Remove\nremove.packages(  i2[,'Package'], libs[2] )\n```\n:::\n\n\n\n## Inputs\n\n#### **Reading Data**. {-}\n\nThe first step in data analysis is getting data into R. There are many ways to do this, depending on your data structure. Perhaps the most common case is reading in a csv file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read in csv (downloaded from online)\n# download source 'http://www.stern.nyu.edu/~wgreene/Text/Edition7/TableF19-3.csv'\n# download destination '~/TableF19-3.csv'\nread.csv('~/TableF19-3.csv')\n \n# Can read in csv (directly from online)\n# dat_csv <- read.csv('http://www.stern.nyu.edu/~wgreene/Text/Edition7/TableF19-3.csv')\n```\n:::\n\n\nReading in other types of data can require the use of \"packages\". For example, the \"wooldridge\" package contains datasets on crime. To use this data, we must first install the package on our computer. Then, to access the data, we must first load the package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install R Data Package and Load in\ninstall.packages('wooldridge') # only once\nlibrary(wooldridge) # anytime you want to use the data\n\ndata('crime2') \ndata('crime4')\n```\n:::\n\n\nWe can use packages to access many different types of data. To read in a Stata data file, for example, we can use the \"haven\" package.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read in stata data file from online\n#library(haven)\n#dat_stata <- read_dta('https://www.ssc.wisc.edu/~bhansen/econometrics/DS2004.dta')\n#dat_stata <- as.data.frame(dat_stata)\n\n# For More Introductory Econometrics Data, see \n# https://www.ssc.wisc.edu/~bhansen/econometrics/Econometrics%20Data.zip\n# https://pages.stern.nyu.edu/~wgreene/Text/Edition7/tablelist8new.htm\n# R packages: wooldridge, causaldata, Ecdat, AER, ....\n```\n:::\n\n\n#### **Cleaning Data**. {-}\n\nData transformation is often necessary before analysis, so remember to be careful and check your code is doing what you want. (If you have large datasets, you can always test out the code on a sample.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to Create Sample Datasets\nmake_noisy_data <- function(n, b=0){\n    # Simple Data Generating Process\n    x <- seq(1,10, length.out=n) \n    e <- rnorm(n, mean=0, sd=10)\n    y <- b*x + e \n    # Obervations\n    xy_mat <- data.frame(ID=seq(x), x=x, y=y)\n    return(xy_mat)\n}\n\n# Two simulated datasets\ndat1 <- make_noisy_data(6)\ndat2 <- make_noisy_data(6)\n\n# Merging data in long format\ndat_merged_long <- rbind(\n    cbind(dat1,DF=1),\n    cbind(dat2,DF=2))\n```\n:::\n\n\nNow suppose we want to transform into wide format\n\n::: {.cell}\n\n```{.r .cell-code}\n# Merging data in wide format, First Attempt\ndat_merged_wide <- cbind( dat1, dat2)\nnames(dat_merged_wide) <- c(paste0(names(dat1),'.1'), paste0(names(dat2),'.2'))\n\n# Merging data in wide format, Second Attempt\n# higher performance\ndat_merged_wide2 <- merge(dat1, dat2,\n    by='ID', suffixes=c('.1','.2'))\n## CHECK they are the same.\nidentical(dat_merged_wide, dat_merged_wide2)\n## [1] FALSE\n# Inspect any differences\n\n# Merging data in wide format, Third Attempt with dedicated package\n# (highest performance but with new type of object)\nlibrary(data.table)\ndat_merged_longDT <- as.data.table(dat_merged_long)\ndat_melted <- melt(dat_merged_longDT, id.vars=c('ID', 'DF'))\ndat_merged_wide3 <- dcast(dat_melted, ID~DF+variable)\n\n## CHECK they are the same.\nidentical(dat_merged_wide, dat_merged_wide3)\n## [1] FALSE\n```\n:::\n\n\nOften, however, we ultimately want data in long format\n\n::: {.cell}\n\n```{.r .cell-code}\n# Merging data in long format, Second Attempt with dedicated package \ndat_melted2 <- melt(dat_merged_wide3, measure=c(\"1_x\",\"1_y\",\"2_x\",\"2_y\"))\nmelt_vars <- strsplit(as.character(dat_melted2$variable),'_')\ndat_melted2$DF <- sapply(melt_vars, `[[`,1)\ndat_melted2$variable <- sapply(melt_vars, `[[`,2)\ndat_merged_long2 <- dcast(dat_melted2, DF+ID~variable)\ndat_merged_long2 <- as.data.frame(dat_merged_long2)\n\n## CHECK they are the same.\nidentical( dat_merged_long2, dat_merged_long)\n## [1] FALSE\n\n# Further Inspect\ndat_merged_long2 <- dat_merged_long2[,c('ID','x','y','DF')]\nmapply( identical, dat_merged_long2, dat_merged_long)\n##    ID     x     y    DF \n##  TRUE  TRUE  TRUE FALSE\n```\n:::\n\n\nFor more tips, see https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-import.pdf and https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reshape.html\n<!--\\url{https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf}-->\n\n\n## Outputs\n\n#### **Polishing**.{-}\n\nYour first figures are typically standard.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random Data\nx <- seq(1, 10, by=.0002)\ne <- rnorm(length(x), mean=0, sd=1)\ny <- .25*x + e \n\n# First Drafts\n# qqplot(x, y)\n# plot(x, y)\n```\n:::\n\n\nEdit your plot to focus on the most useful information. For others to easily comprehend your work, you must also polish the plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Second Draft: Focus\n# (In this example: comparing shapes)\nxs <- scale(x)\nys <- scale(y)\n# qqplot(xs, ys)\n\n# Third Draft: Polish\nqqplot(ys, xs, \n    xlab=expression('['~X-bar(X)~'] /'~s[X]),\n    ylab=expression('['~Y-bar(Y)~'] /'~s[Y]),\n    pch=16, cex=.5, col=grey(0,.2))\nabline(a=0, b=1, lty=2)\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-128-1.png){width=672}\n:::\n:::\n\n\nWhen polishing, you must do two things\n\n* Add details that are necessary to understand the figure\n* Remove unnecessary details (see e.g., <https://www.edwardtufte.com/notebook/chartjunk/> and <https://www.biostat.wisc.edu/~kbroman/topten_worstgraphs/>)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Another Example\nxy_dat <- data.frame(x=x, y=y)\npar(fig=c(0,1,0,0.9), new=F)\nplot(y~x, xy_dat, pch=16, col=rgb(0,0,0,.05), cex=.5,\n    xlab='', ylab='') # Format Axis Labels Seperately\nmtext( 'y=0.25 x + e\\n e ~ standard-normal', 2, line=2.2)\nmtext( expression(x%in%~'[0,10]'), 1, line=2.2)\n#abline( lm(y~x, data=xy_dat), lty=2)\ntitle('Plot with good features, but too excessive in several ways',\n    adj=0, font.main=1)\n\n# Outer Legend (https://stackoverflow.com/questions/3932038/)\nouter_legend <- function(...) {\n  opar <- par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), \n    mar=c(0, 0, 0, 0), new=TRUE)\n  on.exit(par(opar))\n  plot(0, 0, type='n', bty='n', xaxt='n', yaxt='n')\n  legend(...)\n}\nouter_legend('topright', legend='single data point',\n    title='do you see the normal distribution?',\n    pch=16, col=rgb(0,0,0,.1), cex=1, bty='n')\n```\n\n::: {.cell-output-display}\n![](01-BasicStats_files/figure-html/unnamed-chunk-129-1.png){width=672}\n:::\n:::\n\n\nFor useful tips, see C. Wilke (2019) \"Fundamentals of Data Visualization: A Primer on Making Informative and\nCompelling Figures\" https://clauswilke.com/dataviz/\n\n#### **Saving**. {-}\nYou can export figures with specific dimensions\n\n::: {.cell}\n\n```{.r .cell-code}\npdf( 'Figures/plot_example.pdf', height=5, width=5)\n# plot goes here\ndev.off()\n```\n:::\n\nFor plotting math, see\nhttps://astrostatistics.psu.edu/su07/R/html/grDevices/html/plotmath.html and \nhttps://library.virginia.edu/data/articles/mathematical-annotation-in-r\n\nFor exporting options, see `?pdf`.\nFor saving other types of files, see `png(\"*.png\")`, `tiff(\"*.tiff\")`, and  `jpeg(\"*.jpg\")`\n\nFor some things to avoid, see https://www.data-to-viz.com/caveats.html\n\n#### **Interactive Figures**. {-}\n\n**Histograms**. See https://plotly.com/r/histograms/\n\n::: {.cell}\n\n```{.r .cell-code}\npop_mean <- mean(USArrests$UrbanPop)\nmurder_lowpop <- USArrests[USArrests$UrbanPop< pop_mean,'Murder']\nmurder_highpop <- USArrests[USArrests$UrbanPop>= pop_mean,'Murder']\n\nlibrary(plotly)\nfig <- plot_ly(alpha=0.6, \n    hovertemplate=\"%{y}\")\nfig <- fig %>% add_histogram(murder_lowpop, name='Low Pop. (< Mean)')\nfig <- fig %>% add_histogram(murder_highpop, name='High Pop (>= Mean)')\nfig <- fig %>% layout(barmode=\"stack\") # barmode=\"overlay\"\nfig <- fig %>% layout(\n    title=\"Crime and Urbanization in America 1975\",\n    xaxis = list(title='Murders Arrests per 100,000 People'),\n    yaxis = list(title='Number of States'),\n    legend=list(title=list(text='<b> % Urban Pop. </b>'))\n)\nfig\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-ad5be67f15e820ee3ed2\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-ad5be67f15e820ee3ed2\">{\"x\":{\"visdat\":{\"563c6d59f399\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"563c6d59f399\",\"attrs\":{\"563c6d59f399\":{\"hovertemplate\":\"%{y}\",\"alpha\":0.59999999999999998,\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"x\":[13.199999999999999,10,8.8000000000000007,17.399999999999999,2.6000000000000001,7.2000000000000002,2.2000000000000002,9.6999999999999993,2.1000000000000001,16.100000000000001,6,4.2999999999999998,2.1000000000000001,13,0.80000000000000004,14.4,3.7999999999999998,13.199999999999999,2.2000000000000002,8.5,5.7000000000000002,6.7999999999999998],\"type\":\"histogram\",\"name\":\"Low Pop. (< Mean)\",\"inherit\":true},\"563c6d59f399.1\":{\"hovertemplate\":\"%{y}\",\"alpha\":0.59999999999999998,\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"x\":[8.0999999999999996,9,7.9000000000000004,3.2999999999999998,5.9000000000000004,15.4,5.2999999999999998,10.4,6,15.4,11.300000000000001,4.4000000000000004,12.1,2.7000000000000002,9,12.199999999999999,7.4000000000000004,11.4,11.1,7.2999999999999998,6.5999999999999996,4.9000000000000004,6.2999999999999998,3.3999999999999999,12.699999999999999,3.2000000000000002,4,2.6000000000000001],\"type\":\"histogram\",\"name\":\"High Pop (>= Mean)\",\"inherit\":true}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"barmode\":\"stack\",\"title\":\"Crime and Urbanization in America 1975\",\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Murders Arrests per 100,000 People\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Number of States\"},\"legend\":{\"title\":{\"text\":\"<b> % Urban Pop. <\\/b>\"}},\"hovermode\":\"closest\",\"showlegend\":true},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"hovertemplate\":[\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\"],\"x\":[13.199999999999999,10,8.8000000000000007,17.399999999999999,2.6000000000000001,7.2000000000000002,2.2000000000000002,9.6999999999999993,2.1000000000000001,16.100000000000001,6,4.2999999999999998,2.1000000000000001,13,0.80000000000000004,14.4,3.7999999999999998,13.199999999999999,2.2000000000000002,8.5,5.7000000000000002,6.7999999999999998],\"type\":\"histogram\",\"name\":\"Low Pop. (< Mean)\",\"marker\":{\"color\":\"rgba(31,119,180,0.6)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,0.6)\"},\"error_x\":{\"color\":\"rgba(31,119,180,0.6)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"hovertemplate\":[\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\",\"%{y}\"],\"x\":[8.0999999999999996,9,7.9000000000000004,3.2999999999999998,5.9000000000000004,15.4,5.2999999999999998,10.4,6,15.4,11.300000000000001,4.4000000000000004,12.1,2.7000000000000002,9,12.199999999999999,7.4000000000000004,11.4,11.1,7.2999999999999998,6.5999999999999996,4.9000000000000004,6.2999999999999998,3.3999999999999999,12.699999999999999,3.2000000000000002,4,2.6000000000000001],\"type\":\"histogram\",\"name\":\"High Pop (>= Mean)\",\"marker\":{\"color\":\"rgba(255,127,14,0.6)\",\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"error_y\":{\"color\":\"rgba(255,127,14,0.6)\"},\"error_x\":{\"color\":\"rgba(255,127,14,0.6)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n**Boxplots**. See https://plotly.com/r/box-plots/\n\n::: {.cell}\n\n```{.r .cell-code}\nUSArrests$ID <- rownames(USArrests)\nfig <- plot_ly(USArrests,\n    y=~Murder, color=~cut(UrbanPop,4),\n    alpha=0.6, type=\"box\",\n    pointpos=0, boxpoints = 'all',\n    hoverinfo='text',    \n    text = ~paste('<b>', ID, '</b>',\n        \"<br>Urban  :\", UrbanPop,\n        \"<br>Assault:\", Assault,\n        \"<br>Murder :\", Murder))    \nfig <- layout(fig,\n    showlegend=FALSE,\n    title='Crime and Urbanization in America 1975',\n    xaxis = list(title = 'Percent of People in an Urban Area'),\n    yaxis = list(title = 'Murders Arrests per 100,000 People'))\nfig\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-1ed3baf16c6ef7730bda\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-1ed3baf16c6ef7730bda\">{\"x\":{\"visdat\":{\"563c3e8921f5\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"563c3e8921f5\",\"attrs\":{\"563c3e8921f5\":{\"y\":{},\"pointpos\":0,\"boxpoints\":\"all\",\"hoverinfo\":\"text\",\"text\":{},\"color\":{},\"alpha\":0.59999999999999998,\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"box\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"showlegend\":false,\"title\":\"Crime and Urbanization in America 1975\",\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Percent of People in an Urban Area\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Murders Arrests per 100,000 People\"},\"hovermode\":\"closest\"},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"fillcolor\":\"rgba(102,194,165,0.6)\",\"y\":[16.100000000000001,13,0.80000000000000004,3.7999999999999998,2.2000000000000002,5.7000000000000002],\"pointpos\":0,\"boxpoints\":\"all\",\"hoverinfo\":[\"text\",\"text\",\"text\",\"text\",\"text\",\"text\"],\"text\":[\"<b> Mississippi <\\/b> <br>Urban  : 44 <br>Assault: 259 <br>Murder : 16.1\",\"<b> North Carolina <\\/b> <br>Urban  : 45 <br>Assault: 337 <br>Murder : 13\",\"<b> North Dakota <\\/b> <br>Urban  : 44 <br>Assault: 45 <br>Murder : 0.8\",\"<b> South Dakota <\\/b> <br>Urban  : 45 <br>Assault: 86 <br>Murder : 3.8\",\"<b> Vermont <\\/b> <br>Urban  : 32 <br>Assault: 48 <br>Murder : 2.2\",\"<b> West Virginia <\\/b> <br>Urban  : 39 <br>Assault: 81 <br>Murder : 5.7\"],\"type\":\"box\",\"name\":\"(31.9,46.8]\",\"marker\":{\"color\":\"rgba(102,194,165,0.6)\",\"line\":{\"color\":\"rgba(102,194,165,1)\"}},\"line\":{\"color\":\"rgba(102,194,165,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"fillcolor\":\"rgba(252,141,98,0.6)\",\"y\":[13.199999999999999,10,8.8000000000000007,17.399999999999999,2.6000000000000001,2.2000000000000002,9.6999999999999993,2.1000000000000001,6,2.1000000000000001,14.4,13.199999999999999,6.7999999999999998],\"pointpos\":0,\"boxpoints\":\"all\",\"hoverinfo\":[\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\"],\"text\":[\"<b> Alabama <\\/b> <br>Urban  : 58 <br>Assault: 236 <br>Murder : 13.2\",\"<b> Alaska <\\/b> <br>Urban  : 48 <br>Assault: 263 <br>Murder : 10\",\"<b> Arkansas <\\/b> <br>Urban  : 50 <br>Assault: 190 <br>Murder : 8.8\",\"<b> Georgia <\\/b> <br>Urban  : 60 <br>Assault: 211 <br>Murder : 17.4\",\"<b> Idaho <\\/b> <br>Urban  : 54 <br>Assault: 120 <br>Murder : 2.6\",\"<b> Iowa <\\/b> <br>Urban  : 57 <br>Assault: 56 <br>Murder : 2.2\",\"<b> Kentucky <\\/b> <br>Urban  : 52 <br>Assault: 109 <br>Murder : 9.7\",\"<b> Maine <\\/b> <br>Urban  : 51 <br>Assault: 83 <br>Murder : 2.1\",\"<b> Montana <\\/b> <br>Urban  : 53 <br>Assault: 109 <br>Murder : 6\",\"<b> New Hampshire <\\/b> <br>Urban  : 56 <br>Assault: 57 <br>Murder : 2.1\",\"<b> South Carolina <\\/b> <br>Urban  : 48 <br>Assault: 279 <br>Murder : 14.4\",\"<b> Tennessee <\\/b> <br>Urban  : 59 <br>Assault: 188 <br>Murder : 13.2\",\"<b> Wyoming <\\/b> <br>Urban  : 60 <br>Assault: 161 <br>Murder : 6.8\"],\"type\":\"box\",\"name\":\"(46.8,61.5]\",\"marker\":{\"color\":\"rgba(252,141,98,0.6)\",\"line\":{\"color\":\"rgba(252,141,98,1)\"}},\"line\":{\"color\":\"rgba(252,141,98,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"fillcolor\":\"rgba(141,160,203,0.6)\",\"y\":[5.9000000000000004,7.2000000000000002,6,15.4,11.300000000000001,12.1,2.7000000000000002,9,4.2999999999999998,11.4,7.2999999999999998,6.5999999999999996,4.9000000000000004,6.2999999999999998,8.5,4,2.6000000000000001],\"pointpos\":0,\"boxpoints\":\"all\",\"hoverinfo\":[\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\"],\"text\":[\"<b> Delaware <\\/b> <br>Urban  : 72 <br>Assault: 238 <br>Murder : 5.9\",\"<b> Indiana <\\/b> <br>Urban  : 65 <br>Assault: 113 <br>Murder : 7.2\",\"<b> Kansas <\\/b> <br>Urban  : 66 <br>Assault: 115 <br>Murder : 6\",\"<b> Louisiana <\\/b> <br>Urban  : 66 <br>Assault: 249 <br>Murder : 15.4\",\"<b> Maryland <\\/b> <br>Urban  : 67 <br>Assault: 300 <br>Murder : 11.3\",\"<b> Michigan <\\/b> <br>Urban  : 74 <br>Assault: 255 <br>Murder : 12.1\",\"<b> Minnesota <\\/b> <br>Urban  : 66 <br>Assault: 72 <br>Murder : 2.7\",\"<b> Missouri <\\/b> <br>Urban  : 70 <br>Assault: 178 <br>Murder : 9\",\"<b> Nebraska <\\/b> <br>Urban  : 62 <br>Assault: 102 <br>Murder : 4.3\",\"<b> New Mexico <\\/b> <br>Urban  : 70 <br>Assault: 285 <br>Murder : 11.4\",\"<b> Ohio <\\/b> <br>Urban  : 75 <br>Assault: 120 <br>Murder : 7.3\",\"<b> Oklahoma <\\/b> <br>Urban  : 68 <br>Assault: 151 <br>Murder : 6.6\",\"<b> Oregon <\\/b> <br>Urban  : 67 <br>Assault: 159 <br>Murder : 4.9\",\"<b> Pennsylvania <\\/b> <br>Urban  : 72 <br>Assault: 106 <br>Murder : 6.3\",\"<b> Virginia <\\/b> <br>Urban  : 63 <br>Assault: 156 <br>Murder : 8.5\",\"<b> Washington <\\/b> <br>Urban  : 73 <br>Assault: 145 <br>Murder : 4\",\"<b> Wisconsin <\\/b> <br>Urban  : 66 <br>Assault: 53 <br>Murder : 2.6\"],\"type\":\"box\",\"name\":\"(61.5,76.2]\",\"marker\":{\"color\":\"rgba(141,160,203,0.6)\",\"line\":{\"color\":\"rgba(141,160,203,1)\"}},\"line\":{\"color\":\"rgba(141,160,203,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"fillcolor\":\"rgba(231,138,195,0.6)\",\"y\":[8.0999999999999996,9,7.9000000000000004,3.2999999999999998,15.4,5.2999999999999998,10.4,4.4000000000000004,12.199999999999999,7.4000000000000004,11.1,3.3999999999999999,12.699999999999999,3.2000000000000002],\"pointpos\":0,\"boxpoints\":\"all\",\"hoverinfo\":[\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\"],\"text\":[\"<b> Arizona <\\/b> <br>Urban  : 80 <br>Assault: 294 <br>Murder : 8.1\",\"<b> California <\\/b> <br>Urban  : 91 <br>Assault: 276 <br>Murder : 9\",\"<b> Colorado <\\/b> <br>Urban  : 78 <br>Assault: 204 <br>Murder : 7.9\",\"<b> Connecticut <\\/b> <br>Urban  : 77 <br>Assault: 110 <br>Murder : 3.3\",\"<b> Florida <\\/b> <br>Urban  : 80 <br>Assault: 335 <br>Murder : 15.4\",\"<b> Hawaii <\\/b> <br>Urban  : 83 <br>Assault: 46 <br>Murder : 5.3\",\"<b> Illinois <\\/b> <br>Urban  : 83 <br>Assault: 249 <br>Murder : 10.4\",\"<b> Massachusetts <\\/b> <br>Urban  : 85 <br>Assault: 149 <br>Murder : 4.4\",\"<b> Nevada <\\/b> <br>Urban  : 81 <br>Assault: 252 <br>Murder : 12.2\",\"<b> New Jersey <\\/b> <br>Urban  : 89 <br>Assault: 159 <br>Murder : 7.4\",\"<b> New York <\\/b> <br>Urban  : 86 <br>Assault: 254 <br>Murder : 11.1\",\"<b> Rhode Island <\\/b> <br>Urban  : 87 <br>Assault: 174 <br>Murder : 3.4\",\"<b> Texas <\\/b> <br>Urban  : 80 <br>Assault: 201 <br>Murder : 12.7\",\"<b> Utah <\\/b> <br>Urban  : 80 <br>Assault: 120 <br>Murder : 3.2\"],\"type\":\"box\",\"name\":\"(76.2,91.1]\",\"marker\":{\"color\":\"rgba(231,138,195,0.6)\",\"line\":{\"color\":\"rgba(231,138,195,1)\"}},\"line\":{\"color\":\"rgba(231,138,195,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n**Scatterplots**. See https://plotly.com/r/bubble-charts/\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple Scatter Plot\n#plot(Assault~UrbanPop, USArrests, col=grey(0,.5), pch=16,\n#    cex=USArrests$Murder/diff(range(USArrests$Murder))*2,\n#    main='US Murder arrests (per 100,000)')\n\n# Scatter Plot\nUSArrests$ID <- rownames(USArrests)\nfig <- plot_ly(\n    USArrests, x = ~UrbanPop, y = ~Assault,\n    mode='markers',\n    type='scatter',\n    hoverinfo='text',\n    text = ~paste('<b>', ID, '</b>',\n        \"<br>Urban  :\", UrbanPop,\n        \"<br>Assault:\", Assault,\n        \"<br>Murder :\", Murder),\n    color=~Murder,\n    marker=list(\n        size=~Murder,\n        opacity=0.5,\n        showscale=T,  \n        colorbar = list(title='Murder Arrests (per 100,000)')))\nfig <- layout(fig,\n    showlegend=F,\n    title='Crime and Urbanization in America 1975',\n    xaxis = list(title = 'Percent of People in an Urban Area'),\n    yaxis = list(title = 'Assault Arrests per 100,000 People'))\nfig\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-698f92e23f6e5bd07007\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-698f92e23f6e5bd07007\">{\"x\":{\"visdat\":{\"563c307f26ea\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"563c307f26ea\",\"attrs\":{\"563c307f26ea\":{\"x\":{},\"y\":{},\"mode\":\"markers\",\"hoverinfo\":\"text\",\"text\":{},\"marker\":{\"size\":{},\"opacity\":0.5,\"showscale\":true,\"colorbar\":{\"title\":\"Murder Arrests (per 100,000)\"}},\"color\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"showlegend\":false,\"title\":\"Crime and Urbanization in America 1975\",\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Percent of People in an Urban Area\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Assault Arrests per 100,000 People\"},\"hovermode\":\"closest\"},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"x\":[58,48,80,50,91,78,77,72,80,60,83,54,83,65,57,66,52,66,51,67,85,74,66,44,70,53,62,81,56,89,70,86,45,44,75,68,67,72,87,48,45,59,80,80,32,63,73,39,66,60],\"y\":[236,263,294,190,276,204,110,238,335,211,46,120,249,113,56,115,109,249,83,300,149,255,72,259,178,109,102,252,57,159,285,254,337,45,120,151,159,106,174,279,86,188,201,120,48,156,145,81,53,161],\"mode\":\"markers\",\"hoverinfo\":[\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\"],\"text\":[\"<b> Alabama <\\/b> <br>Urban  : 58 <br>Assault: 236 <br>Murder : 13.2\",\"<b> Alaska <\\/b> <br>Urban  : 48 <br>Assault: 263 <br>Murder : 10\",\"<b> Arizona <\\/b> <br>Urban  : 80 <br>Assault: 294 <br>Murder : 8.1\",\"<b> Arkansas <\\/b> <br>Urban  : 50 <br>Assault: 190 <br>Murder : 8.8\",\"<b> California <\\/b> <br>Urban  : 91 <br>Assault: 276 <br>Murder : 9\",\"<b> Colorado <\\/b> <br>Urban  : 78 <br>Assault: 204 <br>Murder : 7.9\",\"<b> Connecticut <\\/b> <br>Urban  : 77 <br>Assault: 110 <br>Murder : 3.3\",\"<b> Delaware <\\/b> <br>Urban  : 72 <br>Assault: 238 <br>Murder : 5.9\",\"<b> Florida <\\/b> <br>Urban  : 80 <br>Assault: 335 <br>Murder : 15.4\",\"<b> Georgia <\\/b> <br>Urban  : 60 <br>Assault: 211 <br>Murder : 17.4\",\"<b> Hawaii <\\/b> <br>Urban  : 83 <br>Assault: 46 <br>Murder : 5.3\",\"<b> Idaho <\\/b> <br>Urban  : 54 <br>Assault: 120 <br>Murder : 2.6\",\"<b> Illinois <\\/b> <br>Urban  : 83 <br>Assault: 249 <br>Murder : 10.4\",\"<b> Indiana <\\/b> <br>Urban  : 65 <br>Assault: 113 <br>Murder : 7.2\",\"<b> Iowa <\\/b> <br>Urban  : 57 <br>Assault: 56 <br>Murder : 2.2\",\"<b> Kansas <\\/b> <br>Urban  : 66 <br>Assault: 115 <br>Murder : 6\",\"<b> Kentucky <\\/b> <br>Urban  : 52 <br>Assault: 109 <br>Murder : 9.7\",\"<b> Louisiana <\\/b> <br>Urban  : 66 <br>Assault: 249 <br>Murder : 15.4\",\"<b> Maine <\\/b> <br>Urban  : 51 <br>Assault: 83 <br>Murder : 2.1\",\"<b> Maryland <\\/b> <br>Urban  : 67 <br>Assault: 300 <br>Murder : 11.3\",\"<b> Massachusetts <\\/b> <br>Urban  : 85 <br>Assault: 149 <br>Murder : 4.4\",\"<b> Michigan <\\/b> <br>Urban  : 74 <br>Assault: 255 <br>Murder : 12.1\",\"<b> Minnesota <\\/b> <br>Urban  : 66 <br>Assault: 72 <br>Murder : 2.7\",\"<b> Mississippi <\\/b> <br>Urban  : 44 <br>Assault: 259 <br>Murder : 16.1\",\"<b> Missouri <\\/b> <br>Urban  : 70 <br>Assault: 178 <br>Murder : 9\",\"<b> Montana <\\/b> <br>Urban  : 53 <br>Assault: 109 <br>Murder : 6\",\"<b> Nebraska <\\/b> <br>Urban  : 62 <br>Assault: 102 <br>Murder : 4.3\",\"<b> Nevada <\\/b> <br>Urban  : 81 <br>Assault: 252 <br>Murder : 12.2\",\"<b> New Hampshire <\\/b> <br>Urban  : 56 <br>Assault: 57 <br>Murder : 2.1\",\"<b> New Jersey <\\/b> <br>Urban  : 89 <br>Assault: 159 <br>Murder : 7.4\",\"<b> New Mexico <\\/b> <br>Urban  : 70 <br>Assault: 285 <br>Murder : 11.4\",\"<b> New York <\\/b> <br>Urban  : 86 <br>Assault: 254 <br>Murder : 11.1\",\"<b> North Carolina <\\/b> <br>Urban  : 45 <br>Assault: 337 <br>Murder : 13\",\"<b> North Dakota <\\/b> <br>Urban  : 44 <br>Assault: 45 <br>Murder : 0.8\",\"<b> Ohio <\\/b> <br>Urban  : 75 <br>Assault: 120 <br>Murder : 7.3\",\"<b> Oklahoma <\\/b> <br>Urban  : 68 <br>Assault: 151 <br>Murder : 6.6\",\"<b> Oregon <\\/b> <br>Urban  : 67 <br>Assault: 159 <br>Murder : 4.9\",\"<b> Pennsylvania <\\/b> <br>Urban  : 72 <br>Assault: 106 <br>Murder : 6.3\",\"<b> Rhode Island <\\/b> <br>Urban  : 87 <br>Assault: 174 <br>Murder : 3.4\",\"<b> South Carolina <\\/b> <br>Urban  : 48 <br>Assault: 279 <br>Murder : 14.4\",\"<b> South Dakota <\\/b> <br>Urban  : 45 <br>Assault: 86 <br>Murder : 3.8\",\"<b> Tennessee <\\/b> <br>Urban  : 59 <br>Assault: 188 <br>Murder : 13.2\",\"<b> Texas <\\/b> <br>Urban  : 80 <br>Assault: 201 <br>Murder : 12.7\",\"<b> Utah <\\/b> <br>Urban  : 80 <br>Assault: 120 <br>Murder : 3.2\",\"<b> Vermont <\\/b> <br>Urban  : 32 <br>Assault: 48 <br>Murder : 2.2\",\"<b> Virginia <\\/b> <br>Urban  : 63 <br>Assault: 156 <br>Murder : 8.5\",\"<b> Washington <\\/b> <br>Urban  : 73 <br>Assault: 145 <br>Murder : 4\",\"<b> West Virginia <\\/b> <br>Urban  : 39 <br>Assault: 81 <br>Murder : 5.7\",\"<b> Wisconsin <\\/b> <br>Urban  : 66 <br>Assault: 53 <br>Murder : 2.6\",\"<b> Wyoming <\\/b> <br>Urban  : 60 <br>Assault: 161 <br>Murder : 6.8\"],\"marker\":{\"colorbar\":{\"title\":\"Murder Arrests (per 100,000)\",\"ticklen\":2},\"cmin\":0.80000000000000004,\"cmax\":17.399999999999999,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666667\",\"rgba(70,19,97,1)\"],[\"0.0833333333333333\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":true,\"color\":[13.199999999999999,10,8.0999999999999996,8.8000000000000007,9,7.9000000000000004,3.2999999999999998,5.9000000000000004,15.4,17.399999999999999,5.2999999999999998,2.6000000000000001,10.4,7.2000000000000002,2.2000000000000002,6,9.6999999999999993,15.4,2.1000000000000001,11.300000000000001,4.4000000000000004,12.1,2.7000000000000002,16.100000000000001,9,6,4.2999999999999998,12.199999999999999,2.1000000000000001,7.4000000000000004,11.4,11.1,13,0.80000000000000004,7.2999999999999998,6.5999999999999996,4.9000000000000004,6.2999999999999998,3.3999999999999999,14.4,3.7999999999999998,13.199999999999999,12.699999999999999,3.2000000000000002,2.2000000000000002,8.5,4,5.7000000000000002,2.6000000000000001,6.7999999999999998],\"size\":[13.199999999999999,10,8.0999999999999996,8.8000000000000007,9,7.9000000000000004,3.2999999999999998,5.9000000000000004,15.4,17.399999999999999,5.2999999999999998,2.6000000000000001,10.4,7.2000000000000002,2.2000000000000002,6,9.6999999999999993,15.4,2.1000000000000001,11.300000000000001,4.4000000000000004,12.1,2.7000000000000002,16.100000000000001,9,6,4.2999999999999998,12.199999999999999,2.1000000000000001,7.4000000000000004,11.4,11.1,13,0.80000000000000004,7.2999999999999998,6.5999999999999996,4.9000000000000004,6.2999999999999998,3.3999999999999999,14.4,3.7999999999999998,13.199999999999999,12.699999999999999,3.2000000000000002,2.2000000000000002,8.5,4,5.7000000000000002,2.6000000000000001,6.7999999999999998],\"opacity\":0.5,\"line\":{\"colorbar\":{\"title\":\"\",\"ticklen\":2},\"cmin\":0.80000000000000004,\"cmax\":17.399999999999999,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666667\",\"rgba(70,19,97,1)\"],[\"0.0833333333333333\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":false,\"color\":[13.199999999999999,10,8.0999999999999996,8.8000000000000007,9,7.9000000000000004,3.2999999999999998,5.9000000000000004,15.4,17.399999999999999,5.2999999999999998,2.6000000000000001,10.4,7.2000000000000002,2.2000000000000002,6,9.6999999999999993,15.4,2.1000000000000001,11.300000000000001,4.4000000000000004,12.1,2.7000000000000002,16.100000000000001,9,6,4.2999999999999998,12.199999999999999,2.1000000000000001,7.4000000000000004,11.4,11.1,13,0.80000000000000004,7.2999999999999998,6.5999999999999996,4.9000000000000004,6.2999999999999998,3.3999999999999999,14.4,3.7999999999999998,13.199999999999999,12.699999999999999,3.2000000000000002,2.2000000000000002,8.5,4,5.7000000000000002,2.6000000000000001,6.7999999999999998]}},\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[32,91],\"y\":[45,337],\"type\":\"scatter\",\"mode\":\"markers\",\"opacity\":0,\"hoverinfo\":\"none\",\"showlegend\":false,\"marker\":{\"colorbar\":{\"title\":\"Murder\",\"ticklen\":2},\"cmin\":0.80000000000000004,\"cmax\":17.399999999999999,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666667\",\"rgba(70,19,97,1)\"],[\"0.0833333333333333\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":true,\"color\":[0.80000000000000004,17.399999999999999],\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\nIf you have many point, you can also use a 2D histogram instead. https://plotly.com/r/2D-Histogram/.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfig <- plot_ly(\n    USArrests, x = ~UrbanPop, y = ~Assault)\nfig <- add_histogram2d(fig, nbinsx=25, nbinsy=25)\nfig\n```\n:::\n\n\n\n#### **Tables**. {-}\n\nYou can also export tables in a variety of formats, for other software programs to easily read \n\n```{.r .cell-code}\nlibrary(stargazer)\n# summary statistics\nstargazer(USArrests,\n    type='html', \n    summary=T,\n    title='Summary Statistics for USArrests')\n```\n\n\n<table style=\"text-align:center\"><caption><strong>Summary Statistics for USArrests</strong></caption>\n<tr><td colspan=\"6\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">Statistic</td><td>N</td><td>Mean</td><td>St. Dev.</td><td>Min</td><td>Max</td></tr>\n<tr><td colspan=\"6\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">Murder</td><td>50</td><td>7.788</td><td>4.356</td><td>0.800</td><td>17.400</td></tr>\n<tr><td style=\"text-align:left\">Assault</td><td>50</td><td>170.760</td><td>83.338</td><td>45</td><td>337</td></tr>\n<tr><td style=\"text-align:left\">UrbanPop</td><td>50</td><td>65.540</td><td>14.475</td><td>32</td><td>91</td></tr>\n<tr><td style=\"text-align:left\">Rape</td><td>50</td><td>21.232</td><td>9.366</td><td>7.300</td><td>46.000</td></tr>\n<tr><td colspan=\"6\" style=\"border-bottom: 1px solid black\"></td></tr></table>\n\nYou can create a basic interactive table to explore raw data.\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"USArrests\")\nlibrary(reactable)\nreactable(USArrests, filterable=T, highlight=T)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"reactable html-widget html-fill-item\" id=\"htmlwidget-f55662fd77682aab1341\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-f55662fd77682aab1341\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\".rownames\":[\"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\"Connecticut\",\"Delaware\",\"Florida\",\"Georgia\",\"Hawaii\",\"Idaho\",\"Illinois\",\"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\"Maryland\",\"Massachusetts\",\"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\"Nebraska\",\"Nevada\",\"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\"North Carolina\",\"North Dakota\",\"Ohio\",\"Oklahoma\",\"Oregon\",\"Pennsylvania\",\"Rhode Island\",\"South Carolina\",\"South Dakota\",\"Tennessee\",\"Texas\",\"Utah\",\"Vermont\",\"Virginia\",\"Washington\",\"West Virginia\",\"Wisconsin\",\"Wyoming\"],\"Murder\":[13.2,10,8.1,8.8,9,7.9,3.3,5.9,15.4,17.4,5.3,2.6,10.4,7.2,2.2,6,9.7,15.4,2.1,11.3,4.4,12.1,2.7,16.1,9,6,4.3,12.2,2.1,7.4,11.4,11.1,13,0.8,7.3,6.6,4.9,6.3,3.4,14.4,3.8,13.2,12.7,3.2,2.2,8.5,4,5.7,2.6,6.8],\"Assault\":[236,263,294,190,276,204,110,238,335,211,46,120,249,113,56,115,109,249,83,300,149,255,72,259,178,109,102,252,57,159,285,254,337,45,120,151,159,106,174,279,86,188,201,120,48,156,145,81,53,161],\"UrbanPop\":[58,48,80,50,91,78,77,72,80,60,83,54,83,65,57,66,52,66,51,67,85,74,66,44,70,53,62,81,56,89,70,86,45,44,75,68,67,72,87,48,45,59,80,80,32,63,73,39,66,60],\"Rape\":[21.2,44.5,31,19.5,40.6,38.7,11.1,15.8,31.9,25.8,20.2,14.2,24,21,11.3,18,16.3,22.2,7.8,27.8,16.3,35.1,14.9,17.1,28.2,16.4,16.5,46,9.5,18.8,32.1,26.1,16.1,7.3,21.4,20,29.3,14.9,8.3,22.5,12.8,26.9,25.5,22.9,11.2,20.7,26.2,9.3,10.8,15.6]},\"columns\":[{\"id\":\".rownames\",\"name\":\"\",\"type\":\"character\",\"sortable\":false,\"filterable\":false,\"rowHeader\":true},{\"id\":\"Murder\",\"name\":\"Murder\",\"type\":\"numeric\"},{\"id\":\"Assault\",\"name\":\"Assault\",\"type\":\"numeric\"},{\"id\":\"UrbanPop\",\"name\":\"UrbanPop\",\"type\":\"numeric\"},{\"id\":\"Rape\",\"name\":\"Rape\",\"type\":\"numeric\"}],\"filterable\":true,\"highlight\":true,\"dataKey\":\"c49af85e0b2038b0b71caf77db2bacb3\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\nFor further data exploration, your plots can also be made [interactive](https://r-graph-gallery.com/interactive-charts.html) via <https://plotly.com/r/>. For more details, see [examples](https://plotly-r.com/) and then [applications](https://bookdown.org/paulcbauer/applied-data-visualization/10-plotly.html).\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"plotly\")\nlibrary(plotly)\n```\n:::\n\n\n\n#### **Custom Figures**. {-}\n\nMany of the best plots are custom made (see https://www.r-graph-gallery.com/). Here are some ones that I have made over the years.\n\n<!-- ## CONVERT IMAGES\nfor pdfile in *.pdf ; do \nconvert -verbose -density 500  \"${pdfile}\" \"${pdfile%.*}\".png;\ndone\n-->\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](./Figures_Manual/Vegetation.png){width=2400}\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](./Figures_Manual/Balances_Trial.png){width=3000}\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](./Figures_Manual/PopulationDensity2.png){width=3000}\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](./Figures_Manual/SampleExample.png){width=1750}\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](./Figures_Manual/SemiInclusive_Example.png){width=3000}\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](./Figures_Manual/Stability_3.png){width=1000}\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](./Figures_Manual/EvolutionaryDynamics.png){width=3000}\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](./Figures_Manual/Experiment_Timeline.png){width=1750}\n:::\n:::\n\n\n\n\n## R-Markdown Reports\n\nWe will use R Markdown for communicating results to each other. Note that R and R Markdown are both languages. R studio interprets R code make statistical computations and interprets R Markdown code to produce pretty documents that contain both writing and statistics. Altogether, your project will use\n\n* R: does statistical computations\n* R Markdown: formats statistical computations for sharing\n* Rstudio: graphical user interface that allows you to easily use both R and R Markdown.\n\nHomework reports are probably the smallest document you can create. These little reports are almost entirely self-contained (showing both code and output). To make them, you will need to \n\nFirst install [Pandoc](http://pandoc.org) on your computer.\n\nThen install any required packages\n\n::: {.cell}\n\n```{.r .cell-code}\n# Packages for Rmarkdown\ninstall.packages(\"knitr\")\ninstall.packages(\"rmarkdown\")\n\n# Other packages frequently used\n#install.packages(\"plotly\") #for interactive plots\n#install.packages(\"sf\") #for spatial data\n```\n:::\n\n\n\nWe will create simple reproducible reports via R Markdown.\n\n#### **Example 1: Data Scientism**. {-}\n<!-- \n**Clean workspace**.\nDelete any temporary files which you do not want (or start a fresh session).\n\n(for example *summarytable_example.txt* and *plot_example.pdf* and section *Data analysis examples: custom figures*)\n-->\n\n\nSee [DataScientism.html](https://jadamso.github.io/Rbooks/Templates/DataScientism.html) and then create it by\n\n* Clicking the \"Code\" button in the top right and then \"Download Rmd\"\n* Open with Rstudio\n* Change the name and title *to your own*, make other edits\n* Then point-and-click \"knit\"\n\nAlternatively,\n\n* Download the source file from [DataScientism.Rmd](https://jadamso.github.io/Rbooks/Templates/DataScientism.Rmd)\n* Change the name and title *to your own*, make other edits\n* Use the console to run\n\n::: {.cell}\n\n```{.r .cell-code}\nrmarkdown::render('DataScientism.Rmd')\n```\n:::\n\n\n#### **Example 2: Homework Assignment**.  {-}\nBelow is a template of what homework questions (and answers) look like. Create a new *.Rmd* file from scratch and produce a *.html* file that looks similar to this:\n\n*Problem:*\nSimulate 100 random observations of the form $y=x\\beta+\\epsilon$ and plot the relationship. Plot and explore the data interactively via plotly, https://plotly.com/r/line-and-scatter/. Then play around with different styles, https://www.r-graph-gallery.com/13-scatter-plot.html, to best express your point.\n\n*Solution:*\nI simulate $400$ observations for $\\epsilon \\sim 2\\times N(0,1)$ and $\\beta=4$, as seen in this single chunk. Notice an upward trend.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulation\nn <- 100\nE <- rnorm(n)\nX <- seq(n)\nY <- 4*X + 2*E\n# Plot\nlibrary(plotly)\ndat <- data.frame(X=X,Y=Y)\nplot_ly( data=dat, x=~X, y=~Y)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-21e81abc8fcc26c5ded2\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-21e81abc8fcc26c5ded2\">{\"x\":{\"visdat\":{\"563c1eea0da9\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"563c1eea0da9\",\"attrs\":{\"563c1eea0da9\":{\"x\":{},\"y\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20]}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"X\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Y\"},\"hovermode\":\"closest\",\"showlegend\":false},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],\"y\":[3.2099031982496227,9.2498680919275156,10.820422189796359,14.397096368498191,20.319445740845367,25.351911496794667,29.947217084671017,33.310452110787004,37.299360397651846,40.475278378943507,44.770855724791026,46.354721833395985,55.021229624624254,56.222879960354007,62.859336043005271,66.224884287956826,69.261357072029099,71.032757370235984,73.119101915215353,78.227962249160726,86.124835008268164,91.209388091384099,94.526198376743352,93.814988212264282,100.13461275468805,102.48791052576253,105.94404517021736,110.22976961304758,117.36314159699107,122.22155749831229,122.86842163201798,129.00507377517226,132.57735455554692,138.11606367246785,141.50323514654858,146.27239939602083,148.14248574007701,151.73663165644379,153.22361626715292,159.96935191622688,163.64092662838789,166.3679285467741,168.90828599008483,172.62064059879356,182.36069730158388,184.97258904498341,183.25800533719666,195.36803833353866,194.46609462283635,196.23649662080177,201.11178608867982,210.06582164105112,213.91263142992057,217.00681541974456,221.39417104010403,224.63372084833875,226.7238307792814,231.70218102590974,239.61443216835329,238.49560396892372,245.04860733617934,245.70862651868487,255.49114234983278,256.06018503227762,263.05990548891896,266.98534882627837,269.96577840410993,274.00831958665037,277.02432003431386,280.91828901485184,285.2976351812228,288.87390025790791,290.46372339574714,293.54795304813922,301.56588154129292,302.98289045836322,307.95373985619602,311.47011214609853,314.26287903426993,320.04956835232258,322.48083729808229,330.17970919910846,332.05747564149101,334.09684200102635,335.44514504164653,343.95991576198975,346.56799580194621,356.12888562041843,358.98258036500931,358.44144729464864,367.6017155265663,372.45123502113427,367.36840691642362,377.15143769211772,379.90783636878109,386.42506523071086,384.18758729731007,389.88696117178517,398.17962192370152,397.74445583228697],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"line\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n\n```{.r .cell-code}\n\n# To Do:\n# 1. Fit a regression line\n# 2. Color points by their residual value\n```\n:::\n\n\n\n<!---\n*Question 2:*\nVerify the definition of a line segment for points $A=(0,3), B=(1,5)$ using a $101 \\times 101$ grid. Recall a line segment is all points $s$ that have $d(s, A) + d(s, B) = d(A, B)$.\n\n*Answer* \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sf)\ns_1 <- c(0,3)\ns_2 <- c(1,5)\nSeg1 <- st_linestring( rbind(s_1,s_2) )\ngrid_pts <- expand.grid(\n    x=seq(s_1[1],s_2[1], length.out=101),\n    y=seq(s_1[2],s_2[2], length.out=101)\n)\n\nSeg1_dist <- dist( Seg1 )\ngrid_pts$dist <- apply(grid_pts, 1, function(s){\n    dist( rbind(s,s_1) ) + dist( rbind(s,s_2) ) })\ngrid_pts$seg <- grid_pts$dist == Seg1_dist\n\nD_point_seg <- st_multipoint( as.matrix(grid_pts[grid_pts$seg==T,1:2]) ) \nD_point_notseg <- st_multipoint( as.matrix(grid_pts[grid_pts$seg==F,1:2]) ) \n\nplot(Seg1)\npoints(D_point_notseg, col=2, pch='.')\npoints(D_point_seg, pch=16)\nbox()\n```\n:::\n\n--->\n\n\n## Further Reading\n\nFor more guidance on how to create Rmarkdown documents, see\n\n* https://github.com/rstudio/cheatsheets/blob/main/rmarkdown.pdf\n* https://cran.r-project.org/web/packages/rmarkdown/vignettes/rmarkdown.html\n* http://rmarkdown.rstudio.com\n* https://bookdown.org/yihui/rmarkdown/\n* https://bookdown.org/yihui/rmarkdown-cookbook/\n* https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/rmarkdown.html\n* An Introduction to the Advanced Theory and Practice of Nonparametric Econometrics. Raccine 2019. Appendices B \\& D.\n* https://rmd4sci.njtierney.com/using-rmarkdown.html\n* https://alexd106.github.io/intro2R/Rmarkdown_intro.html\n\nIf you are still lost, try one of the many online tutorials (such as these)\n\n* https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf\n* https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\n* https://www.neonscience.org/resources/learning-hub/tutorials/rmd-code-intro\n* https://m-clark.github.io/Introduction-to-Rmarkdown/\n* https://www.stat.cmu.edu/~cshalizi/rmarkdown/\n* http://math.wsu.edu/faculty/xchen/stat412/HwWriteUp.Rmd\n* http://math.wsu.edu/faculty/xchen/stat412/HwWriteUp.html\n* https://holtzy.github.io/Pimp-my-rmd/\n* https://ntaback.github.io/UofT_STA130/Rmarkdownforclassreports.html\n* https://crd150.github.io/hw_guidelines.html\n* https://r4ds.had.co.nz/r-markdown.html\n* http://www.stat.cmu.edu/~cshalizi/rmarkdown\n* http://www.ssc.wisc.edu/sscc/pubs/RFR/RFR_RMarkdown.html\n* http://kbroman.org/knitr_knutshell/pages/Rmarkdown.html\n\n\n\n# Bivariate Data\n\n\n## Statistics\n\nThere are several ways to quantitatively describe the relationship between two variables, $Y$ and $X$. The major differences surround whether the variables are cardinal, ordinal, or categorical.\n\n#### **Correlation**. {-}\n**Pearson (Linear) Correlation**.\nSuppose $X$ and $Y$ are both cardinal data. As such, you can compute the most famous measure of association, the covariance:\n$$\nC_{XY} =  \\sum_{i} [X_i - \\overline{X}] [Y_i - \\overline{Y}] / N\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bivariate Data from USArrests\nxy <- USArrests[,c('Murder','UrbanPop')]\n#plot(xy, pch=16, col=grey(0,.25))\ncov(xy)\n##             Murder   UrbanPop\n## Murder   18.970465   4.386204\n## UrbanPop  4.386204 209.518776\n```\n:::\n\nNote that $C_{XX}=V_{X}$.\nFor ease of interpretation, we rescale this statistic to always lay between $-1$ and $1$ \n$$\nr_{XY} = \\frac{ C_{XY} }{ \\sqrt{V_X} \\sqrt{V_Y}}\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(xy)[2]\n## [1] 0.06957262\n```\n:::\n\n\n**Falk Codeviance**.\nThe Codeviance is a robust alternative to Covariance. Instead of relying on means (which can be sensitive to outliers), it uses medians ($\\tilde{X}$) to capture the central tendency.^[See also *Theil-Sen Estimator*, which may be seen as a precursor.] We can also scale the Codeviance by the median absolute deviation to compute the median correlation.\n\\[\n\\text{CoDev}(X,Y) = \\text{Med}\\left\\{ |X_i - \\tilde{X}| |Y_i - \\tilde{Y}| \\right\\} \\\\\n\\tilde{r}_{XY} = \\frac{ \\text{CoDev}(X,Y) }{ \\text{MAD}(X) \\text{MAD}(Y) }.\n\\]\n\n::: {.cell}\n\n```{.r .cell-code}\ncor_m <- function(xy) {\n  # Compute medians for each column\n  med <- apply(xy, 2, median)\n  # Subtract the medians from each column\n  xm <- sweep(xy, 2, med, \"-\")\n  # Compute CoDev\n  CoDev <- median(xm[, 1] * xm[, 2])\n  # Compute the medians of absolute deviation\n  MadProd <- prod( apply(abs(xm), 2, median) )\n  # Return the robust correlation measure\n  return( CoDev / MadProd)\n}\ncor_m(xy)\n## [1] 0.005707763\n```\n:::\n\n\n#### **Kendall's Tau**. {-}\nSuppose $X$ and $Y$ are both *ordered* variables. Kendall's Tau measures the strength and direction of association by counting the number of concordant pairs (where the ranks agree) versus discordant pairs (where the ranks disagree). A value of \\(\\tau = 1\\) implies perfect agreement in rankings, \\(\\tau = -1\\) indicates perfect disagreement, and \\(\\tau = 0\\) suggests no association in the ordering.\n\\[\n\\tau = \\frac{2}{n(n-1)} \\sum_{i} \\sum_{j > i} \\text{sgn} \\Bigl( (X_i - X_j)(Y_i - Y_j) \\Bigr),\n\\]\nwhere the sign function is:\n$$\n\\text{sgn}(z) = \n\\begin{cases}\n+1 & \\text{if } z > 0\\\\\n0  & \\text{if } z = 0 \\\\\n-1 & \\text{if} z < 0 \n\\end{cases}.\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\nxy[,1] <- rank(xy[,1] )\nxy[,2] <- rank(xy[,2] )\n# plot(xy, pch=16, col=grey(0,.25))\ntau <- cor(xy[, 1], xy[, 2], method = \"kendall\")\nround(tau, 3)\n## [1] 0.074\n```\n:::\n\n\n#### **Cramer's V**. {-}\nSuppose $X$ and $Y$ are both *categorical* variables; the value of $X$ is one of $1...r$ categories and the value of $Y$ is one of $1...k$ categories. Cramer's V quantifies the strength of association by adjusting a \"chi-squared\" statistic to provide a measure that ranges from 0 to 1; 0 indicates no association while a value closer to 1 signifies a strong association. \n\nFirst, consider a contingency table for $X$ and $Y$ with \\(r\\) rows and \\(k\\) columns. The chi-square statistic is then defined as:\n\n\\[\n\\chi^2 = \\sum_{i=1}^{r} \\sum_{j=1}^{k} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}.\n\\]\n\nwhere\n\n- \\(O_{ij}\\) denote the observed frequency in cell \\((i, j)\\),\n- \\(E_{ij} = \\frac{R_i \\cdot C_j}{n}\\) is the expected frequency for each cell if $X$ and $Y$ are independent\n- \\(R_i\\) denote the total frequency for row \\(i\\) (i.e., \\(R_i = \\sum_{j=1}^{k} O_{ij}\\)),\n- \\(C_j\\) denote the total frequency for column \\(j\\) (i.e., \\(C_j = \\sum_{i=1}^{r} O_{ij}\\)),\n- \\(n\\) be the grand total of observations, so that \\(n = \\sum_{i=1}^{r} \\sum_{j=1}^{k} O_{ij}\\).\n\n\nSecond, normalize the chi-square statistic with the sample size and the degrees of freedom to compute Cramer's V. \n\n\\[\nV = \\sqrt{\\frac{\\chi^2 / n}{\\min(k - 1, \\, r - 1)}},\n\\]\n\nwhere:\n\n- \\(n\\) is the total sample size,\n- \\(k\\) is the number of categories for one variable,\n- \\(r\\) is the number of categories for the other variable.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\nxy[,1] <- cut(xy[,1],3)\nxy[,2] <- cut(xy[,2],4)\ntable(xy)\n##               UrbanPop\n## Murder         (31.9,46.8] (46.8,61.5] (61.5,76.2] (76.2,91.1]\n##   (0.783,6.33]           4           5           8           5\n##   (6.33,11.9]            0           4           7           6\n##   (11.9,17.4]            2           4           2           3\n\ncor_v <- function(xy){\n    # Create a contingency table from the categorical variables\n    tbl <- table(xy)\n    # Compute the chi-square statistic (without Yates' continuity correction)\n    chi2 <- chisq.test(tbl, correct=FALSE)$statistic\n    # Total sample size\n    n <- sum(tbl)\n    # Compute the minimum degrees of freedom (min(rows-1, columns-1))\n    df_min <- min(nrow(tbl) - 1, ncol(tbl) - 1)\n    # Calculate Cramer's V\n    V <- sqrt((chi2 / n) / df_min)\n    return(V)\n}\ncor_v(xy)\n## X-squared \n## 0.2307071\n\n# DescTools::CramerV( table(xy) )\n```\n:::\n\n\n\n## Probability Theory\nSuppose we have two discrete variables $X_{1}$ and $X_{2}$.\n\nTheir **joint distribution** is denoted as\n\\begin{eqnarray}\nProb(X_{1} = x_{1}, X_{2} = x_{2})\n\\end{eqnarray}\nThe **conditional distributions** are defined as\n\\begin{eqnarray}\nProb(X_{1} = x_{1} | X_{2} = x_{2}) = \\frac{ Prob(X_{1} = x_{1}, X_{2} = x_{2})}{ Prob( X_{2} = x_{2} )}\\\\\nProb(X_{2} = x_{2} | X_{1} = x_{1}) = \\frac{ Prob(X_{1} = x_{1}, X_{2} = x_{2})}{ Prob( X_{1} = x_{1} )}\n\\end{eqnarray}\nThe **marginal distributions** are then defined as\n\\begin{eqnarray}\nProb(X_{1} = x_{1}) = \\sum_{x_{2}} Prob(X_{1} = x_{1} | X_{2} = x_{2}) Prob( X_{2} = x_{2} ) \\\\\nProb(X_{2} = x_{2}) = \\sum_{x_{1}} Prob(X_{2} = x_{2} | X_{1} = x_{1}) Prob( X_{1} = x_{1} ),\n\\end{eqnarray}\nwhich is also known as the *law of total probability*.\n\n#### **Example: Fair Coin**. {-}\nFor one example, Consider flipping two coins. Denoted each coin as $i \\in \\{1, 2\\}$, and mark whether \"heads\" is face up; $X_{i}=1$ if Heads and $=0$ if Tails. Suppose both coins are \"fair\": $Prob(X_{1}=1)= 1/2$ and $Prob(X_{2}=1|X_{1})=1/2$, then the four potential outcomes have equal probabilities. The joint distribution is \n\\begin{eqnarray}\nProb(X_{1} = x_{1}, X_{2} = x_{2}) &=& Prob(X_{1} = x_{1}) Prob(X_{2} = x_{2})\\\\\nProb(X_{1} = 0, X_{2} = 0) &=& 1/2 \\times 1/2 = 1/4 \\\\\nProb(X_{1} = 0, X_{2} = 1) &=& 1/4 \\\\\nProb(X_{1} = 1, X_{2} = 0) &=& 1/4 \\\\\nProb(X_{1} = 1, X_{2} = 1) &=& 1/4 .\n\\end{eqnarray}\nThe marginal distribution of the second coin is \n\\begin{eqnarray}\nProb(X_{2} = 0) &=& Prob(X_{2} = 0 | X_{1} = 0) Prob(X_{1}=0) + Prob(X_{2} = 0 | X_{1} = 1) Prob(X_{1}=1)\\\\\n&=& 1/2 \\times 1/2 + 1/2 \\times 1/2 = 1/2\\\\\nProb(X_{2} = 1) &=& Prob(X_{2} = 1 | X_{1} = 0) Prob(X_{1}=0) + Prob(X_{2} = 1 | X_{1} = 1) Prob(X_{1}=1)\\\\\n&=& 1/2 \\times 1/2 + 1/2 \\times 1/2 = 1/2\n\\end{eqnarray}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a 2x2 matrix for the joint distribution.\n# Rows correspond to X1 (coin 1), and columns correspond to X2 (coin 2).\nP_fair <- matrix(1/4, nrow = 2, ncol = 2)\nrownames(P_fair) <- c(\"X1=0\", \"X1=1\")\ncolnames(P_fair) <- c(\"X2=0\", \"X2=1\")\nP_fair\n##      X2=0 X2=1\n## X1=0 0.25 0.25\n## X1=1 0.25 0.25\n\n# Compute the marginal distributions.\n# Marginal for X1: sum across columns.\nP_X1 <- rowSums(P_fair)\nP_X1\n## X1=0 X1=1 \n##  0.5  0.5\n# Marginal for X2: sum across rows.\nP_X2 <- colSums(P_fair)\nP_X2\n## X2=0 X2=1 \n##  0.5  0.5\n\n# Compute the conditional probabilities Prob(X2 | X1).\ncond_X2_given_X1 <- matrix(0, nrow = 2, ncol = 2)\nfor (j in 1:2) {\n  cond_X2_given_X1[, j] <- P_fair[, j] / P_X1[j]\n}\nrownames(cond_X2_given_X1) <- c(\"X2=0\", \"X2=1\")\ncolnames(cond_X2_given_X1) <- c(\"given X1=0\", \"given X1=1\")\ncond_X2_given_X1\n##      given X1=0 given X1=1\n## X2=0        0.5        0.5\n## X2=1        0.5        0.5\n```\n:::\n\n\n#### **Example: Unfair Coin**. {-}\nConsider a second example, where the second coin is \"Completely Unfair\", so that it is always the same as the first. The outcomes generated with a Completely Unfair coin are the same as if we only flipped one coin.\n\\begin{eqnarray}\nProb(X_{1} = x_{1}, X_{2} = x_{2}) &=& Prob(X_{1} = x_{1}) \\mathbf{1}( x_{1}=x_{2} )\\\\\nProb(X_{1} = 0, X_{2} = 0) &=& 1/2 \\\\\nProb(X_{1} = 0, X_{2} = 1) &=& 0 \\\\\nProb(X_{1} = 1, X_{2} = 0) &=& 0 \\\\\nProb(X_{1} = 1, X_{2} = 1) &=& 1/2 .\n\\end{eqnarray}\nNote that $\\mathbf{1}(X_{1}=1)$ means $X_{1}= 1$ and $0$ if $X_{1}\\neq0$.\nThe marginal distribution of the second coin is \n\\begin{eqnarray}\nProb(X_{2} = 0) \n&=& Prob(X_{2} = 0 | X_{1} = 0) Prob(X_{1}=0) + Prob(X_{2} = 0 | X_{1} = 1) Prob(X_{1} = 1)\\\\\n&=& 1/2 \\times 1 + 0 \\times 1/2 = 1/2 .\\\\\nProb(X_{2} = 1)\n&=& Prob(X_{2} = 1 | X_{1} =0) Prob( X_{1} = 0) + Prob(X_{2} = 1 | X_{1} = 1) Prob( X_{1} = 1)\\\\\n&=& 0\\times 1/2 + 1 \\times 1/2 = 1/2 .\n\\end{eqnarray}\nwhich is the same as in the first example! Different joint distributions can have the same marginal distributions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the joint distribution matrix for the unfair coin case.\nP_unfair <- matrix(c(0.5, 0, 0, 0.5), nrow = 2, ncol = 2, byrow = TRUE)\nrownames(P_unfair) <- c(\"X1=0\", \"X1=1\")\ncolnames(P_unfair) <- c(\"X2=0\", \"X2=1\")\nP_unfair\n##      X2=0 X2=1\n## X1=0  0.5  0.0\n## X1=1  0.0  0.5\n\n# Compute the marginal distribution for X2 in the unfair case.\nP_X2_unfair <- colSums(P_unfair)\nP_X1_unfair <- rowSums(P_unfair)\n\n# Compute the conditional probabilities Prob(X1 | X2) for the unfair coin.\ncond_X2_given_X1_unfair <- matrix(NA, nrow = 2, ncol = 2)\nfor (j in 1:2) {\n  if (P_X1_unfair[j] > 0) {\n    cond_X2_given_X1_unfair[, j] <- P_unfair[, j] / P_X1_unfair[j]\n  }\n}\nrownames(cond_X2_given_X1_unfair) <- c(\"X2=0\", \"X2=1\")\ncolnames(cond_X2_given_X1_unfair) <- c(\"given X1=0\", \"given X1=1\")\ncond_X2_given_X1_unfair\n##      given X1=0 given X1=1\n## X2=0          1          0\n## X2=1          0          1\n```\n:::\n\n\n#### **Bayes' Theorem**. {-}\nFinally, note **Bayes' Theorem**:\n\\begin{eqnarray}\nProb(X_{1} = x_{1} | X_{2} = x_{2})  Prob( X_{2} = x_{2}) \n    &=& Prob(X_{1} = x_{1}, X_{2} = x_{2}) = Prob(X_{2} = x_{2} | X_{1} = x_{1}) Prob(X_{1} = x_{1}).\\\\\nProb(X_{1} = x_{1} | X_{2} = x_{2})\n    &=& \\frac{ Prob(X_{2} = x_{2} | X_{1} = x_{1}) Prob(X_{1}=x_{1}) }{ Prob( X_{2} = x_{2}) }.\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Verify Bayes' theorem for the unfair coin case:\n# Compute Prob(X1=1 | X2=1) using the formula:\n#   Prob(X1=1 | X2=1) = [Prob(X2=1 | X1=1) * Prob(X1=1)] / Prob(X2=1)\n\nP_X1_1 <- 0.5\nP_X2_1_given_X1_1 <- 1  # Since coin 2 copies coin 1.\nP_X2_1 <- P_X2_unfair[\"X2=1\"]\n\nbayes_result <- (P_X2_1_given_X1_1 * P_X1_1) / P_X2_1\nbayes_result\n## X2=1 \n##    1\n```\n:::\n\n\n\n\n## Further Reading \n\nMany introductory econometrics textbooks have a good appendix on probability and statistics. There are many useful texts online too\n\nSee the Further reading about Probability Theory in the Statistics chaper.\n\n* https://www.r-bloggers.com/2024/03/calculating-conditional-probability-in-r/\n\n\nOther Statistics \n* https://cran.r-project.org/web/packages/qualvar/vignettes/wilcox1973.html\n\n\n",
    "supporting": [
      "01-BasicStats_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"site_libs/plotly-binding-4.11.0/plotly.js\"></script>\n<script src=\"site_libs/typedarray-0.1/typedarray.min.js\"></script>\n<script src=\"site_libs/jquery-3.5.1/jquery.min.js\"></script>\n<link href=\"site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\n<link href=\"site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/plotly-main-2.11.1/plotly-latest.min.js\"></script>\n<script src=\"site_libs/core-js-2.5.3/shim.min.js\"></script>\n<script src=\"site_libs/react-18.2.0/react.min.js\"></script>\n<script src=\"site_libs/react-18.2.0/react-dom.min.js\"></script>\n<script src=\"site_libs/reactwidget-2.0.0/react-tools.js\"></script>\n<link href=\"site_libs/reactable-0.4.4/reactable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/reactable-binding-0.4.4/reactable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}