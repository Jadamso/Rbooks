{
  "hash": "963d35483d61791f60921c34dea77f1f",
  "result": {
    "engine": "knitr",
    "markdown": "# (Re)Sampling \n***\n\nA *sample* is a subset of the population.\nA *simple random sample* is a sample where each possible sample of size n has the same probability of being selected.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#All possible samples of two from {1,2,3,4}\n#{1,2} {1,3}, {3,4}\n#{2,3} {2,4}\n#{3,4}\nchoose(4,2)\n## [1] 6\n\n# Simple random sample (no duplicates, equal probability)\nsample(1:4, 2, replace=F)\n## [1] 3 4\n```\n:::\n\n\nOften, we think of the population as being infinitely large. This is an approximation that makes mathematical and computational work much simpler. \n\n::: {.cell}\n\n```{.r .cell-code}\n#All possible samples of two from an enormous bag of numbers {1,2,3,4}\n#{1,1} {1,2} {1,3}, {3,4}\n#{2,2} {2,3} {2,4}\n#{3,3} {3,4}\n#{4,4}\n\n# Simple random sample (duplicates, equal probability)\nsample(1:4, 2, replace=T)\n## [1] 4 4\n```\n:::\n\n\nIntuition for infinite populations: imagine drawing names from a giant urn. If the urn has only 10 names, then removing one name slightly changes the composition of the urn, and the probabilities shift for the next name you draw. Now imagine the urn has 100 billion names, so that removing one makes no noticeable difference. We can pretend the composition never changes. Each draw is essentially identical and independent (iid).\n\n\n\n## Sample Distributions\n\nThe *sampling distribution* of a statistic shows us how much a statistic varies from sample to sample. \n\nFor example, see how the mean statistic varies from sample to sample to sample.\n\n:::{.callout-note icon=false collapse=\"true\"}\nGiven ages for population of $4$ students, compute the sampling distribution for the mean with samples of $n=2$.\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- c(19,21,20,17) #student ages\n# six possible samples\nmean1 <- mean( X[c(1,2)] ) #{1,2}\nmean2 <- mean( X[c(1,3)] ) #{1,3}\nmean3 <- mean( X[c(1,4)] ) #{3,4}\nmean4 <- mean( X[c(2,3)] ) #{2,3}\nmean5 <- mean( X[c(2,4)] ) #{2,4}\nmean6 <- mean( X[c(3,4)] ) #{3,4}\n# sampling distribution\nmeans <- c(mean1, mean2, mean3, mean4, mean5, mean6)\nhist(means, freq=F, breaks=100, border=F)\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nNow compute the sampling distribution for the median with samples of $n=3$.\n:::\n\n:::{.callout-note icon=false collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Three Sample Example from infinite population\nx_one <- runif(100)\nmean_one <- mean(x_one)\nx_two <- runif(100)\nmean_two <- mean(x_two)\nx_three <- runif(100)\nmean_three <- mean(x_three)\nsample_means <- c(mean_one, mean_two, mean_three)\nsample_means\n## [1] 0.5139512 0.4873696 0.5264171\n\n# An Equivalent Approach: fill vector in a loop\nsample_means <- vector(length=3)\nfor(i in seq(sample_means)){\n    x <- runif(100)\n    m <- mean(x)\n    sample_means[i] <- m\n}\nsample_means\n## [1] 0.5449075 0.5420176 0.4740485\n```\n:::\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Three Sample Example w/ Visual\npar(mfrow=c(1,3))\nfor(b in 1:3){\n    x <- runif(100) \n    m <-  mean(x)\n    hist(x,\n        breaks=seq(0,1,by=.1), #for comparability\n        freq=F, main=NA, border=NA)\n    abline(v=m, col=2, lwd=2)\n    title(paste0('mean= ', round(m,2)),  font.main=1)\n}\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nExamine the sampling distribution of the mean\n\n::: {.cell}\n\n```{.r .cell-code}\n# Many sample example\nsample_means <- vector(length=500)\nfor(i in seq(sample_means)){\n    x <- runif(1000)\n    m <- mean(x)\n    sample_means[i] <- m\n}\nhist(sample_means, \n    breaks=seq(0.45,0.55,by=.001),\n    border=NA, freq=F,\n    col=2, font.main=1, \n    main='Sampling Distribution of the mean')\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nIn this figure, you see two the most profound results known in statistics\n\n* *Law of Large Numbers*: the sample mean is centered around the true mean.\n* *Central Limit Theorem*: the sampling distribution of the statistic is approximately Normal.\n\n\n\n#### **Law of Large Numbers**. {-}\n\nThere are different variants of the Law of Large Numbers (LLN), but they all say some version of \"the sample mean is centered around the true mean\".\n\n:::{.callout-note icon=false collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# LLLN example\nm_LLLN <- mean(sample_means)\nround(m_LLLN, 3)\n## [1] 0.5\n```\n:::\n\n:::\n\n\n#### **Central Limit Theorem**. {-}\nThere are different variants of the central limit theorem (CLT), but they all say some version of \"the sampling distribution of a statistic is approximately normal\". For example, the sampling distribution of the mean, shown above, is approximately normal.\n\n:::{.callout-note icon=false collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(sample_means,\n    breaks=seq(0.45,0.55,by=.001),\n    border=NA, freq=F,\n    col=2, font.main=1, \n    main='Sampling Distribution of the mean')\n    \n## Approximately normal?\nmu <- mean(sample_means)\nmu_sd <- sd(sample_means)\nx <- seq(0.1, 0.9, by=0.001)\nfx <- dnorm(x, mu, mu_sd)\nlines(x, fx, col='red')\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n:::\n\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nFor an example with another statistic, let's the sampling distribution of the standard deviation.\n\n::: {.cell}\n\n```{.r .cell-code}\n# CLT example of the \"sd\" statistic\nsample_sds <- vector(length=1000)\nfor(i in seq(sample_sds)){\n    x <- runif(100) # same distribution\n    s <- sd(x) # different statistic\n    sample_sds[i] <- s\n}\nhist(sample_sds,\n    breaks=seq(0.2,0.4,by=.01),\n    border=NA, freq=F,\n    col=4, font.main=1, \n    main='Sampling Distribution of the sd')\n\n## Approximately normal?\nmu <- mean(sample_sds)\nmu_sd <- sd(sample_sds)\nx <- seq(0.1, 0.9, by=0.001)\nfx <- dnorm(x, mu, mu_sd)\nlines(x, fx, col='blue')\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Try another function, such as\nmy_function <- function(x){ diff(range(exp(x))) }\n\n# try another random variable, such as rexp(100) instead of runif(100)\n```\n:::\n\n:::\n\n\nIt is beyond this class to prove this result mathematically, but you should know that not all sampling distributions are standard normal. The CLT approximation is better for \"large $n$\" datasets with \"well behaved\" variances. The CLT does not apply to \"extreme\" statistics. \n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example of \"extreme\" statistics, examine the sampling distribution of min, median, max statistics.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 300 samples, each with 1000 random uniform variables\nx_samples <- matrix(nrow=300, ncol=1000)\nfor(i in seq(nrow(x_samples))){\n    x_samples[i,] <- runif(1000)\n}\n# Each row is a new sample\nlength(x_samples[1,])\n## [1] 1000\n\n# Compute min, median, and max for each sample\nx_mins <- apply(x_samples, 1, quantile, probs=0)\nx_meds <- apply(x_samples, 1, quantile, probs=.5)\nx_maxs <- apply(x_samples, 1, quantile, probs=1)\n\n# Plot the sampling distributions of min, median, and max\n# Median looks normal. Maximum and Minumum do not!\npar(mfrow=c(1,3))\nhist(x_mins, breaks=100, main='Min', font.main=1, border=NA, freq=F)\nhist(x_meds, breaks=100, main='Med', font.main=1, border=NA, freq=F)\nhist(x_maxs, breaks=100, main='Max', font.main=1, border=NA, freq=F)\ntitle('Sampling Distributions', outer=T, line=-1)\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n:::\n\n:::{.callout-tip icon=false collapse=\"true\"}\nHere is an example where variance is not \"well behaved\" .\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_means <- vector(length=1000)\nfor(i in seq(sample_means)){\n    x <- rcauchy(1000,0,10)\n    m <- mean(x)\n    sample_means[i] <- m\n} )\nhist(sample_means, breaks=100, border=NA, freq=F) # Tails look too \"fat\"\n```\n:::\n\n:::\n\n\n\n\n## Resampling\n\nOften, we only have one sample. How then can we estimate the sampling distribution of a statistic? \n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests[,'Murder']\nsample_mean <- mean(sample_dat)\nsample_mean\n## [1] 7.788\n```\n:::\n\n\nWe can \"resample\" our data. *Hesterberg (2015)* provides a nice illustration of the idea. The two most basic versions are the jackknife and the bootstrap, which are discussed below. \n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nNote that we do not use the mean of the resampled statistics as a replacement for the original estimate. This is because the resampled distributions are centered at the observed statistic, not the population parameter. (The bootstrapped mean is centered at the sample mean, for example, not the population mean.) This means that we cannot use resampling to improve on $\\hat{M}$. We use resampling to estimate sampling variability.\n\n#### **Jackknife Distribution**. {-}\nHere, we compute all \"leave-one-out\" estimates. Specifically, for a dataset with $n$ observations, the jackknife uses $n-1$ observations other than $i$ for each unique subsample. Taking the mean, for example, we have \n\\begin{itemize}\n\\item jackknifed estimates: $\\overline{x}^{Jack}_{i}=\\frac{1}{n-1} \\sum_{j \\neq i}^{n-1} \\hat{X}_{j}$\n\\item mean of the jackknife: $\\overline{x}^{Jack}=\\frac{1}{n} \\sum_{i}^{n} \\overline{x}^{Jack}_{i}$.\n\\item standard error of the jackknife: $\\hat{\\sigma}^{Jack}= \\sqrt{ \\frac{1}{n} \\sum_{i}^{n} \\left[\\overline{x}^{Jack}_{i} - \\overline{x}^{Jack} \\right]^2 }$.\n\\end{itemize}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests[,'Murder']\nsample_mean <- mean(sample_dat)\n\n# Jackknife Estimates\njackknife_means <- vector(length=length(sample_dat))\nfor(i in seq_along(jackknife_means)){\n    dat_noti <- sample_dat[-i]\n    mean_noti <- mean(dat_noti)\n    jackknife_means[i] <- mean_noti\n}\nhist(jackknife_means, breaks=25,\n    border=NA, freq=F,\n    main='', xlab=expression(hat(M)[-i]))\nabline(v=sample_mean, col='red', lty=2)\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n#### **Bootstrap Distribution**. {-}\nHere, we draw $n$ observations with replacement from the original data to create a bootstrap sample and calculate a statistic. Each bootstrap sample $b=1...B$ uses a random subset of observations to compute a statistic. We repeat that many times, say $B=9999$, to estimate the sampling distribution.\n\nConsider the sample mean as an example;\n\\begin{itemize}\n\\item bootstrap estimate: $\\overline{x}^{\\text{boot}}_{b}= \\frac{1}{n} \\sum_{i \\in N_b} \\hat{X}_{i} $\n\\item mean of the bootstrap: $\\overline{x}^{\\text{boot}}= \\frac{1}{B} \\sum_{b} \\overline{x}^{\\text{boot}}_{b}$.\n\\item standard error of the bootstrap: $\\hat{\\sigma}^{\\text{boot}}= \\sqrt{ \\frac{1}{B} \\sum_{b=1}^{B} \\left[\\overline{x}^{\\text{boot}}_{b} - \\overline{x}^{\\text{boot}} \\right]^2 }$.\n\\end{itemize}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap estimates\nbootstrap_means <- vector(length=9999)\nfor(b in seq_along(bootstrap_means)){\n    dat_b <- sample(sample_dat, replace=T) # c.f. jackknife\n    mean_b <- mean(dat_b)\n    bootstrap_means[b] <-mean_b\n}\n\nhist(bootstrap_means, breaks=25,\n    border=NA, freq=F,\n    main='', xlab=expression(hat(M)[b]))\nabline(v=sample_mean, col='red', lty=2)\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nWhy does this work? The sample: $\\{\\hat{X}_{1}, \\hat{X}_{2}, ... \\hat{X}_{n}\\}$ is drawn from a CDF $F$. Each bootstrap sample: $\\{\\hat{X}_{1}^{(b)}, \\hat{X}_{2}^{(b)}, ... \\hat{X}_{n}^{(b)}\\}$ is drawn from the ECDF $\\hat{F}$. With $\\hat{F} \\approx F$, each bootstrap sample is approximately a random sample. So when we compute a statistic on each bootstrap sample, we approximate the sampling distribution of the statistic.\n:::{.callout-note icon=false collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# theoretical probabilities\nx <- c(0,1)\nx_probs <- c(1/4, 3/4)\n# sample draws\ncoin_sample <- sample(x, prob=x_probs, 1000, replace=T)\nFhat <- ecdf(coin_sample)\n\nx_probs_boot <- c(Fhat(0), 1-Fhat(0))\nx_probs_boot # approximately the theoretical value\n## [1] 0.248 0.752\ncoin_resample <- sample(x, prob=x_probs_boot, 999, replace=T)\n# any draw from here is almost the same as the original process\n```\n:::\n\n:::\n\nNote that both resampling methods provide imperfect estimates, and can give different numbers. Percentiles of jackknife resamples are systematically less variable than they should be. Until you know more, a conservative approach is to take the larger estimate (typically the bootstrap).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Boot CI\nboot_ci <- quantile(bootstrap_means, probs=c(.025, .975))\nboot_ci\n##   2.5%  97.5% \n## 6.6139 8.9980\n\n# Jack CI\njack_ci <- quantile(jackknife_means, probs=c(.025, .975))\njack_ci\n##     2.5%    97.5% \n## 7.621582 7.904082\n\n# more conservative estimate\nci <- boot_ci\n```\n:::\n\n\n\n## Intervals\n\nUsing either the bootstrap or jackknife distribution for subsamples, or across multiple samples if we can get them, we can calculate \n\n* *Confidence Interval:* range your statistic varies across different samples.\n* *Standard Error*: variance of your statistic across different samples (square rooted).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 300 samples, each with 1000 random uniform variables\nx_samples <- matrix(nrow=300, ncol=1000)\nfor(i in seq(nrow(x_samples))){\n    x_samples[i,] <- runif(1000)\n}\n# Each row is a new sample\nsample_dat1 <- x_samples[1,]\nsd(sample_dat1) # standard deviation\n## [1] 0.2868836\n\n# Compute means for each row (for each sample)\nsample_means <- apply(x_samples, 1, mean)\nsd(sample_means) # standard error\n## [1] 0.009191805\n```\n:::\n\n\n\n#### **Percentile Intervals**.  {-}\nThis type of confidence interval is simply the upper and lower quantiles of the sampling distribution.\n\nFor example, consider the sample mean. We simulate the sampling distribution of the sample mean and construct a $90\\%$ confidence interval by taking the $5^{th}$ and $95^{th}$ percentiles of the simulated means. \n\n::: {.cell}\n\n```{.r .cell-code}\n\n# Middle 90%\nmq <- quantile(sample_means, probs=c(.05,.95))\npaste0('we are 90% confident that the mean is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\n## [1] \"we are 90% confident that the mean is between 0.48 and 0.52\"\n\nhist(sample_means,\n    breaks=seq(.4,.6, by=.001), \n    border=NA, freq=F,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='90% Confidence Interval for the Mean')\nabline(v=mq)\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nFor another example, consider the median. We now repeat the above process to estimate the median for each sample, instead of the mean. \n\n::: {.cell}\n\n```{.r .cell-code}\n## Sample Quantiles (medians)\nsample_quants <- apply(x_samples, 1, quantile, probs=0.5)\n\n# Middle 90% of estimates\nmq <- quantile(sample_quants, probs=c(.05,.95))\npaste0('we are 90% confident that the median is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\n## [1] \"we are 90% confident that the median is between 0.48 and 0.53\"\n\nhist(sample_quants,\n    breaks=seq(.4,.6, by=.001),\n    border=NA, freq=F,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='90% Confidence Interval for the Median')\nabline(v=mq)\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nNote that $Z\\%$ confidence intervals do not generally cover $Z\\%$ of the data (those types of intervals are covered later). In the examples above, notice the confidence interval for the mean differs from the confidence interval of the median, and so both cannot cover $90\\%$ of the data. The confidence interval for the mean is roughly $[0.48, 0.52]$, which theoretically covers only a $0.52-0.48=0.04$ proportion of uniform random data, much less than $90%$.\n\nOften, a $Z\\%$ interval means that $Z\\%$ of the intervals we generate will contain the true mean. E.g., suppose that we repeatedly sample data and construct $95\\%$ bootstrap confidence interval for the mean, then we expect that $95\\%$ of our constructed confidence intervals contain the theoretical population mean. A $95\\%$ coverage level does not imply a $95\\%$ probability that the true mean lies within the particular confidence interval you calculated for a particular sample.\n\n\nNote that confidence intervals shrink with more data, as averaging washes out random fluctuations. Here is the intuition.\n\n* With $n=1$ apple, your estimate depends entirely on that one draw. If it happens to be unusually large or small, your estimate can be far off.\n* With $n=2$ apples, the estimate averages out their idiosyncrasies. An unusually heavy apple can be balanced by a lighter one, lowering how far off you can be. You are less likely to get two extreme values than just one.\n* With $n=100$ apples, individual apples barely move the needle. The average becomes stable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- c(19,21,20,17) #student ages\n# six possible samples of size 2\nmean1 <- mean( X[c(1,2)] ) #{1,2}\nmean2 <- mean( X[c(1,3)] ) #{1,3}\nmean3 <- mean( X[c(1,4)] ) #{3,4}\nmean4 <- mean( X[c(2,3)] ) #{2,3}\nmean5 <- mean( X[c(2,4)] ) #{2,4}\nmean6 <- mean( X[c(3,4)] ) #{3,4}\nmeans_2 <- c(mean1, mean2, mean3, mean4, mean5, mean6)\nsd(means_2)\n## [1] 0.9354143\n# four possible samples of size 3\nmean1 <- mean( X[c(1,2,3)] ) \nmean2 <- mean( X[c(1,2,4)] ) \nmean3 <- mean( X[c(1,3,4)] ) \nmean4 <- mean( X[c(2,3,4)] ) \nmeans_3 <- c(mean1, mean2, mean3, mean4, mean5, mean6)\nsd(means_3)\n## [1] 0.5348936\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 300 samples, each of size n\nn <- 10000\nx_samples <- matrix(nrow=300, ncol=n)\nfor(i in seq(nrow(x_samples))){\n    x_samples[i,] <- runif(1000)\n}\n# Compute means for each row (for each sample)\nsample_means <- apply(x_samples, 1, mean)\nmq <- quantile(sample_means, probs=c(.05,.95))\npaste0('we are 90% confident that the mean is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\nhist(sample_means,\n    breaks=seq(.4,.6, by=.001), \n    border=NA, freq=F,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='90% Confidence Interval for the Mean (larger n)')\nabline(v=mq)\n```\n:::\n\n\n## Probability Theory\n\n#### **Means**. {-}\nThe LLN follows from a famous theoretical result in statistics, *Linearity of Expectations*: the expected value of a sum of random variables equals the sum of their individual expected values.  To be concrete, suppose we take $n$ random variables, each one denoted as $X_{i}$. Then, for constants $a,b,1/n$, we have\n\\begin{eqnarray}\n\\mathbb{E}[a X_{1}+ b X_{2}] &=& a \\mathbb{E}[X_{1}]+ b \\mathbb{E}[X_{2}]\\\\\n\\mathbb{E}\\left[M\\right] &=& \\mathbb{E}\\left[ \\sum_{i=1}^{n} X_{i}/n \\right] = \\sum_{i=1}^{n} \\mathbb{E}[X_{i}]/n\n\\end{eqnarray}\nAssuming each data point has identical means; $\\mathbb{E}[X_{i}]=\\mu$, the expected value of the sample average is the mean; $\\mathbb{E}\\left[M\\right] = \\sum_{i=1}^{n} \\mu/n = \\mu$.\n\nNote that the estimator $M$ differs from the particular estimate you calculated for your sample, $\\hat{M}$. For example, consider flipping a coin three times: $M$ corresponds to a theoretical value before you flip the coins and $\\hat{M}$ corresponds to a specific value after you flip the coins.\n\n:::{.callout-tip icon=false collapse=\"true\"}\nFor example, consider a coin flip with Heads $X_{i}=1$ having probability $p$ and Tails $X_{i}=0$ having probability $1-p$. First notice that $\\mathbb{E}[X_{i}]=p$. Then notice we can first \n\\begin{align*}\n\\mathbb{E}[X_{1}+X_{2}] \n&= [1+1][p \\times p] + [1+0][p \\times (1-p)] + [0+1][(1-p) \\times p] + [0+0][(1-p) \\times (1-p)] &  \\text{``HH + HT + TH + TT''} \\\\\n&= [1][p \\times p] + [1][p \\times (1-p)] + [0][(1-p) \\times p] + [0][(1-p) \\times (1-p)] &  \\text{first outcomes times prob.} \\\\\n&+ [1][p \\times p] + [0][p \\times (1-p)] + [1][(1-p) \\times p] + [0][(1-p) \\times (1-p)] &\n\\text{+second outcomes times prob.} \\\\\n&= [1][p \\times p] + [1][p \\times (1-p)] + [1][p \\times p] + [1][(1-p) \\times p] & \\text{drop zeros}\\\\\n&= 1p (p + [1-p]) +  1p (p + [1-p]) = p+p & \\text{algebra}\\\\\n&= \\mathbb{E}[X_{1}] +  \\mathbb{E}[X_{2}] .\n\\end{align*}\nThe theoretical mean is $\\mathbb{E}[\\frac{X_{1}+X_{2}}{2}]=\\frac{p+p}{2}=p$.\n:::\n\n\n#### **Variances**. {-}\nAnother famous theoretical result in statistics is that if we have independent and identical data (i.e., that each random variable $X_{i}$ has the same mean $\\mu$ and same variance $\\sigma^2$ and is drawn without any dependence on the previous draws), then the standard error of the sample mean is \"root n\" proportional to the theoretical standard error. Intuitively, this follows from thinking of the variance as a type of mean (the theoretical mean squared deviation from $\\mu$).\n\\begin{eqnarray}\n\\mathbb{V}\\left( M \\right) \n&=& \\mathbb{V}\\left( \\frac{\\sum_{i}^{n} X_{i}}{n} \\right) \n= \\sum_{i}^{n} \\mathbb{V}\\left(\\frac{X_{i}}{n}\\right)\n= \\sum_{i}^{n} \\frac{\\sigma^2}{n^2}\n= \\sigma^2/n\\\\\n\\mathbb{s}\\left(M\\right) &=& \\sqrt{\\mathbb{V}\\left( M \\right) } = \\sqrt{\\sigma^2/n} = \\sigma/\\sqrt{n}.\n\\end{eqnarray}\n\n\nNote that the *standard deviation* refers to variance within a single sample, and is hence different from the standard error. Nonetheless, it can be used to estimate the variability of the mean: we can estimate $\\mathbb{s}\\left(M\\right)$ with $\\hat{S}/\\sqrt{n}$, where $\\hat{S}$ is the standard deviation of the sample. This estimate is often a little different from than the bootstrap estimate, as it is based on idealistic theoretical assumptions whereas the bootstrap estimate is driven by data that are often not ideal.\n\n::: {.cell}\n\n```{.r .cell-code}\nboot_se <- sd(bootstrap_means)\n\ntheory_se <- sd(sample_dat)/sqrt(n)\n\nc(boot_se, theory_se)\n## [1] 0.6137248 0.8711020\n```\n:::\n\n\nAlso note that each additional data point you have provides more information, which ultimately decreases the standard error of your estimates. This is why statisticians will often recommend that you to get more data. However, the improvement in the standard error increases at a diminishing rate. In economics, this is known as diminishing returns and why economists may recommend you do not get more data. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nB <- 1000 # number of bootstrap samples\nNseq <- seq(1,100, by=1) # different sample sizes\n\nSE <- vector(length=length(Nseq))\nfor(n in Nseq){\n    sample_statistics_n <- vector(length=B)\n    for(b in seq(sample_statistics_n)){\n        x_b <- rnorm(n) # Sample of size n\n        x_stat_b <- quantile(x_b, probs=.4) # Stat of interest\n        sample_statistics_n[b] <- x_stat_b\n    }\n    se_n <- sd(sample_statistics_n) # How much the stat varies across samples\n    SE[n] <- se_n\n}\n\n\nplot(Nseq, SE, pch=16, col=grey(0,.5),\n    main='Absolute Gain', font.main=1,\n    ylab='standard error', xlab='sample size')\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n#plot(Nseq[-1], abs(diff(SE)), pch=16, col=grey(0,.5),\n#    main='Marginal Gain', font.main=1,\n#    ylab='decrease in standard error', xlab='sample size')\n```\n:::\n\n\n#### **Shape**. {-}\nSometimes, the sampling distribution is approximately normal (according to the CLT). In this case, you can use a standard error and the normal distribution to get a confidence interval.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standard Errors\nsd_theory <- sd(sample_dat1)/sqrt(length(sample_dat1))\n## Normal CI\nspread_theory <- qnorm(c(0.025, 0.975))\nci <- mean(sample_dat1) + spread_theory*sd_theory\n```\n:::\n\n\n\n\n#### **CI Coverage**. {-}\n\nGiven the sample mean, $M$, and the sample size, $n$, is large enough for the mean to be approximately normally distributed, we can construct a symmetric confidence interval $[M - E, M + E]$, where $E$ is some \"margin of error\" on either side of $M$. A coverage level of $1-\\alpha$ means $Prob( M - E < \\mu < M + E)=1-\\alpha$. I.e., if the same sampling procedure were repeated $100$ times from the same population, approximately $95$ of the resulting intervals would be expected to contain the true population mean.^[Notice that $Prob( M - E < \\mu < M + E) = Prob( - E < \\mu - M < + E) = Prob( E + mu > M > mu - E)$. So if the interval $[\\mu - 10, \\mu + 10]$ contains $95%$ of all $M$, then the interval $[M-10, M+10]$ will also contain $\\mu$ $95%$ of the time because whenever $M$ is within $10$ of $\\mu$, $\\mu$ is also within $10$ of $M$.] Note that a $95\\%$ coverage level does not imply a $95%$ probability that the true parameter lies within a particular calculated interval. E.g., if you compute $\\hat{M}=9$ for your particular sample, a coverage level of $1-\\alpha=95\\%$ does not mean $Prob(9 - E < \\mu < 9 + E)=95\\%$.\n\n:::{.callout-tip icon=false collapse=\"true\"}\nGiven the sample mean, $M$, and the sample size, $n$, is large enough for the mean to be approximately normally distributed. What confidence interval satisfies the following: the theoretical mean $\\mu$ is inside of the interval with probability $(1 - 0.05)%$ (i.e., for $(1 - 0.05)%$ of samples).\n:::\n\nFor a fixed sample size $n$, there is a trade-off between *Precision*: the width of a confidence interval, *Accuracy*: the probability that a confidence interval contains the theoretical value.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Confidence Interval for each sample\nxq <- apply(x_samples, 1, function(r){ #theoretical se's \n    mean(r) + c(-1,1)*sd(r)/sqrt(length(r))\n})\n# First 4 interval estimates\nxq[,1:4]\n##           [,1]      [,2]      [,3]      [,4]\n## [1,] 0.4990344 0.4984030 0.4841576 0.4756585\n## [2,] 0.5171785 0.5164427 0.5020911 0.4938873\n\n# Explicit calculation\nmu_true <- 0.5 # theoretical result for x_samples ~ uniform\n# Logical vector: whether the true mean is in each CI\ncovered <- mu_true >= xq[1, ] & mu_true <= xq[2, ]\n# Empirical coverage rate\ncoverage_rate <- mean(covered)\ncat(sprintf(\"Estimated coverage probability: %.2f%%\\n\", 100 * coverage_rate))\n## Estimated coverage probability: 70.33%\n\n# Theoretically: [-1 sd, +1 sd] has 2/3 coverage\n# Change to [-2 sd, +2 sd] to see Precision-Accuracy tradeoff.\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize first 100 confidence intervals\nN <- 100\nplot.new()\nplot.window(xlim = range(xq), ylim = c(0, N))\nfor (i in 1:N) {\n  col_i <- if (covered[i]) rgb(0, 0, 0, 0.3) else rgb(1, 0, 0, 0.5)\n  segments(xq[1, i], i, xq[2, i], i, col = col_i, lwd = 2)\n}\nabline(v = mu_true, col = \"blue\", lwd = 2)\naxis(1)\ntitle(\"Visualizing CI Coverage (Red = Missed)\")\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n\n\n## Further Reading\n\nSee \n\n* <https://www.r-bloggers.com/2025/02/bootstrap-vs-standard-error-confidence-intervals/>\n\n\n\nSee\n\n* <https://dlsun.github.io/probability/linearity.html>\n\n",
    "supporting": [
      "01_06_Sampling_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}