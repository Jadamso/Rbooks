{
  "hash": "78400606672da3d857b906a90a984de4",
  "result": {
    "engine": "knitr",
    "markdown": "\n\n# (Re)Sampling \n***\n\n## Sample Distributions\n\nThe *sampling distribution* of a statistic shows us how much a statistic varies from sample to sample.\n\nFor example, see how the mean varies from sample to sample to sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Three Sample Example\npar(mfrow=c(1,3))\nfor(i in 1:3){\n    x <- runif(100) \n    m <-  mean(x)\n    hist(x,\n        breaks=seq(0,1,by=.1), #for comparability\n        main=NA, border=NA)\n    abline(v=m, col=2, lwd=2)\n    title(paste0('mean= ', round(m,2)),  font.main=1)\n}\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nExamine the sampling distribution of the mean\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_means <- sapply(1:1000, function(i){\n    m <- mean(runif(100))\n    return(m)\n})\nhist(sample_means, breaks=50, border=NA,\n    col=2, font.main=1,\n    main='Sampling Distribution of the mean')\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nIn this figure, you see two the most profound results known in statistics\n\n* *Law of Large Numbers*: the sample mean is centered around the true mean.\n* *Central Limit Theorem*: the sampling distribution of the mean is approximately standard normal.\n\n#### **Central Limit Theorem**. {-}\nThere are actually many different variants of the central limit theorem, as it applies more generally: the sampling distribution of many statistics are standard normal. For example, examine the sampling distribution of the standard deviation.\n\n::: {.cell}\n\n```{.r .cell-code}\nthree_sds <- c(  sd(runif(100)),  sd(runif(100)),  sd(runif(100))  )\nthree_sds\n## [1] 0.2815339 0.2789808 0.3059586\n\nsample_sds <- sapply(1:1000, function(i){\n    s <- sd(runif(100))\n    return(s)\n})\nhist(sample_sds, breaks=50, border=NA,\n    col=4, font.main=1,\n    main='Sampling Distribution of the sd')\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nIt is beyond this class to prove this result, but you should know that not all sampling distributions are standard normal. For example, examine the sampling distribution of the three main \"order statistics\"\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 300 samples, each with 1000 random uniform variables\nx <- sapply(1:300, function(i) runif(1000) )\n# Each row is a new sample\nlength(x[1,])\n## [1] 300\n\n# Median looks normal, Maximum and Minumum do not!\nxmin <- apply(x,1,quantile, probs=0)\nxmed <- apply(x,1,quantile, probs=.5)\nxmax <- apply(x,1,quantile, probs=1)\npar(mfrow=c(1,3))\nhist(xmin, breaks=100, border=NA, main='Min', font.main=1)\nhist(xmed, breaks=100, border=NA, main='Med', font.main=1)\nhist(xmax, breaks=100, border=NA, main='Max', font.main=1)\ntitle('Sampling Distributions', outer=T, line=-1)\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# To explore, try any function!\nfun_of_rv <- function(f, n=100){\n  x <- runif(n)\n  y <- f(x)\n  return(y)\n}\n\nfun_of_rv( f=mean )\n## [1] 0.5123479\n\nfun_of_rv( f=function(i){ diff(range(exp(i))) } )\n## [1] 1.707181\n```\n:::\n\n\n\n## Intervals\n\nUsing either the bootstrap or jackknife distribution, we can calculate \n\n* *Confidence Interval:* range your statistic varies across different samples.\n* *Standard Error*: variance of your statistic across different samples.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_means <- apply(x,1,mean)\n# standard error\nsd(sample_means)\n## [1] 0.01709495\n```\n:::\n\n\nNote that in some cases, you can estimate the standard error to get a confidence interval.\n\n::: {.cell}\n\n```{.r .cell-code}\nx00 <- x[1,]\n# standard error\ns00 <- sd(x00)/sqrt(length(x00))\nci <- mean(x00) + c(1.96, -1.96)*s00\n```\n:::\n\n\n#### **Confidence Interval**.  {-}\nCompute the upper and lower quantiles of the sampling distribution.\n\n***Sample Mean***. We simulate the sampling distribution of the sample mean and construct a 90% confidence interval by taking the 5th and 95th percentiles of the simulated means. This gives an empirical estimate of the interval within which the true mean is expected to lie with 90% confidence, assuming repeated sampling.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Middle 90%\nmq <- quantile(sample_means, probs=c(.05,.95))\npaste0('we are 90% confident that the mean is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\n## [1] \"we are 90% confident that the mean is between 0.47 and 0.53\"\n\nbks <- seq(.4,.6,by=.001)\nhist(sample_means, breaks=bks, border=NA,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='Confidence Interval for the mean')\nabline(v=mq)\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n***Sample Percentile***. We repeat the process to estimate the 99th percentile for each sample. We then construct a 95% confidence interval for the 99th percentile estimator, using the 2.5th and 97.5th quantiles of these estimates.\n\n::: {.cell}\n\n```{.r .cell-code}\n## Upper Percentile\nsample_quants <- apply(x,1,quantile, probs=.99)\n\n# Middle 95% of estimates\nmq <- quantile(sample_quants, probs=c(.025,.975))\npaste0('we are 95% confident that the upper percentile is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\n## [1] \"we are 95% confident that the upper percentile is between 0.97 and 1\"\n\nbks <- seq(.92,1,by=.001)\nhist(sample_quants, breaks=bks, border=NA,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='95% Confidence Interval for the 99% percentile')\nabline(v=mq)\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nNote that X% confidence intervals do not generally cover X% of the data. Those intervals are a type of prediction interval that is covered later. See also <https://online.stat.psu.edu/stat200/lesson/4/4.4/4.4.2>.\n\n#### **Advanced Intervals**. {-}\nIn many cases, we want a X% interval to mean that X% of the intervals we generate will contain the true mean. E.g., in repeated sampling, 50% of constructed confidence intervals are expected to contain the true population mean.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Theoretically: [-1 sd, +1 sd] has 2/3 coverage\n\n# Confidence Interval for each sample\nxq <- apply(x,1, function(r){ #theoretical se's \n    mean(r) + c(-1,1)*sd(r)/sqrt(length(r))\n})\n# First 4 interval estimates\nxq[,1:4]\n##           [,1]      [,2]      [,3]      [,4]\n## [1,] 0.5036362 0.4868789 0.5126083 0.5040012\n## [2,] 0.5372960 0.5199911 0.5462762 0.5356384\n\n# Explicit calculation\nmu_true <- 0.5\n# Logical vector: whether the true mean is in each CI\ncovered <- mu_true >= xq[1, ] & mu_true <= xq[2, ]\n# Empirical coverage rate\ncoverage_rate <- mean(covered)\ncat(sprintf(\"Estimated coverage probability: %.2f%%\\n\", 100 * coverage_rate))\n## Estimated coverage probability: 68.90%\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize first N confidence intervals\nN <- 100\nplot.new()\nplot.window(xlim = range(xq), ylim = c(0, N))\nfor (i in 1:N) {\n  col_i <- if (covered[i]) rgb(0, 0, 0, 0.3) else rgb(1, 0, 0, 0.5)\n  segments(xq[1, i], i, xq[2, i], i, col = col_i, lwd = 2)\n}\nabline(v = mu_true, col = \"blue\", lwd = 2)\naxis(1)\ntitle(\"Visualizing CI Coverage (Red = Missed)\")\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThis differs from a *pointwise inclusion frequency interval*\n\n::: {.cell}\n\n```{.r .cell-code}\n# Frequency each point was in an interval\nbks <- seq(0,1,by=.01)\nxcovr <- sapply(bks, function(b){\n    bl <- b >= xq[1,]\n    bu <- b <= xq[2,]\n    mean( bl & bu )\n})\n# 50\\% Coverage\nc_ul <- range(bks[xcovr>=.5])\nc_ul # 50% confidence interval\n## [1] 0.49 0.51\n\nplot.new()\nplot.window(xlim=c(0,1), ylim=c(0,1))\npolygon( c(bks, rev(bks)), c(xcovr, xcovr*0), col=grey(.5,.5), border=NA)\nmtext('Frequency each value was in an interval',2, line=2.5)\naxis(1)\naxis(2)\nabline(h=.5, lwd=2)\nsegments(c_ul,0,c_ul,.5, lty=2)\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n## Resampling\n\nOften, we only have one sample. How then can we estimate the sampling distribution of a statistic? \n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests$Murder\nmean(sample_dat)\n## [1] 7.788\n```\n:::\n\n\nWe can \"resample\" our data. *Hesterberg (2015)* provides a nice illustration of the idea. The two most basic versions are the jackknife and the bootstrap, which are discussed below.\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n#### **Jackknife Distribution**. {-}\nHere, we compute all \"leave-one-out\" estimates. Specifically, for a dataset with $n$ observations, the jackknife uses $n-1$ observations other than $i$ for each unique subsample. Taking the mean, for example, we have \n\\begin{itemize}\n\\item jackknifed estimates: $\\overline{x}^{Jack}_{i}=\\frac{1}{n-1} \\sum_{j \\neq i}^{n-1} X_{j}$\n\\item mean of the jackknife: $\\overline{x}^{Jack}=\\frac{1}{n} \\sum_{i}^{n} \\overline{x}^{Jack}_{i}$.\n\\item standard error of the jackknife: $\\widehat{\\sigma}^{Jack}= \\sqrt{ \\frac{1}{n} \\sum_{i}^{n} \\left[\\overline{x}^{Jack}_{i} - \\overline{x}^{Jack} \\right]^2 }$.\n\\end{itemize}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests$Murder\nsample_mean <- mean(sample_dat)\n\n# Jackknife Estimates\nn <- length(sample_dat)\nJmeans <- sapply(1:n, function(i){\n    dati <- sample_dat[-i]\n    mean(dati)\n})\nhist(Jmeans, breaks=25, border=NA,\n    main='', xlab=expression(bar(X)[-i]))\nabline(v=sample_mean, col='red', lty=2)\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n#### **Bootstrap Distribution**. {-}\nHere, we draw $n$ observations with replacement from the original data to create a bootstrap sample and calculate a statistic. Each bootstrap sample $b=1...B$ uses a random set of observations (denoted $N_{b}$) to compute a statistic. We repeat that many times, say $B=9999$, to estimate the sampling distribution. Consider the sample mean as an example;\n\\begin{itemize}\n\\item bootstrap estimate: $\\overline{x}^{Boot}_{b}= \\frac{1}{n} \\sum_{i \\in N_b} X_{i} $\n\\item mean of the bootstrap: $\\overline{x}^{Boot}= \\frac{1}{B} \\sum_{b} \\overline{x}^{Boot}_{b}$.\n\\item standard error of the bootstrap: $\\widehat{\\sigma}^{Boot}= \\sqrt{ \\frac{1}{B} \\sum_{b=1}^{B} \\left[\\overline{x}^{Boot}_{b} - \\overline{x}^{Boot} \\right]^2 }$.\n\\end{itemize}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap estimates\nset.seed(2)\nBmeans <- sapply(1:10^4, function(i) {\n    dat_b <- sample(sample_dat, replace=T) # c.f. jackknife\n    mean(dat_b)\n})\n\nhist(Bmeans, breaks=25, border=NA,\n    main='', xlab=expression(bar(X)[b]))\nabline(v=sample_mean, col='red', lty=2)\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n***Caveat***. Note that we do not use the mean of the bootstrap or jackknife statistics as a replacement for the original estimate. This is because the bootstrap and jackknife distributions are centered at the observed statistic, not the population parameter. (The bootstrapped mean is centered at the sample mean, not the population mean.) This means that we cannot use the bootstrap to improve on $\\overline{x}$; no matter how many bootstrap samples we take. We can, however, use the jackknife and bootstrap to estimate sampling variability.\n\n\n#### **Intervals**. {-}\nNote that both methods provide imperfect estimates, and can give different numbers. Percentiles of jackknife resamples are systematically less variable than they should be. Until you know more, a conservative approach is to take the larger estimate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Boot CI\nboot_ci <- quantile(Bmeans, probs=c(.025, .975))\nboot_ci\n##    2.5%   97.5% \n## 6.58200 8.97005\n\n# Jack CI\njack_ci <- quantile(Jmeans, probs=c(.025, .975))\njack_ci\n##     2.5%    97.5% \n## 7.621582 7.904082\n\n# more conservative estimate\nci_est <- boot_ci\n```\n:::\n\n\nAlso note that the *standard deviation* refers to variance within a single sample, and is hence different from the standard error. Nonetheless, they can both be used to estimate the variability of a statistic.\n\n::: {.cell}\n\n```{.r .cell-code}\nboot_se <- sd(Bmeans)\n\nsample_sd <- sd(sample_dat)\n\nc(boot_se, sample_sd/sqrt(n))\n## [1] 0.6056902 0.6159621\n```\n:::\n\n\n#### **Value of More Data**.{-}\nEach additional data point you have provides more information, which ultimately decreases the standard error of your estimates. However, it does so at a decreasing rate (known in economics as diminishing returns).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNseq <- seq(1,100, by=1) # Sample sizes\nB <- 1000 # Number of draws per sample\n\nSE <- sapply(Nseq, function(n){\n    sample_statistics <- sapply(1:B, function(b){\n        x <- rnorm(n) # Sample of size N\n        quantile(x,probs=.4) # Statistic\n    })\n    sd(sample_statistics)\n})\n\npar(mfrow=c(1,2))\nplot(Nseq, SE, pch=16, col=grey(0,.5),\n    main='Absolute Gain', font.main=1,\n    ylab='standard error', xlab='sample size')\nplot(Nseq[-1], abs(diff(SE)), pch=16, col=grey(0,.5),\n    main='Marginal Gain', font.main=1,\n    ylab='decrease in standard error', xlab='sample size')\n```\n\n::: {.cell-output-display}\n![](01_06_Sampling_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n## Further Reading\n\nSee \n\n* https://www.r-bloggers.com/2025/02/bootstrap-vs-standard-error-confidence-intervals/\n\n\n",
    "supporting": [
      "01_06_Sampling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}