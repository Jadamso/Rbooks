{
  "hash": "e64430b5ddff0a04ca831a26138097b5",
  "result": {
    "engine": "knitr",
    "markdown": "# Basic Regression\n***\n\nSuppose we have some bivariate data. First, we inspect it as in Part I.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bivariate Data from USArrests\nxy <- USArrests[,c('Murder','UrbanPop')]\ncolnames(xy) <- c('y','x')\n\n# Inspect Dataset\n# head(xy)\n# summary(xy)\nplot(y~x, xy, col=grey(0,.5), pch=16)\ntitle('Murder and Urbanization in America 1975', font.main=1)\n```\n\n::: {.cell-output-display}\n![](02_01_BasicRegression_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nNow we will assess the association between variables by fitting a line through the data points using a \"regression\".\n\n## Simple Linear Regression\nThis refers to fitting a linear model to bivariate data. Specifically, our model is \n$$\ny_i=\\beta_{0}+\\beta_{1} x_i+\\epsilon_{i}\n$$\nand our objective function is\n$$\nmin_{\\beta_{0}, \\beta_{1}} \\sum_{i=1}^{N} \\left( \\epsilon_{i} \\right)^2 =  min_{\\beta_{0}, \\beta_{1}} \\sum_{i=1} \\left( y_i - [\\beta_{0}+\\beta_{1} x_i] \\right).\n$$\nMinimizing the sum of squared errors yields parameter estimates\n$$\n\\hat{\\beta_{0}}=\\bar{Y}-\\hat{\\beta_{1}}\\bar{X} \\\\\n\\hat{\\beta_{1}}=\\frac{\\sum_{i}^{}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i}^{}(x_i-\\bar{x})^2} = \\frac{C_{XY}}{V_{X}}\n$$\nand predictions\n$$\n\\hat{y}_i=\\hat{\\beta_{0}}+\\hat{\\beta}x_i\\\\\n\\hat{\\epsilon}_i=y_i-\\hat{y}_i\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run a Regression Coefficients\nreg <- lm(y~x, dat=xy)\n# predict(reg)\n# resid(reg)\n# coef(reg)\n```\n:::\n\n\n#### **Goodness of Fit**. {-}\nFirst, we qualitatively analyze the ''Goodness of fit'' of our model, we plot our predictions for a qualitative analysis\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot Data and Predictions\nlibrary(plotly)\nxy$ID <- rownames(USArrests)\nxy$pred <- predict(reg)\nxy$resid <- resid(reg)\nfig <- plotly::plot_ly(\n  xy, x=~x, y=~y,\n  mode='markers',\n  type='scatter',\n  hoverinfo='text',\n  marker=list(color=grey(0,.25), size=10),\n  text=~paste('<b>', ID, '</b>',\n              '<br>Urban  :', x,\n              '<br>Murder :', y,\n              '<br>Predicted Murder :', round(pred,2),\n              '<br>Residual :', round(resid,2)))              \n# Add Legend\nfig <- plotly::layout(fig,\n          showlegend=F,\n          title='Crime and Urbanization in America 1975',\n          xaxis = list(title='Percent of People in an Urban Area'),\n          yaxis = list(title='Homicide Arrests per 100,000 People'))\n# Plot Model Predictions\nadd_trace(fig, x=~x, y=~pred,\n    inherit=F, hoverinfo='none',\n    mode='lines+markers', type='scatter',\n    color=I('black'),\n    line=list(width=1/2),\n    marker=list(symbol=134, size=5))\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-41400c0b7b6379068f31\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-41400c0b7b6379068f31\">{\"x\":{\"visdat\":{\"5e58745f8438\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"5e58745f8438\",\"attrs\":{\"5e58745f8438\":{\"x\":{},\"y\":{},\"mode\":\"markers\",\"hoverinfo\":\"text\",\"marker\":{\"color\":\"#00000040\",\"size\":10},\"text\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\"},\"5e58745f8438.1\":{\"x\":{},\"y\":{},\"hoverinfo\":\"none\",\"mode\":\"lines+markers\",\"type\":\"scatter\",\"color\":[\"black\"],\"line\":{\"width\":0.5},\"marker\":{\"symbol\":134,\"size\":5},\"inherit\":false}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"showlegend\":false,\"title\":\"Crime and Urbanization in America 1975\",\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Percent of People in an Urban Area\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Homicide Arrests per 100,000 People\"},\"hovermode\":\"closest\"},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"x\":[58,48,80,50,91,78,77,72,80,60,83,54,83,65,57,66,52,66,51,67,85,74,66,44,70,53,62,81,56,89,70,86,45,44,75,68,67,72,87,48,45,59,80,80,32,63,73,39,66,60],\"y\":[13.199999999999999,10,8.0999999999999996,8.8000000000000007,9,7.9000000000000004,3.2999999999999998,5.9000000000000004,15.4,17.399999999999999,5.2999999999999998,2.6000000000000001,10.4,7.2000000000000002,2.2000000000000002,6,9.6999999999999993,15.4,2.1000000000000001,11.300000000000001,4.4000000000000004,12.1,2.7000000000000002,16.100000000000001,9,6,4.2999999999999998,12.199999999999999,2.1000000000000001,7.4000000000000004,11.4,11.1,13,0.80000000000000004,7.2999999999999998,6.5999999999999996,4.9000000000000004,6.2999999999999998,3.3999999999999999,14.4,3.7999999999999998,13.199999999999999,12.699999999999999,3.2000000000000002,2.2000000000000002,8.5,4,5.7000000000000002,2.6000000000000001,6.7999999999999998],\"mode\":\"markers\",\"hoverinfo\":[\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\"],\"marker\":{\"color\":\"#00000040\",\"size\":10,\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"text\":[\"<b> Alabama <\\/b> <br>Urban  : 58 <br>Murder : 13.2 <br>Predicted Murder : 7.63 <br>Residual : 5.57\",\"<b> Alaska <\\/b> <br>Urban  : 48 <br>Murder : 10 <br>Predicted Murder : 7.42 <br>Residual : 2.58\",\"<b> Arizona <\\/b> <br>Urban  : 80 <br>Murder : 8.1 <br>Predicted Murder : 8.09 <br>Residual : 0.01\",\"<b> Arkansas <\\/b> <br>Urban  : 50 <br>Murder : 8.8 <br>Predicted Murder : 7.46 <br>Residual : 1.34\",\"<b> California <\\/b> <br>Urban  : 91 <br>Murder : 9 <br>Predicted Murder : 8.32 <br>Residual : 0.68\",\"<b> Colorado <\\/b> <br>Urban  : 78 <br>Murder : 7.9 <br>Predicted Murder : 8.05 <br>Residual : -0.15\",\"<b> Connecticut <\\/b> <br>Urban  : 77 <br>Murder : 3.3 <br>Predicted Murder : 8.03 <br>Residual : -4.73\",\"<b> Delaware <\\/b> <br>Urban  : 72 <br>Murder : 5.9 <br>Predicted Murder : 7.92 <br>Residual : -2.02\",\"<b> Florida <\\/b> <br>Urban  : 80 <br>Murder : 15.4 <br>Predicted Murder : 8.09 <br>Residual : 7.31\",\"<b> Georgia <\\/b> <br>Urban  : 60 <br>Murder : 17.4 <br>Predicted Murder : 7.67 <br>Residual : 9.73\",\"<b> Hawaii <\\/b> <br>Urban  : 83 <br>Murder : 5.3 <br>Predicted Murder : 8.15 <br>Residual : -2.85\",\"<b> Idaho <\\/b> <br>Urban  : 54 <br>Murder : 2.6 <br>Predicted Murder : 7.55 <br>Residual : -4.95\",\"<b> Illinois <\\/b> <br>Urban  : 83 <br>Murder : 10.4 <br>Predicted Murder : 8.15 <br>Residual : 2.25\",\"<b> Indiana <\\/b> <br>Urban  : 65 <br>Murder : 7.2 <br>Predicted Murder : 7.78 <br>Residual : -0.58\",\"<b> Iowa <\\/b> <br>Urban  : 57 <br>Murder : 2.2 <br>Predicted Murder : 7.61 <br>Residual : -5.41\",\"<b> Kansas <\\/b> <br>Urban  : 66 <br>Murder : 6 <br>Predicted Murder : 7.8 <br>Residual : -1.8\",\"<b> Kentucky <\\/b> <br>Urban  : 52 <br>Murder : 9.7 <br>Predicted Murder : 7.5 <br>Residual : 2.2\",\"<b> Louisiana <\\/b> <br>Urban  : 66 <br>Murder : 15.4 <br>Predicted Murder : 7.8 <br>Residual : 7.6\",\"<b> Maine <\\/b> <br>Urban  : 51 <br>Murder : 2.1 <br>Predicted Murder : 7.48 <br>Residual : -5.38\",\"<b> Maryland <\\/b> <br>Urban  : 67 <br>Murder : 11.3 <br>Predicted Murder : 7.82 <br>Residual : 3.48\",\"<b> Massachusetts <\\/b> <br>Urban  : 85 <br>Murder : 4.4 <br>Predicted Murder : 8.2 <br>Residual : -3.8\",\"<b> Michigan <\\/b> <br>Urban  : 74 <br>Murder : 12.1 <br>Predicted Murder : 7.97 <br>Residual : 4.13\",\"<b> Minnesota <\\/b> <br>Urban  : 66 <br>Murder : 2.7 <br>Predicted Murder : 7.8 <br>Residual : -5.1\",\"<b> Mississippi <\\/b> <br>Urban  : 44 <br>Murder : 16.1 <br>Predicted Murder : 7.34 <br>Residual : 8.76\",\"<b> Missouri <\\/b> <br>Urban  : 70 <br>Murder : 9 <br>Predicted Murder : 7.88 <br>Residual : 1.12\",\"<b> Montana <\\/b> <br>Urban  : 53 <br>Murder : 6 <br>Predicted Murder : 7.53 <br>Residual : -1.53\",\"<b> Nebraska <\\/b> <br>Urban  : 62 <br>Murder : 4.3 <br>Predicted Murder : 7.71 <br>Residual : -3.41\",\"<b> Nevada <\\/b> <br>Urban  : 81 <br>Murder : 12.2 <br>Predicted Murder : 8.11 <br>Residual : 4.09\",\"<b> New Hampshire <\\/b> <br>Urban  : 56 <br>Murder : 2.1 <br>Predicted Murder : 7.59 <br>Residual : -5.49\",\"<b> New Jersey <\\/b> <br>Urban  : 89 <br>Murder : 7.4 <br>Predicted Murder : 8.28 <br>Residual : -0.88\",\"<b> New Mexico <\\/b> <br>Urban  : 70 <br>Murder : 11.4 <br>Predicted Murder : 7.88 <br>Residual : 3.52\",\"<b> New York <\\/b> <br>Urban  : 86 <br>Murder : 11.1 <br>Predicted Murder : 8.22 <br>Residual : 2.88\",\"<b> North Carolina <\\/b> <br>Urban  : 45 <br>Murder : 13 <br>Predicted Murder : 7.36 <br>Residual : 5.64\",\"<b> North Dakota <\\/b> <br>Urban  : 44 <br>Murder : 0.8 <br>Predicted Murder : 7.34 <br>Residual : -6.54\",\"<b> Ohio <\\/b> <br>Urban  : 75 <br>Murder : 7.3 <br>Predicted Murder : 7.99 <br>Residual : -0.69\",\"<b> Oklahoma <\\/b> <br>Urban  : 68 <br>Murder : 6.6 <br>Predicted Murder : 7.84 <br>Residual : -1.24\",\"<b> Oregon <\\/b> <br>Urban  : 67 <br>Murder : 4.9 <br>Predicted Murder : 7.82 <br>Residual : -2.92\",\"<b> Pennsylvania <\\/b> <br>Urban  : 72 <br>Murder : 6.3 <br>Predicted Murder : 7.92 <br>Residual : -1.62\",\"<b> Rhode Island <\\/b> <br>Urban  : 87 <br>Murder : 3.4 <br>Predicted Murder : 8.24 <br>Residual : -4.84\",\"<b> South Carolina <\\/b> <br>Urban  : 48 <br>Murder : 14.4 <br>Predicted Murder : 7.42 <br>Residual : 6.98\",\"<b> South Dakota <\\/b> <br>Urban  : 45 <br>Murder : 3.8 <br>Predicted Murder : 7.36 <br>Residual : -3.56\",\"<b> Tennessee <\\/b> <br>Urban  : 59 <br>Murder : 13.2 <br>Predicted Murder : 7.65 <br>Residual : 5.55\",\"<b> Texas <\\/b> <br>Urban  : 80 <br>Murder : 12.7 <br>Predicted Murder : 8.09 <br>Residual : 4.61\",\"<b> Utah <\\/b> <br>Urban  : 80 <br>Murder : 3.2 <br>Predicted Murder : 8.09 <br>Residual : -4.89\",\"<b> Vermont <\\/b> <br>Urban  : 32 <br>Murder : 2.2 <br>Predicted Murder : 7.09 <br>Residual : -4.89\",\"<b> Virginia <\\/b> <br>Urban  : 63 <br>Murder : 8.5 <br>Predicted Murder : 7.73 <br>Residual : 0.77\",\"<b> Washington <\\/b> <br>Urban  : 73 <br>Murder : 4 <br>Predicted Murder : 7.94 <br>Residual : -3.94\",\"<b> West Virginia <\\/b> <br>Urban  : 39 <br>Murder : 5.7 <br>Predicted Murder : 7.23 <br>Residual : -1.53\",\"<b> Wisconsin <\\/b> <br>Urban  : 66 <br>Murder : 2.6 <br>Predicted Murder : 7.8 <br>Residual : -5.2\",\"<b> Wyoming <\\/b> <br>Urban  : 60 <br>Murder : 6.8 <br>Predicted Murder : 7.67 <br>Residual : -0.87\"],\"type\":\"scatter\",\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"line\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[58,48,80,50,91,78,77,72,80,60,83,54,83,65,57,66,52,66,51,67,85,74,66,44,70,53,62,81,56,89,70,86,45,44,75,68,67,72,87,48,45,59,80,80,32,63,73,39,66,60],\"y\":[7.630152672499273,7.4208060843020238,8.0907151665332222,7.4626754019414738,8.3209964135501959,8.0488458488937713,8.0279111900740467,7.9232378959754222,8.0907151665332222,7.672021990138723,8.1535191429923959,7.546414037220373,8.1535191429923959,7.7766952842373476,7.6092180136795484,7.7976299430570721,7.5045447195809238,7.7976299430570721,7.4836100607611984,7.8185646018767976,8.1953884606318468,7.9651072136148722,7.7976299430570721,7.3370674490231238,7.8813685783359722,7.5254793784006484,7.713891307778173,8.1116498253529468,7.588283354859823,8.279127095910745,7.8813685783359722,8.2163231194515713,7.3580021078428492,7.3370674490231238,7.9860418724345967,7.8394992606965221,7.8185646018767976,7.9232378959754222,8.2372577782712959,7.4208060843020238,7.3580021078428492,7.6510873313189975,8.0907151665332222,8.0907151665332222,7.0858515431864246,7.7348259665978976,7.9441725547951467,7.2323941549244992,7.7976299430570721,7.672021990138723],\"hoverinfo\":[\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\"],\"mode\":\"lines+markers\",\"type\":\"scatter\",\"line\":{\"color\":\"rgba(0,0,0,1)\",\"width\":0.5},\"marker\":{\"color\":\"rgba(0,0,0,1)\",\"symbol\":134,\"size\":5,\"line\":{\"color\":\"rgba(0,0,0,1)\"}},\"textfont\":{\"color\":\"rgba(0,0,0,1)\"},\"error_y\":{\"color\":\"rgba(0,0,0,1)\"},\"error_x\":{\"color\":\"rgba(0,0,0,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\nFor a quantitative summary, we can also compute the linear correlation between the predictions and the data \n$$\nR = Cor( \\hat{y}_i, y)\n$$\nWith linear models, we typically compute $R^2$, known as the \"coefficient of determination\", using the sums of squared errors (Total, Explained, and Residual)\n$$\n\\underbrace{\\sum_{i}(y_i-\\bar{y})^2}_\\text{TSS}=\\underbrace{\\sum_{i}(\\hat{y}_i-\\bar{y})^2}_\\text{ESS}+\\underbrace{\\sum_{i}\\hat{\\epsilon_{i}}^2}_\\text{RSS}\\\\\nR^2 = \\frac{ESS}{TSS}=1-\\frac{RSS}{TSS}\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manually Compute R2\nEhat <- resid(reg)\nRSS  <- sum(Ehat^2)\nY <- xy$y\nTSS  <- sum((Y-mean(Y))^2)\nR2 <- 1 - RSS/TSS\nR2\n## [1] 0.00484035\n\n# Check R2\nsummary(reg)$r.squared\n## [1] 0.00484035\n\n# Double Check R2\nR <- cor(xy$y, predict(reg))\nR^2\n## [1] 0.00484035\n```\n:::\n\n\n\n## Variability Estimates\n\nA regression coefficient is a statistic. And, just like all statistics, we can calculate \n\n* *standard deviation*: variability within a single sample.\n* *standard error*: variability across different samples.\n* *confidence interval:* range your statistic varies across different samples.\n\n\nNote that values reported by your computer do not necessarily satisfy this definition. To calculate these statistics, we will estimate variability using *data-driven* methods. (For some theoretical background, see, e.g., https://www.sagepub.com/sites/default/files/upm-binaries/21122_Chapter_21.pdf.)\n\n#### **Jackknife**. {-}\nWe first consider the simplest, the jackknife. In this procedure, we loop through each row of the dataset. And, in each iteration of the loop, we drop that observation from the dataset and reestimate the statistic of interest. We then calculate the standard deviation of the statistic across all ``subsamples''.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Jackknife Standard Errors for OLS Coefficient\njack_regs <- lapply(1:nrow(xy), function(i){\n    xy_i <- xy[-i,]\n    reg_i <- lm(y~x, dat=xy_i)\n})\njack_coefs <- sapply(jack_regs, coef)['x',]\njack_se <- sd(jack_coefs)\n# classic_se <- sqrt(diag(vcov(reg)))[['x']]\n\n\n# Jackknife Sampling Distribution\nhist(jack_coefs, breaks=25,\n    main=paste0('SE est. = ', round(jack_se,4)),\n    font.main=1, border=NA,\n    xlab=expression(beta[-i]))\n# Original Estimate\nabline(v=coef(reg)['x'], lwd=2)\n# Jackknife Confidence Intervals\njack_ci_percentile <- quantile(jack_coefs, probs=c(.025,.975))\nabline(v=jack_ci_percentile, lty=2)\n```\n\n::: {.cell-output-display}\n![](02_01_BasicRegression_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n\n# Plot Normal Approximation\n# jack_ci_normal <- jack_mean+c(-1.96, +1.96)*jack_se\n# abline(v=jack_ci_normal, col=\"red\", lty=3)\n```\n:::\n\n\n#### **Bootstrap**. {-}\n\nThere are several resampling techniques. The other main one is the bootstrap, which resamples with *replacement* for an *arbitrary* number of iterations. When bootstrapping a dataset with $n$ observations, you randomly resample all $n$ rows in your data set $B$ times. Random subsampling is one of many hybrid approaches that tries to combine the best of both worlds.\n\n| | Sample Size per Iteration | Number of Iterations | Resample |\n| -------- | ------- | ------- | ------- |\nBootstrap | $n$     | $B$  | With Replacement |\nJackknife | $n-1$   | $n$  | Without Replacement |\nRandom Subsample | $m < n$ | $B$  | Without Replacement |\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap\nboot_regs <- lapply(1:399, function(b){\n    b_id <- sample( nrow(xy), replace=T)\n    xy_b <- xy[b_id,]\n    reg_b <- lm(y~x, dat=xy_b)\n})\nboot_coefs <- sapply(boot_regs, coef)['x',]\nboot_se <- sd(boot_coefs)\n\nhist(boot_coefs, breaks=25,\n    main=paste0('SE est. = ', round(boot_se,4)),\n    font.main=1, border=NA,\n    xlab=expression(beta[b]))\nboot_ci_percentile <- quantile(boot_coefs, probs=c(.025,.975))\nabline(v=boot_ci_percentile, lty=2)\nabline(v=coef(reg)['x'], lwd=2)\n```\n\n::: {.cell-output-display}\n![](02_01_BasicRegression_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random Subsamples\nrs_regs <- lapply(1:399, function(b){\n    b_id <- sample( nrow(xy), nrow(xy)-10, replace=F)\n    xy_b <- xy[b_id,]\n    reg_b <- lm(y~x, dat=xy_b)\n})\nrs_coefs <- sapply(rs_regs, coef)['x',]\nrs_se <- sd(rs_coefs)\n\nhist(rs_coefs, breaks=25,\n    main=paste0('SE est. = ', round(rs_se,4)),\n    font.main=1, border=NA,\n    xlab=expression(beta[b]))\nabline(v=coef(reg)['x'], lwd=2)\nrs_ci_percentile <- quantile(rs_coefs, probs=c(.025,.975))\nabline(v=rs_ci_percentile, lty=2)\n```\n\n::: {.cell-output-display}\n![](02_01_BasicRegression_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nWe can also bootstrap other statistics, such as a t-statistic or $R^2$. We do such things to test a null hypothesis, which is often ``no relationship''. We are rarely interested in computing standard errors and conducting hypothesis tests for two variables. However, we work through the ideas in the two-variable case to better understand the multi-variable case.\n\n## Hypothesis Tests\n\n#### **Invert a CI**. {-}\n\nOne main way to conduct hypothesis tests is to examine whether a confidence interval contains a hypothesized value. Does the slope coefficient equal $0$? For reasons we won't go into in this class, we typically normalize the coefficient by its standard error: $$ \\hat{t} = \\frac{\\hat{\\beta}}{\\hat{\\sigma}_{\\hat{\\beta}}} $$\n\n::: {.cell}\n\n```{.r .cell-code}\ntvalue <- coef(reg)['x']/jack_se\n\njack_t <- sapply(jack_regs, function(reg_b){\n    # Data\n    xy_b <- reg_b$model\n    # Coefficient\n    beta_b <- coef(reg_b)[['x']]\n    t_hat_b <- beta_b/jack_se\n    return(t_hat_b)\n})\n\nhist(jack_t, breaks=25,\n    main='Jackknife t Density',\n    font.main=1, border=NA,\n    xlab=expression(hat(t)[b]), \n    xlim=range(c(0, jack_t)) )\nabline(v=quantile(jack_t, probs=c(.025,.975)), lty=2)\nabline(v=0, col=\"red\", lwd=2)\n```\n\n::: {.cell-output-display}\n![](02_01_BasicRegression_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n#### **Impose the Null**.{-}\nWe can also compute a null distribution. We focus on the simplest: bootstrap simulations that each impose the null hypothesis and re-estimate the statistic of interest. Specifically, we compute the distribution of t-values on data with randomly reshuffled outcomes (imposing the null), and compare how extreme the observed value is.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Null Distribution for Beta\nboot_t0 <- sapply( 1:399, function(b){\n    xy_b <- xy\n    xy_b$y <- sample( xy_b$y, replace=T)\n    reg_b <- lm(y~x, dat=xy_b)\n    beta_b <- coef(reg_b)[['x']]\n    t_hat_b <- beta_b/jack_se\n    return(t_hat_b)\n})\n\n# Null Bootstrap Distribution\nboot_ci_percentile0 <- quantile(boot_t0, probs=c(.025,.975))\nhist(boot_t0, breaks=25,\n    main='Null Bootstrap Density',\n    font.main=1, border=NA,\n    xlab=expression(hat(t)[b]),\n    xlim=range(boot_t0))\nabline(v=boot_ci_percentile0, lty=2)\nabline(v=tvalue, col=\"red\", lwd=2)\n```\n\n::: {.cell-output-display}\n![](02_01_BasicRegression_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nAlternatively, you can impose the null by recentering the sampling distribution around the theoretical value; $$\\hat{t} = \\frac{\\hat{\\beta} - \\beta_{0} }{\\hat{\\sigma}_{\\hat{\\beta}}}.$$ Under some assumptions, the null distribution follows a t-distribution. (For more on parametric t-testing based on statistical theory, see https://www.econometrics-with-r.org/4-lrwor.html.)\n\n\nIn any case, we can calculate a *p-value*: the probability you would see something as extreme as your statistic under the null (assuming your null hypothesis was true). We can always calculate a p-value from an explicit null distribution.\n\n::: {.cell}\n\n```{.r .cell-code}\n# One Sided Test for P(t > boot_t | Null) = 1 - P(t < boot_t | Null)\nThat_NullDist1 <- ecdf(boot_t0)\nPhat1  <- 1-That_NullDist1(jack_t)\n\n# Two Sided Test for P(t > jack_t or t < -jack_t | Null)\nThat_NullDist2 <- ecdf(abs(boot_t0))\nplot(That_NullDist2, xlim=range(boot_t0, jack_t),\n    xlab=expression( abs(hat(t)[b]) ),\n    main='Null Bootstrap Distribution', font.main=1)\nabline(v=tvalue, col='red')\n```\n\n::: {.cell-output-display}\n![](02_01_BasicRegression_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\nPhat2  <-  1-That_NullDist2( abs(tvalue))\nPhat2\n## [1] 0.6240602\n```\n:::\n\n\n## Local Linear Regression\n\nIt is generally safe to assume that you could be analyzing data with nonlinear relationships. Here, our model can be represented as\n\\begin{eqnarray}\ny_{i} = m(x_{i}) + e_{i},\n\\end{eqnarray}\nwith $m$ being some unknown but smooth function. In such cases, linear regressions can still be useful.\n\n#### **Piecewise Regression**.{-}\nThe simplest case is *segmented/piecewise regression*, which runs a separate regression for different subsets of the data.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Globally Linear\nreg <- lm(y~x, data=xy)\n\n# Diagnose Fit\n#plot( fitted(reg), resid(reg), pch=16, col=grey(0,.5))\n#plot( xy$x, resid(reg), pch=16, col=grey(0,.5))\n\n# Linear in 2 Pieces (subsets)\nxcut2 <- cut(xy$x,2)\nxy_list2 <- split(xy, xcut2)\nregs2 <- lapply(xy_list2, function(xy_s){\n    lm(y~x, data=xy_s)\n})\nsapply(regs2, coef)\n##             (31.9,61.5] (61.5,91.1]\n## (Intercept)  -0.2836303  4.15337509\n## x             0.1628157  0.04760783\n\n# Linear in 3 Pieces (subsets or bins)\nxcut3 <- cut(xy$x, seq(32,92,by=20)) # Finer Bins\nxy_list3 <- split(xy, xcut3)\nregs3 <- lapply(xy_list3, function(xy_s){\n    lm(y~x, data=xy_s)\n})\nsapply(regs3, coef)\n##                (32,52]    (52,72]      (72,92]\n## (Intercept) 4.60313390 2.36291848  8.653829140\n## x           0.08233618 0.08132841 -0.007174454\n```\n:::\n\n\nCompare Predictions\n\n::: {.cell}\n\n```{.r .cell-code}\npred1 <- data.frame(yhat=predict(reg), x=reg$model$x)\npred1 <- pred1[order(pred1$x),]\n\npred2 <- lapply(regs2, function(reg){\n    data.frame(yhat=predict(reg), x=reg$model$x)\n})\npred2 <- do.call(rbind,pred2)\npred2 <- pred2[order(pred2$x),]\n\npred3 <- lapply(regs3, function(reg){\n    data.frame(yhat=predict(reg), x=reg$model$x)\n})\npred3 <- do.call(rbind,pred3)\npred3 <- pred3[order(pred3$x),]\n\n# Compare Predictions\nplot(y ~ x, pch=16, col=grey(0,.5), dat=xy)\nlines(yhat~x, pred1, lwd=2, col=2)\nlines(yhat~x, pred2, lwd=2, col=4)\nlines(yhat~x, pred3, lwd=2, col=3)\nlegend('topleft',\n    legend=c('Globally Linear', 'Peicewise Linear (2)','Peicewise Linear (3)'),\n    lty=1, col=c(2,4,3), cex=.8)\n```\n\n::: {.cell-output-display}\n![](02_01_BasicRegression_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n#### **Locally Linear**.{-}\nA less simple case is a **local linear regression** which conducts a linear regression for each data point using a subsample of data around it. \n\n::: {.cell}\n\n```{.r .cell-code}\n# ``Naive\" Smoother\npred_fun <- function(x0, h, xy){\n    # Assign equal weight to observations within h distance to x0\n    # 0 weight for all other observations\n    ki   <- dunif(xy$x, x0-h, x0+h) \n    llls <- lm(y~x, data=xy, weights=ki)\n    yhat_i <- predict(llls, newdata=data.frame(x=x0))\n}\n\nX0 <- sort(unique(xy$x))\npred_lo1 <- sapply(X0, pred_fun, h=2, xy=xy)\npred_lo2 <- sapply(X0, pred_fun, h=20, xy=xy)\n\nplot(y~x, pch=16, data=xy, col=grey(0,.5),\n    ylab='Murder Rate', xlab='Population Density')\ncols <- c(rgb(.8,0,0,.5), rgb(0,0,.8,.5))\nlines(X0, pred_lo1, col=cols[1], lwd=1, type='o')\nlines(X0, pred_lo2, col=cols[2], lwd=1, type='o')\nlegend('topleft', title='Locally Linear',\n    legend=c('h=2 ', 'h=20'),\n    lty=1, col=cols, cex=.8)\n```\n\n::: {.cell-output-display}\n![](02_01_BasicRegression_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nNote that there are more complex versions of local linear regressions (see https://shinyserv.es/shiny/kreg/ for a nice illustration.) An even more complex (and more powerful) version is **loess**, which uses adaptive bandwidths in order to have a similar number of data points in each subsample (especially useful when $X$ is not uniform.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Adaptive-width subsamples with non-uniform weights\nxy0 <- xy[order(xy$x),]\nplot(y~x, pch=16, col=grey(0,.5), dat=xy0)\n\nreg_lo4 <- loess(y~x, data=xy0, span=.4)\nreg_lo8 <- loess(y~x, data=xy0, span=.8)\n\ncols <- hcl.colors(3,alpha=.75)[-3]\nlines(xy0$x, predict(reg_lo4),\n    col=cols[1], type='o', pch=2)\nlines(xy0$x, predict(reg_lo8),\n    col=cols[2], type='o', pch=2)\n\nlegend('topleft', title='Loess',\n    legend=c('span=.4 ', 'span=.8'),\n    lty=1, col=cols, cex=.8)\n```\n\n::: {.cell-output-display}\n![](02_01_BasicRegression_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n#### **Confidence Bands**. {-}\nThe smoothed predicted values estimate the local means. So we can also construct confidence bands\n\n::: {.cell}\n\n```{.r .cell-code}\n# Loess\nxy0 <- xy[order(xy$x),]\nX0 <- unique(xy0$x)\nreg_lo <- loess(y~x, data=xy0, span=.8)\n\n# Jackknife CI\njack_lo <- sapply(1:nrow(xy), function(i){\n    xy_i <- xy[-i,]\n    reg_i <- loess(y~x, dat=xy_i, span=.8)\n    predict(reg_i, newdata=data.frame(x=X0))\n})\njack_cb <- apply(jack_lo,1, quantile,\n    probs=c(.025,.975), na.rm=T)\n\n# Plot\nplot(y~x, pch=16, col=grey(0,.5), dat=xy0)\npreds_lo <- predict(reg_lo, newdata=data.frame(x=X0))\nlines(X0, preds_lo,\n    col=hcl.colors(3,alpha=.75)[2],\n    type='o', pch=2)\n# Plot CI\npolygon(\n    c(X0, rev(X0)),\n    c(jack_cb[1,], rev(jack_cb[2,])),\n    col=hcl.colors(3,alpha=.25)[2],\n    border=NA)\n```\n\n::: {.cell-output-display}\n![](02_01_BasicRegression_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n",
    "supporting": [
      "02_01_BasicRegression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"site_libs/plotly-binding-4.11.0/plotly.js\"></script>\n<script src=\"site_libs/typedarray-0.1/typedarray.min.js\"></script>\n<script src=\"site_libs/jquery-3.5.1/jquery.min.js\"></script>\n<link href=\"site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\n<link href=\"site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/plotly-main-2.11.1/plotly-latest.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}