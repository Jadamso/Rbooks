{
  "hash": "416347f4fde463cc648ddd9223ed0db7",
  "result": {
    "engine": "knitr",
    "markdown": "# Statistical Theory\n***\n\n## Theoretical Statistics\n\n#### **Conditional Expectation**. {-}\n\nThe conditional expectation function\n\\begin{eqnarray}\nm(x) = \\mathbb{E}[Y_i|X_i=x]\n\\end{eqnarray}\nis the average value of $Y_i$ among observations with $X_i=x$. In empirical work, this is the population object that regressions try to approximate.\n\nFor discrete $X_i$, it is a weighted average over conditional probabilities:\n\\begin{eqnarray}\n\\mathbb{E}[Y_i|X_i=x] = \\sum_y y \\cdot Prob(Y_i=y|X_i=x).\n\\end{eqnarray}\nFor continuous $X_i$, we interpret $m(x)$ as a smooth curve indexed by $x$.\n\nThe key property is that conditional expectation is the best mean-squared predictor:\n\\begin{eqnarray}\nm(x) = \\arg\\min_{a(x)} \\mathbb{E}\\left[(Y_i-a(X_i))^2\\right].\n\\end{eqnarray}\nThis is why local and global least-squares methods are central: both are trying to estimate $m(x)$ under different shape restrictions.\n\nExample (discrete):\n\n|  | $x=0$ | $x=1$ |\n|:--:|:--:|:--:|\n| $y=0$ | 0.30 | 0.10 |\n| $y=1$ | 0.10 | 0.20 |\n| $y=2$ | 0.10 | 0.20 |\n\nFrom the table, $Prob(X_i=0)=0.5$ and $Prob(X_i=1)=0.5$. Then\n\\begin{eqnarray}\n\\mathbb{E}[Y_i|X_i=0] &=& 0\\cdot 0.6 + 1\\cdot 0.2 + 2\\cdot 0.2 = 0.6,\\\\\n\\mathbb{E}[Y_i|X_i=1] &=& 0\\cdot 0.2 + 1\\cdot 0.4 + 2\\cdot 0.4 = 1.2.\n\\end{eqnarray}\nSo moving from $x=0$ to $x=1$ increases the conditional mean by $0.6$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Joint probabilities: rows are y, columns are x\ny_vals <- c(0,1,2)\nx_vals <- c(0,1)\nP_yx <- matrix(c(\n  0.30, 0.10,\n  0.10, 0.20,\n  0.10, 0.20\n), nrow=3, byrow=TRUE)\n\n# Marginal Prob(X_i=x)\nP_x <- colSums(P_yx)\n\n# Conditional probabilities Prob(Y_i=y | X_i=x)\nP_y_given_x <- sweep(P_yx, 2, P_x, \"/\")\n\n# Conditional expectation m(x)=E[Y_i|X_i=x]\nm_x <- colSums(P_y_given_x * y_vals)\nm_x\n## [1] 0.6 1.2\n```\n:::\n\n\n#### **Consistency of Local Regression (LLLS/LOESS)**.{-}\n\nLocal least squares methods (LLLS, LOESS) estimate $m(x)=\\mathbb{E}[Y_i|X_i=x]$ using nearby points. Their consistency comes from two conditions as sample size grows:\n\n* neighborhoods shrink, so local bias decreases\n* local sample size still grows, so local variance decreases\n\nFor kernel/local methods this is often written as bandwidth conditions:\n\\begin{eqnarray}\nh_n \\to 0\n\\quad\\text{and}\\quad\nn h_n \\to \\infty.\n\\end{eqnarray}\nIn LOESS language, this corresponds to the span shrinking with $n$, but not too fast.\n\nThe simulation below illustrates this at one target point $x_0$: absolute error in estimating $m(x_0)$ tends to decrease with larger $n$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n\ntrue_m <- function(x) sin(2*x) + 0.5*x\nx0 <- 0.25\nn_grid <- c(60, 120, 240, 480)\nR <- 120\n\navg_abs_err <- sapply(n_grid, function(n){\n  span_n <- min(0.9, 1.8*n^(-1/4)) # shrinks with n\n  errs <- replicate(R, {\n    x <- runif(n, -1.5, 1.5)\n    y <- true_m(x) + rnorm(n, sd=0.35)\n    fit <- loess(y~x, span=span_n, degree=1)\n    mhat <- predict(fit, newdata=data.frame(x=x0))\n    abs(mhat - true_m(x0))\n  })\n  mean(errs, na.rm=TRUE)\n})\n\nplot(n_grid, avg_abs_err, type='b', pch=16, col=2,\n     xlab='Sample size (n)',\n     ylab='Average |mhat(x0)-m(x0)|',\n     main='LLLS/LOESS consistency illustration',\n     font.main=1)\n```\n\n::: {.cell-output-display}\n![](02_17_StatisticalTheory_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n#### **Bayes' Theorem**. {-}\n\nBayes' theorem maps predictive statements into inferential statements. In bivariate form,\n\\begin{eqnarray}\nProb(X_i=x \\mid Y_i=y)\n&=&\n\\frac{Prob(Y_i=y \\mid X_i=x)Prob(X_i=x)}{Prob(Y_i=y)}.\n\\end{eqnarray}\n\nInterpretation:\n\n* $Prob(X_i=x)$ is the prior probability for $X_i=x$.\n* $Prob(Y_i=y|X_i=x)$ is the likelihood of seeing $Y_i=y$ if $X_i=x$ is true.\n* $Prob(X_i=x|Y_i=y)$ is the posterior, your updated probability after seeing $Y_i=y$.\n\nA useful way to remember this is\n\\begin{eqnarray}\n\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}.\n\\end{eqnarray}\nFor two states, posterior odds are prior odds times a likelihood ratio.\n\nFor a concrete example, suppose a screening test has sensitivity 0.90 and false positive rate 0.08, while prevalence is 0.12:\n\\begin{eqnarray}\nProb(Y_i=1|X_i=1)=0.90,\\quad\nProb(Y_i=1|X_i=0)=0.08,\\quad\nProb(X_i=1)=0.12,\n\\end{eqnarray}\nwhere $X_i=1$ means \"condition present\" and $Y_i=1$ means \"test positive\".\n\nThen\n\\begin{eqnarray}\nProb(X_i=1|Y_i=1)\n&=&\n\\frac{0.90\\times0.12}{0.90\\times0.12 + 0.08\\times0.88}\n\\approx 0.605.\n\\end{eqnarray}\nEven with a good test, posterior probability depends strongly on prevalence.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# States: X in {0,1}, signal Y in {0,1}\n# Prior Prob(X_i=1)\np_x1 <- 0.12\n\n# Test characteristics\np_y1_x1 <- 0.90\np_y1_x0 <- 0.08\n\n# Law of total probability for Prob(Y_i=1)\np_y1 <- p_y1_x1 * p_x1 + p_y1_x0 * (1 - p_x1)\n\n# Bayes posterior Prob(X_i=1 | Y_i=1)\np_x1_y1 <- (p_y1_x1 * p_x1) / p_y1\np_x1_y1\n## [1] 0.6053812\n\n# Also compute Prob(X_i=1 | Y_i=0)\np_y0_x1 <- 1 - p_y1_x1\np_y0_x0 <- 1 - p_y1_x0\np_y0 <- p_y0_x1 * p_x1 + p_y0_x0 * (1 - p_x1)\np_x1_y0 <- (p_y0_x1 * p_x1) / p_y0\np_x1_y0\n## [1] 0.01460565\n```\n:::\n\n## Testing Theory\n\n#### **Theoretical Distributions**. {-}\nJust as with one sample tests, we can compute a standardized differences, where is converted into a statistic. Note, however, that we have to compute the standard error for the difference statistic, which is a bit more complicated. Under the assumption that both populations are independent distributed, we can analytically derive the sampling distribution for the differences between two groups.\n\nIn particular, the $t$-statistic is used to compare two groups.\n\\begin{eqnarray}\n\\hat{t} = \\frac{\n    \\hat{M}_{Y1} - \\hat{M}_{Y2}\n}{\n    \\sqrt{\\hat{S}_{Y1}+\\hat{S}_{Y2}}/\\sqrt{n}\n},\n\\end{eqnarray}\nWith normally distributed means, this statistic follows Student's [t-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution).\nWelch's [$t$-statistic](https://en.wikipedia.org/wiki/Welch%27s_t-test) is an adjustment for two normally distributed populations with potentially unequal variances or sample sizes. With the above assumptions, one can conduct hypothesis tests entirely using math.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample 1 (e.g., males)\nn1 <- 100\nY1 <- rnorm(n1, 0, 2)\n#hist(Y1, freq=F, main='Sample 1')\n\n# Sample 2 (e.g., females)\nn2 <- 80\nY2 <- rnorm(n2, 1, 1)\n#hist(Y2, freq=F, main='Sample 2')\n\nt.test(Y1, Y2, var.equal=F)\n## \n## \tWelch Two Sample t-test\n## \n## data:  Y1 and Y2\n## t = -4.6719, df = 141.64, p-value = 6.869e-06\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -1.4444412 -0.5855021\n## sample estimates:\n##  mean of x  mean of y \n## -0.0303432  0.9846285\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\nIf we want to test for the differences in medians across groups with independent observations, we can also use notches in the boxplot. If the notches of two boxes do not overlap, then there is rough evidence that the difference in medians is statistically significant. The square root of the sample size is also shown as the bin width in each boxplot.^[Let each group $g$ have median $\\tilde{M}_{g}$, interquartile range $\\hat{IQR}_{g}$, observations $n_{g}$. We can compute standard deviation of the median as $\\tilde{S}_{g}= \\frac{1.25 \\hat{IQR}_{g}}{1.35 \\sqrt{n_{g}}}$. As a rough guess, the interval $\\tilde{M}_{g} \\pm 1.7 \\tilde{S}_{g}$ is the historical default and displayed as a *notch* in the boxplot. See also <https://www.tandfonline.com/doi/abs/10.1080/00031305.1978.10479236>.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nY3 <- rnorm(n1, 3, 3)\n\nboxplot(Y1, Y2, Y3,\n    col=c(\n        rgb(1,0,0,.5),\n        rgb(0,1,0,.5),\n        rgb(0,0,1,.5)),\n    notch=T,\n    varwidth=T)\n```\n\n::: {.cell-output-display}\n![](02_17_StatisticalTheory_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nThere are also theoretical results for distributional comparisons\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Ecdat)\ndata(Caschool)\nCaschool[,'stratio'] <- Caschool[,'enrltot']/Caschool[,'teachers']\nkruskal.test(Caschool[,'stratio'], Caschool[,'county'])\n\n# Multiple pairwise tests\n# pairwise.wilcox.test(Caschool[,'stratio'], Caschool[,'county'])\n```\n:::\n\n\n\n## Type II Errors\n\nWhen we test a hypothesis, we start with a claim called the null hypothesis $H_0$ and an alternative claim $H_A$. Because we base conclusions on sample data, which has variability, mistakes are possible. There are two types of errors:\n\n* *Type I Error*: Rejecting a true null hypothesis. (False Positive).\n* *Type II Error*: Failing to reject a false null hypothesis (False Negative).\n\n| True Situation | Decision: Fail to Reject $H_0$ | Decision: Reject $H_0$ |\n|---|---|---|\n| $H_0$ is True  |  Correct (no detection)  |  Type I Error (False Positive) |\n| $H_0$ is False |  Type II Error (False Negative; missed detection) | Correct (effect detected) |\n\n:::{.callout-tip icon=false collapse=\"true\"}\nHere is a Courtroom example: Someone suspected of committing a crime is at trial, and they are either guilty or not (a Bernoulli random variable). You hypothesize that the suspect is innocent, and a jury can either convict them (decide guilty) or free them (decide not-guilty). Recall that fail-to-reject a hypothesis does mean accepting it, so deciding not-guilty does not necessarily mean innocent.\n\n| True Situation | Decision: Free | Decision: Convict |\n|---|---|---|\n| Suspect Innocent |  Correctly Freed  | Falsely Convicted |\n| Suspect Guilty   |  Falsely Freed    | Correctly Convicted |\n:::\n\n#### **Statistical Power**. {-}\n\nThe probability of Type I Error is called *significance level* and denoted by $Prob(\\text{Type I Error}) = \\alpha$. The probability of correctly rejecting a false null is called *power* and denoted by $\\text{Power} = 1 - \\beta = 1 -  Prob(\\text{Type II Error})$.\n\nSignificance is often chosen by statistical analysts to be $\\alpha=0.05$. Power is less often chosen, instead following from a decision about power.\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nThe code below runs a small simulation using a shifted, nonparametric bootstrap. Two-sided test; studentized statistic, for $H0: \\mu = 0$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Power for Two-sided test;\n# nonparametric bootstrap, studentized statistic\nn <- 25\nmu <- 0\nalpha <- 0.05\nB <- 299\n\nsim_reps <- 100\n\np_values <- vector(length=sim_reps)\nfor (i in seq(p_values)) {\n    # Generate data\n    X <- rnorm(n, mean=0.2, sd=1)\n    # Observed statistic\n    X_bar <- mean(X)\n    T_obs <-  (X_bar - mu) / (sd(X)/ sqrt(n)) ##studentized\n    # Bootstrap null distribution of the statistic\n    T_boot <- vector(length=B)\n    X_null <- X - X_bar + mu # Impose the null by recentering\n    for (b in seq(T_boot)) {\n      X_b <- sample(X_null, size = n, replace = TRUE)\n      T_b <- (mean(X_b) - mu) / (sd(X_b)/sqrt(n))\n      T_boot[b] <- T_b\n    }\n    # Two-sided bootstrap p-value\n    pval <- mean(abs(T_boot) >= abs(T_obs))\n    p_values[i] <- pval\n    }\npower <- mean(p_values < alpha)\npower\n```\n:::\n\n:::\n\nThere is an important Trade-off for fixed sample sizes: Increasing significance (fewer false positive) often lowers power (more false negatives). Generally, power depends on the effect size and sample size: bigger true effects and larger $n$ make it easier to detect real differences (higher power, lower $\\beta$).\n\n\n## Further Reading\n\nMany introductory econometrics textbooks have a good appendix on probability and statistics. There are many useful statistical texts online too\n\nSee the Further reading about Probability Theory in the Statistics chapter.\n\n* <https://www.r-bloggers.com/2024/03/calculating-conditional-probability-in-r/>\n",
    "supporting": [
      "02_17_StatisticalTheory_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}