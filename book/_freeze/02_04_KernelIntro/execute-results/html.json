{
  "hash": "60a110521a45b1de669fb2b95c56c24c",
  "result": {
    "engine": "knitr",
    "markdown": "# Local Regression\n***\n\n## Local Relationships \n\n#### **The Effect**.{-}\n\nThe interpretation of regression coefficients as ``the effect'' assumes the linear model is true. If you fit a line to a non-linear relationship then you will get back a number, but there is no singular *the* effect if the true relationship is non-linear! Consider a classic example, Anscombe's Quartet, which shows four very different datasets that give the same linear regression coefficient. You understand the problem because we used scatterplots to visual the data.^[The same principles holds when comparing two groups: <http://www.stat.columbia.edu/~gelman/research/published/causal_quartet_second_revision.pdf>]\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##################\n# Anscombe\n##################\n\npar(mfrow=c(2,2))\nfor(i in 1:4){\n    xi <- anscombe[,paste0('x',i)]\n    yi <- anscombe[,paste0('y',i)]\n    plot(xi, yi, ylim=c(4,13), xlim=c(4,20),\n        pch=16, col=grey(0,.6))\n    reg <- lm(yi~xi)\n    b <- round(coef(reg)[2],2)\n    p <- round(summary(reg)$coefficients[2,4],4)\n    abline(reg, col='orange')\n    title(paste0(\"Slope=\", b,', p=',p), font.main=1)\n}\n```\n\n::: {.cell-output-display}\n![](02_04_KernelIntro_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n## For an even better example, see `Datasaurus Dozen'\n#browseURL(\n#'https://bookdown.org/paul/applied-data-visualization/\n#why-look-the-datasaurus-dozen.html')\n```\n:::\n\n\nIt is true that \"OLS is the best linear predictor of the nonlinear regression function if the mean-squared error is used as the loss function.\" \\parencite[p.92]{camerontrivedi2005}. But this is not a carte-blanche justification for OLS, as the best of the bad predictors is still a bad predictor. For many economic applications, it is more helpful to think and speak of \"dose response curves\" instead of \"the effect\".\n\nWhile adding interaction terms or squared terms allows one incorporate heterogeneity and non-linearity, they change several features of the model (most of which are not intended). Often, there are nonsensical predicted values. For example, if the most of your age data are between $[23,65]$, a quadratic term can imply silly things for people aged $10$ or $90$. \n\nNonetheless, OLS provides an important piece of quantitative information that is understood by many. All models are an approximation, and sometimes only unimportant nuances are missing from a vanilla linear model. Other times, that model can be seriously misleading. (This is especially true if your making policy recommondations based on a universal ``the effect''.) As an exploratory tool, OLS is a good guess but one whose point estimates should not be taken too seriously (in which case, the standard errors are also much less important).  Before trying to find a regression specification that makes sense for the entire dataset, explore local relationships.\n\n#### **Local Relationships**.{-}\nScatterplots are a great and simplest plot for bivariate data that simply plots each observation. There are many extensions and similar tools. The example below shows two ways of summarizing the information; in both cases helping you understand how the central tendency and dispersion change. \n\n::: {.cell}\n\n```{.r .cell-code}\n##################\n# Application: Summarizing wages\n##################\nlibrary(wooldridge)\n\n## Plot 1\nplot(wage~educ, data=wage1, pch=16, col=grey(0,.1))\neduc_means <- aggregate(wage1[,c(\"wage\",\"educ\")], list(wage1$educ), mean)\npoints(wage~educ, data=educ_means, pch=17, col='blue', type='b')\ntitle(\"Grouped Means and Scatterplot\", font.main=1)\n```\n\n::: {.cell-output-display}\n![](02_04_KernelIntro_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n## Plot 2 (Less informative!)\n#barplot(wage~educ, data=educ_means)\n#title(\"Bar Plot of Grouped Means\")\n```\n:::\n\n\n\n#### **Regressograms**.{-}\n\nJust as we use histograms to describe the distribution of a random variable, we can use a regressogram for *conditional* relationships. Specifically, we can use dummies for exclusive intervals or bins to estimate how the average value of $Y$ varies with $X$.\n\nAfter dividing $X$ into $1,...L$ exclusive bins of width $h$. Each bin has a midpoint, $x$, and an associated dummy variable $\\hat{D}_{i}(x,h) = \\mathbf{1}\\left(\\hat{X}_{i} \\in \\left(x-h/2,x+h/2\\right] \\right)$.^[The bins do not all need to have the same width, but that is a good default and more notationally convenient than letting the bandwidth depend on the bin; $h(x)$.] Then conduct a dummy variable regression\n\\begin{eqnarray}\n\\hat{Y}_{i} &=& \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} b_{0}(x,h) \\hat{D}_{i}(x,h)  + e_{i},\n\\end{eqnarray}\nwhere each bin has a coefficient $b_{0}(x,h)$.\n\nNotice that each bin has $n(x,h) = \\sum_{i}^{n}\\hat{D}_{i}(x,h)$ observations. This means se can split the dataset into parts associated with each bin\n\\begin{eqnarray}\n\\label{eqn:regressogram1}\n\\sum_{i}^{n}\\left[e_{i}\\right]^2 \n&=& \\sum_{i}^{n}\\left[\\hat{Y}_{i}- \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} b_{0}(x,h) \\hat{D}_{i}(x,h) \\right]^2 \\\\\n&=& \\sum_{i}^{n(x_{1},h)}\\left[\\hat{Y}_{i}- \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} b_{0}(x,h) \\hat{D}_{i}(x,h) \\right]^2 + ...  \\nonumber  \\\\\n& & \\sum_{i}^{n(x_{L},h)}\\left[\\hat{Y}_{i}- \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} b_{0}(x,h) \\hat{D}_{i}(x,h) \\right]^2 \\\\\n&=& \\sum_{i}^{n(x_{1},h)}\\left[\\hat{Y}_{i}- b_{0}\\left(x_1,h\\right) \\right]^2 + ... \\sum_{i}^{n(x_L,h)}\\left[\\hat{Y}_{i}-b_{0}\\left(x_L,h\\right) \\right]^2 % +~ (N-1)\\sum_{i}\\hat{Y}_{i}.\n\\end{eqnarray}\nThis separation allows us optimize for each bin separately\n\\begin{eqnarray}\n\\label{eqn:regressogram2}\n\\min_{ \\left\\{ b_{0}(x,h) \\right\\} } \\sum_{i}^{n}\\left[e_{i}\\right]^2\n&=& \\min_{ \\left\\{ b_{0}(x,h) \\right\\} } \\sum_{i}^{n(x,h)}\\left[\\hat{Y}_{i}- b\\left(x,h\\right) \\right]^2,\n\\end{eqnarray}\nsince, in either case, minimizing yields \n\\begin{eqnarray}\n0 &=& -2 \\sum_{i}^{n(x,h)}\\left[ \\hat{Y}_{i} - b_{0}(x,h)  \\right] \\\\\n\\hat{b}_{0}(x,h) &=& \\frac{\\sum_{i}^{n(x,h)} \\hat{Y}_{i}}{ n(x,h) } = \\hat{M}_{Y}(x,h) .\n\\end{eqnarray}\nAs such, the OLS regression yields coefficients that are interpreted as the conditional mean: $\\hat{M}_{Y}(x,h)$. We can directly compute the same statistic directly by simply takes the average value of $\\hat{Y}_{i}$ for all $i$ observations in a particular bin. The values predicted by the model are then found as $\\hat{y}_{i} = \\sum_{x} \\hat{b}(x,h) \\hat{D}_{i}(x,h)$.\n\n\nConsider this two-bin example of how age affects wage for people aged $10$ to $70$.\n\\begin{eqnarray}\n\\text{Wage}_{i} &=& b_{0}(x=25, h=15) \\mathbf{1}\\left(\\text{Age}_{i} \\in (10,40]\\right) + b(x=55, h=15) \\mathbf{1}\\left(\\text{Age}_{i} \\in (40,70] \\right) + e_{i}.\n\\end{eqnarray}\nYou could also look at yearly bins and see if a tri-part grouping emerges naturally or not (e.g., whether the main effect on wages is whether your not in school or retired). You can choose other bins as well.\n\n::: {.cell}\n\n```{.r .cell-code}\n##################\n# Regressogram\n##################\n\n## Ages\nXmx <- 70\nXmn <- 15\n\n##Generate N Observations\ndat_sim <- function(n=1000){\n    n  <- 1000\n    X <- seq(Xmn,Xmx,length.out=n)\n    ## Random Productivity\n    e <- runif(n, 0, 1E6)\n    beta <-  1E-10*exp(1.4*X -.015*X^2)\n    Y    <-  (beta*X + e)/10\n    return(data.frame(Y,X))\n}\n\n\ndat <- dat_sim(1000)\nX <- dat$X\n## Plot\nplot(Y~X, data=dat, pch=16, col=grey(0,.1),\n    ylab='Yearly Productivity ($)', xlab='Age' )\n\n## Regression Estimates\nreg1  <- lm(Y~X, data=dat) ## OLS\npred1 <- cbind( Y=predict(reg1), X)[order(X),]\n\ndat$xcc   <- cut(X, seq(Xmn-1,Xmx,length.out=6)) ## Course Age Bins\nreg2  <- lm(Y~xcc, data=dat)\npred2 <- cbind( Y=predict(reg2), X)[order(X),]\n\ndat$xcf   <- cut(X, seq(Xmn-1, Xmx, length.out=31)) ## Fine Age Bins\nreg3  <- lm(Y~xcf, data=dat)\npred3 <- cbind( Y=predict(reg3), X)[order(X),]\n\n## Compare Models\nlines(Y~X, data=pred1, lwd=2, col=2)\nlines(Y~X, data=pred2, lwd=2, col=3)\nlines(Y~X, data=pred3, lwd=2, col=4)\nlegend('topleft',\n    legend=c('Linear Regression','Regressogram (5)','Regressogram (30)'),\n    lty=1, col=2:4, cex=.8)\n```\n\n::: {.cell-output-display}\n![](02_04_KernelIntro_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nInterestingly, we can obtain the same statistic from weighted least squares regression. For some specific design point, $x$, we can find $\\hat{b}(x, h)$ by minimizing\n\\begin{eqnarray}\n\\sum_{i}^{n}\\left[ e_{i} \\right]^2  \\hat{D}_{i}(x,h) &=& \\sum_{i}^{n}\\left[ \\hat{Y}_{i}- b_{0}(x,h) \\right]^2  \\hat{D}_{i}(x,h) \\\\\n&=& \\sum_{i}^{n(x_{1},h)}\\left[ \\hat{Y}_{i}- b_{0}(x_{1},h) \\right]^2  \\hat{D}_{i}(x_{1},h) + ... \\sum_{i}^{n(x_{L},h)}\\left[ \\hat{Y}_{i}- b_{0}(x_{L},h) \\right]^2  \\hat{D}_{i}(x_{L},h) \\\\\n&=& \\sum_{i}^{n(x,h)}\\left[\\hat{Y}_{i}- b_{0}\\left(x,h\\right) \\right]^2 \n\\end{eqnarray}\n\n#### **Piecewise Regression**.{-}\nThe regressogram depicts locally constant relationships. We can also included slope terms within each bin to allow for locally linear relationships. This is often called *segmented/piecewise regression*, which runs a separate regression for different subsets of the data.\n\n\\begin{eqnarray}\n\\hat{Y}_{i} &=& \\sum_{x} \\left[b_{0}(x,h) + b_{1}(x,h)\\hat{X}_{i} \\right] \\hat{D}_{i}(x,h) + e_{i}.\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\n##################\n# Regressogram w/ Slopes\n##################\n\n## Plot\ndat <- dat_sim(1000)\nX <- dat$X\nplot(Y~X, data=dat, pch=16, col=grey(0,.1),\n    ylab='Yearly Productivity ($)', xlab='Age' )\n\n## Course Age Bins\n#### Single Regression\ndat$xcc   <- cut(X, seq(Xmn-1,Xmx,length.out=6)) ## Course Age Bins\npred4 <- cbind( Y=predict( lm(Y~xcc*X, data=dat) ), X)[order(X),]\n#### Split Sample Regressions\ndat4 <- split( dat, dat$xcc)\npred4_B <- lapply(dat4, function(d){\n    pred_d <- cbind( Y=predict( lm(Y~X, d)), X=d$X)\n})\npred4_B <- as.data.frame(do.call('rbind', pred4_B))\npred4_B <- pred4_B[order(pred4_B$X),]\n\nlines(Y~X, data=pred4, lwd=2, col=5, lty=1)\nlines(Y~X, data=pred4_B, lwd=4, col=5, lty=3)\n\n\n## Fine Age Bins\n#### Single Regression\ndat$xcf  <- cut(X, seq(Xmn-1,Xmx,length.out=31)) ## Course Age Bins\npred5 <- cbind( Y=predict(lm(Y~xcf*X,data=dat)), X)[order(X),]\n#### Split Sample Regressions\ndat5 <- split(dat, dat$xcf)\npred5_B <- lapply(dat5, function(d){\n    pred_d <- cbind( Y=predict(lm(Y~X, d)), X=d$X)\n})\npred5_B <- as.data.frame(do.call('rbind', pred5_B))\npred5_B <- pred5_B[order(pred5_B$X),]\n\n## Compare Models\nlines(Y~X, data=pred5, lwd=2, col=6, lty=1)\nlines(Y~X, data=pred5_B, lwd=4, col=6, lty=3)\nlegend('topleft', \n    legend=c('5 bins','30 bins'),\n    lty=1, col=5:6, cex=.8)\n```\n\n::: {.cell-output-display}\n![](02_04_KernelIntro_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\nHere is another example with a real dataset\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\ncolnames(xy) <- c('y','x')\n\n# Globally Linear\nreg <- lm(y~x, data=xy)\n\n# Diagnose Fit\n#plot( fitted(reg), resid(reg), pch=16, col=grey(0,.5))\n#plot( xy$x, resid(reg), pch=16, col=grey(0,.5))\n\n# Linear in 2 Pieces (subsets)\nxcut2 <- cut(xy$x,2)\nxy_list2 <- split(xy, xcut2)\nregs2 <- lapply(xy_list2, function(xy_s){\n    lm(y~x, data=xy_s)\n})\nsapply(regs2, coef)\n##             (31.9,61.5] (61.5,91.1]\n## (Intercept)  -0.2836303  4.15337509\n## x             0.1628157  0.04760783\n\n# Linear in 3 Pieces (subsets or bins)\nxcut3 <- cut(xy$x, seq(32,92,by=20)) # Finer Bins\nxy_list3 <- split(xy, xcut3)\nregs3 <- lapply(xy_list3, function(xy_s){\n    lm(y~x, data=xy_s)\n})\nsapply(regs3, coef)\n##                (32,52]    (52,72]      (72,92]\n## (Intercept) 4.60313390 2.36291848  8.653829140\n## x           0.08233618 0.08132841 -0.007174454\n```\n:::\n\n\nCompare Predictions\n\n::: {.cell}\n\n```{.r .cell-code}\npred1 <- data.frame(yhat=predict(reg), x=reg$model$x)\npred1 <- pred1[order(pred1$x),]\n\npred2 <- lapply(regs2, function(reg){\n    data.frame(yhat=predict(reg), x=reg$model$x)\n})\npred2 <- do.call(rbind,pred2)\npred2 <- pred2[order(pred2$x),]\n\npred3 <- lapply(regs3, function(reg){\n    data.frame(yhat=predict(reg), x=reg$model$x)\n})\npred3 <- do.call(rbind,pred3)\npred3 <- pred3[order(pred3$x),]\n\n# Compare Predictions\nplot(y ~ x, pch=16, col=grey(0,.5), dat=xy)\nlines(yhat~x, pred1, lwd=2, col=2)\nlines(yhat~x, pred2, lwd=2, col=4)\nlines(yhat~x, pred3, lwd=2, col=3)\nlegend('topleft',\n    legend=c('Globally Linear', 'Peicewise Linear (2)','Peicewise Linear (3)'),\n    lty=1, col=c(2,4,3), cex=.8)\n```\n\n::: {.cell-output-display}\n![](02_04_KernelIntro_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nFor many things, a simple linear regression, regressograms, or piecewise regression is \"good enough\". Simple linear regressions struggle with nonlinear relationships but are very easy to run with a computer. Regressograms and piecewise regressions are intuitive ways to capture nonlinear relationships that are computationally efficient but have obvious problems where the bins change. Sometimes we want smoother predictions or to estimate derivatives (gradients). To cover more advanced regression methods that do those things, we will need to first learn about kernel density estimation.\n\n\n## Kernel Density Estimation\n\nA kernel density is generally a \"smooth\" version of a histogram. We estimate the density at many points (e.g., all unique values $x$ in the dataset), not just the midpoints of exclusive bins. The uniform kernel and density estimator is \n\\begin{eqnarray}\n\\label{eqn:uniform}\n\\widehat{f}_{U}(x) &=& \\frac{1}{n} \\sum_{i}^{n} \\frac{k_{U}(\\hat{X}_{i}, x, h) }{2h} \\\\\nk_{U}\\left( \\hat{X}_{i}, x, h \\right) &=& \\mathbf{1}\\left(\\frac{|\\hat{X}_{i}-x|}{h}<1\\right)\n= \\mathbf{1}\\left( \\hat{X}_{i} \\in \\left( x-h, x + h\\right) \\right).\n\\end{eqnarray} \nComparing equations \\ref{eqn:uniform} to \\ref{eqn:indicator}, we can see the uniform kernel is essentially the histogram but without the restriction that $x$ must be a midpoint of *exclusive* bins. Typically, the points $x$ are chosen to be either the unique observations or some equidistant set of \"design points\".\n\n\nWe can also replace the uniform kernel with a more general kernel function $k$, which is then normalized to an easier to read and program $K$ function: $k\\left( \\hat{X}_{i}, x, h \\right)= K\\left( \\frac{|\\hat{X}_{i}-x|}{h} \\right)$. We can then define a general kernel function as a non-negative real-valued function $K$ that integrates to unity:\n\\begin{eqnarray}\n\\int_{-\\infty}^{\\infty} K(v) dv &=& 1\n\\end{eqnarray}\nThe general idea behind kernel density is to use windows around each $x$ that potentially overlap, rather than partitioning the range of $X$ into exclusive bins.^[We only examine symmetric kernels, as some texts also include symmetric in the definition of a kernel; $K(v) = K(-v)$.] For examples of some common kernels, see <https://en.wikipedia.org/wiki/Kernel_(statistics)#In_non-parametric_statistics>. In my view, these are the most intuitive and common.\n\n::: {.cell}\n\n```{.r .cell-code}\n##################\n# Kernel Density Functions\n##################\n\nX <- seq(-2,2, length.out=1001)\n\nplot.new()\nplot.window(xlim=c(-1.2,1.2), ylim=c(0,1))\n\nh <- 1\nlines( dunif(X,-h,h)~X, col=1, lty=1)\n\nh <- 1/2\nlines( dnorm(X,0,h)~X, col=2, lty=1)\n\ndtricub <- function(X, x=0, h){\n    u <- abs(X-x)/h\n    fu <- 70/81*(1-u^3)^3/h*(u <= 1)\n    return(fu)\n}\nh <- 1\nlines( dtricub(X,0,h)~X, col=3, lty=1)\n\nh <- 1/2\nlines(density(x=0, bw=h, kernel=\"epanechnikov\"), col=4, lty=1)\n## Note that \"density\" defines h slightly differently\n\nrug(0, lwd=2)\naxis(1)\naxis(2)\n\nlegend('topright', lty=1, col=1:4,\n    legend=c('uniform(1)', 'gaussian(1/2)', 'tricubic(1)', 'epanechnikov(1)'))\n```\n\n::: {.cell-output-display}\n![](02_04_KernelIntro_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n\n## Try others:\n## lines(density(x=0, bw=1/2, kernel=\"triangular\"),col=4, lty=1)\n```\n:::\n\n\nOnce we have picked a kernel (which particular one is not particularly important) we can use it to compute density estimates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##################\n# Kernel Density Estimation\n##################\n\nN <- 1000\ne <- rweibull(N,100,100)\nebins <- seq(floor(min(e)), ceiling(max(e)), length.out=12)\n\n## Histogram Estimates at 12 points\nxbks <- c(ebins[1]-diff(ebins)[1]/2, ebins+diff(ebins)[1]/2)\nhist(e, freq=F, main='', breaks=xbks, ylim=c(0,.4), border=NA)\nrug(e, lwd=.07, col=grey(0,.5))  ## Sample\n\n\n## Manually Compute Uniform Estimate at X=100 with h=2\n# w100 <- (e < 101)*(e > 99)\n# sum(w100)/(N*2)\n\n## Gaussian Estimates at same points as histogram\nF_hat <- sapply(ebins, function(x,h=.5){\n    kx <- dnorm( abs(e-x)/h )\n    fx <- sum(kx,na.rm=T)/(h*N)\n    fx\n})\n## Verify the same\nfhat1 <- density(e, n=12, from=min(ebins), to=max(ebins), bw=.5)\npoints(fhat1$x, fhat1$y, pch=16, col=rgb(0,0,1,.5), cex=1.5)\n\n## Gaussian Estimates at all sample points\nfhat2 <- density(e, n=1000, from=min(ebins), to=max(ebins), bw=.5)\npoints(fhat2$x, fhat2$y, pch=16, col=rgb(1,0,0,.25), cex=.5)\n\nlegend('topleft', pch=c(15,16,16),\n    col=c(grey(0,.5),rgb(0,0,1,.5), rgb(1,0,0,.25)),\n    title='Type (# Design Points)', bty='n',\n    legend=c('Histogram (12)',\n    'Gaussian-Kernel (12)',\n    'Gaussian-Kernel (1000)'))\n```\n\n::: {.cell-output-display}\n![](02_04_KernelIntro_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n## Local Linear Regression\n\n\nIt is safer to assume that you could be analyzing data with nonlinear relationships. A general nonparametric model is written as\n\\begin{eqnarray}\n\\hat{Y}_{i} = m(\\hat{X}_{i}) + \\epsilon_{i}\n\\end{eqnarray}\nwhere $m$ is an unknown continuous function and $\\epsilon$ is white noise. (As such, the linear model is a special case.) You can estimate the mean of $Y_{i}$ conditional on $X_{i}=x$ with a regressogram or a variety of other least-squares procedures.\n\n#### **Locally Constant**.{-}\n\nConsider a point $x$ and suppose $\\hat{Y}_{i} = b(x,h) + e_{i}$ locally. Then notice a weighted OLS estimator with uniform kernel weights yields\n\\begin{eqnarray} \\label{eqn:lcls}\n& & \\min_{b(x,h)}~ \\sum_{i}^{n}\\left[e_{i} \\right]^2 k_{U}\\left( \\hat{X}_{i}, x, h \\right) \\\\\n\\Rightarrow & & -2 \\sum_{i}^{n}\\left[\\hat{Y}_{i}- b(x,h) \\right] k_{U}\\left(\\hat{X}_{i}, x, h\\right) = 0\\\\\n\\label{eqn:lcls1}\n\\Rightarrow & & \\hat{b_{U}}(x) \n= \\frac{\\sum_{i} \\hat{Y}_{i} k_{U} \\left( \\hat{X}_{i}, x, h \\right) }{ \\sum_{i} k_{U}\\left( \\hat{X}_{i}, x, h \\right) } \n= \\sum_{i} \\hat{Y}_{i} \\left[ \\frac{ k_{U} \\left( \\hat{X}_{i}, x, h \\right) }{ \\sum_{i} k_{U}\\left( \\hat{X}_{i}, x, h \\right)} \\right] =  \\sum_{i} \\hat{Y}_{i} w_{i},\n\\end{eqnarray}\nwhere weight $w_{i} = \\mathbf{1}\\left( |\\hat{X}_{i} - x| < h \\right)/N$. The last equality is derived analogously to equation \\ref{eqn:sum}; where $k_{U} \\left( \\hat{X}_{i}, x, h \\right)$ is either one or zero, and $\\sum_{i} k_{U} \\left( \\hat{X}_{i}, x, h \\right) = n(x,h)$. \n \nWhen $n$ is small, $\\hat{b}_{U}(x)$ is typically estimated for each unique observed value: $x \\in \\{ x_{1},...x_{n} \\}$. For large datasets, you can select a subset or evenly spaced values of $x$ for which to estimate $\\hat{b}_{U}(x)$. If we use exclusive bins, then equation \\ref{eqn:regressogram1} equals \\ref{eqn:lcls1}, which shows the regressogram is a kernel regression weights that recovers the conditional mean. This regressogram is more crude but can be estimated with OLS.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##################\n# LCLS\n##################\n## Generate Sample Data\nx <- 1:5\ny <- runif(length(x))\n## plot(x,y)\n\n## Manually Compute Estimate at X=3\nw3 <- dunif(x-3,-1,1) #(x < 4)*(x > 2)\nyhat_3 <- sum(w3*y)\nyhat_3\n## [1] 0.5782872\n```\n:::\n\n\nThe basic idea also generalizes other kernels. As such, a kernel regression using uniform weights is often called a ``naive kernel regression''. Typically, kernel regressions use kernels that weight nearby observations more heavily. We can also add a slope term to improve the fit.\n\nIf $X$ represents time, then the local constant regressions is also called a *moving average*. With non-uniform kernel weights, we have a *weighted* moving average.\n\n#### **Locally Linear**.{-}\nA less simple case is a *local linear regression* which conducts a linear regression for each data point using a subsample of data around it. Consider a point $x$ and suppose $\\hat{Y}_{i} = b_{0}(x,h) + b_{1}(x) \\hat{X}_{i} + e_{i}$ for data near $x$. The weighted OLS estimator with kernel weights is\n\\begin{eqnarray}\n& & \\min_{b_{0}(x,h),b_{1}(x,h)}~ \\sum_{i}^{n}\\left[\\hat{Y}_{i}- b_{0}(x,h) - b_{1}(x,h) \\hat{X}_{i} \\right]^2 K\\left(\\frac{|\\hat{X}_{i}-x|}{h}\\right)\n\\end{eqnarray} \nDeriving the optimal values $\\hat{b}_{0}(x,h)$ and  $\\hat{b}_{1}(x,h)$ for $k_{U}$ is left as a homework exercise.^[Note that one general benefit of LLLS is with edge effects (see homework). Another is that it is theoretically motivated: assuming that $Y_{i}=m(X_{i}) + \\epsilon_{i}$, we can then take a Taylor approximation: $m(X_{i}) + \\epsilon_{i} \\approx m(x) + m'(x)[X_{i}-x] + \\epsilon_{i} = [m(x) - m'(x)x ] + m'(x)X_{i} + \\epsilon_{i} = b_{0}(x,h) + b_{1}(x,h) X_{i}$. As such, a third benefit is that the estimated slope coefficient $\\hat{b}_{1}(x,h)$ can be interpreted as the estimated gradient at $x$.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ``Naive\" Smoother\npred_fun <- function(x0, h, xy){\n    # Assign equal weight to observations within h distance to x0\n    # 0 weight for all other observations\n    ki   <- dunif(xy$x, x0-h, x0+h) \n    llls <- lm(y~x, data=xy, weights=ki)\n    yhat_i <- predict(llls, newdata=data.frame(x=x0))\n}\n\nX0 <- sort(unique(xy$x))\npred_lo1 <- sapply(X0, pred_fun, h=2, xy=xy)\npred_lo2 <- sapply(X0, pred_fun, h=20, xy=xy)\n\nplot(y~x, pch=16, data=xy, col=grey(0,.5),\n    ylab='Murder Rate', xlab='Population Density')\ncols <- c(rgb(.8,0,0,.5), rgb(0,0,.8,.5))\nlines(X0, pred_lo1, col=cols[1], lwd=1, type='o')\nlines(X0, pred_lo2, col=cols[2], lwd=1, type='o')\nlegend('topleft', title='Locally Linear',\n    legend=c('h=2 ', 'h=20'),\n    lty=1, col=cols, cex=.8)\n```\n\n::: {.cell-output-display}\n![](02_04_KernelIntro_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nNote that there are more complex versions of local linear regressions (see <https://shinyserv.es/shiny/kreg/> for a nice illustration.) An even more complex (and more powerful) version is **loess**, which uses adaptive bandwidths in order to have a similar number of data points in each subsample (especially useful when $X$ is not uniform.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Adaptive-width subsamples with non-uniform weights\nxy0 <- xy[order(xy$x),]\nplot(y~x, pch=16, col=grey(0,.5), dat=xy0)\n\nreg_lo4 <- loess(y~x, data=xy0, span=.4)\nreg_lo8 <- loess(y~x, data=xy0, span=.8)\n\ncols <- hcl.colors(3,alpha=.75)[-3]\nlines(xy0$x, predict(reg_lo4),\n    col=cols[1], type='o', pch=2)\nlines(xy0$x, predict(reg_lo8),\n    col=cols[2], type='o', pch=2)\n\nlegend('topleft', title='Loess',\n    legend=c('span=.4 ', 'span=.8'),\n    lty=1, col=cols, cex=.8)\n```\n\n::: {.cell-output-display}\n![](02_04_KernelIntro_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n#### **Confidence Bands**. {-}\n\nThe smoothed predicted values estimate the local means. So we can also construct confidence bands\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Loess\nxy0 <- xy[order(xy$x),]\nX0 <- unique(xy0$x)\nreg_lo <- loess(y~x, data=xy0, span=.8)\n\n# Jackknife CI\njack_lo <- sapply(1:nrow(xy), function(i){\n    xy_i <- xy[-i,]\n    reg_i <- loess(y~x, dat=xy_i, span=.8)\n    predict(reg_i, newdata=data.frame(x=X0))\n})\njack_cb <- apply(jack_lo,1, quantile,\n    probs=c(.025,.975), na.rm=T)\n\n# Plot\nplot(y~x, pch=16, col=grey(0,.5), dat=xy0)\npreds_lo <- predict(reg_lo, newdata=data.frame(x=X0))\nlines(X0, preds_lo,\n    col=hcl.colors(3,alpha=.75)[2],\n    type='o', pch=2)\n# Plot CI\npolygon(\n    c(X0, rev(X0)),\n    c(jack_cb[1,], rev(jack_cb[2,])),\n    col=hcl.colors(3,alpha=.25)[2],\n    border=NA)\n```\n\n::: {.cell-output-display}\n![](02_04_KernelIntro_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n",
    "supporting": [
      "02_04_KernelIntro_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}