{
  "hash": "0032c7e8d40c1dd54a0b648c8776c657",
  "result": {
    "engine": "knitr",
    "markdown": "# Local Regression\n***\n\n## Local Relationships \n\n\n#### **Local Relationships**.{-}\nScatterplots are a great and simplest plot for bivariate data that simply plots each observation. There are many extensions and similar tools. The example below helps understand how both the central tendency and dispersion change. \n\n::: {.cell}\n\n```{.r .cell-code}\n# Local relationship: wages and education\nlibrary(wooldridge)\n\n## Plot 1\nplot(wage~educ, data=wage1, pch=16, col=grey(0,.1))\neduc_means <- aggregate(wage1[,c(\"wage\",\"educ\")],\n    list(wage1[,'educ']), mean)\npoints(wage~educ, data=educ_means, pch=17, col='blue', type='b')\ntitle(\"Grouped Means and Scatterplot\", font.main=1)\n```\n\n::: {.cell-output-display}\n![](02_05_KernelIntro_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n## Plot 2 (Alternative for big datasets)\n# boxplot(wage~educ, data=wage1, \n#    pch=16, col=grey(0,.1), varwidth=T)\n#title(\"Boxplots\", font.main=1)\n\n## Plot 3 (Less informative!)\n#barplot(wage~educ, data=educ_means)\n#title(\"Bar Plot of Grouped Means\", font.main=1)\n```\n:::\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nExamine the local relationship between 'Murder' and 'Urbanization' in the `USArrests` dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\nxy[,'bins'] <- cut(USArrests[,'UrbanPop'], 6) \n```\n:::\n\n\n:::\n\n#### **Regressograms**.{-}\n\nJust as we use histograms to describe the distribution of a random variable, we can use a regressogram for *conditional* relationships. Specifically, we can use dummies for exclusive intervals or bins to estimate how the average value of $Y$ varies with $X$.\n\nAfter dividing $X$ into $1,...L$ exclusive bins of width $h$. Each bin has a midpoint, $x$, and an associated dummy variable $\\hat{D}_{i}(x,h) = \\mathbf{1}\\left(\\hat{X}_{i} \\in \\left(x-h/2,x+h/2\\right] \\right)$.^[The bins do not all need to have the same width, but that is a good default and more notationally convenient than letting the bandwidth depend on the bin; $h(x)$.] Then conduct a dummy variable regression\n\\begin{eqnarray}\n\\hat{Y}_{i} &=& \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} b_{0}(x,h) \\hat{D}_{i}(x,h)  + e_{i},\n\\end{eqnarray}\nwhere each bin has a coefficient $b_{0}(x,h)$.\n\n\nConsider this two-bin example of how age affects wage for people with $\\leq 10$ years of school complete vs $> 10$. \n\\begin{eqnarray}\n\\text{Wage}_{i} &=& b_{0}(x=5, h=10) \\mathbf{1}\\left(\\text{Educ}_{i} \\in [0,10]\\right) + b_{0}(x=15, h=10) \\mathbf{1}\\left(\\text{Age}_{i} \\in (10,20] \\right) + e_{i}.\n\\end{eqnarray}\nYou could also look at three bins to see if that is a better fit. You can also try finer bins and see what grouping emerges naturally  (e.g., whether the main effect on wages is whether your not in school or retired). \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data\nplot(wage~educ, data=wage1, pch=16, col=grey(0,.1))\ndat <- wage1[order(wage1[,'educ']),]\n\n## Simple Regression\nreg1  <- lm(wage~educ, data=dat) ## OLS\n\n# Regressogram: 2 Bins\ndat[,'xcc'] <- cut(dat[,'educ'],\n    c(-1,10,20))\nreg2  <- lm(wage~xcc, data=dat)\n\n# Regressogram: Finer Age Bins\ndat[,'xcf']   <- cut(dat[,'educ'],\n    c(-1,6,12,18)) \nreg3  <- lm(wage~xcf, data=dat)\n\n## Compare Models (Only 2 for simplicity)\nlines( dat[,'educ'], predict(reg1), lwd=2, col=2)\nlines( dat[,'educ'], predict(reg3), lwd=2, col=4, type='s')\nlegend('topleft',\n    legend=c('Linear Regression','Regressogram (3)'),\n    col=c(2,4),\n    lty=1, cex=.8)\n```\n\n::: {.cell-output-display}\n![](02_05_KernelIntro_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\nNotice that each bin has $n(x,h) = \\sum_{i}^{n}\\hat{D}_{i}(x,h)$ observations. This means we can split the dataset into parts associated with each bin\n\\begin{eqnarray}\n\\label{eqn:regressogram1}\n\\sum_{i}^{n}\\left[e_{i}\\right]^2 \n&=& \\sum_{i}^{n}\\left[\\hat{Y}_{i}- \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} b_{0}(x,h) \\hat{D}_{i}(x,h) \\right]^2 \\\\\n&=& \\sum_{i}^{n(x_{1},h)}\\left[\\hat{Y}_{i}- \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} b_{0}(x,h) \\hat{D}_{i}(x,h) \\right]^2 + ...  \\nonumber  \\\\\n& & \\sum_{i}^{n(x_{L},h)}\\left[\\hat{Y}_{i}- \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} b_{0}(x,h) \\hat{D}_{i}(x,h) \\right]^2 \\\\\n&=& \\sum_{i}^{n(x_{1},h)}\\left[\\hat{Y}_{i}- b_{0}\\left(x_1,h\\right) \\right]^2 + ... \\sum_{i}^{n(x_L,h)}\\left[\\hat{Y}_{i}-b_{0}\\left(x_L,h\\right) \\right]^2 % +~ (N-1)\\sum_{i}\\hat{Y}_{i}.\n\\end{eqnarray}\nThis separation allows us optimize for each bin separately\n\\begin{eqnarray}\n\\label{eqn:regressogram2}\n\\min_{ \\left\\{ b_{0}(x,h) \\right\\} } \\sum_{i}^{n}\\left[e_{i}\\right]^2\n&=& \\min_{ \\left\\{ b_{0}(x,h) \\right\\} } \\sum_{i}^{n(x,h)}\\left[\\hat{Y}_{i}- b_{0}\\left(x,h\\right) \\right]^2,\n\\end{eqnarray}\nsince, in either case, minimizing yields \n\\begin{eqnarray}\n0 &=& -2 \\sum_{i}^{n(x,h)}\\left[ \\hat{Y}_{i} - b_{0}(x,h)  \\right] \\\\\n\\hat{b}_{0}(x,h) &=& \\frac{\\sum_{i}^{n(x,h)} \\hat{Y}_{i}}{ n(x,h) } = \\hat{M}_{Y}(x,h) .\n\\end{eqnarray}\nAs such, the OLS regression yields coefficients that are interpreted as the conditional mean: $\\hat{M}_{Y}(x,h)$. We can directly compute the same statistic directly by simply takes the average value of $\\hat{Y}_{i}$ for all $i$ observations in a particular bin. The values predicted by the model are then found as $\\hat{y}_{i} = \\sum_{x} \\hat{b}_{0}(x,h) \\hat{D}_{i}(x,h)$.\n\n\nInterestingly, we can obtain the same statistic from weighted least squares regression. For some specific design point, $x$, we can find $\\hat{b}(x, h)$ by minimizing\n\\begin{eqnarray}\n\\sum_{i}^{n}\\left[ e_{i} \\right]^2  \\hat{D}_{i}(x,h) &=& \\sum_{i}^{n}\\left[ \\hat{Y}_{i}- b_{0}(x,h) \\right]^2  \\hat{D}_{i}(x,h) \\\\\n&=& \\sum_{i}^{n(x_{1},h)}\\left[ \\hat{Y}_{i}- b_{0}(x_{1},h) \\right]^2  \\hat{D}_{i}(x_{1},h) + ... \\sum_{i}^{n(x_{L},h)}\\left[ \\hat{Y}_{i}- b_{0}(x_{L},h) \\right]^2  \\hat{D}_{i}(x_{L},h) \\\\\n&=& \\sum_{i}^{n(x,h)}\\left[\\hat{Y}_{i}- b_{0}\\left(x,h\\right) \\right]^2 \n\\end{eqnarray}\n\n#### **Piecewise Regression**.{-}\nThe regressogram depicts locally constant relationships. We can also included slope terms within each bin to allow for locally linear relationships. This is often called *segmented/piecewise regression*, which runs a separate regression for different subsets of the data.\n\n\\begin{eqnarray}\n\\hat{Y}_{i} &=& \\sum_{x} \\left[b_{0}(x,h) + b_{1}(x,h)\\hat{X}_{i} \\right] \\hat{D}_{i}(x,h) + e_{i}.\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data\nplot(wage~educ, data=wage1, pch=16, col=grey(0,.1))\ndat <- wage1[order(wage1[,'educ']),]\ndat[,'educ'] <- as.numeric(dat[,'educ'])\n\n# Piecewise: 2 Bins\ndat[,'xcc'] <- cut(dat[,'educ'],\n    c(-1,10,20))\nreg4  <- lm(wage~xcc*educ, data=dat)\n\n# Piecewise: Finer Age Bins\ndat[,'xcf']   <- cut(dat[,'educ'],\n    c(-1,6,12,18)) \nreg5  <- lm(wage~xcf*educ, data=dat)\n\n## Compare Models\nlines( dat[,'educ'], predict(reg4), lwd=2, col=5)\nlines( dat[,'educ'], predict(reg5), lwd=2, col=6)\nlegend('topleft',\n    legend=c('2 bins','3 bins'),\n    lty=1, col=5:6, cex=.8)\ntitle('Piecewise Regressions')\n```\n\n::: {.cell-output-display}\n![](02_05_KernelIntro_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# See that two methods give the same predictions\n## Course Age Bins\npred2 <- predict(reg2)\n## Split Sample Regressions\ndat2 <- split( dat, dat[,'xcc'])\npred2B <- lapply(dat2, function(d){\n    reg2 <- lm(wage~educ, d)\n    pred_d <- predict(reg2)\n})\n# Any differences?\nall( pred2 - unlist(pred2B) < 1e-10 )\n## [1] FALSE\n```\n:::\n\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nCompare a simple regression to a regressogram and a piecewise regression. Examine the relationship between 'Murder' and 'Urbanization' in the `USArrests` dataset. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\ncolnames(xy) <- c('y','x')\n\n# Globally Linear\nreg <- lm(y~x, data=xy)\n```\n:::\n\n\nTry to do so on your own before looking at the code below, which compares two piecewise regressions to a simple linear regression.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Diagnose Fit\n#plot( fitted(reg), resid(reg), pch=16, col=grey(0,.5))\n#plot( xy[,'x'], resid(reg), pch=16, col=grey(0,.5))\n\n# Linear in 2 Pieces (subsets)\nxcut2 <- cut(xy[,'x'],2)\nxy_list2 <- split(xy, xcut2)\nregs2 <- lapply(xy_list2, function(xy_s){\n    lm(y~x, data=xy_s)\n})\n\n# Linear in 3 Pieces (subsets or bins)\nxcut3 <- cut(xy[,'x'], seq(32,92,by=20)) # Finer Bins\nxy_list3 <- split(xy, xcut3)\nregs3 <- lapply(xy_list3, function(xy_s){\n    lm(y~x, data=xy_s)\n})\n\n## Make Predictions\npred1 <- data.frame(yhat=predict(reg), x=reg[['model']][,'x'])\npred1 <- pred1[order(pred1[,'x']),]\n\npred2 <- lapply(regs2, function(reg){\n    data.frame(yhat=predict(reg), x=reg[['model']][,'x'])\n})\npred2 <- do.call(rbind,pred2)\npred2 <- pred2[order(pred2[,'x']),]\n\npred3 <- lapply(regs3, function(reg){\n    data.frame(yhat=predict(reg), x=reg[['model']][,'x'])\n})\npred3 <- do.call(rbind,pred3)\npred3 <- pred3[order(pred3[,'x']),]\n\n# Compare Predictions\nplot(y ~ x, pch=16, col=grey(0,.5), dat=xy)\nlines(yhat~x, pred1, lwd=2, col=2)\nlines(yhat~x, pred2, lwd=2, col=4)\nlines(yhat~x, pred3, lwd=2, col=3)\nlegend('topleft',\n    legend=c('Globally Linear', 'Piecewise Linear (2)','Piecewise Linear (3)'),\n    lty=1, col=c(2,4,3), cex=.8)\n```\n:::\n\n:::\n\nFor many things, a simple linear regression, regressograms, or piecewise regression is \"good enough\". Simple linear regressions struggle with nonlinear relationships but are very easy to run with a computer. Regressograms and piecewise regressions are intuitive ways to capture nonlinear relationships that are computationally efficient but have obvious problems where the bins change. Sometimes we want smoother predictions or to estimate derivatives (gradients). To cover more advanced regression methods that do those things, we will need to first learn about kernel density estimation.\n\n\n## Kernel Density Estimation\n\nA kernel density is generally a \"smooth\" version of a histogram. We estimate the density at many points (e.g., all unique values $x$ in the dataset), not just the midpoints of exclusive bins. The uniform kernel and density estimator is \n\\begin{eqnarray}\n\\label{eqn:uniform}\n\\hat{f}_{U}(x) &=& \\frac{1}{n} \\sum_{i}^{n} \\frac{k_{U}(\\hat{X}_{i}, x, h) }{2h} \\\\\nk_{U}\\left( \\hat{X}_{i}, x, h \\right) &=& \\mathbf{1}\\left(\\frac{|\\hat{X}_{i}-x|}{h}<1\\right)\n= \\mathbf{1}\\left( \\hat{X}_{i} \\in \\left( x-h, x + h\\right) \\right).\n\\end{eqnarray} \nNotice that the uniform kernel is essentially the histogram but without the restriction that $x$ must be a midpoint of *exclusive* bins. Typically, the points $x$ are chosen to be either the unique observations or some equidistant set of \"design points\".\n\n\nWe can also replace the uniform kernel with a more general kernel function $k$, which is then normalized to an easier to read and program $K$ function: $k\\left( \\hat{X}_{i}, x, h \\right)= K\\left( \\frac{|\\hat{X}_{i}-x|}{h} \\right)$. We can then define a general kernel function as a non-negative real-valued function $K$ that integrates to unity:\n\\begin{eqnarray}\n\\int_{-\\infty}^{\\infty} K(v) dv &=& 1\n\\end{eqnarray}\nThe general idea behind kernel density is to use windows around each $x$ that potentially overlap, rather than partitioning the range of $X$ into exclusive bins.^[We only examine symmetric kernels, as some texts also include symmetric in the definition of a kernel; $K(v) = K(-v)$.] There are many [kernels](https://en.wikipedia.org/wiki/Kernel_(statistics)#In_non-parametric_statistics), but these are the most intuitive and commonly used.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Kernel Density Functions\n\nX <- seq(-2,2, length.out=1001)\n\nplot.new()\nplot.window(xlim=c(-1.2,1.2), ylim=c(0,1))\n\nh <- 1\nlines( dunif(X,-h,h)~X, col=1, lty=1)\n\nh <- 1/2\nlines( dnorm(X,0,h)~X, col=2, lty=1)\n\ndtricub <- function(X, x=0, h){\n    u <- abs(X-x)/h\n    fu <- 70/81*(1-u^3)^3/h*(u <= 1)\n    return(fu)\n}\nh <- 1\nlines( dtricub(X,0,h)~X, col=3, lty=1)\n\nh <- 1/2\nlines(density(x=0, bw=h, kernel=\"epanechnikov\"), col=4, lty=1)\n## Note that \"density\" defines h slightly differently\n\nrug(0, lwd=2)\naxis(1)\naxis(2)\n\nlegend('topright', lty=1, col=1:4,\n    legend=c('uniform(1)', 'gaussian(1/2)', 'tricubic(1)', 'epanechnikov(1)'))\n```\n\n::: {.cell-output-display}\n![](02_05_KernelIntro_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n\n## Try others:\n## lines(density(x=0, bw=1/2, kernel=\"triangular\"),col=4, lty=1)\n```\n:::\n\n\nOnce we have picked a kernel (which particular one is not particularly important) we can use it to compute density estimates at each design point.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Practical Example\nX <- wage1[,'educ']\nhist(X,\n    breaks=seq(0,20,by=2), #bin width=1\n    freq=F,\n    border=NA, \n    main='',\n    xlab='Murder Arrests')\n# Density Estimate\nlines(density(X, bw=1))\n# Raw Observations\nrug(wage1[,'educ'], col=grey(0,.5))\n```\n\n::: {.cell-output-display}\n![](02_05_KernelIntro_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nHere is an example going into the details of KDE.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Kernel Density Estimation\n\nN <- 1000\ne <- rweibull(N,100,100)\nebins <- seq(floor(min(e)), ceiling(max(e)), length.out=12)\n\n## Histogram Estimates at 12 points\nxbks <- c(ebins[1]-diff(ebins)[1]/2, ebins+diff(ebins)[1]/2)\nhist(e, freq=F, main='', breaks=xbks, ylim=c(0,.4), border=NA)\nrug(e, lwd=.07, col=grey(0,.5))  ## Sample\n\n\n## Manually Compute Uniform Estimate at X=100 with h=2\n# w100 <- (e < 101)*(e > 99)\n# sum(w100)/(N*2)\n\n## Gaussian Estimates at same points as histogram\nF_hat <- sapply(ebins, function(x,h=.5){\n    kx <- dnorm( abs(e-x)/h )\n    fx <- sum(kx,na.rm=T)/(h*N)\n    fx\n})\n## Verify the same\nfhat1 <- density(e, n=12, from=min(ebins), to=max(ebins), bw=.5)\npoints(fhat1[['x']], fhat1[['y']], pch=16, col=rgb(0,0,1,.5), cex=1.5)\n\n## Gaussian Estimates at all sample points\nfhat2 <- density(e, n=1000, from=min(ebins), to=max(ebins), bw=.5)\npoints(fhat2[['x']], fhat2[['y']], pch=16, col=rgb(1,0,0,.25), cex=.5)\n\nlegend('topleft', pch=c(15,16,16),\n    col=c(grey(0,.5),rgb(0,0,1,.5), rgb(1,0,0,.25)),\n    title='Type (# Design Points)', bty='n',\n    legend=c('Histogram (12)',\n    'Gaussian-Kernel (12)',\n    'Gaussian-Kernel (1000)'))\n```\n\n::: {.cell-output-display}\n![](02_05_KernelIntro_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n:::\n\n## Local Linear Regression\n\n\nIt is safer to assume that you could be analyzing data with nonlinear relationships. A general nonparametric model is written as\n\\begin{eqnarray}\n\\hat{Y}_{i} = m(\\hat{X}_{i}) + \\epsilon_{i}\n\\end{eqnarray}\nwhere $m$ is an unknown continuous function and $\\epsilon$ is white noise. (As such, the linear model is a special case.) You can estimate the mean of $Y_{i}$ conditional on $X_{i}=x$ with a regressogram or a variety of other least-squares procedures.\n\n#### **Locally Constant (Moving Average)**.{-}\n\nConsider a point $x$ and suppose $\\hat{Y}_{i} = b(x,h) + e_{i}$ locally. Then notice a weighted OLS estimator with uniform kernel weights yields\n\\begin{eqnarray} \\label{eqn:lcls}\n& & \\min_{b(x,h)}~ \\sum_{i}^{n}\\left[e_{i} \\right]^2 k_{U}\\left( \\hat{X}_{i}, x, h \\right) \\\\\n\\Rightarrow & & -2 \\sum_{i}^{n}\\left[\\hat{Y}_{i}- b(x,h) \\right] k_{U}\\left(\\hat{X}_{i}, x, h\\right) = 0\\\\\n\\label{eqn:lcls1}\n\\Rightarrow & & \\hat{b}_{U}(x) \n= \\frac{\\sum_{i} \\hat{Y}_{i} k_{U} \\left( \\hat{X}_{i}, x, h \\right) }{ \\sum_{i} k_{U}\\left( \\hat{X}_{i}, x, h \\right) } \n= \\sum_{i} \\hat{Y}_{i} \\left[ \\frac{ k_{U} \\left( \\hat{X}_{i}, x, h \\right) }{ \\sum_{i} k_{U}\\left( \\hat{X}_{i}, x, h \\right)} \\right] =  \\sum_{i} \\hat{Y}_{i} w_{i},\n\\end{eqnarray}\nwhere weight $w_{i} = \\mathbf{1}\\left( |\\hat{X}_{i} - x| < h \\right)/N$ because $k_{U} \\left( \\hat{X}_{i}, x, h \\right)$ is either one or zero, and $\\sum_{i} k_{U} \\left( \\hat{X}_{i}, x, h \\right) = n(x,h)$. So locally constant kernel regression recovers the weighted mean of $Y_{i}$ around design point $x$. If we use exclusive bins, then we are running a regressogram, which is more crude but can be estimated easily.\n \nWhen $n$ is small, $\\hat{b}_{U}(x)$ is typically estimated for each unique observed value: $x \\in \\{ x_{1},...x_{n} \\}$. For large datasets, you can select a subset or evenly spaced values of $x$ for which to estimate $\\hat{b}_{U}(x)$. \n\n:::{.callout-note icon=false collapse=\"true\"}\nHere is an example going into the details of LCLS.\n\n::: {.cell}\n\n```{.r .cell-code}\n## Generate Sample Data\nx <- 1:5\ny <- runif(length(x))\n## plot(x,y)\n\n## Manually Compute Estimate at x=3\nw3 <- dunif(x-3,-1,1) #(x < 4)*(x > 2)\nyhat_3 <- sum(w3*y)\nyhat_3\n## [1] 1.167304\n```\n:::\n\n:::\n\nThe basic idea also generalizes other kernels. As such, a kernel regression using uniform weights is often called a \"naive kernel regression\". Typically, kernel regressions use kernels that weight nearby observations more heavily. We can also add a slope term to improve the fit.\n\nIf $X_{i}$ represents time, then the local constant regressions is also called a *moving average*. With non-uniform kernel weights, we have a *weighted* moving average.\n\n#### **Locally Linear**.{-}\nA less simple case is a *local linear regression* which conducts a linear regression for each data point using a subsample of data around it. Consider a point $x$ and suppose $\\hat{Y}_{i} = b_{0}(x,h) + b_{1}(x) \\hat{X}_{i} + e_{i}$ for data near $x$. The weighted OLS estimator with kernel weights is\n\\begin{eqnarray}\n& & \\min_{b_{0}(x,h),b_{1}(x,h)}~ \\sum_{i}^{n}\\left[\\hat{Y}_{i}- b_{0}(x,h) - b_{1}(x,h) \\hat{X}_{i} \\right]^2 K\\left(\\frac{|\\hat{X}_{i}-x|}{h}\\right)\n\\end{eqnarray} \nDeriving the optimal values $\\hat{b}_{0}(x,h)$ and  $\\hat{b}_{1}(x,h)$ for $k_{U}$ is left as a homework exercise.^[Note that one general benefit of LLLS is with edge effects (see homework). Another is that it is theoretically motivated: assuming that $Y_{i}=m(X_{i}) + \\epsilon_{i}$, we can then take a Taylor approximation: $m(X_{i}) + \\epsilon_{i} \\approx m(x) + m'(x)[X_{i}-x] + \\epsilon_{i} = [m(x) - m'(x)x ] + m'(x)X_{i} + \\epsilon_{i} = b_{0}(x,h) + b_{1}(x,h) X_{i}$. As such, a third benefit is that the estimated slope coefficient $\\hat{b}_{1}(x,h)$ can be interpreted as the estimated gradient at $x$.]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ``Naive\" Smoother\npred_fun <- function(x0, h, xy){\n    # Assign equal weight to observations within h distance to x0\n    # 0 weight for all other observations\n    ki   <- dunif(xy[,'x'], x0-h, x0+h) \n    llls <- lm(y~x, data=xy, weights=ki)\n    yhat_i <- predict(llls, newdata=data.frame(x=x0))\n}\n\nxy <- wage1[,c('educ','wage')]\nnames(xy) <- c('x','y')\nX0 <- sort(unique(xy[,'x']))\npred_lo1 <- sapply(X0, pred_fun, h=2, xy=xy)\npred_lo2 <- sapply(X0, pred_fun, h=12, xy=xy)\n\nplot(wage~educ, pch=16, data=wage1, col=grey(0,.1),\n    ylab='Murder Rate', xlab='Population Density')\ncols <- c(rgb(.8,0,0,.5), rgb(0,0,.8,.5))\nlines(X0, pred_lo1, col=cols[1], lwd=1, type='o')\nlines(X0, pred_lo2, col=cols[2], lwd=1, type='o')\nlegend('topleft', title='Locally Linear',\n    legend=c('h=2 ', 'h=12'),\n    lty=1, col=cols, cex=.8)\n```\n\n::: {.cell-output-display}\n![](02_05_KernelIntro_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nCompare local linear and local constant regressions using <https://shinyserv.es/shiny/kreg/>, with degree $0$ and $1$.\nAlso try different kernels and different datasets.\n:::\n\n:::{.callout-tip icon=false collapse=\"true\"}\nExamine the local relationship between 'Murder' and 'Urbanization' in the `USArrests` dataset using LLLS with a Gaussian kernel.\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\nX0 <- sort(unique(xy[,'x']))\npred_lo1 <- sapply(X0, pred_fun, h=2, xy=xy)\npred_lo2 <- sapply(X0, pred_fun, h=20, xy=xy)\n\nplot(y~x, pch=16, data=xy, col=grey(0,.5),\n    ylab='Murder Rate', xlab='Population Density')\ncols <- c(rgb(.8,0,0,.5), rgb(0,0,.8,.5))\nlines(X0, pred_lo1, col=cols[1], lwd=1, type='o')\nlines(X0, pred_lo2, col=cols[2], lwd=1, type='o')\nlegend('topleft', title='Locally Linear',\n    legend=c('h=2 ', 'h=20'),\n    lty=1, col=cols, cex=.8)\n```\n:::\n\n:::\n\n\nAn important extension of locally linear regressions is called *loess*, which uses adaptive bandwidths in order to have a similar number of data points around each design point. This is especially useful when $X$ is not uniform.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Adaptive-width subsamples with non-uniform weights\nxy0 <- xy[order(xy[,'x']),]\nplot(y~x, pch=16, col=grey(0,.1), dat=xy0)\n\nreg_lo4 <- loess(y~x, data=xy0, span=.4)\nreg_lo8 <- loess(y~x, data=xy0, span=.8)\n\ncols <- hcl.colors(3,alpha=.75)[-3]\nlines(xy0[,'x'], predict(reg_lo4),\n    col=cols[1], type='o', pch=2)\nlines(xy0[,'x'], predict(reg_lo8),\n    col=cols[2], type='o', pch=2)\n\nlegend('topleft', title='Loess',\n    legend=c('span=.4 ', 'span=.8'),\n    lty=1, col=cols, cex=.8)\n```\n\n::: {.cell-output-display}\n![](02_05_KernelIntro_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n#### **Confidence Bands**. {-}\n\nThe smoothed predicted values estimate the local means. So we can also construct confidence bands using the jackknife or bootstrap.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Loess\nxy0 <- xy[order(xy[,'x']),]\nreg_lo <- loess(y~x, data=xy0, span=.8)\n\n# Predictions at design points\npred_design <- data.frame(x=unique(xy0[,'x']))\npreds_lo <- predict(reg_lo, newdata=pred_design)\n\n# Plot\nplot(y~x, pch=16, col=grey(0,.1), dat=xy0)\nlines(X0, preds_lo,\n    col=hcl.colors(3,alpha=.75)[2],\n    type='o', pch=2)\n    \n# Boot CI\nboot_lo <- sapply(1:399, function(b){\n    # xy_i <- xy[-i,] #jackknife for i in 1:nrow(xy)\n    xy_b <- xy[sample(nrow(xy0), replace=T),]\n    reg_b <- loess(y~x, dat=xy_b, span=.8)\n    predict(reg_b, newdata=pred_design)\n})\nboot_cb <- apply(boot_lo,1, quantile,\n    probs=c(.025,.975), na.rm=T)\n    \n\n# Plot CI\npolygon(\n    c(pred_design[[1]], rev(pred_design[[1]])),\n    c(boot_cb[1,], rev(boot_cb[2,])),\n    col=hcl.colors(3,alpha=.25)[2],\n    border=NA)\n```\n\n::: {.cell-output-display}\n![](02_05_KernelIntro_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nThese confidence bands are approximations to what we want: how much do the predicted values vary from sample to sample. To make that clear, we will use a simulation where we can actually generate different samples.\n\n::: {.cell}\n\n```{.r .cell-code}\n## Ages\nXmx <- 70\nXmn <- 15\n\n##Generate Sample Data\ndat_sim <- function(n=1000){\n    n  <- 1000\n    X <- seq(Xmn,Xmx,length.out=n)\n    ## Random Productivity\n    e <- runif(n, 0, 1E6)\n    beta <-  1E-10*exp(1.4*X -.015*X^2)\n    Y    <-  (beta*X + e)/10\n    return(data.frame(Y,X))\n}\ndat <- dat_sim(1000)\n\n## Data from one sample\nplot(Y~X, data=dat, pch=16, col=grey(0,.1),\n    ylab='Yearly Productivity ($)', xlab='Age' )\ndat0 <- dat[order(dat[,'X']),]\nreg_lo <- loess(Y~X, data=dat0, span=.8)\n\n## Plot Bootstrap CI for Single Sample\npred_design <- data.frame(X=seq(Xmn, Xmx))\npreds_lo <- predict(reg_lo, newdata=pred_design)\nboot_lo <- sapply(1:399, function(b){\n    dat0_i <- dat0[sample(nrow(dat0), replace=T),]\n    reg_i <- loess(Y~X, dat=dat0_i, span=.8)\n    predict(reg_i, newdata=pred_design)\n})\nboot_cb <- apply(boot_lo,1, quantile,\n    probs=c(.025,.975), na.rm=T)\npolygon(\n    c(pred_design[[1]], rev(pred_design[[1]])),\n    c(boot_cb[1,], rev(boot_cb[2,])),\n    col=hcl.colors(3,alpha=.25)[2],\n    border=NA)\n\n# Construct CI across Multiple Samples\nsample_lo <- sapply(1:399, function(b){\n    xy_b <- dat_sim(1000) #Entirely new sample\n    reg_b <- loess(Y~X, dat=xy_b, span=.8)\n    predict(reg_b, newdata=pred_design)\n})\nci_lo <- apply(sample_lo,1, quantile,\n    probs=c(.025,.975), na.rm=T)\npolygon(\n    c(pred_design[[1]], rev(pred_design[[1]])),\n    c(ci_lo[1,], rev(ci_lo[2,])),\n    col=grey(0,alpha=.25),\n    border=NA)\n```\n\n::: {.cell-output-display}\n![](02_05_KernelIntro_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n:::\n\n",
    "supporting": [
      "02_05_KernelIntro_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}