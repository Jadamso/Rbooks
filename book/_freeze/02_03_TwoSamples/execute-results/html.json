{
  "hash": "2f725308cbc590efe72a1380f532735b",
  "result": {
    "engine": "knitr",
    "markdown": "\n# Mixed Data\n***\n\n\nFor mixed data, $\\hat{Y}_{i}$ is a cardinal variable and $\\hat{X}_{i}$ is a factor variable (typically unordered). For such data, we analyze associations via group comparisons. The basic idea is best seen in a comparison of two samples, which corresponds to an $\\hat{X}_{i}$ with two categories.\n\nSuppose we have two samples of data. For example, the heights of men and women in Canada. For another example, homicide rates in two different American states. For another example, the wages for people with and without completing a degree.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wooldridge)\nx1 <- wage1[wage1$educ == 15, 'wage']\nx2 <- wage1[wage1$educ == 16, 'wage']\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample 1 (e.g., males)\nn1 <- 100\nx1 <- rnorm(n1, 0, 2)\n# Sample 2 (e.g., females)\nn2 <- 80\nx2 <- rnorm(n2, 1, 1)\n\npar(mfrow=c(1,2))\nbks <- seq(-8,8, by=.5)\nhist(x1, border=NA, breaks=bks,\n    main='Sample 1', font.main=1)\n\nhist(x2, border=NA, breaks=bks, \n    main='Sample 2', font.main=1)\n```\n\n::: {.cell-output-display}\n![](02_03_TwoSamples_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Differences\n\nThere may be several differences between these samples. Often, the first summary statistic we investigate is the difference in means. \n\n#### **Mean Differences**. {-}\n\nWe often want to know if the means of different sample are different. To test this hypothesis, we compute the means separately for each sample and then examine the differences term\n\\begin{eqnarray} \n\\hat{D} = \\hat{M}_{X1} - \\hat{M}_{X2},\n\\end{eqnarray}\nwith a null hypothesis of $D=0$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Differences between means\nm1 <- mean(x1)\nm2 <- mean(x2)\nd <- m1-m2\n    \n# Bootstrap Distribution\nbootstrap_diff <- vector(length=9999)\nfor(b in seq(bootstrap_diff) ){\n    x1_b <- sample(x1, replace=T)\n    x2_b <- sample(x2, replace=T)\n    m1_b <- mean(x1_b)\n    m2_b <- mean(x2_b)\n    d_b <- m1_b - m2_b\n    bootstrap_diff[b] <- d_b\n}\nhist(bootstrap_diff,\n    border=NA, font.main=1,\n    main='Difference in Means')\n\n# 2-Sided Test\nboot_ci <- quantile(bootstrap_diff, probs=c(.025, .975))\nabline(v=boot_ci, lwd=2)\nabline(v=0, lwd=2, col=2)\n```\n\n::: {.cell-output-display}\n![](02_03_TwoSamples_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# p-value\n1 - ecdf(bootstrap_diff)(0)\n## [1] 0\n```\n:::\n\n\n\nJust as with one sample tests, we can compute a *standardized differences*, where $D$ is converted into a $t$ statistic.  Note, however, that we have to compute the standard error for the difference statistic, which is a bit more complicated. However, this allows us to easily conduct one or two sided hypothesis tests using a standard normal approximation.\n\n::: {.cell}\n\n```{.r .cell-code}\nse_hat <- sqrt(var(x1)/n1 + var(x2)/n2);\nt_obs <- d/se_hat\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n#### **Quantile Differences**. {-}\nThe above procedure generalized from differences in means to other quantiles statistics like medians.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap Distribution Function\nboot_quant <- function(x1, x2, B=9999, prob=0.5, ...){\n    bootstrap_diff <- vector(length=B)\n    for(b in seq(bootstrap_diff)){\n        x1_b <- sample(x1, replace=T)\n        x2_b <- sample(x2, replace=T)\n        q1_b <- quantile(x1_b, probs=0.5, ...)\n        q2_b <- quantile(x2_b, probs=0.5, ...)\n        d_b <- q1_b - q2_b\n        bootstrap_diff[b] <- d_b\n    }\n    return(bootstrap_diff)\n}\n\n# 2-Sided Test for Median Differences\n# d <- median(x2) - median(x1)\nboot_d <- boot_quant(x1, x1, B=999, prob=0.5)\nhist(boot_d, border=NA, font.main=1,\n    main='Difference in Medians')\nabline(v=quantile(boot_d, probs=c(.025, .975)), lwd=2)\nabline(v=0, lwd=2, col=2)\n```\n\n::: {.cell-output-display}\n![](02_03_TwoSamples_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n1 - ecdf(boot_d)(0)\n## [1] 0.5015015\n```\n:::\n\n\n\nIf we want to test for the differences in medians across groups with independent observations, we can also use notches in the boxplot. If the notches of two boxes do not overlap, then there is rough evidence that the difference in medians is statistically significant. The square root of the sample size is also shown as the bin width in each boxplot.^[Let each group $g$ have median $\\tilde{M}_{g}$, interquartile range $\\hat{IQR}_{g}$, observations $n_{g}$. We can compute standard deviation of the median as $\\tilde{S}_{g}= \\frac{1.25 \\hat{IQR}_{g}}{1.35 \\sqrt{n_{g}}}$. As a rough guess, the interval $\\tilde{M}_{g} \\pm 1.7 \\tilde{S}_{g}$ is the historical default and displayed as a *notch* in the boxplot. See also <https://www.tandfonline.com/doi/abs/10.1080/00031305.1978.10479236>.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(x1, x2,\n    col=c(2,4),\n    notch=T,\n    varwidth=T)\n```\n\n::: {.cell-output-display}\n![](02_03_TwoSamples_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nNote that bootstrap tests can perform poorly with highly unequal variances or skewed data. To see this yourself, make a simulation with skewed data and unequal variances.\n\n\nIn principle, we can also examine whether there are differences in spread or shape statistics such as `sd` and `IQR`, or `skew` and `kurtosis`. More often, however, we examine whether there are any differences in the distributions.\n\n:::{.callout-tip icon=false collapse=\"true\"}\nHere is an example to look at differences in \"spread\"\n\n::: {.cell}\n\n```{.r .cell-code}\nboot_fun <- function( fun, B=9999, ...){\n    bootstrap_diff <- vector(length=B)\n    for(b in seq(bootstrap_diff)){\n        x1_b <- sample(x1, replace=T)\n        x2_b <- sample(x2, replace=T)\n        f1_b <- fun(x1_b, ...)\n        f2_b <- fun(x2_b, ...)\n        d_b <- f1_b - f2_b\n        bootstrap_diff[b] <- d_b\n    }\n    return(bootstrap_diff)\n}\n\n# 2-Sided Test for SD Differences\n#d <- sd(x2) - sd(x1)\nboot_d <- boot_fun(sd)\nhist(boot_d, border=NA, font.main=1,\n    main='Difference in Standard Deviations')\nabline(v=quantile(boot_d, probs=c(.025, .975)), lwd=2)\nabline(v=0, lwd=2, col=2)\n1 - ecdf(boot_d)(0)\n\n\n# Try any function!\n# boot_fun( function(xs) { IQR(xs)/median(xs) } )\n```\n:::\n\n:::\n\n\n\n## Distributional Comparisons\n\nWe can also examine whether there are any differences between the entire *distributions*\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample Wage Data\nlibrary(wooldridge)\nx1 <- sort( wage1[wage1$educ == 15,  'wage'])  \nx2 <- sort( wage1[wage1$educ == 16,  'wage'] )\nx <- sort(c(x1, x2))\n\n# Compute Quantiles\nquants <- seq(0,1,length.out=101)\nQ1 <- quantile(x1, probs=quants)\nQ2 <- quantile(x2, probs=quants)\n\n# Compare Distributions via Quantiles\nrx <- range(c(x1, x2))\npar(mfrow=c(1,2))\nplot(rx, c(0,1), type='n', font.main=1,\n    main='Distributional Comparison',\n    xlab=\"Quantile\",\n    ylab=\"ECDF\")\nlines(Q1, quants, col=2)\nlines(Q2, quants, col=4)\nlegend('bottomright', col=c(2,4), lty=1,\n\tlegend=c(\n\t    expression(hat(F)[1]),\n\t    expression(hat(F)[2]) \n))\n\n# Compare Quantiles\nplot(Q1, Q2, xlim=rx, ylim=rx,\n    xlab=expression(Q[1]),\n    ylab=expression(Q[2]),\n    main='Quantile-Quantile Plot', font.main=1,\npch=16, col=grey(0,.25))\nabline(a=0,b=1,lty=2)\n```\n\n::: {.cell-output-display}\n![](02_03_TwoSamples_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThe starting point for hypothesis testing is the Kolmogorov-Smirnov Statistic: the maximum absolute difference between two CDF's over all sample data $x \\in \\{X_1\\} \\cup \\{X_2\\}$.\n\\begin{eqnarray}\n\\hat{KS} &=& \\max_{x} |\\hat{F}_{1}(x)- \\hat{F}_{2}(x)|^{p},\n\\end{eqnarray}\nwhere $p$ is an integer (typically 1). An intuitive alternative is the Cramer-von Mises Statistic: the sum of absolute differences (raised to an integer, typically 2) between two CDF's. \n\\begin{eqnarray}\n\\hat{CVM} &=& \\sum_{x} | \\hat{F}_{1}(x)- \\hat{F}_{2}(x)|^{p}.\n\\end{eqnarray}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Distributions\nF1 <- ecdf(x1)(x)\nF2 <- ecdf(x2)(x)\n\nlibrary(twosamples)\n\n# Kolmogorov-Smirnov\nKSq <- which.max(abs(F2 - F1))\nKSqv <- round(twosamples::ks_stat(x1, x2),2)\n\n# Cramer-von Mises Statistic (p=2)\nCVMqv <- round(twosamples::cvm_stat(x1, x2, power=2), 2) \n\n# Visualize Differences\nplot(range(x), c(0,1), type=\"n\", xlab='x', ylab='ECDF')\nlines(x, F1, col=2, lwd=2)\nlines(x, F2, col=4, lwd=2)\n\n# KS\ntitle( paste0('KS: ', KSqv), adj=0, font.main=1)\nsegments(x[KSq], F1[KSq], x[KSq], F2[KSq], lwd=1.5, col=grey(0,.75), lty=2)\n\n# CVM\ntitle( paste0('CVM: ', CVMqv), adj=1, font.main=1)\nsegments(x, F1, x, F2, lwd=.5, col=grey(0,.2))\n```\n\n::: {.cell-output-display}\n![](02_03_TwoSamples_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nJust as before, you use bootstrapping for hypothesis testing.\n\n::: {.cell}\n\n```{.r .cell-code}\ntwosamples::ks_test(x1, x2)\n## Test Stat   P-Value \n## 0.2892157 0.0965000\n\ntwosamples::cvm_test(x1, x2)\n## Test Stat   P-Value \n##  2.084253  0.085500\n```\n:::\n\n\n#### **Comparing Multiple Groups**. {-}\nFor multiple groups, we can tests the equality of all distributions (whether at least one group is different). The *Kruskal-Wallis* test examines $H_0:\\; F_1 = F_2 = \\dots = F_G$ versus $H_A:\\; \\text{at least one } F_g \\text{ differs}$, where $F_g$ is the continuous distribution of group $g=1,...G$. This test does not tell us which group is different.\n\nTo conduct the test, first denote individuals $i=1,...n$ with overall ranks $\\hat{r}_1,....\\hat{r}_{n}$. Each individual belongs to group $g=1,...G$, and each group $g$ has $n_{g}$ individuals with average rank $\\bar{r}_{g} = \\sum_{i} \\hat{r}_{i} /n_{g}$. The Kruskal Wallis statistic is \n\\begin{eqnarray}\n\\hat{KW} &=& (N-1) \\frac{\\sum_{g=1}^{G} n_{g}( \\bar{r}_{g} - \\bar{r}  )^2  }{\\sum_{i=1}^{n} ( \\hat{r}_{i} - \\bar{r}  )^2}, \n\\end{eqnarray}\nwhere  $\\bar{r} = \\frac{n+1}{2}$ is the grand mean rank.\n\nIn the special case with only two groups, $G=2$, the Kruskal Wallis test reduces to the *Mannâ€“Whitney U* test (also known as the *Wilcoxon rank-sum* test). In this case, we can write the hypotheses in terms of individual outcomes in each group, $Y_i$ in one group $Y_j$ in the other; $H_0: Prob(Y_i > Y_j)=Prob(Y_i > Y_i)$  versus $H_A: Prob(Y_i > Y_j) \\neq Prob(Y_i > Y_j)$. The corresponding test statistic is\n\\begin{eqnarray}\n\\hat{U}   &=& \\min(\\hat{U}_1, \\hat{U}_2) \\\\\n\\hat{U}_g &=& \\sum_{i\\in g}\\sum_{j\\in -g}\n           \\Bigl[\\mathbf 1( \\hat{Y}_{i} > \\hat{Y}_{j}) + \\tfrac12\\mathbf 1(\\hat{Y}_{i} = \\hat{Y}_{j})\\Bigr].\n\\end{eqnarray}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\ndata(CASchools)\nCASchools$stratio <- CASchools$students/CASchools$teachers\n\n# Do student/teacher ratio differ for at least 1 county?\n# Single test of multiple distributions\nkruskal.test(CASchools$stratio, CASchools$county)\n## \n## \tKruskal-Wallis rank sum test\n## \n## data:  CASchools$stratio and CASchools$county\n## Kruskal-Wallis chi-squared = 161.18, df = 44, p-value = 2.831e-15\n\n# Multiple pairwise tests\n# pairwise.wilcox.test(CASchools$stratio, CASchools$county)\n```\n:::\n\n\n\n\n## Further Reading \n\nOther Statistics \n\n* <https://cran.r-project.org/web/packages/qualvar/vignettes/wilcox1973.html>\n\n\n",
    "supporting": [
      "02_03_TwoSamples_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}