{
  "hash": "2b038b2eb8a95d37154d95b8a95d22a6",
  "result": {
    "engine": "knitr",
    "markdown": "# Sampling & Resampling\n***\n\nA *sample* is a subset of the population.\nA *simple random sample* is a sample where each possible sample of size $n$ has the same probability of being selected.\n\n:::{.callout-note icon=false collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple random sample (no duplicates, equal probability)\nx <- c(1,2,3,4) # population\nsample(x, 2, replace=F) #sample\n## [1] 3 4\n```\n:::\n\nHow many possible samples of two are there from a population with this data: ${7,10,122,55}$?\n:::\n\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nFactorials are used for counting permutations: different ways of rearranging $n$ distinct objects into a sequence. The factorial is denoted as $n!=1\\times2\\times3 ... (n-2)\\times(n-1)\\times n$.\n\nE.g., how many ways are there to order the numbers $\\{1, 2, 3\\}$?\n\n::: {.cell}\n\n```{.r .cell-code}\n#{1,2,3} {1,3,2}\n#{2,1,3} {2,3,1}  \n#{3,2,1} {3,1,2}\nfactorial(3)\n## [1] 6\n```\n:::\n\n\nThe [binomial coefficient](https://en.wikipedia.org/wiki/Binomial_coefficient) counts the subsets of $k$ elements from a set with $n$ elements, and is mathematically defined as $\\tbinom{n}{k}=\\frac{n!}{k!(n-k)!}$.\n\nFor example, how many subsets with $k=2$ are there for the set $\\{1,2,3,4\\}$?\n\n::: {.cell}\n\n```{.r .cell-code}\n#Ways to draw k=2 from a set with n=4\n#{1,2} {1,3} {1,4}  \n#      {2,3} {2,4}  \n#            {3,4} \n\nchoose(4,2)\n## [1] 6\n```\n:::\n\n:::\n\n\n\nOften, we think of the population as being infinitely large. This is an approximation that makes mathematical and computational work much simpler. \n\n::: {.cell}\n\n```{.r .cell-code}\n#All possible samples of two from a bag of numbers {1,2,3,4} with replacement \n#{1,1} {1,2} {1,3}, {3,4}\n#{2,2} {2,3} {2,4}\n#{3,3} {3,4}\n#{4,4}\n\n# Simple random sample (duplicates, equal probability)\nsample(x, 2, replace=T)\n## [1] 3 3\n```\n:::\n\n\nIntuition for infinite populations: imagine drawing names from a giant urn. If the urn has only $10$ names, then removing one name slightly changes the composition of the urn, and the probabilities shift for the next name you draw. Now imagine the urn has $100$ billion names, so that removing one makes no noticeable difference. We can pretend the composition never changes: each draw is essentially identical and independent (iid). We can actually guarantee the names are iid by putting any names drawn back into the urn (sampling with replacement).\n\n\n\n## Sampling Distributions\n\nThe *sampling distribution* of a statistic shows us how much a statistic varies from sample to sample. \n\nFor example, the sampling distribution of the mean shows how the sample mean varies from sample to sample to sample. The sampling distribution of mean can also be referred to as the probability distribution of the sample mean.\n\n:::{.callout-note icon=false collapse=\"true\"}\nGiven ages for population of $4$ students, compute the sampling distribution for the mean with samples of $n=2$.\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- c(18,20,22,24) # Ages for student population\n# six possible samples\nm1 <- mean( X[c(1,2)] ) #{1,2}\nm2 <- mean( X[c(1,3)] ) #{1,3}\nm3 <- mean( X[c(1,4)] ) #{3,4}\nm4 <- mean( X[c(2,3)] ) #{2,3}\nm5 <- mean( X[c(2,4)] ) #{2,4}\nm6 <- mean( X[c(3,4)] ) #{3,4}\n# sampling distribution\nsample_means <- c(m1, m2, m3, m4, m5, m6)\nhist(sample_means,\n    freq=F, breaks=100,\n    main='', border=F)\n```\n\n::: {.cell-output-display}\n![](01_05_Sampling_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nNow compute the sampling distribution for the median with samples of $n=3$.\n:::\n\n:::{.callout-note icon=false collapse=\"true\"}\nTo consider an infinite population, expand the loop below to consider 3000 samples and then make a histogram.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Three Sample Example from infinite population\nx1 <- runif(100)\nm1 <- mean(x1)\nx2 <- runif(100)\nm2 <- mean(x2)\nx3 <- runif(100)\nm3 <- mean(x3)\nsample_means <- c(m1, m2, m3)\nsample_means\n## [1] 0.5292126 0.4768009 0.5189463\n\n# An Equivalent Approach: fill vector in a loop\nsample_means <- vector(length=3)\nfor(i in seq(sample_means)){\n    x <- runif(100)\n    m <- mean(x)\n    sample_means[i] <- m\n}\nsample_means\n## [1] 0.5153382 0.5029452 0.5199177\n```\n:::\n\nFor more on loops, see <https://jadamso.github.io/Rbooks/00_01_FirstSteps.html#loops>.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Three Sample Example w/ Visual\npar(mfrow=c(1,3))\nfor(b in 1:3){\n    x <- runif(100) \n    m <-  mean(x)\n    hist(x,\n        breaks=seq(0,1,by=.1), #for comparability\n        freq=F, main=NA, border=NA)\n    abline(v=m, col=2, lwd=2)\n    title(paste0('mean= ', round(m,2)),  font.main=1)\n}\n```\n\n::: {.cell-output-display}\n![](01_05_Sampling_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nExamine the sampling distribution of the mean\n\n::: {.cell}\n\n```{.r .cell-code}\n# Many sample example\nsample_means <- vector(length=500)\nfor(i in seq_along(sample_means)){\n    x <- runif(1000)\n    m <- mean(x)\n    sample_means[i] <- m\n}\nhist(sample_means, \n    breaks=seq(0.45,0.55,by=.001),\n    border=NA, freq=F,\n    col=2, font.main=1, \n    xlab=expression(hat(M)),\n    main='Sampling Distribution of the mean')\n```\n\n::: {.cell-output-display}\n![](01_05_Sampling_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nIn this figure, you see two the most profound results known in statistics\n\n* *Law of Large Numbers*: the sample mean is centered around the true mean, and more tightly centered with more data\n* *Central Limit Theorem*: the sampling distribution of the mean is approximately Normal.\n\n\n\n#### **Law of Large Numbers**. {-}\n\nThere are different variants of the Law of Large Numbers (LLN), but they all say some version of \"the sample mean becomes more tightly centered around the true mean as we get more data\".\n\n:::{.callout-note icon=false collapse=\"true\"}\nNotice where the sampling distribution is centered\n\n::: {.cell}\n\n```{.r .cell-code}\nm_LLLN <- mean(sample_means)\nround(m_LLLN, 3)\n## [1] 0.5\n```\n:::\n\nand more tightly centered with more data\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,3))\nfor(n in c(5,50,500)){\n    sample_means_n <- vector(length=299)\n    for(i in seq_along(sample_means_n)){\n        x <- runif(n)\n        m <- mean(x)\n        sample_means_n[i] <- m\n    }\n    hist(sample_means_n, \n        breaks=seq(0,1,by=.01),\n        border=NA, freq=F,\n        col=2, font.main=1, \n        xlab=expression(hat(M)),\n        main=paste0('n=',n) )\n}\n```\n\n::: {.cell-output-display}\n![](01_05_Sampling_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nPlot the variability of the sample mean as a function of sample size\n\n::: {.cell}\n\n```{.r .cell-code}\nn_seq <- seq(1, 40)\nsd_seq <- vector(length=length(n_seq))\nfor(n in seq_along(sd_seq)){\n    sample_means_n <- vector(length=499)\n    for(i in seq_along(sample_means_n)){\n        x <- runif(n)\n        m <- mean(x)\n        sample_means_n[i] <- m\n    }\n    sd_seq[n] <- sd(sample_means_n)\n}\nplot(n_seq, sd_seq, pch=16, col=grey(0,0.5),\n    xlab='n', ylab='sd of sample means', main='')\n```\n\n::: {.cell-output-display}\n![](01_05_Sampling_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n:::\n\n\n#### **Central Limit Theorem**. {-}\nThere are different variants of the central limit theorem (CLT), but they all say some version of \"the sampling distribution of the mean is approximately normal\". For example, the sampling distribution of the mean, shown above, is approximately normal.\n\n:::{.callout-note icon=false collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(sample_means,\n    breaks=seq(0.45,0.55,by=.001),\n    border=NA, freq=F,\n    col=2, font.main=1,\n    xlab=expression(hat(M)),\n    main='Sampling Distribution of the mean')\n    \n# Approximately normal?\nmu <- mean(sample_means)\nmu_sd <- sd(sample_means)\nx <- seq(0.1, 0.9, by=0.001)\nfx <- dnorm(x, mu, mu_sd)\nlines(x, fx, col='red')\n```\n\n::: {.cell-output-display}\n![](01_05_Sampling_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n:::\n\n\nMany statistics have an approximately Normal sampling distribution.\n\n:::{.callout-tip icon=false collapse=\"true\"}\nFor an example with another statistic, let's the sampling distribution of the standard deviation.\n\n::: {.cell}\n\n```{.r .cell-code}\n# CLT example of the \"sd\" statistic\nsample_sds <- vector(length=1000)\nfor(i in seq_along(sample_sds)){\n    x <- runif(100) # same distribution\n    s <- sd(x) # different statistic\n    sample_sds[i] <- s\n}\nhist(sample_sds,\n    breaks=seq(0.2,0.4,by=.01),\n    border=NA, freq=F,\n    col=4, font.main=1,\n    xlab=expression(hat(S)),\n    main='Sampling Distribution of the sd')\n\n# Approximately normal?\nmu <- mean(sample_sds)\nmu_sd <- sd(sample_sds)\nx <- seq(0.1, 0.9, by=0.001)\nfx <- dnorm(x, mu, mu_sd)\nlines(x, fx, col='blue')\n```\n\n::: {.cell-output-display}\n![](01_05_Sampling_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# try another random variable, such as rexp(100) instead of runif(100)\n```\n:::\n\n:::\n\n\nIt is beyond this class to prove this result mathematically, but you should know that not all sampling distributions are standard normal. The CLT approximation is better for \"large $n$\" datasets with \"well behaved\" variances. The CLT also does not apply to \"extreme\" statistics. \n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example of \"extreme\" statistics, examine the sampling distribution of min and max statistics.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 300 samples, each with 1000 random uniform variables\nx_samples <- matrix(nrow=300, ncol=1000)\nfor(i in seq(1,nrow(x_samples))){\n    x_samples[i,] <- runif(1000)\n}\n# Each row is a new sample\nlength(x_samples[1,])\n## [1] 1000\n\n# Compute min and max for each sample\nx_mins <- apply(x_samples, 1, quantile, probs=0)\nx_maxs <- apply(x_samples, 1, quantile, probs=1)\n\n# Plot the sampling distributions of min, median, and max\n# Median looks normal. Maximum and minimum do not!\npar(mfrow=c(1,2))\nhist(x_mins, breaks=100, main='Min', font.main=1,\n    xlab=expression(hat(Q)[0]), border=NA, freq=F)\nhist(x_maxs, breaks=100, main='Max', font.main=1,\n    xlab=expression(hat(Q)[1]), border=NA, freq=F)\ntitle('Sampling Distributions', outer=T, line=-1, adj=0)\n```\n\n::: {.cell-output-display}\n![](01_05_Sampling_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\nExplore another function, such as\n` my_function <- function(x){ diff(range(x)) } `\n\n:::\n\n:::{.callout-tip icon=false collapse=\"true\"}\nHere is an example where variance is not \"well behaved\" .\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_means <- vector(length=999)\nfor(i in seq_along(sample_means)){\n    x <- rcauchy(1000,0,10)\n    m <- mean(x)\n    sample_means[i] <- m\n}\nhist(sample_means, breaks=100,\n    main='',\n    border=NA, freq=F) # Tails look too \"fat\"\n```\n:::\n\n:::\n\n\n\n#### **The Fundamental Theorem of Statistics**. {-}\n\nThe Law of Large Numbers generalizes to many other statistics, like `median` or `sd`.^[When a statistic converges in probability to the quantity they are meant to estimate, they are called *consistent*.] \n\n:::{.callout-note icon=false collapse=\"true\"}\nHere is a sampling distribution for a quantile, for three different sample sizes\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,3))\nfor(n in c(5,50,500)){\n    sample_quants_n <- vector(length=299)\n    for(i in seq_along(sample_quants_n)){\n        x <- runif(n)\n        m <- quantile(x, probs=0.75) #upper quartile\n        sample_quants_n[i] <- m\n    }\n    hist(sample_quants_n, \n        breaks=seq(0,1,by=.01),\n        border=NA, freq=F,\n        col=2, font.main=1, \n        xlab=\"Sample Quantile\",\n        main=paste0('n=',n) )\n}\n```\n\n::: {.cell-output-display}\n![](01_05_Sampling_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nHere is a sampling distribution for a proportion, for three different sample sizes\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,3))\nfor(n in c(5,50,500)){\n    sample_props_n <- vector(length=299)\n    for(i in seq_along(sample_props_n)){\n        x <- sample(1:5, size=n, prob=c(1:5)/15, replace=T)\n        p3 <- mean(x==3)\n        sample_props_n[i] <- p3\n    }\n    hist(sample_props_n, \n        breaks=seq(0,1,by=.01),\n        border=NA, freq=F,\n        col=2, font.main=1, \n        xlab=\"Sample Quantile\",\n        main=paste0('n=',n) )\n}\n```\n\n::: {.cell-output-display}\n![](01_05_Sampling_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n:::\n\nIn fact, the Glivenko-Cantelli Theorem (GCT) shows entire empirical distribution converges: the ECDF gets increasingly close to the CDF as the sample sizes grow. This result is often termed the *The Fundamental Theorem of Statistics*.\n\n:::{.callout-note icon=false collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 3))\nfor (n in c(50, 500, 5000)) {\n  x <- runif(n)\n  Fx <- ecdf(x)\n  plot(Fx)\n}\n```\n\n::: {.cell-output-display}\n![](01_05_Sampling_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n:::\n\n\n## Resampling Distributions \n\nOften, we only have one sample. How then can we estimate the sampling distribution of a statistic? \n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests[,'Murder']\nsample_mean <- mean(sample_dat)\nsample_mean\n## [1] 7.788\n```\n:::\n\n\nWe can \"resample\" our data. *Hesterberg (2015)* provides a nice illustration of the idea. The two most basic versions are the jackknife and the bootstrap, which are discussed below. \n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_05_Sampling_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nNote that we do not use the mean of the resampled statistics as a replacement for the original estimate. This is because the resampled distributions are centered at the observed statistic, not the population parameter. (The bootstrapped mean is centered at the sample mean, for example, not the population mean.) This means that we do not use resampling to improve on $\\hat{M}$. We use resampling to estimate sampling variability.\n\n\n#### **Jackknife Distribution**. {-}\nHere, we compute all \"leave-one-out\" estimates. Specifically, for a dataset with $n$ observations, the jackknife uses $n-1$ observations other than $i$ for each unique subsample. \n\n:::{.callout-note icon=false collapse=\"true\"}\nGiven the sample $\\{1,6,7,22\\}$, compute the jackknife estimate of the median. Show the result mathematically by hand and also with the computer.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests[,'Murder']\nsample_mean <- mean(sample_dat)\n\n# Jackknife Estimates\nn <- length(sample_dat)\njackknife_means <- vector(length=n)\nfor(i in seq_along(jackknife_means)){\n    dat_noti <- sample_dat[-i]\n    mean_noti <- mean(dat_noti)\n    jackknife_means[i] <- mean_noti\n}\nhist(jackknife_means, breaks=25,\n    border=NA, freq=F,\n    main='', xlab=expression(hat(M)[-i]))\nabline(v=sample_mean, col='red', lty=2)\n```\n\n::: {.cell-output-display}\n![](01_05_Sampling_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n#### **Bootstrap Distribution**. {-}\nHere, we draw $n$ observations with replacement from the original data to create a bootstrap sample and calculate a statistic.\nEach bootstrap sample $b=1...B$ uses a random resample of the observations to recompute a statistic.\nWe repeat that many times, say $B=9999$, to estimate the sampling distribution.\n\n:::{.callout-note icon=false collapse=\"true\"}\nGiven the sample $\\{1,6,7,22\\}$, compute the bootstrap estimate of the median, with $B=8$.\nShow the result with the computer and then show what is happening in each sample by hand.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap estimates\nbootstrap_means <- vector(length=9999)\nfor(b in seq_along(bootstrap_means)){\n    boot_id <- sample(n, replace=T)\n    dat_b  <- sample_dat[boot_id] # c.f. jackknife\n    mean_b <- mean(dat_b)\n    bootstrap_means[b] <-mean_b\n}\n\nhist(bootstrap_means, breaks=25,\n    border=NA, freq=F,\n    main='', xlab=expression(hat(M)[b]))\nabline(v=sample_mean, col='red', lty=2)\n```\n\n::: {.cell-output-display}\n![](01_05_Sampling_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nWhy does this work? The sample: $\\{\\hat{X}_{1}, \\hat{X}_{2}, ... \\hat{X}_{n}\\}$ is drawn from a CDF $F$. Each bootstrap sample: $\\{\\hat{X}_{1}^{(b)}, \\hat{X}_{2}^{(b)}, ... \\hat{X}_{n}^{(b)}\\}$ is drawn from the ECDF $\\hat{F}$. With $\\hat{F} \\approx F$, each bootstrap sample is approximately a random sample. So when we compute a statistic on each bootstrap sample, we approximate the sampling distribution of the statistic.\n\n:::{.callout-tip icon=false collapse=\"true\"}\nHere is an intuitive example with Bernoulli random variables (unfair coin flips)\n\n::: {.cell}\n\n```{.r .cell-code}\n# theoretical probabilities\nx <- c(0,1)\nx_probs <- c(1/4, 3/4)\n# sample draws\ncoin_sample <- sample(x, prob=x_probs, 1000, replace=T)\nFhat <- ecdf(coin_sample)\n\nx_probs_boot <- c(Fhat(0), 1-Fhat(0))\nx_probs_boot # approximately the theoretical value\n## [1] 0.239 0.761\ncoin_resample <- sample(x, prob=x_probs_boot, 999, replace=T)\n# any draw from here is almost the same as the original process\n```\n:::\n\n:::\n\n\n#### **Comparison**. {-}\n\nNote that both Jackknife and Bootstrap resampling methods provide imperfect estimates, and can give different numbers.\n\n* Jackknife resamples are often less variable than they should be and sample $n-1$ instead of $n$.\n* Bootstrap resamples have the right $n$ but often have duplicated data. \n\n\n#### **Generalization**. {-}\n\nThe above procedure works for many different statistics\n\n::: {.cell}\n\n```{.r .cell-code}\nmed <- quantile(sample_dat, prob=0.5)\n\n# Bootstrap estimates\nbootstrap_stat <- vector(length=9999)\nfor(b in seq_along(bootstrap_stat)){\n    boot_id <- sample(n, replace=T)\n    dat_b  <- sample_dat[boot_id] # c.f. jackknife\n    stat_b <- quantile(dat_b, prob=0.5)\n    bootstrap_stat[b] <- stat_b\n}\n\nhist(bootstrap_stat, breaks=25,\n    border=NA, freq=F,\n    main='', xlab=expression(Med[b]))\nabline(v=med, col='red', lty=2)\n```\n\n::: {.cell-output-display}\n![](01_05_Sampling_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n\n## Standard Errors\n\nUsing either the bootstrap or jackknife distribution, we typically communicate the variability of the sampling distribution of a statistic using the *Standard Error*. In either case, this differs from the standard deviation of the data within your sample.\n\n* sample *standard deviation*: variability of individual observations within a single sample.\n* sample *standard error*: variability of a statistic across repeated samples.\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, we have\n\n* jackknifed estimates: $\\hat{M}_{i}^{\\text{jack}}=\\frac{1}{n-1} \\sum_{j \\neq i}^{n} \\hat{X}_{j}$.\n* mean of the jackknife distribution: $\\bar{\\hat{M}}^{\\text{jack}}=\\frac{1}{n} \\sum_{i}^{n} \\hat{M}_{i}^{\\text{jack}}$.\n* standard deviation of the jackknife distribution: $\\hat{SE}^{\\text{jack}}= \\sqrt{ \\frac{1}{n} \\sum_{i}^{n} \\left[\\hat{M}_{i}^{\\text{jack}} - \\bar{\\hat{M}}^{\\text{jack}} \\right]^2 }$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsd(sample_dat) # standard deviation\n## [1] 4.35551\nsd(jackknife_means) # standard error estimate for the mean\n## [1] 0.08888795\n```\n:::\n\n:::\n\nThere are \"corrections\" that can improve the basic jackknife SEs.\nAlternatively, we can compute bootstrap SEs.\n\n:::{.callout-tip icon=false collapse=\"true\"}\nTaking the bootstrap distribution of the median, for example, we have\n\n* bootstrapped estimate: $\\tilde{M}_{b}^{\\text{boot}}= \\text{Med}(\\hat{X}_{i}^{(b)})$, for resampled data $\\hat{X}_{i}^{(b)}$\n* mean of the bootstrap distribution: $\\bar{\\hat{M}}^{\\text{boot}}= \\frac{1}{B} \\sum_{b} \\tilde{M}_{b}^{\\text{boot}}$.\n* standard deviation of the bootstrap distribution: $\\hat{SE}^{\\text{boot}}= \\sqrt{ \\frac{1}{B} \\sum_{b=1}^{B} \\left[\\hat{M}_{b}^{\\text{boot}} - \\bar{\\hat{M}}^{\\text{boot}} \\right]^2}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standard error estimate for the median\nsd(bootstrap_stat)\n## [1] 0.8723213\n```\n:::\n\n:::\n\n\nAlso note that each additional data point you have provides more information, which ultimately decreases the standard error of your estimates. This is why statisticians will often recommend that you to get more data. However, the improvement in the standard error increases at a diminishing rate. In economics, this is known as diminishing returns and why economists may recommend you do not get more data. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nB <- 999 # number of bootstrap samples\nNseq <- seq(1, length(sample_dat), by=1) # different resample sizes\nn_id <- seq_along(sample_dat) # who to potentially resample\n\n## For each sample size, compute the bootstrap SE\nSE <- vector(length=length(Nseq))\nfor(n in seq_along(Nseq)){\n    sample_stats_n <- vector(length=B)\n    for(b in seq(1,B)){\n    \tb_id <- sample(n_id, size=n, replace=T)\n        x_b <- sample_dat[b_id]\n        x_stat_b <- mean(x_b) # statistic of interest\n        sample_stats_n[b] <- x_stat_b\n    }\n    se_n <- sd(sample_stats_n) # How much the statistic varies across samples\n    SE[n] <- se_n\n}\n\nplot(Nseq, SE, pch=16, col=grey(0,.5),\n    ylab='standard error', xlab='sample size')\n```\n\n::: {.cell-output-display}\n![](01_05_Sampling_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n\n## Drawing Samples\n\nTo generate a random variable from known distributions, you can use some type of physical machine. E.g., you can roll a fair die to generate Discrete Uniform data or you can roll weighted die to generate Categorical data.\n\nThere are also several ways to computationally generate random variables from a probability distribution. Perhaps the most common one is \"inverse sampling\". To generate a random variable using inverse sampling, first sample $p$ from a uniform distribution and then find the associated quantile  quantile function $\\hat{F}^{-1}(p)$.^[Drawing random uniform samples with computers is actually quite complex and beyond the scope of this course.]\n\n\n#### **Empirically**. {-}\n\nYou can generate a random variable from a known empirical distribution. Inverse sampling randomly selects observations from the dataset with equal probabilities. To implement this, we \n\n* order the data and associate each observation with an ECDF value\n* draw $p \\in [0,1]$ as a uniform random variable\n* find the associated quantile via the ECDF\n\n:::{.callout-tip icon=false collapse=\"true\"}\nHere is an example of generating random murder rates for US states.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Empirical Distribution\nX <- USArrests[,'Murder']\nFX_hat <- ecdf(X)\nplot(FX_hat, lwd=2, xlim=c(0,20),\n    pch=16, col=grey(0,.5), main='')\n```\n\n::: {.cell-output-display}\n![](01_05_Sampling_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Generating random variables via inverse ECDF\np <- runif(3000) ## Multiple Draws\nQX_hat <- quantile(FX_hat, p, type=1)\nQX_hat[c(1,2,3)]\n## 67.03324958% 22.58107369%  8.96678311% \n##          9.7          3.8          2.2\n\n## Can also do directly from the data\nQX_hat <- quantile(X, p, type=1)\nQX_hat[c(1,2,3)]\n## 67.03324958% 22.58107369%  8.96678311% \n##          9.7          3.8          2.2\n```\n:::\n\n:::\n\n#### **Theoretically**. {-}\n\nIf you know the distribution function that generates the data, then you can derive the quantile function and do inverse sampling. \nThat is how computers generate random data from a distribution.\n\n::: {.cell}\n\n```{.r .cell-code}\n# 4 random data points from 3 different distributions\nqunif(4)\n## [1] NaN\nqexp(4)\n## [1] NaN\nqnorm(4)\n## [1] NaN\n```\n:::\n\n\nHere is an in-depth example of the [Dagum distribution](https://en.wikipedia.org/wiki/Dagum_distribution). The distribution function is $F(x)=(1+(x/b)^{-a})^{-c}$. For a given probability $p$, we can then solve for the quantile as $F^{-1}(p)=\\frac{ b p^{\\frac{1}{ac}} }{(1-p^{1/c})^{1/a}}$. Afterwhich, we sample $p$ from a uniform distribution and then find the associated quantile.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Theoretical Quantile Function (from VGAM::qdagum)\nqdagum <- function(p, scale.b=1, shape1.a, shape2.c) {\n  # Quantile function (theoretically derived from the CDF)\n  ans <- scale.b * (expm1(-log(p) / shape2.c))^(-1 / shape1.a)\n  # Special known cases\n  ans[p == 0] <- 0\n  ans[p == 1] <- Inf\n  # Safety Checks\n  ans[p < 0] <- NaN\n  ans[p > 1] <- NaN\n  if(scale.b <= 0 | shape1.a <= 0 | shape2.c <= 0){ ans <- ans*NaN }\n  # Return\n  return(ans)\n}\n\n# Generate Random Variables (VGAM::rdagum)\nrdagum <-function(n, scale.b=1, shape1.a, shape2.c){\n    p <- runif(n) # generate random probabilities\n    x <- qdagum(p, scale.b=scale.b, shape1.a=shape1.a, shape2.c=shape2.c) #find the inverses\n    return(x)\n}\n\n# Example\nset.seed(123)\nX <- rdagum(3000,1,3,1)\nX[c(1,2,3)]\n## [1] 0.7390476 1.5499868 0.8845006\n```\n:::\n\n\n",
    "supporting": [
      "01_05_Sampling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}