{
  "hash": "9bd1837b667187fa1c26207bae6b33a9",
  "result": {
    "engine": "knitr",
    "markdown": "# Misc Topics\n\n\n## Functions for Data Analysis\n\n#### **Data Handling**. {-}\n\nYou can find a value by a particular criterion\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- 1:10\n\n# Return Y-value with minimum absolute difference from 3\nabs_diff_y <- abs( y - 3 ) \nabs_diff_y # is this the luckiest number?\n##  [1] 2 1 0 1 2 3 4 5 6 7\n\nmin(abs_diff_y)\n## [1] 0\nwhich.min(abs_diff_y)\n## [1] 3\ny[ which.min(abs_diff_y) ]\n## [1] 3\n```\n:::\n\n\n\nThere are also some useful built in functions for standardizing data\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- matrix(c(1:3,2*(1:3)),byrow=TRUE,ncol=3)\nm\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    2    4    6\n\n# normalize rows\nm/rowSums(m)\n##           [,1]      [,2] [,3]\n## [1,] 0.1666667 0.3333333  0.5\n## [2,] 0.1666667 0.3333333  0.5\n\n# normalize columns\nt(t(m)/colSums(m))\n##           [,1]      [,2]      [,3]\n## [1,] 0.3333333 0.3333333 0.3333333\n## [2,] 0.6666667 0.6666667 0.6666667\n\n# de-mean rows\nsweep(m,1,rowMeans(m), '-')\n##      [,1] [,2] [,3]\n## [1,]   -1    0    1\n## [2,]   -2    0    2\n\n# de-mean columns\nsweep(m,2,colMeans(m), '-')\n##      [,1] [,2] [,3]\n## [1,] -0.5   -1 -1.5\n## [2,]  0.5    1  1.5\n```\n:::\n\n\n\n#### **Binning and Aggregating**. {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 1:10\ncut(x, 4)\n##  [1] (0.991,3.25] (0.991,3.25] (0.991,3.25] (3.25,5.5]   (3.25,5.5]  \n##  [6] (5.5,7.75]   (5.5,7.75]   (7.75,10]    (7.75,10]    (7.75,10]   \n## Levels: (0.991,3.25] (3.25,5.5] (5.5,7.75] (7.75,10]\nsplit(x, cut(x, 4))\n## $`(0.991,3.25]`\n## [1] 1 2 3\n## \n## $`(3.25,5.5]`\n## [1] 4 5\n## \n## $`(5.5,7.75]`\n## [1] 6 7\n## \n## $`(7.75,10]`\n## [1]  8  9 10\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxs <- split(x, cut(x, 4))\nsapply(xs, mean)\n## (0.991,3.25]   (3.25,5.5]   (5.5,7.75]    (7.75,10] \n##          2.0          4.5          6.5          9.0\n\n# shortcut\naggregate(x, list(cut(x,4)), mean)\n##        Group.1   x\n## 1 (0.991,3.25] 2.0\n## 2   (3.25,5.5] 4.5\n## 3   (5.5,7.75] 6.5\n## 4    (7.75,10] 9.0\n```\n:::\n\n\nSee also <https://bookdown.org/rwnahhas/IntroToR/logical.html>\n\n\n## Transformations\n\nTransformations can stabilize variance, reduce skewness, and make model errors closer to Gaussian.\n\nPerhaps the most common examples are *power transformations*: $y= x^\\lambda$, which includes $\\sqrt{x}$ and $x^2$.\n\nOther examples include the *exponential transformation*: $y=exp(x)$ for any $x\\in (-\\infty, \\infty)$ and *logarithmic transformation*: $y=\\log(x)$ for any $x>0$.\n\nThe *Box–Cox Transform* nests many cases. For $x>0$ and parameter $\\lambda$,\n\\begin{eqnarray}\ny=\\begin{cases}\n\\dfrac{x^\\lambda-1}{\\lambda}, & \\lambda\\neq 0,\\\\\n\\log x, & \\lambda=0.\n\\end{cases}\n\\end{eqnarray}\nThis function is continuous over $\\lambda$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Box–Cox transform and inverse\nbc_transform <- function(x, lambda) {\n  if (any(x <= 0)) stop(\"Box-Cox requires x > 0\")\n  if (abs(lambda) < 1e-8) log(x) else (x^lambda - 1)/lambda\n}\nbc_inverse <- function(t, lambda) {\n  if (abs(lambda) < 1e-8) exp(t) else (lambda*t + 1)^(1/lambda)\n}\n\nX <- USArrests$Murder\nhist(X, main='', border=NA, freq=F)\n```\n\n::: {.cell-output-display}\n![](01_10_MiscTopics_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\npar(mfrow=c(1,3))\nfor(lambda in c(-1,0,1)){\n    Y <- bc_transform(X, lambda)\n    hist(Y, \n        main=bquote(paste(lambda,'=',.(lambda))),\n        border=NA, freq=F)\n}\n```\n\n::: {.cell-output-display}\n![](01_10_MiscTopics_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n:::\n\n\n#### **Law of the Unconscious Statistician (LOTUS)**. {-}\n\nAs before, we will represent the random variable as $X_{i}$, which can take on values $x$ from the sample space. If $X_{i}$ is a discrete random variable (a random variable with a discrete sample space) and $g$ is a function, then\n$\\mathbb E[g(X)] = \\sum_x g(x)Prob(X_{i}=x)$.\n\n:::{.callout-note icon=false collapse=\"true\"}\nLet $X_{i}$ take values $\\{1,2,3\\}$ with\n$$\nPr(X_{i}=1)=0.2,\\quad Prob(X_{i}=2)=0.5,\\quad Prob(X_{i}=3)=0.3.\n$$\nLet $g(x)=x^2+1$. Then $g(1)=1^2+1=2$, $g(2)=2^2+1=5$, $g(3)=3^2+1=10$. \n\nThen, by LOTUS,\n\\begin{eqnarray}\n\\mathbb E[g(X_{i})]=\\sum_x g(x)Prob(X_{i}=x)\n&=& g(1)\\cdot 0.2 + g(2)\\cdot 0.5 + g(3)\\cdot 0.3 \\\\\n&=& 2 \\cdot 0.2 + 5 \\cdot 0.5 + 10 \\cdot 0.3 \\\\\n&=& 0.4 + 2.5 + 3 = 5.9.\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\nx  <- c(1,2,3)\nx_probs <- c(0.2,0.5,0.3)\ng  <- function(x) x^2 + 1\nsum(g(x) * x_probs) \n## [1] 5.9\n```\n:::\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng  <- function(x) x^2 + 1\n\n# A theoretical example\nx <- c(1,2,3,4)\nx_probs <- c(1/4, 1/4, 1/4, 1/4)\nsum(g(x) * x_probs) \n## [1] 8.5\n\n# A simulation example\nX <- sample(x, x_probs, size=1000, replace=T)\nmean(g(X))\n## [1] 8.518\n```\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nIf $X_{i}$ is a continuous random variable (a random variable with a continuous sample space) and $g$ is a function, then\n$\\mathbb E[g(X_{i})] = \\int_{-\\infty}^{\\infty} g(x)f(x) dx$.\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rexp(5e5, rate = 1)           # X ~ Exp(1)\nmean(sqrt(x))                      # LOTUS Simulation\n## [1] 0.8869553\nsqrt(pi) / 2                       # Exact via LOTUS integral\n## [1] 0.8862269\n```\n:::\n\n:::\n\nNote that you have already seen the special case where $g(X_{i})=\\left(X_{i}-\\mathbb{E}[X_{i}]\\right)^2$.\n\n#### **Jensen’s inequality**. {-}\n\nConcave functions curve inwards, like the inside of a cave.\nConvex functions curve outward, the opposite of concave.\n\nIf $g$ is a *concave* function, then $g(\\mathbb E[X_{i}]) \\geq \\mathbb E[g(X_{i})]$.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Continuous Example 1\nmean( sqrt(x) )\n## [1] 0.8869553\nsqrt( mean(x) ) \n## [1] 1.000834\n\n# Continuous Example 2\nmean( log(x) )\n## [1] -0.5751418\nlog( mean(x) ) \n## [1] 0.001667872\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Discrete Example\nx  <- c(1,2,3)\npx <- c(0.2,0.5,0.3)\nEX <- sum(x * px)\nEX\n## [1] 2.1\n\ng  <- sqrt\ngEX <- g(EX)\nEgX <- sum(g(x) * px)\nc(gEX, EgX)\n## [1] 1.449138 1.426722\n```\n:::\n\n\n\nIf $g$ is a *convex* function, then the inequality reverses: $g(\\mathbb E[X_{i}]) \\leq \\mathbb E[g(X_{i})]$.\n\n::: {.cell}\n\n```{.r .cell-code}\nmean( exp(x) )\n## [1] 10.06429\nexp( mean(x) )  \n## [1] 7.389056\n```\n:::\n\n\n## Drawing Samples\n\nTo generate a random variable from known distributions, you can use some type of physical machine. E.g., you can roll a fair die to generate Discrete Uniform data or you can roll weighted die to generate Categorical data.\n\nThere are also several ways to computationally generate random variables from a probability distribution. Perhaps the most common one is \"inverse sampling\". To generate a random variable using inverse sampling, first sample $p$ from a uniform distribution and then find the associated quantile  quantile function $\\widehat{F}^{-1}(p)$.^[Drawing random uniform samples with computers is actually quite complex and beyond the scope of this course.]\n\n\n#### **Using Data**. {-}\n\nYou can generate a random variable from a known empirical distribution. Inverse sampling randomly selects observations from the dataset with equal probabilities. To implement this, we \n\n* order the data and associate each observation with an ECDF value\n* draw $p \\in [0,1]$ as a uniform random variable\n* find the associated data point on the ECDF\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Empirical Distribution\nX <- USArrests$Murder\nFX_hat <- ecdf(X)\nplot(FX_hat, lwd=2, xlim=c(0,20),\n    pch=16, col=grey(0,.5), main='')\n```\n\n::: {.cell-output-display}\n![](01_10_MiscTopics_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Generating a random variable\np <- runif(3000) ## Multiple Draws\nQX_hat <- quantile(X, p, type=1)\nQX_hat[1:5]\n## 42.83342932% 99.93823159% 50.07819012% 19.01619665% 43.60562169% \n##          6.3         17.4          7.3          3.3          6.3\n```\n:::\n\n\n#### **Using Math**. {-}\n\nIf you know the distribution function that generates the data, then you can derive the quantile function and do inverse sampling. Here is an in-depth example of the [Dagum distribution](https://en.wikipedia.org/wiki/Dagum_distribution). The distribution function is $F(x)=(1+(x/b)^{-a})^{-c}$. For a given probability $p$, we can then solve for the quantile as $F^{-1}(p)=\\frac{ b p^{\\frac{1}{ac}} }{(1-p^{1/c})^{1/a}}$. Afterwhich, we sample $p$ from a uniform distribution and then find the associated quantile.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Theoretical Quantile Function (from VGAM::qdagum)\nqdagum <- function(p, scale.b=1, shape1.a, shape2.c) {\n  # Quantile function (theoretically derived from the CDF)\n  ans <- scale.b * (expm1(-log(p) / shape2.c))^(-1 / shape1.a)\n  # Special known cases\n  ans[p == 0] <- 0\n  ans[p == 1] <- Inf\n  # Safety Checks\n  ans[p < 0] <- NaN\n  ans[p > 1] <- NaN\n  if(scale.b <= 0 | shape1.a <= 0 | shape2.c <= 0){ ans <- ans*NaN }\n  # Return\n  return(ans)\n}\n\n# Generate Random Variables (VGAM::rdagum)\nrdagum <-function(n, scale.b=1, shape1.a, shape2.c){\n    p <- runif(n) # generate random probabilities\n    x <- qdagum(p, scale.b=scale.b, shape1.a=shape1.a, shape2.c=shape2.c) #find the inverses\n    return(x)\n}\n\n# Example\nset.seed(123)\nX <- rdagum(3000,1,3,1)\nX[1:5]\n## [1] 0.7390476 1.5499868 0.8845006 1.9616251 2.5091656\n```\n:::\n\n\n",
    "supporting": [
      "01_10_MiscTopics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}