{
  "hash": "c0d8de5729324e12dab7e8fd7e268ca0",
  "result": {
    "engine": "knitr",
    "markdown": "\n# Probability Theory\n***\n\n## Theoretical Distributions\n\nWe now consider a bivariate *random vector* $(X_{i}, Y_{i})$, which is a theoretical version of the bivariate observations $(\\hat{X}_{i}, \\hat{Y}_{i})$. E.g., If we are going to flip two coins, then $(X_{i}, Y_{i})$ corresponds to the unflipped coins and $(\\hat{X}_{i}, \\hat{Y}_{i})$ corresponds to concrete values after they are flipped.\n\n\n#### **Definitions for Discrete Data**. {-}\nThe *joint distribution* is defined as\n\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y)\n\\end{eqnarray}\nFor example, consider a bivariate random vector with three outcomes for $X_{i}$ and three outcomes for $Y_{i}$. The plot below shows each outcome $(x,y)$ with deeper colors reflecting higher probability events.\n\n::: {.cell}\n\n```{.r .cell-code}\nprob_table <- expand.grid(x=c(0,1,2), y=c(0,10,20))\nprob_table[,'probabilities'] <- c(\n    0.0, 0.1, 0.0,\n    0.1, 0.3, 0.1,\n    0.1, 0.2, 0.2)\nplot(prob_table[,'x'], prob_table[,'y'],\n     xlim=c(-0.5,2.5), ylim=c(-5, 25),\n     pch=21, cex=8, col='blue',\n     bg=rgb(0,0,1, prob_table[,'probabilities']),\n     xlab = \"X\", ylab = \"Y\")\n```\n\n::: {.cell-output-display}\n![](02_15_ProbabilityTheory_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nNote that variables are *statistically independent* if $Prob(X_{i} = x, Y_{i} = y)= Prob(X_{i} = x) Prob(Y_{i} = y)$ for all $x, y$. Independence is sometimes assumed for mathematical simplicity, not because it generally fits data well.^[The same can be said about assuming normally distributed errors, although at least that can be motivated by the Central Limit Theorems.]\n\n\nThe *conditional distributions* are defined as\n\\begin{eqnarray}\nProb(X_{i} = x | Y_{i} = y) = \\frac{ Prob(X_{i} = x, Y_{i} = y)}{ Prob( Y_{i} = y )}\\\\\nProb(Y_{i} = y | X_{i} = x) = \\frac{ Prob(X_{i} = x, Y_{i} = y)}{ Prob( X_{i} = x )}\n\\end{eqnarray}\nThe *marginal distributions* are then defined as\n\\begin{eqnarray}\nProb(X_{i} = x) = \\sum_{y} Prob(X_{i} = x | Y_{i} = y) Prob( Y_{i} = y ) \\\\\nProb(Y_{i} = y) = \\sum_{x} Prob(Y_{i} = y | X_{i} = x) Prob( X_{i} = x ),\n\\end{eqnarray}\nwhich is also known as the *law of total probability*.\n\n\n\n#### **Coin Flips Example**. {-}\n\nFor one example, Consider flipping two coins, where we mark whether \"heads\" is face up with a $1$ and \"tail\" with a $0$. E.g., the first coin has a value of $x=1$ if it shows heads and $x=0$ if it shows tails.\nThis table shows both the joint distribution and also each marginal distribution.\n\n|    | $x=0$  | $x=1$   | Marginal    |\n|:------:|:---:|:---:|:---:|\n| $y=0$    |$Prob(X_{i}=0,Y_{i}=0)$|$Prob(X_{i}=1,Y_{i}=0)$|$Prob(Y_{i}=0)$|\n| $y=1$    |$Prob(X_{i}=0,Y_{i}=1)$|$Prob(X_{i}=1,Y_{i}=1)$|$Prob(Y_{i}=1)$|\n| **Marginal** |$Prob(X_{i}=0)$    |$Prob(X_{i}=1)$        | $1$           |\n\nNote that different joint distributions can have the same marginal distributions.\n\n:::{.callout-note icon=false collapse=\"true\"}\nSuppose both coins are \"fair\": $Prob(X_{i}=1)= 1/2$ and $Prob(Y_{i}=1|X_{i}=x)=1/2$ for either $x=1$ or $x=0$, then the four potential outcomes have equal probabilities.\n\\begin{eqnarray}\nProb(X_{i} = 0, Y_{i} = 0) &=& 1/2 \\times 1/2 = 1/4 \\\\\nProb(X_{i} = 0, Y_{i} = 1) &=& 1/4 \\\\\nProb(X_{i} = 1, Y_{i} = 0) &=& 1/4 \\\\\nProb(X_{i} = 1, Y_{i} = 1) &=& 1/4 .\n\\end{eqnarray}\nThe joint distribution is written generally as\n\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y) &=& Prob(X_{i} = x) Prob(Y_{i} = y).\n\\end{eqnarray}\n\nThe marginal distribution of the second coin is\n\\begin{eqnarray}\nProb(Y_{i} = 0) &=& Prob(Y_{i} = 0 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 0 | X_{i} = 1) Prob(X_{i}=1)\\\\\n&=& 1/2 (1/2) + 1/2 (1/2) = 1/2\\\\\nProb(Y_{i} = 1) &=& Prob(Y_{i} = 1 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 1 | X_{i} = 1) Prob(X_{i}=1)\\\\\n&=& 1/2 (1/2) + 1/2 (1/2) = 1/2\n\\end{eqnarray}\n\nThe marginal distribution of the first coin is found in the exact same way\n\\begin{eqnarray}\nProb(X_{i} = 0) &=& Prob(X_{i} = 0 | Y_{i} = 0) Prob(Y_{i}=0) + Prob(X_{i} = 0 | Y_{i} = 1) Prob(Y_{i}=1)\\\\\n&=& 1/2 (1/2) + 1/2 (1/2) = 1/2\\\\\nProb(X_{i} = 1) &=& Prob(X_{i} = 1 | Y_{i} = 0) Prob(Y_{i}=0) + Prob(X_{i} = 1 | Y_{i} = 1) Prob(Y_{i}=1)\\\\\n&=& 1/2 (1/2) + 1/2 (1/2) = 1/2\n\\end{eqnarray}\n\nAltogether, we find\n\n|    | $x=0$  | $x=1$   | Marginal    |\n|:------:|:---:|:---:|:---:|\n| $y=0$        |$1/4$|$1/4$|$1/2$|\n| $y=1$        |$1/4$|$1/4$|$1/2$|\n| **Marginal** |$1/2$|$1/2$| $1$ |\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a 2x2 matrix for the joint distribution.\n# Rows correspond to X1 (coin 1), and columns correspond to X2 (coin 2).\nP_fair <- matrix(1/4, nrow = 2, ncol = 2)\nrownames(P_fair) <- c(\"X1=0\", \"X1=1\")\ncolnames(P_fair) <- c(\"X2=0\", \"X2=1\")\nP_fair\n##      X2=0 X2=1\n## X1=0 0.25 0.25\n## X1=1 0.25 0.25\n\n# Compute the marginal distributions.\n# Marginal for X1: sum across columns.\nP_X1 <- rowSums(P_fair)\nP_X1\n## X1=0 X1=1 \n##  0.5  0.5\n# Marginal for X2: sum across rows.\nP_X2 <- colSums(P_fair)\nP_X2\n## X2=0 X2=1 \n##  0.5  0.5\n\n# Compute the conditional probabilities Prob(X2 | X1).\ncond_X2_given_X1 <- matrix(0, nrow = 2, ncol = 2)\nfor (j in c(1,2)) {\n  cond_X2_given_X1[, j] <- P_fair[, j] / P_X1[j]\n}\nrownames(cond_X2_given_X1) <- c(\"X2=0\", \"X2=1\")\ncolnames(cond_X2_given_X1) <- c(\"given X1=0\", \"given X1=1\")\ncond_X2_given_X1\n##      given X1=0 given X1=1\n## X2=0        0.5        0.5\n## X2=1        0.5        0.5\n```\n:::\n\n:::\n\n:::{.callout-note icon=false collapse=\"true\"}\nNow consider a second example, where the second coin is \"Completely Unfair\", so that it is always the same as the first. The outcomes generated with a Completely Unfair coin are the same as if we only flipped one coin.\n\\begin{eqnarray}\nProb(X_{i} = 0, Y_{i} = 0) &=& 1/2 \\\\\nProb(X_{i} = 0, Y_{i} = 1) &=& 0 \\\\\nProb(X_{i} = 1, Y_{i} = 0) &=& 0 \\\\\nProb(X_{i} = 1, Y_{i} = 1) &=& 1/2 .\n\\end{eqnarray}\nThe joint distribution is written generally as\n\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y) &=& Prob(X_{i} = x) \\mathbf{1}( x=y ),\n\\end{eqnarray}\nwhere $\\mathbf{1}(X_{i}=1)$ means $X_{i}= 1$ and $0$ if $X_{i}\\neq0$.\nThe marginal distribution of the second coin is\n\\begin{eqnarray}\nProb(Y_{i} = 0)\n&=& Prob(Y_{i} = 0 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 0 | X_{i} = 1) Prob(X_{i} = 1)\\\\\n&=& 1 (1/2) + 0(1/2) = 1/2 .\\\\\nProb(Y_{i} = 1)\n&=& Prob(Y_{i} = 1 | X_{i} =0) Prob( X_{i} = 0) + Prob(Y_{i} = 1 | X_{i} = 1) Prob( X_{i} = 1)\\\\\n&=& 0 (1/2) + 1 (1/2) = 1/2 .\n\\end{eqnarray}\nwhich is the same marginal as in the first example!\n\nThe marginal distribution of the first coin is found in the exact same way (show this yourself).\n\nAlltogether, we find\n\n|    | $x=0$  | $x=1$   | Marginal    |\n|:------:|:---:|:---:|:---:|\n| $y=0$        |$1/2$|$0$  |$1/2$|\n| $y=1$        |$0$  |$1/2$|$1/2$|\n| **Marginal** |$1/2$|$1/2$| $1$ |\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the joint distribution matrix for the unfair coin case.\nP_unfair <- matrix(c(0.5, 0, 0, 0.5), nrow = 2, ncol = 2, byrow = TRUE)\nrownames(P_unfair) <- c(\"X1=0\", \"X1=1\")\ncolnames(P_unfair) <- c(\"X2=0\", \"X2=1\")\nP_unfair\n##      X2=0 X2=1\n## X1=0  0.5  0.0\n## X1=1  0.0  0.5\n\n# Compute the marginal distribution for X2 in the unfair case.\nP_X2_unfair <- colSums(P_unfair)\nP_X1_unfair <- rowSums(P_unfair)\n\n# Compute the conditional probabilities Prob(X1 | X2) for the unfair coin.\ncond_X2_given_X1_unfair <- matrix(NA, nrow = 2, ncol = 2)\nfor (j in c(1,2)) {\n  if (P_X1_unfair[j] > 0) {\n    cond_X2_given_X1_unfair[, j] <- P_unfair[, j] / P_X1_unfair[j]\n  }\n}\nrownames(cond_X2_given_X1_unfair) <- c(\"X2=0\", \"X2=1\")\ncolnames(cond_X2_given_X1_unfair) <- c(\"given X1=0\", \"given X1=1\")\ncond_X2_given_X1_unfair\n##      given X1=0 given X1=1\n## X2=0          1          0\n## X2=1          0          1\n```\n:::\n\n:::\n\n\n\n\n#### **Definitions for Continuous Data**. {-}\nThe *joint distribution* is defined as\n\\begin{eqnarray}\nF(x, y) &=& Prob(X_{i} \\leq x, Y_{i} \\leq y)\n\\end{eqnarray}\nThe *marginal distributions* are then defined as\n\\begin{eqnarray}\n F_{X}(x) &=& F(x, \\infty)\\\\\n F_{Y}(y) &=& F(\\infty, y).\n\\end{eqnarray}\nwhich is also known as the *law of total probability*.\nVariables are statistically independent if $F(x, y) = F_{X}(x)F_{Y}(y)$ for all $x, y$.\n\n\nFor example, suppose $(X_{i},Y_{i})$ is bivariate normal with means $(\\mu_{X}, \\mu_{Y})$, variances $(\\sigma_{X}, \\sigma_{Y})$ and covariance $\\rho$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate Bivariate Data\nN <- 10000\nMu <- c(2,2) ## Means\n\nSigma1 <- matrix(c(2,-.8,-.8,1),2,2) ## CoVariance Matrix\nMVdat1 <- mvtnorm::rmvnorm(N, Mu, Sigma1)\ncolnames(MVdat1) <- c('X','Y')\n\nSigma2 <- matrix(c(2,.4,.4,1),2,2) ## CoVariance Matrix\nMVdat2 <- mvtnorm::rmvnorm(N, Mu, Sigma2)\ncolnames(MVdat2) <- c('X','Y')\n\npar(mfrow=c(1,2))\n## Different diagonals\nplot(MVdat2, col=rgb(1,0,0,0.02), pch=16,\n    main='Joint Distributions', font.main=1,\n    ylim=c(-4,8), xlim=c(-4,8),\n    xlab='X', ylab='Y')\npoints(MVdat1,col=rgb(0,0,1,0.02),pch=16)\n## Same marginal distributions\nxbks <- seq(-4,8,by=.2)\nhist(MVdat2[,2], col=rgb(1,0,0,0.5),\n    breaks=xbks, border=NA,\n    xlab='Y',\n    main='Marginal Distributions', font.main=1)\nhist(MVdat1[,2], col=rgb(0,0,1,0.5),\n    add=T, breaks=xbks, border=NA)\n```\n\n::: {.cell-output-display}\n![](02_15_ProbabilityTheory_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# See that independent data are a special case\nn <- 2e4\n## 2 Indepenant RV\nXYiid <- cbind( rnorm(n),  rnorm(n))\n## As a single Joint Draw\nXYjoint <- mvtnorm::rmvnorm(n, c(0,0))\n## Plot\npar(mfrow=c(1,2))\nplot(XYiid, xlab=\n    col=grey(0,.05), pch=16, xlim=c(-5,5), ylim=c(-5,5))\nplot(XYjoint,\n    col=grey(0,.05), pch=16, xlim=c(-5,5), ylim=c(-5,5))\n\n# Compare densities\n#d1 <- dnorm(XYiid[,1],0)*dnorm(XYiid[,2],0)\n#d2 <- mvtnorm::dmvnorm(XYiid, c(0,0))\n#head(cbind(d1,d2))\n```\n:::\n\n\n\nThe multivariate normal is a workhorse for analytical work on multivariate random variables, but there are many more. See e.g., <https://cran.r-project.org/web/packages/NonNorMvtDist/NonNorMvtDist.pdf>\n\n\n#### **Bayes' Theorem**. {-}\n\nAlso note *Bayes' Theorem*:\n\\begin{eqnarray}\nProb(X_{i} = x | Y_{i} = y)  Prob( Y_{i} = y)\n    &=& Prob(X_{i} = x, Y_{i} = y) = Prob(Y_{i} = y | X_{i} = x) Prob(X_{i} = x).\\\\\nProb(X_{i} = x | Y_{i} = y)\n    &=& \\frac{ Prob(Y_{i} = y | X_{i} = x) Prob(X_{i}=x) }{ Prob( Y_{i} = y) }.\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Verify Bayes' theorem for the unfair coin case:\n# Compute Prob(X1=1 | X2=1) using the formula:\n#   Prob(X1=1 | X2=1) = [Prob(X2=1 | X1=1) * Prob(X1=1)] / Prob(X2=1)\n\nP_X1_1 <- 0.5\nP_X2_1_given_X1_1 <- 1  # Since coin 2 copies coin 1.\nP_X2_1 <- P_X2_unfair[\"X2=1\"]\n\nbayes_result <- (P_X2_1_given_X1_1 * P_X1_1) / P_X2_1\nbayes_result\n## X2=1 \n##    1\n```\n:::\n\n\n\n\n## Theoretical Statistics\n\n#### **Conditional Expectation**. {-}\n\n#### **Correlation**. {-}\n\nWe will now dig a little deeper theoretically into the statistics we compute. When we know how the data are generated theoretically, we can often compute the theoretical value of the most basic and often-used bivariate statistic: the Pearson correlation. To see this, we focus on two discrete random variables, first showing their covariance, $\\mathbb{C}[X_{i}, Y_{i}]$, and then their correlation $\\mathbb{R}[X_{i}, Y_{i}]$. Referring to and $\\mu_{X}=\\mathbb{E}[X_{i}]$ and $\\mu_{Y}=\\mathbb{E}[Y_{i}]$, we have\n\\begin{eqnarray}\n\\mathbb{C}[X_{i}, Y_{i}]\n&=& \\mathbb{E}[(X_{i} –  \\mu_{X})(Y_{i} – \\mu_{Y}])]\n= \\sum_{x}\\sum_{y} (x –  \\mu_{X})(y –  \\mu_{Y}) Prob(X_{i} = x, Y_{i} = y)\n\\\\\n\\mathbb{R}[X_{i}, Y_{i}] &=& \\frac{\\mathbb{C}[X_{i}, Y_{i}] }{ \\sqrt{\\mathbb{V}[X_{i}]} \\sqrt{\\mathbb{V}[Y_{i}]} }\n\\end{eqnarray}\n\nFor example, suppose we have discrete data with the following outcomes and probabilities. Note that cells reflect the probabilities of the outcomes depicted on the row and column labels, e.g. $Prob(X_{i}=1, Y_{i}=0)=0.1$.\n\n|    | $x=0$ | $x=1$ | $x=2$ |\n|:--:|:--:|:--:|:--:|\n| $y=0$  | $0.0$ | $0.1$ | $0.0$ |\n| $y=10$ | $0.1$ | $0.3$ | $0.1$ |\n| $y=20$ | $0.1$ | $0.1$ | $0.2$ |\n\n\nAfter verifying that the probabilities sum to $1$, we then compute the marginal distributions\n\\begin{eqnarray}\nProb(X_{i}=0)=0.2,\\quad Prob(X_{i}=1)=0.5,\\quad Prob(X_{i}=2) = 0.3 \\\\\nProb(Y_{i}=0)=0.1,\\quad Prob(Y_{i}=10)=0.5,\\quad Prob(Y_{i}=20) = 0.4\n\\end{eqnarray}\nwhich allows us to compute the means:\n\\begin{eqnarray}\n\\mathbb{E}[X_{i}] &=& 0(0.2)+1(0.5)+2(0.3) = 1.1 \\\\\n\\mathbb{E}[Y_{i}] &=& 0(0.1)+10(0.5)+20(0.4) = 13\n\\end{eqnarray}\nWe can then compute the cell-by-cell contributions: $Prob(X_{i} = x, Y_{i} = y) (x-\\mathbb{E}[X_{i}])(y-\\mathbb{E}[Y_{i}])$, which lead plug in to the covariance formula;\n\\begin{eqnarray}\n\\begin{array}{l l r r r r r}\n\\hline\nx & y & Prob(X_{i}=x, Y_{i}=y) & x-\\mathbb{E}[X_{i}] & y-\\mathbb{E}[Y_{i}] & (x-\\mathbb{E}[X_{i}])(y-\\mathbb{E}[Y_{i}]) & \\text{Contribution}\\\\\n\\hline\n0 & 0  & 0.0 & -1.1 & -13 & 14.3  & 0\\\\\n0 & 10 & 0.1 & -1.1 & -3  & 3.3   & 0.330\\\\\n0 & 20 & 0.1 & -1.1 & 7   & -7.7  & -0.770\\\\\n1 & 0  & 0.1 & -0.1 & -13 & 1.3   & 0.130\\\\\n1 & 10 & 0.3 & -0.1 & -3  & 0.3   & 0.090\\\\\n1 & 20 & 0.1 & -0.1 & 7   & -0.7  & -0.070\\\\\n2 & 0  & 0.0 & 0.9  & -13 & -11.7 & 0\\\\\n2 & 10 & 0.1 & 0.9  & -3  & -2.7  & -0.270\\\\\n2 & 20 & 0.2 & 0.9  & 7   & 6.3   & 1.260\\\\\n\\hline\n\\end{array}\n\\end{eqnarray}\n\\begin{eqnarray}\n\\mathbb{C}[X_{i},Y_{i}] &=& \\sum_{x} \\sum_{y} \\left(x-\\mathbb{E}[X_{i}]\\right)\\left(y-\\mathbb{E}[Y_{i}]\\right) Prob\\left(X_{i} = x, Y_{i} = y\\right) \\\\\n&=& 0 + 0.330 -0.770 + 0.130 + 0.090  -0.070 +0 -0.270 + 1.260\n= 0.7\n\\end{eqnarray}\n\nTo compute the correlation value, we first need the standard deviations\n\\begin{eqnarray}\n\\mathbb{V}[X_{i}] &=& \\sum_{x} (x-\\mathbb{E}[X_{i}])^2 Prob(X_{i} = x) \\\\\n&=& (0-1.1)^2(0.2)+(1-1.1)^2(0.5)+(2-1.1)^2(0.3)=0.49 \\\\\n\\mathbb{V}[Y_{i}] &=& \\sum_{y}  (y-\\mathbb{E}[Y_{i}])^2 Prob(Y_{i} = y) \\\\\n&=& (0-13)^2(0.1)+(10-13)^2(0.5)+(20-13)^2(0.4)=41 \\\\\n\\end{eqnarray}\nThen we can find the correlation as\n\\begin{eqnarray}\n\\frac{\\mathbb{C}[X_{i},Y_{i}]}{ \\sqrt{\\mathbb{V}[X_{i}]} \\sqrt{\\mathbb{V}[Y_{i}]} }\n&=& \\frac{0.7}{\\sqrt{0.49} \\sqrt{41}} \\approx 0.156,\n\\end{eqnarray}\nwhich suggests a weak positive association between the variables.\n\nNote that you can do all of the above calculations using the computer instead of by hand.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make a Probability Table\nx <- c(0,1,2)\ny <- c(0,10,20)\nxy_probs <- matrix(c(\n    0.0, 0.1, 0.0,\n    0.1, 0.3, 0.1,\n    0.1, 0.1, 0.2\n), nrow=3, ncol=3, byrow=TRUE)\nrownames(xy_probs) <- paste0('y=',y)\ncolnames(xy_probs) <-  paste0('x=',x)\nxy_probs\n##      x=0 x=1 x=2\n## y=0  0.0 0.1 0.0\n## y=10 0.1 0.3 0.1\n## y=20 0.1 0.1 0.2\n\n# Compute Marginals and Means\npX  <- colSums(xy_probs)\npY  <- rowSums(xy_probs)\nEX  <- sum(x * pX)\nEY  <- sum(y * pY)\n\n# Compute Covariance\ndxy_grid <- expand.grid(dy=y-EY, dx=x-EX)[,c(2,1)]\ndxy_grid[,'p'] <- as.vector(xy_probs)\ndxy_grid[,'contribution'] <- dxy_grid[,'dx'] * dxy_grid[,'dy'] * dxy_grid[,'p']\nCovXY <- sum(dxy_grid[,'contribution'])\nCovXY\n## [1] 0.7\n\n# Compute Variances\nVX <- sum( (x-EX)^2 * pX)\nSX <- sqrt(VX)\nVY <- sum( (y-EY)^2 * pY)\nSY <- sqrt(VY)\n\n# Compute Correlation\nCorXY <- CovXY / (SX * SY)\nCorXY\n## [1] 0.1561738\n```\n:::\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nCompute the correlation for bivariate data with these probabilities, using both math (first) and the computer (second)\n\n|    | $x=-10$ | $x=10$ |\n|:--:|:--:|:--:|\n| $y=0$ | $0.05$ | $0.20$ |\n| $y=1$ | $0.05$ | $0.20$ |\n| $y=2$ | $0.05$ | $0.20$ |\n| $y=3$ | $0.05$ | $0.20$ |\n\n:::\n\n:::{.callout-tip icon=false collapse=\"true\"}\n\nAlso compute the correlation for bivariate data with these probabilities\n\n|    | $x=-10$ | $x=10$ |\n|:--:|:--:|:--:|\n| $y=0$ | $0.05$ | $0.05$ |\n| $y=1$ | $0.10$ | $0.10$ |\n| $y=2$ | $0.15$ | $0.15$ |\n| $y=3$ | $0.20$ | $0.20$ |\n\nAlso compute the correlation for bivariate data with these probabilities\n\n|    | $x=-10$ | $x=10$ |\n|:--:|:--:|:--:|\n| $y=0$ | $0.05$ | $0.15$ |\n| $y=1$ | $0.05$ | $0.15$ |\n| $y=2$ | $0.10$ | $0.20$ |\n| $y=3$ | $0.10$ | $0.20$ |\n\nExplain intuitively when the correlation equals $0$ and when it does not.\n:::\n\n\n\n## Testing Theory\n\n#### **Theoretical Distributions**. {-}\nJust as with one sample tests, we can compute a standardized differences, where is converted into a statistic. Note, however, that we have to compute the standard error for the difference statistic, which is a bit more complicated. Under the assumption that both populations are independent distributed, we can analytically derive the sampling distribution for the differences between two groups.\n\nIn particular, the $t$-statistic is used to compare two groups.\n\\begin{eqnarray}\n\\hat{t} = \\frac{\n    \\hat{M}_{Y1} - \\hat{M}_{Y2}\n}{\n    \\sqrt{\\hat{S}_{Y1}+\\hat{S}_{Y2}}/\\sqrt{n}\n},\n\\end{eqnarray}\nWith normally distributed means, this statistic follows Student's [t-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution).\nWelch's [$t$-statistic](https://en.wikipedia.org/wiki/Welch%27s_t-test) is an adjustment for two normally distributed populations with potentially unequal variances or sample sizes. With the above assumptions, one can conduct hypothesis tests entirely using math.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample 1 (e.g., males)\nn1 <- 100\nY1 <- rnorm(n1, 0, 2)\n#hist(Y1, freq=F, main='Sample 1')\n\n# Sample 2 (e.g., females)\nn2 <- 80\nY2 <- rnorm(n2, 1, 1)\n#hist(Y2, freq=F, main='Sample 2')\n\nt.test(Y1, Y2, var.equal=F)\n## \n## \tWelch Two Sample t-test\n## \n## data:  Y1 and Y2\n## t = -4.1266, df = 164.48, p-value = 5.836e-05\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -1.3470652 -0.4751704\n## sample estimates:\n##  mean of x  mean of y \n## 0.04569887 0.95681667\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\nIf we want to test for the differences in medians across groups with independent observations, we can also use notches in the boxplot. If the notches of two boxes do not overlap, then there is rough evidence that the difference in medians is statistically significant. The square root of the sample size is also shown as the bin width in each boxplot.^[Let each group $g$ have median $\\tilde{M}_{g}$, interquartile range $\\hat{IQR}_{g}$, observations $n_{g}$. We can compute standard deviation of the median as $\\tilde{S}_{g}= \\frac{1.25 \\hat{IQR}_{g}}{1.35 \\sqrt{n_{g}}}$. As a rough guess, the interval $\\tilde{M}_{g} \\pm 1.7 \\tilde{S}_{g}$ is the historical default and displayed as a *notch* in the boxplot. See also <https://www.tandfonline.com/doi/abs/10.1080/00031305.1978.10479236>.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nY3 <- rnorm(n1, 3, 3)\n\nboxplot(Y1, Y2, Y3,\n    col=c(\n        rgb(1,0,0,.5),\n        rgb(0,1,0,.5),\n        rgb(0,0,1,.5)),\n    notch=T,\n    varwidth=T)\n```\n\n::: {.cell-output-display}\n![](02_15_ProbabilityTheory_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThere are also theoretical results for distributional comparisons\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Ecdat)\ndata(Caschool)\nCaschool[,'stratio'] <- Caschool[,'enrltot']/Caschool[,'teachers']\nkruskal.test(Caschool[,'stratio'], Caschool[,'county'])\n\n# Multiple pairwise tests\n# pairwise.wilcox.test(Caschool[,'stratio'], Caschool[,'county'])\n```\n:::\n\n\n\n## Type II Errors\n\nWhen we test a hypothesis, we start with a claim called the null hypothesis $H_0$ and an alternative claim $H_A$. Because we base conclusions on sample data, which has variability, mistakes are possible. There are two types of errors:\n\n* *Type I Error*: Rejecting a true null hypothesis. (False Positive).\n* *Type II Error*: Failing to reject a false null hypothesis (False Negative).\n\n| True Situation | Decision: Fail to Reject $H_0$ | Decision: Reject $H_0$ |\n|---|---|---|\n| $H_0$ is True  |  Correct (no detection)  |  Type I Error (False Positive) |\n| $H_0$ is False |  Type II Error (False Negative; missed detection) | Correct (effect detected) |\n\n:::{.callout-tip icon=false collapse=\"true\"}\nHere is a Courtroom example: Someone suspected of committing a crime is at trial, and they are either guilty or not (a Bernoulli random variable). You hypothesize that the suspect is innocent, and a jury can either convict them (decide guilty) or free them (decide not-guilty). Recall that fail-to-reject a hypothesis does mean accepting it, so deciding not-guilty does not necessarily mean innocent.\n\n| True Situation | Decision: Free | Decision: Convict |\n|---|---|---|\n| Suspect Innocent |  Correctly Freed  | Falsely Convicted |\n| Suspect Guilty   |  Falsely Freed    | Correctly Convicted |\n:::\n\n#### **Statistical Power**. {-}\n\nThe probability of Type I Error is called *significance level* and denoted by $Prob(\\text{Type I Error}) = \\alpha$. The probability of correctly rejecting a false null is called *power* and denoted by $\\text{Power} = 1 - \\beta = 1 -  Prob(\\text{Type II Error})$.\n\nSignificance is often chosen by statistical analysts to be $\\alpha=0.05$. Power is less often chosen, instead following from a decision about power.\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nThe code below runs a small simulation using a shifted, nonparametric bootstrap. Two-sided test; studentized statistic, for $H0: \\mu = 0$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Power for Two-sided test;\n# nonparametric bootstrap, studentized statistic\nn <- 25\nmu <- 0\nalpha <- 0.05\nB <- 299\n\nsim_reps <- 100\n\np_values <- vector(length=sim_reps)\nfor (i in seq(p_values)) {\n    # Generate data\n    X <- rnorm(n, mean=0.2, sd=1)\n    # Observed statistic\n    X_bar <- mean(X)\n    T_obs <-  (X_bar - mu) / (sd(X)/ sqrt(n)) ##studentized\n    # Bootstrap null distribution of the statistic\n    T_boot <- vector(length=B)\n    X_null <- X - X_bar + mu # Impose the null by recentering\n    for (b in seq(T_boot)) {\n      X_b <- sample(X_null, size = n, replace = TRUE)\n      T_b <- (mean(X_b) - mu) / (sd(X_b)/sqrt(n))\n      T_boot[b] <- T_b\n    }\n    # Two-sided bootstrap p-value\n    pval <- mean(abs(T_boot) >= abs(T_obs))\n    p_values[i] <- pval\n    }\npower <- mean(p_values < alpha)\npower\n```\n:::\n\n:::\n\nThere is an important Trade-off for fixed sample sizes: Increasing significance (fewer false positive) often lowers power (more false negatives). Generally, power depends on the effect size and sample size: bigger true effects and larger $n$ make it easier to detect real differences (higher power, lower $\\beta$).\n\n\n## Further Reading\n\nMany introductory econometrics textbooks have a good appendix on probability and statistics. There are many useful statistical texts online too\n\nSee the Further reading about Probability Theory in the Statistics chapter.\n\n* <https://www.r-bloggers.com/2024/03/calculating-conditional-probability-in-r/>\n\n",
    "supporting": [
      "02_15_ProbabilityTheory_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}