{
  "hash": "d078832f05ad2e419b117dfc215dbbf0",
  "result": {
    "engine": "knitr",
    "markdown": "# Comparing Groups\n***\n\n\n## Group Differences\n\nFor mixed data, $\\hat{Y}_{i}$ is a cardinal variable and $\\hat{X}_{i}$ is a factor variable (typically unordered). For such \"grouped data\", we analyze associations via group comparisons. The basic idea is best seen in a comparison of two samples, which corresponds to an $\\hat{X}_{i}$ with two categories. For example, the heights of men and women in Canada or the homicide rates in two different American states.\n\n\nWe will consider the wages for people with and without completing a degree. pTo have this data on our computer, we must first install the \"wooldridge\" package once. Then we can access the data at any time by loading the package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install R Data Package and Load in\ninstall.packages('wooldridge') # only once\nlibrary('wooldridge') # \"load\" anytime you want to use the data\n?wooldridge # details about the package\n?wage1 # details about the dataset we want\n```\n:::\n\n\nTo compare wages for people with and without completing a degree, we start by making a [histogram](https://jadamso.github.io/Rbooks/01_02_Data.html#histogram-density-estimate) for both groups.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary('wooldridge')\n## Group 1\nX1_id <- wage1[,'educ'] == 15\nY1 <- wage1[X1_id, 'wage']\n## Group 2\nX2_id <- wage1[,'educ'] == 16\nY2 <- wage1[X2_id, 'wage']\n\n# Initial Summary Figure\nbks <- seq(0, 24, by=1.5)\ndlim <- c(0,.2)\ncols <- c(rgb(1,0,0,.5), rgb(0,0,1,.5))\n\nhist(Y1, breaks=bks, ylim=dlim,\n     col=cols[1], xlab='Wages',\n     freq=F, border=NA, main='')\nhist(Y2, breaks=bks, ylim=dlim,\n     col=cols[2],\n     freq=F, border=NA, add=T)\nlegend('topright',\n       col=cols, pch=15,\n       legend=c('15 Years', '16 Years'),\n       title='School Completed')\n```\n\n::: {.cell-output-display}\n![](02_12_ComparingGroups_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThe histogram may show several differences between groups or none at all. Often, the first statistic we investigate for hypothesis testing is the mean.\n\n#### **Mean Differences**. {-}\n\nWe often want to know if the means of different samples are the same or different. To test this hypothesis, we compute the means separately for each sample and then examine the differences term\n\\begin{eqnarray} \n\\hat{D} = \\hat{M}_{Y1} - \\hat{M}_{Y2},\n\\end{eqnarray}\nwith a null hypothesis that there is no difference in the population means.\n\n:::{.callout-note icon=false collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Differences between means\nm1 <- mean(Y1)\nm2 <- mean(Y2)\nd <- m1-m2\n    \n# Bootstrap Distribution\nbootstrap_diff <- vector(length=9999)\nfor(b in seq(bootstrap_diff) ){\n    Y1_b <- sample(Y1, replace=T)\n    Y2_b <- sample(Y2, replace=T)\n    m1_b <- mean(Y1_b)\n    m2_b <- mean(Y2_b)\n    d_b <- m1_b - m2_b\n    bootstrap_diff[b] <- d_b\n}\nhist(bootstrap_diff,\n    border=NA, font.main=1,\n    main='Difference in Means')\n\n# 2-Sided Test via Confidence Interval\nboot_ci <- quantile(bootstrap_diff, probs=c(.025, .975))\nabline(v=boot_ci, lwd=2)\nabline(v=0, lwd=2, col=2)\n```\n\n::: {.cell-output-display}\n![](02_12_ComparingGroups_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n:::\n\nJust as with one sample tests, we can compute a *standardized differences*, where $D$ is converted into a $t$ statistic. Note, however, that we have to compute the standard error for the difference statistic, which is a bit more complicated. However, this allows us to easily conduct one or two sided hypothesis tests using a standard normal approximation.\n\n:::{.callout-tip icon=false collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nse_hat <- sqrt(var(Y1)/n1 + var(Y2)/n2);\nt_obs <- d/se_hat\n\nt_2sample <- function(Y1, Y2){\n    # Differences between means\n    m1 <- mean(Y1)\n    m2 <- mean(Y2)\n    d <- (m1-m2)\n\n    # SE estimate\n    n1  <- length(Y1)\n    n2  <- length(Y2)\n    s1  <- var(Y1)\n    s2  <- var(Y2)\n    s   <- ((n1-1)*s1 + (n2-1)*s2)/(n1+n2-2)\n    d_se <- sqrt(s*(1/n1+1/n2))\n\n    # t stat\n    t_stat <- d/d_se\n    return(t_stat)\n}\n \ntstat <- twosam(data[,'male'], data[,'female'])\ntstat\n```\n:::\n\n:::\n\n\n#### **Quantile Differences**. {-}\nThe above procedure generalized from differences in means to other quantiles statistics like medians. To start, we plot the [ECDF](\nhttps://jadamso.github.io/Rbooks/docs/01_02_Data.html#empirical-cumulative-distribution-function)'s for both groups.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Quantile Comparison\n\n## Distribution 1\nF1 <- ecdf(Y1)\nplot(F1, col=cols[1],\n     pch=16, xlab='Wages',\n     main='Comparing Medians',\n     font.main=1, bty='n')\n## Median 1\nmed1 <- quantile(F1, probs=0.5)\nsegments(med1, 0, med1, 0.5, col=cols[1], lty=2)\nabline(h=0.5, lty=2)\n\n## Distribution 2\nF2 <- ecdf(Y2)\nplot(F2, add=TRUE, col=cols[2], pch=16)\n## Median 2\nmed2 <- quantile(F2, probs=0.5)\nsegments(med2, 0, med2, 0.5, col=cols[2], lty=2)\n\n## Legend\nlegend('bottomright',\n       col=cols, pch=15,\n       legend=c('Grade 15', 'Grade 16'),\n       title='School Completed')\n```\n\n::: {.cell-output-display}\n![](02_12_ComparingGroups_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-note icon=false collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap Distribution Function\nboot_quant <- function(Y1, Y2, B=9999, probs=0.5, ...){\n    bootstrap_diff <- vector(length=B)\n    for(b in seq(bootstrap_diff)){\n        Y1_b <- sample(Y1, replace=T)\n        Y2_b <- sample(Y2, replace=T)\n        q1_b <- quantile(Y1_b, probs=probs, ...)\n        q2_b <- quantile(Y2_b, probs=probs, ...)\n        d_b <- q1_b - q2_b\n        bootstrap_diff[b] <- d_b\n    }\n    return(bootstrap_diff)\n}\n\n# 2-Sided Test for Median Differences\n# d <- quantile(Y2, probs=0.5) - quantile(Y1, probs=0.5)\nboot_d <- boot_quant(Y1, Y1, B=999, probs=0.5)\nhist(boot_d, border=NA, font.main=1,\n    main='Difference in Medians')\nabline(v=quantile(boot_d, probs=c(.025, .975)), lwd=2)\nabline(v=0, lwd=2, col=2)\n```\n\n::: {.cell-output-display}\n![](02_12_ComparingGroups_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n1 - ecdf(boot_d)(0)\n## [1] 0.4224224\n```\n:::\n\n:::\n\nThe above procedure is quite general and extends  to other quantiles. Note that bootstrap tests can perform poorly with highly unequal variances or skewed data. To see this yourself, make a simulation with skewed data and unequal variances.\n\n\nIf we want to test for the differences in medians across groups with independent observations, we can also use notches in the boxplot. If the notches of two boxes do not overlap, then there is rough evidence that the difference in medians is statistically significant. The square root of the sample size is also shown as the bin width in each boxplot.^[Let each group $g$ have median $\\tilde{M}_{g}$, interquartile range $\\hat{IQR}_{g}$, observations $n_{g}$. We can compute standard deviation of the median as $\\tilde{S}_{g}= \\frac{1.25 \\hat{IQR}_{g}}{1.35 \\sqrt{n_{g}}}$. As a rough guess, the interval $\\tilde{M}_{g} \\pm 1.7 \\tilde{S}_{g}$ is the historical default and displayed as a *notch* in the boxplot. See also <https://www.tandfonline.com/doi/abs/10.1080/00031305.1978.10479236>.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(Y1, Y2,\n    col=cols,\n    notch=T,\n    varwidth=T)\n```\n\n::: {.cell-output-display}\n![](02_12_ComparingGroups_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nIn principle, we can also examine whether there are differences in spread (`sd` or `IQR`) or shape (`skew` or `kurtosis`). More often, however, we examine whether there are any differences in the distributions.\n\n:::{.callout-tip icon=false collapse=\"true\"}\nHere is an example to look at differences in \"spread\"\n\n::: {.cell}\n\n```{.r .cell-code}\nboot_fun <- function( fun, B=9999, ...){\n    bootstrap_diff <- vector(length=B)\n    for(b in seq(bootstrap_diff)){\n        Y1_b <- sample(Y1, replace=T)\n        Y2_b <- sample(Y2, replace=T)\n        f1_b <- fun(Y1_b, ...)\n        f2_b <- fun(Y2_b, ...)\n        d_b <- f1_b - f2_b\n        bootstrap_diff[b] <- d_b\n    }\n    return(bootstrap_diff)\n}\n\n# 2-Sided Test for SD Differences\n#d <- sd(Y2) - sd(Y1)\nboot_d <- boot_fun(sd)\nhist(boot_d, border=NA, font.main=1,\n    main='Difference in Standard Deviations')\nabline(v=quantile(boot_d, probs=c(.025, .975)), lwd=2)\nabline(v=0, lwd=2, col=2)\n1 - ecdf(boot_d)(0)\n\n\n# Try any function!\n# boot_fun( function(xs) { IQR(xs)/median(xs) } )\n```\n:::\n\n:::\n\n\n\n## Distributional Comparisons\n\nWe can also examine whether there are any differences between the entire *distributions*. We typically start by plotting the data using ECDF's or a boxplot, and then calculate a statistic for hypothesis testing. Which plot and test statistic depends on how many groups there are.\n\n#### **Two groups**.{-}\n\nOne useful visualization for two groups is to plot the quantiles against one another: a quantile-quantile plot. I.e., the first data point on the bottom left shows the first quantile for both distributions. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Wage Data (same as from before)\n#library(wooldridge)\n#Y1 <- sort( wage1[wage1[,'educ'] == 15,  'wage'])  \n#Y2 <- sort( wage1[wage1[,'educ'] == 16,  'wage'] )\n\n# Compute Quantiles\nquants <- seq(0,1,length.out=101)\nQ1 <- quantile(Y1, probs=quants)\nQ2 <- quantile(Y2, probs=quants)\n\n# Compare Distributions via Quantiles\n#ry <- range(c(Y1, Y2))\n#plot(ry, c(0,1), type='n', font.main=1,\n#    main='Distributional Comparison',\n#    xlab=\"Quantile\",\n#    ylab=\"Probability\")\n#lines(Q1, quants, col=2)\n#lines(Q2, quants, col=4)\n#legend('bottomright', col=c(2,4), lty=1,\n#\tlegend=c(\n#\t    expression(hat(F)[1]),\n#\t    expression(hat(F)[2]) \n#))\n\n# Compare Quantiles\nry <- range(c(Y1, Y2))\nplot(Q1, Q2, xlim=ry, ylim=ry,\n    xlab=expression(Q[1]),\n    ylab=expression(Q[2]),\n    main='Quantile-Quantile Plot',\n    font.main=1,\n    pch=16, col=grey(0,.25))\nabline(a=0,b=1,lty=2)\n```\n\n::: {.cell-output-display}\n![](02_12_ComparingGroups_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nThe starting point for hypothesis testing is the Kolmogorov-Smirnov Statistic: the maximum absolute difference between two CDF's over all sample data $y \\in \\{Y_1\\} \\cup \\{Y_2\\}$.\n\\begin{eqnarray}\n\\hat{KS} &=& \\max_{y} |\\hat{F}_{1}(y)- \\hat{F}_{2}(y)|^{p},\n\\end{eqnarray}\nwhere $p$ is an integer (typically 1). An intuitive alternative is the Cramer-von Mises Statistic: the sum of absolute differences (raised to an integer, typically 2) between two CDF's. \n\\begin{eqnarray}\n\\hat{CVM} &=& \\sum_{y} | \\hat{F}_{1}(y)- \\hat{F}_{2}(y)|^{p}.\n\\end{eqnarray}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Distributions\ny <- sort(c(Y1, Y2))\nF1 <- ecdf(Y1)(y)\nF2 <- ecdf(Y2)(y)\n\nlibrary(twosamples)\n\n# Kolmogorov-Smirnov\nKSq <- which.max(abs(F2 - F1))\nKSqv <- round(twosamples::ks_stat(Y1, Y2),2)\n\n# Cramer-von Mises Statistic (p=2)\nCVMqv <- round(twosamples::cvm_stat(Y1, Y2, power=2), 2) \n\n# Visualize Differences\nplot(range(y), c(0,1), type=\"n\", xlab='x', ylab='ECDF')\nlines(y, F1, col=cols[1], lwd=2)\nlines(y, F2, col=cols[2], lwd=2)\n\n# KS\ntitle( paste0('KS: ', KSqv), adj=0, font.main=1)\nsegments(y[KSq], F1[KSq], y[KSq], F2[KSq], lwd=1.5, col=grey(0,.75), lty=2)\n\n# CVM\ntitle( paste0('CVM: ', CVMqv), adj=1, font.main=1)\nsegments(y, F1, y, F2, lwd=.5, col=grey(0,.2))\n```\n\n::: {.cell-output-display}\n![](02_12_ComparingGroups_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nJust as before, you use bootstrapping for hypothesis testing.\n\n::: {.cell}\n\n```{.r .cell-code}\ntwosamples::ks_test(Y1, Y2)\n## Test Stat   P-Value \n## 0.2892157 0.0915000\n\ntwosamples::cvm_test(Y1, Y2)\n## Test Stat   P-Value \n##  2.084253  0.089500\n```\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nCompare the distribution of arrests for two different counties, each with data over time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wooldridge)\ncountymurders\n```\n:::\n\n\n:::\n\n#### **Multiple groups**. {-}\nWith multiple groups, you will want to begin with a summary figure (such as a boxplot). We can also tests the equality of all distributions (whether at least one group is different). The *Kruskal-Wallis* test examines $H_0:\\; F_1 = F_2 = \\dots = F_G$ versus $H_A:\\; \\text{at least one } F_g \\text{ differs}$, where $F_g$ is the continuous distribution of group $g=1,...G$. This test does not tell us which group is different.\n\nTo conduct the test, first denote individuals $i=1,...n$ with overall ranks $\\hat{r}_1,....\\hat{r}_{n}$. Each individual belongs to group $g=1,...G$, and each group $g$ has $n_{g}$ individuals with average rank $\\bar{r}_{g} = \\sum_{i} \\hat{r}_{i} /n_{g}$. The Kruskal Wallis statistic is \n\\begin{eqnarray}\n\\hat{KW} &=& (N-1) \\frac{\\sum_{g=1}^{G} n_{g}( \\bar{r}_{g} - \\bar{r}  )^2  }{\\sum_{i=1}^{n} ( \\hat{r}_{i} - \\bar{r}  )^2}, \n\\end{eqnarray}\nwhere  $\\bar{r} = \\frac{n+1}{2}$ is the grand mean rank.\n\nIn the special case with only two groups, $G=2$, the Kruskal Wallis test reduces to the *Mannâ€“Whitney U* test (also known as the *Wilcoxon rank-sum* test). In this case, we can write the hypotheses in terms of individual outcomes in each group, $Y_i$ in one group $Y_j$ in the other; $H_0: Prob(Y_i > Y_j)=Prob(Y_i > Y_i)$  versus $H_A: Prob(Y_i > Y_j) \\neq Prob(Y_i > Y_j)$. The corresponding test statistic is\n\\begin{eqnarray}\n\\hat{U}   &=& \\min(\\hat{U}_1, \\hat{U}_2) \\\\\n\\hat{U}_g &=& \\sum_{i\\in g}\\sum_{j\\in -g}\n           \\Bigl[\\mathbf 1( \\hat{Y}_{i} > \\hat{Y}_{j}) + \\tfrac12\\mathbf 1(\\hat{Y}_{i} = \\hat{Y}_{j})\\Bigr].\n\\end{eqnarray}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Ecdat)\ndata(Caschool)\nCaschool[,'stratio'] <- Caschool[,'enrltot']/Caschool[,'teachers']\n\n# Do student/teacher ratio differ for at least 1 county?\n# Single test of multiple distributions\nkruskal.test(Caschool[,'stratio'], Caschool[,'county'])\n## \n## \tKruskal-Wallis rank sum test\n## \n## data:  Caschool[, \"stratio\"] and Caschool[, \"county\"]\n## Kruskal-Wallis chi-squared = 161.18, df = 44, p-value = 2.831e-15\n\n# Multiple pairwise tests\n# pairwise.wilcox.test(Caschool[,'stratio'], Caschool[,'county'])\n```\n:::\n\n\n\n## Type II Errors\n\nWhen we test a hypothesis, we start with a claim called the null hypothesis $H_0$ and an alternative claim $H_A$. Because we base conclusions on sample data, which has variability, mistakes are possible. There are two types of errors:\n\n* *Type I Error*: Rejecting a true null hypothesis. (False Positive). \n* *Type II Error*: Failing to reject a false null hypothesis (False Negative). \n\n| True Situation | Decision: Fail to Reject $H_0$ | Decision: Reject $H_0$ |\n|---|---|---|\n| $H_0$ is True  |  Correct (no detection)  |  Type I Error (False Positive) |\n| $H_0$ is False |  Type II Error (False Negative; missed detection) | Correct (effect detected) |\n\n:::{.callout-tip icon=false collapse=\"true\"}\nHere is a Courtroom example: Someone suspected of committing a crime is at trial, and they are either guilty or not (a Bernoulli random variable). You hypothesize that the suspect is innocent, and a jury can either convict them (decide guilty) or free them (decide not-guilty). Recall that fail-to-reject a hypothesis does mean accepting it, so deciding not-guilty does not necessarily mean innocent.\n\n| True Situation | Decision: Free | Decision: Convict |\n|---|---|---|\n| Suspect Innocent |  Correctly Freed  | Falsely Convicted |\n| Suspect Guilty   |  Falsely Freed    | Correctly Convicted |\n:::\n\n#### **Statistical Power**. {-}\n\nThe probability of Type I Error is called *significance level* and denoted by $Prob(\\text{Type I Error}) = \\alpha$. The probability of correctly rejecting a false null is called *power* and denoted by $\\text{Power} = 1 - \\beta = 1 -  Prob(\\text{Type II Error})$. \n\nSignificance is often chosen by statistical analysts to be $\\alpha=0.05$. Power is less often chosen, instead following from a decision about power.\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nThe code below runs a small simulation using a shifted, nonparametric bootstrap. Two-sided test; studentized statistic, for $H0: \\mu = 0$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Power for Two-sided test;\n# nonparametric bootstrap, studentized statistic\nn <- 25\nmu <- 0\nalpha <- 0.05\nB <- 299\n\nsim_reps <- 100 \n\np_values <- vector(length=sim_reps)\nfor (i in seq(p_values)) {\n    # Generate data\n    X <- rnorm(n, mean=0.2, sd=1)\n    # Observed statistic\n    X_bar <- mean(X)\n    T_obs <-  (X_bar - mu) / (sd(X)/ sqrt(n)) ##studentized\n    # Bootstrap null distribution of the statistic\n    T_boot <- vector(length=B)\n    X_null <- X - X_bar + mu # Impose the null by recentering\n    for (b in seq(T_boot)) {\n      X_b <- sample(X_null, size = n, replace = TRUE)\n      T_b <- (mean(X_b) - mu) / (sd(X_b)/sqrt(n))\n      T_boot[b] <- T_b\n    }\n    # Two-sided bootstrap p-value\n    pval <- mean(abs(T_boot) >= abs(T_obs))\n    p_values[i] <- pval\n    }\npower <- mean(p_values < alpha)\npower\n```\n:::\n\n:::\n\nThere is an important Trade-off for fixed sample sizes: Increasing significance (fewer false positive) often lowers power (more false negatives). Generally, power depends on the effect size and sample size: bigger true effects and larger $n$ make it easier to detect real differences (higher power, lower $\\beta$).\n\n\n\n## Further Reading \n\nOther Statistics \n\n* <https://cran.r-project.org/web/packages/qualvar/vignettes/wilcox1973.html>\n\n\n",
    "supporting": [
      "02_12_ComparingGroups_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}