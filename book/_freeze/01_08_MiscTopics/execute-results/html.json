{
  "hash": "bbc89c71022a2ba251d173a985d82183",
  "result": {
    "engine": "knitr",
    "markdown": "# Misc. Univariate Topics\n\n\n\n## Data Transformations\n\nTransformations can stabilize variance, reduce skewness, and make model errors closer to Gaussian.\n\nPerhaps the most common examples are *power transformations*: $y= x^\\lambda$, which includes $\\sqrt{x}$ and $x^2$.\n\nOther examples include the *exponential transformation*: $y=\\exp(x)$ for any $x\\in (-\\infty, \\infty)$ and *logarithmic transformation*: $y=\\log(x)$ for any $x>0$.\n\nThe *Box–Cox Transform* nests many cases. For $x>0$ and parameter $\\lambda$,\n\\begin{eqnarray}\ny=\\begin{cases}\n\\dfrac{x^\\lambda-1}{\\lambda}, & \\lambda\\neq 0,\\\\\n\\log\\left(x\\right) & \\lambda=0.\n\\end{cases}\n\\end{eqnarray}\nThis function is continuous over $\\lambda$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Box–Cox transform and inverse\nbc_transform <- function(x, lambda) {\n  if (any(x <= 0)) stop(\"Box-Cox requires x > 0\")\n  if (abs(lambda) < 1e-8) log(x) else (x^lambda - 1)/lambda\n}\nbc_inverse <- function(t, lambda) {\n  if (abs(lambda) < 1e-8) exp(t) else (lambda*t + 1)^(1/lambda)\n}\n\nX <- USArrests$Murder\nhist(X, main='', border=NA, freq=F)\n```\n\n::: {.cell-output-display}\n![](01_08_MiscTopics_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\npar(mfrow=c(1,3))\nfor(lambda in c(-1,0,1)){\n    Y <- bc_transform(X, lambda)\n    hist(Y, \n        main=bquote(paste(lambda,'=',.(lambda))),\n        border=NA, freq=F)\n}\n```\n\n::: {.cell-output-display}\n![](01_08_MiscTopics_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n\n#### **Law of the Unconscious Statistician (LOTUS)**. {-}\n\nAs before, we will represent the random variable as $X_{i}$, which can take on values $x$ from the sample space. If $X_{i}$ is a discrete random variable (a random variable with a discrete sample space) and $g$ is a function, then\n$\\mathbb E[g(X)] = \\sum_x g(x)Prob(X_{i}=x)$.\n\n:::{.callout-note icon=false collapse=\"true\"}\nLet $X_{i}$ take values $\\{1,2,3\\}$ with\n\\begin{eqnarray}\nPr(X_{i}=1)=0.2,\\quad Prob(X_{i}=2)=0.5,\\quad Prob(X_{i}=3)=0.3.\n\\end{eqnarray}\nLet $g(x)=x^2+1$. Then $g(1)=1^2+1=2$, $g(2)=2^2+1=5$, $g(3)=3^2+1=10$. \n\nThen, by LOTUS,\n\\begin{eqnarray}\n\\mathbb E[g(X_{i})]=\\sum_x g(x)Prob(X_{i}=x)\n&=& g(1)\\cdot 0.2 + g(2)\\cdot 0.5 + g(3)\\cdot 0.3 \\\\\n&=& 2 \\cdot 0.2 + 5 \\cdot 0.5 + 10 \\cdot 0.3 \\\\\n&=& 0.4 + 2.5 + 3 = 5.9.\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\nx  <- c(1,2,3)\nx_probs <- c(0.2,0.5,0.3)\ng  <- function(x) x^2 + 1\nsum(g(x) * x_probs) \n## [1] 5.9\n```\n:::\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng  <- function(x) x^2 + 1\n\n# A theoretical example\nx <- c(1,2,3,4)\nx_probs <- c(1/4, 1/4, 1/4, 1/4)\nsum(g(x) * x_probs) \n## [1] 8.5\n\n# A simulation example\nX <- sample(x, x_probs, size=1000, replace=T)\nmean(g(X))\n## [1] 8.543\n```\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nIf $X_{i}$ is a continuous random variable (a random variable with a continuous sample space) and $g$ is a function, then\n$\\mathbb E[g(X_{i})] = \\int_{-\\infty}^{\\infty} g(x)f(x) dx$.\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rexp(5e5, rate = 1)           # X ~ Exp(1)\nmean(sqrt(x))                      # LOTUS Simulation\n## [1] 0.8868495\nsqrt(pi) / 2                       # Exact via LOTUS integral\n## [1] 0.8862269\n```\n:::\n\n:::\n\nNote that you have already seen the special case where $g(X_{i})=\\left(X_{i}-\\mathbb{E}[X_{i}]\\right)^2$.\n\n#### **Jensen’s inequality**. {-}\n\nConcave functions curve inwards, like the inside of a cave.\nConvex functions curve outward, the opposite of concave.\n\nIf $g$ is a *concave* function, then $g(\\mathbb E[X_{i}]) \\geq \\mathbb E[g(X_{i})]$.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Continuous Example 1\nmean( sqrt(x) )\n## [1] 0.8868495\nsqrt( mean(x) ) \n## [1] 1.001076\n\n# Continuous Example 2\nmean( log(x) )\n## [1] -0.5764185\nlog( mean(x) ) \n## [1] 0.002151259\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Discrete Example\nx  <- c(1,2,3)\npx <- c(0.2,0.5,0.3)\nEX <- sum(x * px)\nEX\n## [1] 2.1\n\ng  <- sqrt\ngEX <- g(EX)\nEgX <- sum(g(x) * px)\nc(gEX, EgX)\n## [1] 1.449138 1.426722\n```\n:::\n\n\n\nIf $g$ is a *convex* function, then the inequality reverses: $g(\\mathbb E[X_{i}]) \\leq \\mathbb E[g(X_{i})]$.\n\n::: {.cell}\n\n```{.r .cell-code}\nmean( exp(x) )\n## [1] 10.06429\nexp( mean(x) )  \n## [1] 7.389056\n```\n:::\n\n\n## Drawing Samples\n\nTo generate a random variable from known distributions, you can use some type of physical machine. E.g., you can roll a fair die to generate Discrete Uniform data or you can roll weighted die to generate Categorical data.\n\nThere are also several ways to computationally generate random variables from a probability distribution. Perhaps the most common one is \"inverse sampling\". To generate a random variable using inverse sampling, first sample $p$ from a uniform distribution and then find the associated quantile  quantile function $\\widehat{F}^{-1}(p)$.^[Drawing random uniform samples with computers is actually quite complex and beyond the scope of this course.]\n\n\n#### **Using Data**. {-}\n\nYou can generate a random variable from a known empirical distribution. Inverse sampling randomly selects observations from the dataset with equal probabilities. To implement this, we \n\n* order the data and associate each observation with an ECDF value\n* draw $p \\in [0,1]$ as a uniform random variable\n* find the associated data point on the ECDF\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Empirical Distribution\nX <- USArrests$Murder\nFX_hat <- ecdf(X)\nplot(FX_hat, lwd=2, xlim=c(0,20),\n    pch=16, col=grey(0,.5), main='')\n```\n\n::: {.cell-output-display}\n![](01_08_MiscTopics_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Generating a random variable\np <- runif(3000) ## Multiple Draws\nQX_hat <- quantile(X, p, type=1)\nQX_hat[1:5]\n## 79.64490056% 85.45562837% 72.71574638% 41.93181463% 24.39832431% \n##         12.1         13.0         11.1          6.0          4.0\n```\n:::\n\n\n#### **Using Math**. {-}\n\nIf you know the distribution function that generates the data, then you can derive the quantile function and do inverse sampling. Here is an in-depth example of the [Dagum distribution](https://en.wikipedia.org/wiki/Dagum_distribution). The distribution function is $F(x)=(1+(x/b)^{-a})^{-c}$. For a given probability $p$, we can then solve for the quantile as $F^{-1}(p)=\\frac{ b p^{\\frac{1}{ac}} }{(1-p^{1/c})^{1/a}}$. Afterwhich, we sample $p$ from a uniform distribution and then find the associated quantile.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Theoretical Quantile Function (from VGAM::qdagum)\nqdagum <- function(p, scale.b=1, shape1.a, shape2.c) {\n  # Quantile function (theoretically derived from the CDF)\n  ans <- scale.b * (expm1(-log(p) / shape2.c))^(-1 / shape1.a)\n  # Special known cases\n  ans[p == 0] <- 0\n  ans[p == 1] <- Inf\n  # Safety Checks\n  ans[p < 0] <- NaN\n  ans[p > 1] <- NaN\n  if(scale.b <= 0 | shape1.a <= 0 | shape2.c <= 0){ ans <- ans*NaN }\n  # Return\n  return(ans)\n}\n\n# Generate Random Variables (VGAM::rdagum)\nrdagum <-function(n, scale.b=1, shape1.a, shape2.c){\n    p <- runif(n) # generate random probabilities\n    x <- qdagum(p, scale.b=scale.b, shape1.a=shape1.a, shape2.c=shape2.c) #find the inverses\n    return(x)\n}\n\n# Example\nset.seed(123)\nX <- rdagum(3000,1,3,1)\nX[1:5]\n## [1] 0.7390476 1.5499868 0.8845006 1.9616251 2.5091656\n```\n:::\n\n\n\n## Set Theory\n\nLet the sample space contain events $A$ and $B$, with their probability denoted as $Prob(A)$ and $Prob(B)$. Then\n\n- the *union*: $A\\cup B$, refers to either event occurring\n- the *intersection*: $A\\cap B$, refers to both events occurring\n- the events $A$ and $B$ are *mutually exclusive* if and only if $A\\cap B$ is empty.\n- the *inclusion–exclusion* rule is: $Prob(A\\cup B)=Prob(A)+Prob(B)-Prob(A\\cap B)$.\n- the events $A$ and $B$ are *mutually independent* if and only if $Prob(A\\cap B)=Prob(A) Prob(B)$.\n- the *conditional probability* is $Prob(A \\mid B) = Prob(A \\cap B)/ P(B)$.\n\nFor example, consider a six sided die where\n\n- $A$: number $>\\!2$, which implies $A=\\{3,4,5,6\\}$\n- $B$: number is even, which implies $B=\\{2,4,6\\}$\n- To check mutually exclusive: notice $A\\cap B=\\{4,6\\}$ which is not empty. So No, event $A$ and event $B$ are not mutually exclusive.\n\nFurther assuming the die are fair\n\n- $Prob(A)=4/6=2/3$ and $Prob(B)=3/6=1/2$\n- $A\\cup B=\\{2,3,4,5,6\\}$, which implies $Prob(A\\cup B)=5/6$.\n- $A\\cap B=\\{4,6\\}$, which implies $Prob(A\\cap B)=2/6=1/3$.\n- To check independence: notice $Prob(A\\cap B)=1/3 = (2/3) (1/2) = Prob(A)Prob(B)$. So Yes, event $A$ and event $B$ are mutually independent.\n- Finally, the conditional probability of $Prob(\\text{die is} >2 \\mid \\text{die shows an even number}) = Prob(A \\mid B) = Prob(A \\cap B)/ P(B) = \\frac{1/3}{2/3}=2/3$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulation verification\nset.seed(123)\nrolls <- sample(1:6, 1e6, replace = TRUE)\nA <- rolls > 2\nB <- rolls %% 2 == 0\nP_AcondB <- mean(A & B) / mean(B)\nround(P_AcondB, 3)\n## [1] 0.667\n```\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nConsider a six sided die with sample space $\\{1,2,3,4,5,6\\}$ where each outcome is each equally likely. What is $Prob(\\text{die is odd or }<5)$?\nDenote the odd events as $A=\\{1,3,5\\}$ and the less than five events as  $B=\\{1,2,3,4\\}$. Then\n\n- $A\\cup B=\\{1,3\\}$, which implies $Prob(A\\cap B)=2/6=1/3$.\n- $Prob(A)=3/6=1/2$ and $Prob(B)=4/6=2/3$.\n- By inclusion–exclusion: $Prob(A\\cup B)=Prob(A)+Prob(B)-Prob(A\\cap B)=1/2+2/3-1/3=5/6$.\n\nNow find $Prob(\\text{die is even and }<5)$. Verify your answer with a computer simulation.\n:::\n\n\n## Count Distributions\n\n#### **Binomial**. {-}\n\nThe sum of $n$ Bernoulli trials (number of successes)\n\n* Discrete, support $\\{0,1,\\ldots,n\\}$\n* Probability Mass Function: $Prob(X_{i}=x)=\\binom{n}{x}p^k(1-p)^{n-x}$\n* See <https://en.wikipedia.org/wiki/Binomial_distribution>\n* A common use case: How many heads will I get when I flip a coin twice?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Minimal example\nn <- 15\np <- 0.3\n\n# PMF plot\nx <- 0:n\nf_x <- dbinom(x, n, p)\nplot(x, f_x, type = \"h\", col = \"blue\",\n    main='',  xlab = \"x\", ylab = \"Prob(X = x)\")\ntitle(bquote(paste('Binom(',.(n),', ',.(p), ')' ) ))\npoints(x, f_x, pch = 16, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](01_08_MiscTopics_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Simulate:  Compare empirical vs theoretical\n#X <- rbinom(1e4, size = n, prob = p)\n#c(emp_mean = mean(X), th_mean = n*p)\n#c(emp_var  = var(X),  th_var  = n*p*(1-p))\n```\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nSuppose that employees at a company are $70%$ female and $30%$ male. If we select a random sample of eight employees, what is the probability that more than $2$ in the sample are female?\n:::\n\n:::{.callout-tip icon=false collapse=\"true\"}\nShow that \n$\\mathbb{E}[X_{i}]=np$ and $\\mathbb{V}[X_{i}]=np(1-p)$.\n:::\n\nThe Binomial Limit Theorem (de Moivre–Laplace theorem) says that as $n$ grows large, with $p \\in (0,1)$ staying fixed, the Binomial distribution is approximately normal with mean $np$ and variance $np(1-p)$\n\n:::{.callout-tip icon=false collapse=\"true\"}\nThe unemployment rate is $10%$. Suppose that $100$ employable people are selected randomly. What is the probability that this sample contains between $9$ and $12$ unemployed people. Use the normal approximation to binomial probabilities (parameters $\\mu=100, \\sigma=9.49$).\n:::\n\n#### **Poisson**. {-}\n\nThe number of events in a fixed interval\n\n* Discrete, support $\\{0,1,2,\\ldots\\}$\n* Probability Mass Function: $Prob(X_{i}=x)=e^{-\\lambda}\\lambda^x/x!$\n* See <https://en.wikipedia.org/wiki/Poisson_distribution>\n* A common use case: How many cars will show up in the lot tomorrow?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Minimal example\nlambda <- 3.5\n\n# PMF plot\nx <- 0:15\nf_x <- dpois(x, lambda)\nplot(x, f_x, type=\"h\", col=\"blue\",\n     xlab = \"x\", ylab = \"Prob(X = x)\")\npoints(x, f_x, pch = 16, col = \"blue\")\ntitle(bquote(paste('Pois(',.(lambda), ')')))\n```\n\n::: {.cell-output-display}\n![](01_08_MiscTopics_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n\n# Simulate: Compare empirical vs theoretical\n#X <- rpois(1e4, lambda)\n#c(emp_mean = mean(X), th_mean = lambda)\n#c(emp_var  = var(X),  th_var  = lambda)\n```\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nShow that $\\mathbb{E}[X_{i}] = \\mathbb{V}[X_{i}]= \\lambda$.\n:::\n\n#### **Irwin–Hall**. {-}\n\nThe sum of $n$ i.i.d. $\\text{Uniform}(0,1)$.  \n\n* Continuous, support $[0,n]$\n* Probability Density Function: $f(x) = \\dfrac{1}{(n-1)!}\n\\displaystyle\\sum_{k=0}^{\\lfloor x \\rfloor} (-1)^k\n\\binom{n}{k} (x - k)^{n-1}$ for $x \\in [0,n]$, and $0$ otherwise\n* See <https://en.wikipedia.org/wiki/Irwin%E2%80%93Hall_distribution>\n* A common use case: representing the sum of many small independent financial shocks\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Minimal example (base R)\n# Irwin–Hall PDF function\ndirwinhall <- function(x, n) {\n    f_x <- vector(length=length(x))\n    for(i in seq(x)) {\n        xx <- x[i]\n        if(xx < 0 | xx > n){\n            f_x[i] <- 0 \n        } else {\n            k <- 0:floor(xx)\n            f_k <- sum((-1)^k*choose(n, k)*(xx-k)^(n-1))/factorial(n-1)\n            f_x[i] <- f_k\n        }}\n    return(f_x)\n}\n\n# Parameters\nn <- 2\nx <- seq(0, n, length.out = 500)\n\n# Compute and plot PDF\nf_x <- dirwinhall(x, n)\nplot(x, f_x, type=\"l\", col=\"blue\",\n    main='', xlab = \"x\", ylab = \"f(x)\")\ntitle(bquote(paste('IrwinHall(',.(n), ')')))\n```\n\n::: {.cell-output-display}\n![](01_08_MiscTopics_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nSee also the Bates Distribution.\n\n\n## Beyond Basic Programming\n\nUse expansion \"packages\" for more functionality.\n\nMost packages can be easily installed, and you only need to install packages once.\n\n::: {.cell}\n\n```{.r .cell-code}\n# common packages for tables/figures\ninstall.packages(\"plotly\")\ninstall.packages(\"reactable\")\ninstall.packages(\"stargazer\")\n\n# common packages for data/teaching\ninstall.packages(\"AER\")\ninstall.packages(\"Ecdat\")\ninstall.packages('wooldridge')\n\n# other packages for statistics and data handling\ninstall.packages(\"extraDistr\")\ninstall.packages(\"twosamples\")\ninstall.packages(\"data.table\")\n```\n:::\n\n\nYou need to load the package via `library` every time you want the extended functionality. For example, to generate exotic probability distributions\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(extraDistr)\n\npar(mfrow=c(1,2))\nfor(p in c(-.5,0)){\n    x <- rgev(2000, mu=0, sigma=1, xi=p)\n    hist(x, breaks=50, border=NA, main=NA, freq=F)\n}\ntitle('GEV densities', outer=T, line=-1)\n```\n\n::: {.cell-output-display}\n![](01_08_MiscTopics_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(extraDistr)\n\npar(mfrow=c(1,3))\nfor(p in c(-1, 0,2)){\n    x <- rtlambda(2000, p)\n    hist(x, breaks=100, border=NA, main=NA, freq=F)\n}\ntitle('Tukey-Lambda densities', outer=T, line=-1)\n```\n\n::: {.cell-output-display}\n![](01_08_MiscTopics_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThe most common tasks also have [cheatsheets](https://www.rstudio.com/resources/cheatsheets/) you can use. \n\n#### **Updating**. {-}\n\nMake sure R and your packages are up to date. The current version of R and any packages used can be found (and recorded) with \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n:::\n\n\nTo update your R packages, use \n\n::: {.cell}\n\n```{.r .cell-code}\nupdate.packages()\n```\n:::\n\n\n#### **Base**. {-}\nWhile additional packages can make your code faster, they also create dependancies that can lead to problems. So learn base R well before becoming dependent on other packages\n\n* <https://bitsofanalytics.org/posts/base-vs-tidy/>\n* <https://jtr13.github.io/cc21fall2/comparison-among-base-r-tidyverse-and-datatable.html>\n\n\n#### **Advanced Programming**. {-}\n\n<details>\n<summary> Advanced and Optional </summary>\n  <p>\n \nSometimes you will want to install a package from GitHub. For this, you can use [devtools](https://devtools.r-lib.org/) or its light-weight version [remotes](https://remotes.r-lib.org/)\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"devtools\")\ninstall.packages(\"remotes\")\n```\n:::\n\n\nNote that to install `devtools`, you also need to have developer tools installed on your computer.\n\n* Windows: [Rtools](https://cran.r-project.org/bin/windows/Rtools/)\n* Mac: [Xcode](https://apps.apple.com/us/app/xcode/id497799835)\n\nTo color terminal output on Linux systems, you can use the colorout package\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(remotes)\n# Install <https://github.com/jalvesaq/colorout\n# to .libPaths()[1]\ninstall_github('jalvesaq/colorout')\nlibrary(colorout)\n```\n:::\n\n\nNote that after updating R, you can update *all* packages stored in *all* `.libPaths()` with the following command\n\n::: {.cell}\n\n```{.r .cell-code}\nupdate.packages(checkBuilt=T, ask=F)\n# install.packages(old.packages(checkBuilt=T)[,\"Package\"])\n```\n:::\n\n\nSometimes there is a problem. To find specific broken packages after an update\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(purrr)\n\nset_names(.libPaths()) %>%\n  map(function(lib) {\n    .packages(all.available = TRUE, lib.loc = lib) %>%\n        keep(function(pkg) {\n            f <- system.file('Meta', 'package.rds', package = pkg, lib.loc = lib)\n            tryCatch({readRDS(f); FALSE}, error = function(e) TRUE)\n        })\n  })\n# https://stackoverflow.com/questions/31935516/installing-r-packages-error-in-readrdsfile-error-reading-from-connection/55997765\n```\n:::\n\n\nTo remove packages duplicated in multiple libraries\n\n::: {.cell}\n\n```{.r .cell-code}\n# Libraries\ni <- installed.packages()\nlibs <- .libPaths()\n# Find Duplicated Packages\ni1 <- i[ i[,'LibPath']==libs[1], ]\ni2 <- i[ i[,'LibPath']==libs[2], ]\ndups <- i2[,'Package'] %in% i1[,'Package']\nall( dups )\n# Remove\nremove.packages(  i2[,'Package'], libs[2] )\n```\n:::\n\n  </p>\n</details>\n",
    "supporting": [
      "01_08_MiscTopics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}