{
  "hash": "dad113265f6621839a74a1618ce58214",
  "result": {
    "engine": "knitr",
    "markdown": "# Data\n***\n\n## Types\n\n#### **Basic Types**. {-}\nThe two basic types of data are *cardinal* (aka numeric) data and *factor* data. We can further distinguish between whether cardinal data are discrete or continuous. We can also further distinguish between whether factor data are ordered or not\n\n* *Cardinal (Numeric)*: the difference between elements always means the same thing. \n    * Discrete: E.g. $\\{ 1,2,3\\}$ and notice that $2-1=3-2$.\n    * Continuous: E.g., $\\{1.4348, 2.4348, 2.9, 3.9 \\}$ and notice that $2.9-1.4348=3.9-2.4348$\n* *Factor*: the difference between elements does not always mean the same thing.\n    * Ordered: E.g., $\\{1^{st}, 2^{nd}, 3^{rd}\\}$ place in a race and notice that $1^{st}$ - $2^{nd}$ place does not equal $2^{nd}$ - $3^{rd}$ place for a very competitive person who cares only about winning.\n    * Unordered (categorical): E.g., $\\{Amanda, Bert, Charlie\\}$ and notice that $Amanda - Bert$ never makes sense.\n\n\nHere are some examples\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_card1 <- 1:3 # Cardinal data (Discrete)\ndat_card1\n## [1] 1 2 3\n\ndat_card2 <- c(1.1, 2/3, 3) # Cardinal data (Continuous)\ndat_card2\n## [1] 1.1000000 0.6666667 3.0000000\n\ndat_fact1 <- factor( c('A','B','C'), ordered=T) # Factor data (Ordinal)\ndat_fact1\n## [1] A B C\n## Levels: A < B < C\n\ndat_fact2 <- factor( c('Leipzig','Los Angeles','Logan'), ordered=F) # Factor data (Categorical)\ndat_fact2\n## [1] Leipzig     Los Angeles Logan      \n## Levels: Leipzig Logan Los Angeles\n\ndat_fact3 <- factor( c(T,F), ordered=F) # Factor data (Categorical)\ndat_fact3\n## [1] TRUE  FALSE\n## Levels: FALSE TRUE\n\n# Explicitly check the data types:\n#class(dat_card1)\n#class(dat_card2)\n```\n:::\n\n\nNote that for theoretical analysis, the types are sometimes grouped differently as\n\n* Discrete (discrete cardinal, ordered factor, and unordered factor data). You can count the potential values. E.g., the set $\\{A,B,C\\}$ has three potential values.\n* Continuous (continuous cardinal data). There are uncountably infinite potential values. E.g., Try counting the numbers between $0$ and $1$ including decimal points, and notice that any two numbers have another potential number between them.\n\n\nIn any case, data are often computationally analyzed as `data.frame` objects, discussed below.\n\n\n#### **Strings**. {-} \nNote that R allows for unstructured plain text, called *character strings*, which we can then format as factors\n\n::: {.cell}\n\n```{.r .cell-code}\nc('A','B','C')  # character strings\n## [1] \"A\" \"B\" \"C\"\nc('Leipzig','Los Angeles','Logan')  # character strings\n## [1] \"Leipzig\"     \"Los Angeles\" \"Logan\"\n```\n:::\n\nAlso note that strings are encounter in a variety of settings, and you often have to format them after reading them into R.^[We will not cover the statistical analysis of text in this course, but strings are amenable to statistical analysis.]\n\n::: {.cell}\n\n```{.r .cell-code}\n# Strings\npaste( 'hi', 'mom')\n## [1] \"hi mom\"\npaste( c('hi', 'mom'), collapse='--')\n## [1] \"hi--mom\"\n\nkingText <- \"The king infringes the law on playing curling.\"\ngsub(pattern=\"ing\", replacement=\"\", kingText)\n## [1] \"The k infres the law on play curl.\"\n# advanced usage\n#gsub(\"[aeiouy]\", \"_\", kingText)\n#gsub(\"([[:alpha:]]{3})ing\\\\b\", \"\\\\1\", kingText) \n```\n:::\n\n\n## Datasets\n\nDatasets can be stored in a variety of formats on your computer. But they can be analyzed in R in three basic ways.\n\n#### **Lists**. {-}\n\nLists are probably the most basic type\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 1:10\ny <- 2*x\nlist(x, y)  # list of vectors\n## [[1]]\n##  [1]  1  2  3  4  5  6  7  8  9 10\n## \n## [[2]]\n##  [1]  2  4  6  8 10 12 14 16 18 20\n\nx_mat1 <- matrix(2:7,2,3)\nx_mat2 <- matrix(4:-1,2,3)\nlist(x_mat1, x_mat2)  # list of matrices\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n```\n:::\n\n\nLists are useful for storing unstructured data\n\n::: {.cell}\n\n```{.r .cell-code}\nlist(list(x_mat1), list(x_mat2))  # list of lists\n## [[1]]\n## [[1]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## \n## [[2]]\n## [[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\nlist(x_mat1, list(x_mat1, x_mat2)) # list of different objects\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]]\n## [[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]][[2]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\n# ...inception...\nlist(x_mat1,\n    list(x_mat1, x_mat2), \n    list(x_mat1, list(x_mat2)\n    )) \n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]]\n## [[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]][[2]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n## \n## \n## [[3]]\n## [[3]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[3]][[2]]\n## [[3]][[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n```\n:::\n\n\n#### **Data.frames**. {-}\n\nA *data.frame* looks like a matrix but each column is actually a list rather than a vector. This allows you to combine different data types into a single object for analysis, which is why it might be your most common object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# data.frames: your most common data type\n    # matrix of different data-types\n    # well-ordered lists\ndata.frame(x, y)  # list of vectors\n##     x  y\n## 1   1  2\n## 2   2  4\n## 3   3  6\n## 4   4  8\n## 5   5 10\n## 6   6 12\n## 7   7 14\n## 8   8 16\n## 9   9 18\n## 10 10 20\n```\n:::\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nCreate a data.frame storing two different types of data. Then show print only the second column\n\n::: {.cell}\n\n```{.r .cell-code}\nd0 <- data.frame(x=dat_fact2, y=dat_card2)\nd0\n##             x         y\n## 1     Leipzig 1.1000000\n## 2 Los Angeles 0.6666667\n## 3       Logan 3.0000000\n\nd0[,'y']\n## [1] 1.1000000 0.6666667 3.0000000\n```\n:::\n\n:::\n\n#### **Arrays**. {-}\n\nArrays are generalization of matrices to multiple dimensions. They are a very efficient way to store well-formatted numeric data, and are often used in spatial econometrics and time series (often in the form of \"data cubes\").\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# data square (matrix)\narray(data = 1:24, dim = c(3,8))\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n## [1,]    1    4    7   10   13   16   19   22\n## [2,]    2    5    8   11   14   17   20   23\n## [3,]    3    6    9   12   15   18   21   24\n\n# data cube\na <- array(data = 1:24, dim = c(3, 2, 4))\na\n## , , 1\n## \n##      [,1] [,2]\n## [1,]    1    4\n## [2,]    2    5\n## [3,]    3    6\n## \n## , , 2\n## \n##      [,1] [,2]\n## [1,]    7   10\n## [2,]    8   11\n## [3,]    9   12\n## \n## , , 3\n## \n##      [,1] [,2]\n## [1,]   13   16\n## [2,]   14   17\n## [3,]   15   18\n## \n## , , 4\n## \n##      [,1] [,2]\n## [1,]   19   22\n## [2,]   20   23\n## [3,]   21   24\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\na[1, , , drop = FALSE]  # Row 1\n#a[, 1, , drop = FALSE]  # Column 1\n#a[, , 1, drop = FALSE]  # Layer 1\n\na[ 1, 1,  ]  # Row 1, column 1\n#a[ 1,  , 1]  # Row 1, \"layer\" 1\n#a[  , 1, 1]  # Column 1, \"layer\" 1\na[1 , 1, 1]  # Row 1, column 1, \"layer\" 1\n```\n:::\n\n\nApply extends to arrays\n\n::: {.cell}\n\n```{.r .cell-code}\napply(a, 1, mean)    # Row means\n## [1] 11.5 12.5 13.5\napply(a, 2, mean)    # Column means\n## [1] 11 14\napply(a, 3, mean)    # \"Layer\" means\n## [1]  3.5  9.5 15.5 21.5\napply(a, 1:2, mean)  # Row/Column combination \n##      [,1] [,2]\n## [1,]   10   13\n## [2,]   11   14\n## [3,]   12   15\n```\n:::\n\n\nOuter products yield arrays\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(1,2,3)\nx_mat1 <- outer(x, x) # x %o% x\nx_mat1\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    2    4    6\n## [3,]    3    6    9\nis.array(x_mat1) # Matrices are arrays\n## [1] TRUE\n\nx_mat2 <- matrix(6:1,2,3)\nouter(x_mat2, x)\n## , , 1\n## \n##      [,1] [,2] [,3]\n## [1,]    6    4    2\n## [2,]    5    3    1\n## \n## , , 2\n## \n##      [,1] [,2] [,3]\n## [1,]   12    8    4\n## [2,]   10    6    2\n## \n## , , 3\n## \n##      [,1] [,2] [,3]\n## [1,]   18   12    6\n## [2,]   15    9    3\n# outer(x_mat2, matrix(x))\n# outer(x_mat2, t(x))\n# outer(x_mat1, x_mat2)\n```\n:::\n\n\n\n## Densities and Distributions\n\n#### **Initial Data Inspection**. {-}\nRegardless of the data types you have, you typically begin by inspecting your data by examining the first few observations.\n \nConsider, for example, historical data on crime in the US.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(USArrests) # Actual Data\n##            Murder Assault UrbanPop Rape\n## Alabama      13.2     236       58 21.2\n## Alaska       10.0     263       48 44.5\n## Arizona       8.1     294       80 31.0\n## Arkansas      8.8     190       50 19.5\n## California    9.0     276       91 40.6\n## Colorado      7.9     204       78 38.7\n\n# Check NA values\nX <- c(3,3.1,NA,0.02) #Small dataset we will use in numerical examples\nsum(is.na(X))\n## [1] 1\n```\n:::\n\n\nTo further examine a particular variable, we look at its distribution. In what follows, we will often work with data as vector $\\hat{X}=(\\hat{X}_{1}, \\hat{X}_{2}, ....\\hat{X}_{n})$, where there are $n$ observations and $\\hat{X}_{i}$ is the value of the $i$th one. We often analyze observations in comparison to some value $x$. \n\n::: {.cell}\n\n```{.r .cell-code}\nX <- c(3,3.1,0.02) # Data: \"big X\"\nx <- 2 # particular value: \"little x\"\nsum(X <= x)\n## [1] 1\nsum(X == x)\n## [1] 0\n```\n:::\n\n\n#### **Histogram Density Estimate**. {-}\nThe [histogram](https://en.wikipedia.org/wiki/Histogram) measures the proportion of the data in different bins. It does so by dividing the range of the data into exclusive bins of equal-width $h$, and count the number of observations within each bin. We often rescale the counts, dividing by the number of observations $n$ multiplied by bin-width $h$, so that the total area of the histogram sums to one, which allows us to interpret the numbers as a *density*.\n\nMathematically, for an exclusive bin $\\left(x-\\frac{h}{2}, x+\\frac{h}{2} \\right]$ defined by their midpoint $x$ and width $h$, we compute\n\\begin{eqnarray}\n\\widehat{f}(x) &=& \\frac{  \\sum_{i=1}^{n} \\mathbf{1}\\left( \\hat{X}_{i} \\in \\left(x-\\frac{h}{2}, x+\\frac{h}{2} \\right] \\right) }{n h},\n\\end{eqnarray}\nwhere $\\mathbf{1}()$ is an indicator function that equals $1$ if the expression inside is TRUE and $0$ otherwise. E.g., if $\\hat{X}_{i}=3.8$ and $h=1$, then for $x=1$ we have $\\mathbf{1}\\left( \\hat{X}_{i} \\in \\left(1-\\frac{h}{2}, 1+\\frac{h}{2} \\right] \\right)=\\mathbf{1}\\left( 3.8 \\in \\left(0.5, 1.5\\right] \\right)=0$ and for $x=4$ $\\mathbf{1}\\left( \\hat{X}_{i} \\in \\left(4-\\frac{h}{2}, 4+\\frac{h}{2} \\right] \\right)=\\mathbf{1}\\left(  3.8 \\in \\left(3.5, 4.5\\right] \\right)=1$.\n\nNote that the area of the rectangle is \"base x height\", which is $h \\times \\widehat{f}(x)= \\sum_{i=1}^{n} \\mathbf{1}\\left( \\hat{X}_{i} \\in \\left(x-\\frac{h}{2}, x+\\frac{h}{2} \\right] \\right) /n$. This means the rectangle area equals the proportion of data in the bin. We compute $\\widehat{f}(x)$ for each bin midpoint $x$, and the area of all rectangles sums to one.^[If $L$ distinct bins exactly span the range, then $h=[\\text{max}(\\hat{X}_{i}) - \\text{min}(\\hat{X}_{i})]/L$ and $x\\in \\left\\{ \\frac{\\ell h}{2} + \\text{min}(\\hat{X}_{i}) \\right\\}_{\\ell=1}^{L}$.]\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, let $X=(3,3.1,0.02)$ and use bins $(0,1], (1,2], (2,3], (3,4]$. In this case, the midpoints are $x=(0.5,1.5,2.5,3.5)$ and $h=1$. Then the counts at each midpoints are $(1,0,1,1)$. Since $\\frac{1}{nh}=\\frac{1}{3\\times1}=frac{1}{3}$, we can rescale the counts to compute the density as $\\widehat{f}(x)=(1,0,1,1) \\frac{1}{3}=(1/3,0,1/3,1/3)$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Intuitive Examples\nX <- c(3,3.1,0.02)\nXhist <- hist(X, breaks=c(0,1,2,3,4), plot=F)\nXhist\n## $breaks\n## [1] 0 1 2 3 4\n## \n## $counts\n## [1] 1 0 1 1\n## \n## $density\n## [1] 0.3333333 0.0000000 0.3333333 0.3333333\n## \n## $mids\n## [1] 0.5 1.5 2.5 3.5\n## \n## $xname\n## [1] \"X\"\n## \n## $equidist\n## [1] TRUE\n## \n## attr(,\"class\")\n## [1] \"histogram\"\n\n# base x height\nbase <- 1\nheight <- Xhist$density\nsum(base*height)\n## [1] 1\n```\n:::\n\n\nFor another example, use the bins $(0,2]$ and $(2,4]$. So the midpoints are $1$ and $3$, and the bin width is $2$. Only one observation, $0.02$, falls in the bin $(0,2]$. The other two observations, $3$ and $3.1$, fall into the bin $(2,4]$. The scaling factor is $\\frac{1}{nh}=\\frac{1}{3\\times 2}=\\frac{1}{6}$. So the first bin has density $f(1)=1\\frac{1}{6}=1/6$ and the second bin has density $f(3)=2\\frac{1}{6}=2/6$. The area of the first bin's rectangle is $2 \\times f(1)=2/6=1/3$ and the area of the second rectangle is $2 \\times f(3)=4/6=2/3$.\n\nNow intuitively work through an example with three bins instead of four. Compute the areas \n\n::: {.cell}\n\n```{.r .cell-code}\nXhist <- hist(X, breaks=c(0,4/3,8/3,4), plot=F)\nbase <- 4/3\nheight <- Xhist$density\nsum(base*height)\n## [1] 1\n\n\n# as a default, R uses bins (,] instead of [,)\n# but you can change that with \"right=F\"\n# hist(X, breaks=c(0,4/3,8/3,4), plot=F, right=F)\n```\n:::\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Practical Example\nX <- USArrests[,'Murder']\nhist(X,\n    breaks=seq(0,20,by=1), #bin width=1\n    freq=F,\n    border=NA, \n    main='',\n    xlab='Murder Arrests')\n# Raw Observations\nrug(USArrests[,'Murder'], col=grey(0,.5))\n```\n\n::: {.cell-output-display}\n![](01_03_Data_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Since h=1, the density equals the proportion of states in each bin\n# Redo this example with h=2\n```\n:::\n\n\nNote that if you your data are factor data, or discrete cardinal data, you can directly plot the proportions in a *bar plot*. For each unique outcome $x$ we compute\n\\begin{eqnarray}\n\\widehat{p}_{x}=\\sum_{i=1}^{n}\\mathbf{1}\\left(\\hat{X}_{i}=x\\right)/n.\n\\end{eqnarray}\nwhere $n$ is the number of observations and $\\sum_{i=1}^{n}\\mathbf{1}\\left(\\hat{X}_{i}=x\\right)$ counts the number of observations equal to $x$. The height of each line equals the proportion of data with a specific value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Discretized data\nX <- USArrests[,'Murder']\nXr <- floor(X) #rounded down\n#table(Xr)\nproportions <- table(Xr)/length(Xr)\nplot(proportions, col=grey(0,.5),\n    xlab='Murder Arrests (Discretized)',\n    ylab='Proportion of States with each value')\n```\n\n::: {.cell-output-display}\n![](01_03_Data_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n#### **Empirical *Cumulative* Distribution Function**. {-}\nThe ECDF counts the proportion of observations whose values are less than or equal to $x$; \n\\begin{eqnarray}\n\\widehat{F}(x) = \\frac{1}{n} \\sum_{i}^{n} \\mathbf{1}( \\hat{X}_{i} \\leq x).\n\\end{eqnarray}\nTypically, we compute this for each unique value of $x$ in the dataset. The ECDF jumps up by $1/n$ at each of the $n$ data points.\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, let $X=(3,3.1,0.02)$. We reorder the observations as $(0.02, 3, 3.1)$, so that there are discrete jumps of $1/n=1/3$ at each value. Consider the points $x \\in \\{0.5,2.5,3.5\\}$. At $x=0.5$, $F(0.5)$ measures the proportion of the data $\\leq 0.5$. Since only one observations, $0.02$, of three is $\\leq 0.5$, we can compute $F(0.5)=1/3$. Similarly, since only one observations, $0.02$, of three is $\\leq 2.5$, we can compute $F(2.5)=1/3$. Since all observations are $\\leq 3.5$, we can compute $F(3.5)=1$.\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- c(3,3.1,0.02)\nFhat <- ecdf(X)\n\n# Visualized\nplot(Fhat)\n```\n\n::: {.cell-output-display}\n![](01_03_Data_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Evaluated at the data\nx <- X\nFhat(X)\n## [1] 0.6666667 1.0000000 0.3333333\n#sum(X<=3)/length(X)\n#sum(X<=3.1)/length(X)\n#sum(X<=0.02)/length(X)\n\n# Evaluated at other points\nx <- c(0.5, 2.5, 3.5)\nFhat(x)\n## [1] 0.3333333 0.3333333 1.0000000\n#sum(X<=0.5)/length(X)\n#sum(X<=2.5)/length(X)\n#sum(X<=3.5)/length(X)\n```\n:::\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nF_murder <- ecdf(USArrests[,'Murder'])\n# proportion of murders <= 10\nF_murder(10)\n## [1] 0.7\n# proportion of murders <= x, for all x\nplot(F_murder, main='', \n    xlab='Murder Arrests (x)',\n    ylab='Proportion of States with Murder Arrests <= x',\n    pch=16, col=grey(0,.5))\nrug(USArrests[,'Murder'])\n```\n\n::: {.cell-output-display}\n![](01_03_Data_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n#### **Quantiles**. {-}\nYou can summarize the distribution of data using *quantiles*: the $q$th quantile is the value where $q$ percent of the data are below and ($1-q$) percent are above.\n\n\n* The *min* is the smallest value (or the most negative value if there are any), where $0%$ of the data has lower values.\n* The *median* is the point where half of the data has lower values and the other half has higher values.\n* The *max* is the smallest value (or the most negative value if there are any), where $100%$ of the data has lower values.\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, if $X=(0,0,0.02,3,5)$ then the median is $0.02$, the lower quartile is $0$, and the upper quartile is $3$. (The number $0$ is also special: the most frequent observation is called the *mode*.) \n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <-  c(3.1, 3, 0.02)\nquantile(X, probs=c(0,.5,1))\n##   0%  50% 100% \n## 0.02 3.00 3.10\n```\n:::\n\n\nNow work through an intuitive example with observations $\\{1,2,...,13\\}$. Hint: split the ordered observations into four groups.\n:::\n\nWe are often also interested in quartiles: where $25\\%$ of the data fall below $x$ (lower quartile) and where $75\\%$ of the data fall below $x$ (upper quartile). Sometimes we are also interested in deciles: where $10\\%$ of the data fall below $x$ (lower decile) and where $90\\%$ of the data fall below $x$ (upper decile). \n\n::: {.cell}\n\n```{.r .cell-code}\n# common quantiles\nX <- USArrests[,'Murder']\nquantile(X)\n##     0%    25%    50%    75%   100% \n##  0.800  4.075  7.250 11.250 17.400\n\n# All deciles are quantiles\nquantile(X, probs=seq(0,1, by=.1))\n##    0%   10%   20%   30%   40%   50%   60%   70%   80%   90%  100% \n##  0.80  2.56  3.38  4.75  6.00  7.25  8.62 10.12 12.12 13.32 17.40\n\n# Visualized: Inverting the Empirical Distribution\nFX_hat <- ecdf(X)\nplot(FX_hat, lwd=2, xlim=c(0,20),\n    pch=16, col=grey(0,.5), main='')\n# Two Examples of Quantiles \np <- c(.25, .9) # Lower Quartile, Upper Decile\ncols <- c(2,4) \nQX_hat <- quantile(X, p, type=1)\nQX_hat\n##  25%  90% \n##  4.0 13.2\nsegments(QX_hat, p, -10, p, col=cols)\nsegments(QX_hat, p, QX_hat, 0, col=cols)\nmtext( round(QX_hat,2), 1, at=QX_hat, col=cols)\n```\n\n::: {.cell-output-display}\n![](01_03_Data_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nThere are some issues with quantiles with smaller datasets. E.g., to compute the median of ${3,3.1,0,1}$, we need some ways to break ties (of which there are many options). Similar issues arise for other quantiles, so quantiles are not used for such small datasets.\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nTo calculate quantiles, the computer sorts the observations from smallest to largest as $\\hat{X}_{(1)}, \\hat{X}_{(2)},... \\hat{X}_{(N)}$, and then computes quantiles as $\\hat{X}_{ (q \\times N) }$. Note that $(q \\times N)$ is rounded and there are different ways to break ties.\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- USArrests[,'Murder']\nXo <- sort(X)\nXo\n##  [1]  0.8  2.1  2.1  2.2  2.2  2.6  2.6  2.7  3.2  3.3  3.4  3.8  4.0  4.3  4.4\n## [16]  4.9  5.3  5.7  5.9  6.0  6.0  6.3  6.6  6.8  7.2  7.3  7.4  7.9  8.1  8.5\n## [31]  8.8  9.0  9.0  9.7 10.0 10.4 11.1 11.3 11.4 12.1 12.2 12.7 13.0 13.2 13.2\n## [46] 14.4 15.4 15.4 16.1 17.4\n\n# median\nXo[length(Xo)*.5]\n## [1] 7.2\nquantile(X, probs=.5, type=4) #tie break rule #4\n## 50% \n## 7.2\n\n# min\nXo[1]\n## [1] 0.8\nmin(Xo)\n## [1] 0.8\nquantile(Xo, probs=0)\n##  0% \n## 0.8\n```\n:::\n\n:::\n\n#### **Boxplots**. {-}\n[Boxplots](https://en.wikipedia.org/wiki/Box_plot) also summarize the distribution. The boxplot shows the median (solid black line) and interquartile range ($IQR=$ upper quartile $-$ lower quartile; filled box).^[Technically, the upper and lower *hinges* use two different versions of the first and third quartile. See <https://stackoverflow.com/questions/40634693/lower-and-upper-quartiles-in-boxplot-in-r>.] As a default, whiskers are shown as $1.5\\times IQR$ and values beyond that are highlighted as outliers (so whiskers do not typically show the data range). You can alternatively show all the raw data points instead of whisker+outliers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(USArrests[,'Murder'],\n    main='', ylab='Murder Arrests',\n    whisklty=0, staplelty=0, outline=F)\n# Raw Observations\nstripchart(USArrests[,'Murder'],\n    pch='-', col=grey(0,.5), cex=2,\n    vert=T, add=T)\n```\n\n::: {.cell-output-display}\n![](01_03_Data_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n## Further Reading\n\nECDF\n\n* <https://library.virginia.edu/data/articles/understanding-empirical-cumulative-distribution-functions>\n\nHandling Strings\n\n* <https://meek-parfait-60672c.netlify.app/docs/M1_R-intro_03_text.html>\n* <https://raw.githubusercontent.com/rstudio/cheatsheets/main/regex.pdf>\n\n\n",
    "supporting": [
      "01_03_Data_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}