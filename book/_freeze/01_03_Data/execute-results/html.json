{
  "hash": "fb32ba9e74e0f97468a8bda8117143db",
  "result": {
    "engine": "knitr",
    "markdown": "# Data\n***\n\n## Types\n\n#### **Basic Types**. {-}\nThe two basic types of data are *cardinal* (aka numeric) data and *factor* data. We can further distinguish between whether cardinal data are discrete or continuous. We can also further distinguish between whether factor data are ordered or not\n\n* *Cardinal (Numeric)*: the difference between elements always means the same thing. \n    * Discrete: E.g. $\\{ 1,2,3\\}$ and notice that $2-1=3-2$.\n    * Continuous: E.g., $\\{1.4348, 2.4348, 2.9, 3.9 \\}$ and notice that $2.9-1.4348=3.9-2.4348$\n* *Factor*: the difference between elements does not always mean the same thing.\n    * Ordered: E.g., $\\{1^{st}, 2^{nd}, 3^{rd}\\}$ place in a race and notice that $1^{st}$ place - $2^{nd}$ place does not equal $2^{nd}$ place - $3^{rd}$ place for a very competitive person.\n    * Unordered (categorical): E.g., $\\{Amanda, Bert, Charlie\\}$ and notice that $Amanda - Bert$ never makes sense.\n\n\nHere are some examples\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_card1 <- 1:3 # Cardinal data (Discrete)\ndat_card1\n## [1] 1 2 3\n\ndat_card2 <- c(1.1, 2/3, 3) # Cardinal data (Continuous)\ndat_card2\n## [1] 1.1000000 0.6666667 3.0000000\n\ndat_fact1 <- factor( c('A','B','C'), ordered=T) # Factor data (Ordinal)\ndat_fact1\n## [1] A B C\n## Levels: A < B < C\n\ndat_fact2 <- factor( c('Leipzig','Los Angeles','Logan'), ordered=F) # Factor data (Categorical)\ndat_fact2\n## [1] Leipzig     Los Angeles Logan      \n## Levels: Leipzig Logan Los Angeles\n\ndat_fact3 <- factor( c(T,F), ordered=F) # Factor data (Categorical)\ndat_fact3\n## [1] TRUE  FALSE\n## Levels: FALSE TRUE\n\n# Explicitly check the data types:\n#class(dat_card1)\n#class(dat_card2)\n```\n:::\n\n\nNote that for theoretical analysis, the types are sometimes grouped differently as\n\n* Discrete (discrete cardinal, ordered factor, and unordered factor data). You can count the potential values. E.g., the set $\\{A,B,C\\}$ has three potential values.\n* Continuous (continuous cardinal data). There are uncountably infinite potential values. E.g., Try counting the numbers between $0$ and $1$ including decimal points, and notice that any two numbers have another potential number between them.\n\n\nIn any case, data are often computationally analyzed as `data.frame` objects, discussed below.\n\n\n#### **Strings**. {-} \nNote that R allows for unstructured plain text, called *character strings*, which we can then format as factors\n\n::: {.cell}\n\n```{.r .cell-code}\nc('A','B','C')  # character strings\n## [1] \"A\" \"B\" \"C\"\nc('Leipzig','Los Angeles','Logan')  # character strings\n## [1] \"Leipzig\"     \"Los Angeles\" \"Logan\"\n```\n:::\n\nAlso note that strings are encounter in a variety of settings, and you often have to format them after reading them into R.^[We will not cover the statistical analysis of text in this course, but strings are amenable to statistical analysis.]\n\n::: {.cell}\n\n```{.r .cell-code}\n# Strings\npaste( 'hi', 'mom')\n## [1] \"hi mom\"\npaste( c('hi', 'mom'), collapse='--')\n## [1] \"hi--mom\"\n\nkingText <- \"The king infringes the law on playing curling.\"\ngsub(pattern=\"ing\", replacement=\"\", kingText)\n## [1] \"The k infres the law on play curl.\"\n# advanced usage\n#gsub(\"[aeiouy]\", \"_\", kingText)\n#gsub(\"([[:alpha:]]{3})ing\\\\b\", \"\\\\1\", kingText) \n```\n:::\n\nSee \n\n* <https://meek-parfait-60672c.netlify.app/docs/M1_R-intro_03_text.html>\n* <https://raw.githubusercontent.com/rstudio/cheatsheets/main/regex.pdf>\n\n\n## Datasets\n\nDatasets can be stored in a variety of formats on your computer. But they can be analyzed in R in three basic ways.\n\n#### **Lists**. {-}\n\nLists are probably the most basic type\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 1:10\ny <- 2*x\nlist(x, y)  # list of vectors\n## [[1]]\n##  [1]  1  2  3  4  5  6  7  8  9 10\n## \n## [[2]]\n##  [1]  2  4  6  8 10 12 14 16 18 20\n\nx_mat1 <- matrix(2:7,2,3)\nx_mat2 <- matrix(4:-1,2,3)\nlist(x_mat1, x_mat2)  # list of matrices\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n```\n:::\n\n\nLists are useful for storing unstructured data\n\n::: {.cell}\n\n```{.r .cell-code}\nlist(list(x_mat1), list(x_mat2))  # list of lists\n## [[1]]\n## [[1]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## \n## [[2]]\n## [[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\nlist(x_mat1, list(x_mat1, x_mat2)) # list of different objects\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]]\n## [[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]][[2]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\n# ...inception...\nlist(x_mat1,\n    list(x_mat1, x_mat2), \n    list(x_mat1, list(x_mat2)\n    )) \n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]]\n## [[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[2]][[2]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n## \n## \n## [[3]]\n## [[3]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n## \n## [[3]][[2]]\n## [[3]][[2]][[1]]\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n```\n:::\n\n\n#### **Data.frames**. {-}\n\nA *data.frame* looks like a matrix but each column is actually a list rather than a vector. This allows you to combine different data types into a single object for analysis, which is why it might be your most common object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# data.frames: your most common data type\n    # matrix of different data-types\n    # well-ordered lists\ndata.frame(x, y)  # list of vectors\n##     x  y\n## 1   1  2\n## 2   2  4\n## 3   3  6\n## 4   4  8\n## 5   5 10\n## 6   6 12\n## 7   7 14\n## 8   8 16\n## 9   9 18\n## 10 10 20\n```\n:::\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nCreate a data.frame storing two different types of data. Then show print only the second column\n\n::: {.cell}\n\n```{.r .cell-code}\nd0 <- data.frame(x=dat_fact2, y=dat_card2)\nd0\n##             x         y\n## 1     Leipzig 1.1000000\n## 2 Los Angeles 0.6666667\n## 3       Logan 3.0000000\n\nd0[,'y']\n## [1] 1.1000000 0.6666667 3.0000000\n```\n:::\n\n:::\n\n#### **Arrays**. {-}\n\nArrays are generalization of matrices to multiple dimensions. They are a very efficient way to store well-formatted numeric data, and are often used in spatial econometrics and time series (often in the form of \"data cubes\").\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# data square (matrix)\narray(data = 1:24, dim = c(3,8))\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n## [1,]    1    4    7   10   13   16   19   22\n## [2,]    2    5    8   11   14   17   20   23\n## [3,]    3    6    9   12   15   18   21   24\n\n# data cube\na <- array(data = 1:24, dim = c(3, 2, 4))\na\n## , , 1\n## \n##      [,1] [,2]\n## [1,]    1    4\n## [2,]    2    5\n## [3,]    3    6\n## \n## , , 2\n## \n##      [,1] [,2]\n## [1,]    7   10\n## [2,]    8   11\n## [3,]    9   12\n## \n## , , 3\n## \n##      [,1] [,2]\n## [1,]   13   16\n## [2,]   14   17\n## [3,]   15   18\n## \n## , , 4\n## \n##      [,1] [,2]\n## [1,]   19   22\n## [2,]   20   23\n## [3,]   21   24\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\na[1, , , drop = FALSE]  # Row 1\n#a[, 1, , drop = FALSE]  # Column 1\n#a[, , 1, drop = FALSE]  # Layer 1\n\na[ 1, 1,  ]  # Row 1, column 1\n#a[ 1,  , 1]  # Row 1, \"layer\" 1\n#a[  , 1, 1]  # Column 1, \"layer\" 1\na[1 , 1, 1]  # Row 1, column 1, \"layer\" 1\n```\n:::\n\n\nApply extends to arrays\n\n::: {.cell}\n\n```{.r .cell-code}\napply(a, 1, mean)    # Row means\n## [1] 11.5 12.5 13.5\napply(a, 2, mean)    # Column means\n## [1] 11 14\napply(a, 3, mean)    # \"Layer\" means\n## [1]  3.5  9.5 15.5 21.5\napply(a, 1:2, mean)  # Row/Column combination \n##      [,1] [,2]\n## [1,]   10   13\n## [2,]   11   14\n## [3,]   12   15\n```\n:::\n\n\nOuter products yield arrays\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(1,2,3)\nx_mat1 <- outer(x, x) # x %o% x\nx_mat1\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    2    4    6\n## [3,]    3    6    9\nis.array(x_mat1) # Matrices are arrays\n## [1] TRUE\n\nx_mat2 <- matrix(6:1,2,3)\nouter(x_mat2, x)\n## , , 1\n## \n##      [,1] [,2] [,3]\n## [1,]    6    4    2\n## [2,]    5    3    1\n## \n## , , 2\n## \n##      [,1] [,2] [,3]\n## [1,]   12    8    4\n## [2,]   10    6    2\n## \n## , , 3\n## \n##      [,1] [,2] [,3]\n## [1,]   18   12    6\n## [2,]   15    9    3\n# outer(x_mat2, matrix(x))\n# outer(x_mat2, t(x))\n# outer(x_mat1, x_mat2)\n```\n:::\n\n\n\n## Densities and Distributions\n\n#### **Initial Data Inspection**. {-}\nRegardless of the data types you have, you typically begin by inspecting your data by examining the first few observations.\n \nConsider, for example, historical data on crime in the US.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(USArrests)\n##            Murder Assault UrbanPop Rape\n## Alabama      13.2     236       58 21.2\n## Alaska       10.0     263       48 44.5\n## Arizona       8.1     294       80 31.0\n## Arkansas      8.8     190       50 19.5\n## California    9.0     276       91 40.6\n## Colorado      7.9     204       78 38.7\n\n# Check NA values\nX <- c(1,NA,2,3)\nsum(is.na(X))\n## [1] 1\n```\n:::\n\n\nTo further examine a particular variable, we look at its distribution. In what follows, we will often work with data as vector $X=(X_{1}, X_{2}, ....X_{N})$, where there are $N$ observations and $X_{i}$ is the value of the $i$th one.\n\n\n#### **Histogram Density Estimate**. {-}\nThe histogram divides the range of the data into $L$ exclusive bins of equal-width $h$, and count the number of observations within each bin. We often rescale the counts so that the total area of all bins sums to one, which allows us to interpret the numbers as a *density* measuring a proportion of the data in each bin. Mathematically, for an exclusive bin $\\left(x-\\frac{h}{2}, x+\\frac{h}{2} \\right]$ defined by their midpoint $x$ and width $h$, we compute\n\\begin{eqnarray}\n\\widehat{f}(x) &=& \\frac{  \\sum_{i=1}^{N} \\mathbf{1}\\left( X_{i} \\in \\left(x-\\frac{h}{2}, x+\\frac{h}{2} \\right] \\right) }{N h}.\n\\end{eqnarray}\nNote that $\\mathbf{1}()$ is an indicator function, which equals $1$ if the expression inside is TRUE and $0$ otherwise. E.g., if $X_{i}=3.8$ then $\\mathbf{1}\\left( X_{i} \\in \\left(1-\\frac{1}{2}, 1+\\frac{1}{2} \\right] \\right)=0$ and $\\mathbf{1}\\left( X_{i} \\in \\left(4-\\frac{1}{2}, 4+\\frac{1}{2} \\right] \\right)=1$. Also note that we compute $\\widehat{f}(x)$ for each bin midpoint $x$.^[If the bins exactly span the range, then $h=[\\text{max}(X_{i}) - \\text{min}(X_{i})]/L$ and $x\\in \\left\\{ \\frac{\\ell h}{2} + \\text{min}(X_{i}) \\right\\}_{\\ell=1}^{L}$.]\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, let $X=(3,3.1,0.02)$ and use bins $(0,1], (1,2], (2,3], (3,4]$. In this case, the midpoints are $x=(0.5,1.5,2.5,3.5)$ and $h=1$. Then the counts at each midpoints are $(1,0,0,2)$. Since $\\frac{1}{Nh}=1/3$, we also have $\\widehat{f}(x)=(1/3,0,1/3,1/3)$.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Intuitive Examples\nX <- c(3,3.1,0.02)\nhist(X, breaks=c(0,1,2,3,4), plot=F)\n## $breaks\n## [1] 0 1 2 3 4\n## \n## $counts\n## [1] 1 0 1 1\n## \n## $density\n## [1] 0.3333333 0.0000000 0.3333333 0.3333333\n## \n## $mids\n## [1] 0.5 1.5 2.5 3.5\n## \n## $xname\n## [1] \"X\"\n## \n## $equidist\n## [1] TRUE\n## \n## attr(,\"class\")\n## [1] \"histogram\"\n```\n:::\n\n\nNow intuitively work through an example with three bins instead of four.\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(X, breaks=c(0,4/3,8/3,4), plot=F)\n## $breaks\n## [1] 0.000000 1.333333 2.666667 4.000000\n## \n## $counts\n## [1] 1 0 2\n## \n## $density\n## [1] 0.25 0.00 0.50\n## \n## $mids\n## [1] 0.6666667 2.0000000 3.3333333\n## \n## $xname\n## [1] \"X\"\n## \n## $equidist\n## [1] TRUE\n## \n## attr(,\"class\")\n## [1] \"histogram\"\n\n# as a default, R uses bins (,] instead of [,)\n# but you can change that \nhist(X, breaks=c(0,4/3,8/3,4), plot=F, right=F)\n## $breaks\n## [1] 0.000000 1.333333 2.666667 4.000000\n## \n## $counts\n## [1] 1 0 2\n## \n## $density\n## [1] 0.25 0.00 0.50\n## \n## $mids\n## [1] 0.6666667 2.0000000 3.3333333\n## \n## $xname\n## [1] \"X\"\n## \n## $equidist\n## [1] TRUE\n## \n## attr(,\"class\")\n## [1] \"histogram\"\n```\n:::\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Practical Example\nhist(USArrests[,'Murder'], freq=F, breaks=20,\n    border=NA, \n    main='',\n    xlab='Murder Arrests',\n    ylab='Proportion of States in each bin')\n# Raw Observations\nrug(USArrests[,'Murder'], col=grey(0,.5))\n```\n\n::: {.cell-output-display}\n![](01_03_Data_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nNote that if you your data are factor data, or discrete cardinal data, you can directly plot the counts or proportions in a *bar plot*. For each unique outcome $x$ we compute\n\\begin{eqnarray}\n\\widehat{p}_{x}=\\sum_{i=1}^{N}\\mathbf{1}\\left(X_{i}=x\\right)/N.\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Discretized data\nxr <- floor(USArrests[,'Murder']) #rounded down\n#table(xr)\nproportions <- table(xr)/length(xr)\nplot(proportions, col=grey(0,.5),\n    xlab='Murder Rate (Discretized)',\n    ylab='Proportion of States with each value')\n```\n\n::: {.cell-output-display}\n![](01_03_Data_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n#### **Empirical *Cumulative* Distribution Function**. {-}\nThe ECDF counts the proportion of observations whose values are less than or equal to $x$; \n\\begin{eqnarray}\n\\widehat{F}(x) = \\frac{1}{N} \\sum_{i}^{N} \\mathbf{1}(X_{i} \\leq x).\n\\end{eqnarray}\nTypically, we compute this for each unique value of $x$ in the dataset, but sometimes other values of $x$ too.\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, let $X=(3,3.1,0.02)$ and consider the points $x=(0.5,1.5,2.5,3.5)$. Then the counts are $(1,1,1,3)$. Since $N=3$, $\\widehat{F}(x)=(1/3,1/3,1/3,1)$.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nF_murder <- ecdf(USArrests[,'Murder'])\n# proportion of murders <= 10\nF_murder(10)\n## [1] 0.7\n# proportion of murders <= x, for all x\nplot(F_murder, main='', \n    xlab='Murder Arrests (x)',\n    ylab='Proportion of States with Murder Arrests <= x',\n    pch=16, col=grey(0,.5))\nrug(USArrests[,'Murder'])\n```\n\n::: {.cell-output-display}\n![](01_03_Data_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n#### **Quantiles**. {-}\nYou can summarize the distribution of data using *quantiles*: the $q$th quantile is the value where $q$ percent of the data are below and ($1-q$) percent are above.\n\n* The *median* is the point where half of the data has lower values and the other half has higher values.\n* The *lower quartile* is the point where $25%$ of the data has lower values and the other $75%$ has higher values.\n* The *min* is the smallest value (or the most negative value if there are any), where $0%$ of the data has lower values.\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, if $X=(0,0,0.02,3,5)$ then the median is $0.02$, the lower quartile is $0$, and the upper quartile is $3$. (The number $0$ is also special: the most frequent observation is called the *mode*.) \n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <-  c(3.1, 3, 0.02)\nquantile(X, probs=c(0,.5,1))\n##   0%  50% 100% \n## 0.02 3.00 3.10\n```\n:::\n\n\nNow work through an intuitive example with $N=24$ data points (hint: split the ordered observations into groups of six).\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# quantiles\nX <- USArrests[,'Murder']\nquantile(X)\n##     0%    25%    50%    75%   100% \n##  0.800  4.075  7.250 11.250 17.400\n\n# deciles are quantiles\nquantile(X, probs=seq(0,1, by=.1))\n##    0%   10%   20%   30%   40%   50%   60%   70%   80%   90%  100% \n##  0.80  2.56  3.38  4.75  6.00  7.25  8.62 10.12 12.12 13.32 17.40\n```\n:::\n\n\nNote that every CDF is associated with a *quantile function* that is the inverse of the CDF: the $x$ value where $p$ percent of the data fall below it. The median is the value $x$ where $50\\%$ of the data fall below $x$, for example.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Empirical Distribution\nX <- USArrests$Murder\nFX_hat <- ecdf(X)\nplot(FX_hat, lwd=2, xlim=c(0,20),\n    pch=16, col=grey(0,.5), main='')\n\n# Two Example Quantiles\np <- c(.25, .9)\ncols <- c(2,4)\nQX_hat <- quantile(X, p, type=1)\nQX_hat\n##  25%  90% \n##  4.0 13.2\n\n# Visualized\nsegments(QX_hat, p, -10, p, col=cols)\nsegments(QX_hat, p, QX_hat, 0, col=cols)\nmtext( round(QX_hat,2), 1, at=QX_hat, col=cols)\n```\n\n::: {.cell-output-display}\n![](01_03_Data_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nTo calculate quantiles, the computer sorts the observations from smallest to largest as $X_{(1)}, X_{(2)},... X_{(N)}$, and then computes quantiles as $X_{ (q \\times N) }$. Note that $(q \\times N)$ is rounded and there are different ways to break ties.\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- USArrests[,'Murder']\nXo <- sort(X)\nXo\n##  [1]  0.8  2.1  2.1  2.2  2.2  2.6  2.6  2.7  3.2  3.3  3.4  3.8  4.0  4.3  4.4\n## [16]  4.9  5.3  5.7  5.9  6.0  6.0  6.3  6.6  6.8  7.2  7.3  7.4  7.9  8.1  8.5\n## [31]  8.8  9.0  9.0  9.7 10.0 10.4 11.1 11.3 11.4 12.1 12.2 12.7 13.0 13.2 13.2\n## [46] 14.4 15.4 15.4 16.1 17.4\n\n# median\nXo[length(Xo)*.5]\n## [1] 7.2\nquantile(X, probs=.5, type=4)\n## 50% \n## 7.2\n\n# min\nXo[1]\n## [1] 0.8\nmin(Xo)\n## [1] 0.8\nquantile(Xo, probs=0)\n##  0% \n## 0.8\n```\n:::\n\n:::\n\n#### **Boxplots**. {-}\nBoxplots also summarize the distribution. The boxplot shows the median (solid black line) and interquartile range ($IQR=$ upper quartile $-$ lower quartile; filled box).^[Technically, the upper and lower *hinges* use two different versions of the first and third quartile. See <https://stackoverflow.com/questions/40634693/lower-and-upper-quartiles-in-boxplot-in-r>.] As a default, whiskers are shown as $1.5\\times IQR$ and values beyond that are highlighted as outliers (so whiskers do not typically show the data range). You can alternatively show all the raw data points instead of whisker+outliers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(USArrests[,'Murder'],\n    main='', ylab='Murder Arrests',\n    whisklty=0, staplelty=0, outline=F)\n# Raw Observations\nstripchart(USArrests[,'Murder'],\n    pch='-', col=grey(0,.5), cex=2,\n    vert=T, add=T)\n```\n\n::: {.cell-output-display}\n![](01_03_Data_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n\n",
    "supporting": [
      "01_03_Data_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}