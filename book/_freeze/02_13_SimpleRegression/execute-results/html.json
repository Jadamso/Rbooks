{
  "hash": "641c44f4770cc45b6e389d5e6c0cfabc",
  "result": {
    "engine": "knitr",
    "markdown": "# Simple Regression\n\n***\n\nSuppose we have some bivariate data: $\\hat{X}_{i}, \\hat{Y}_{i}$. First, we inspect it as in Part I. We then assess the association between variables by fitting a line through the data points using a \"regression\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bivariate Data from USArrests\nxy <- USArrests[,c('Murder','UrbanPop')]\ncolnames(xy) <- c('y','x')\n\n# Inspect Dataset\n# head(xy)\n# summary(xy)\nplot(y~x, xy, col=grey(0,.5), pch=16,\n\txlab='Population Share in Urban Area',\n\tylab='Murder Arrests per 100K')\ntitle('Data from American States, 1975', font.main=1)\n\nreg <- lm(y~x, dat=xy)\nabline(reg, lty=2)\n```\n\n::: {.cell-output-display}\n![](02_13_SimpleRegression_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n\n## Simple Linear Regression\nThis refers to fitting a linear model to bivariate data. Specifically, our model is \n\\begin{eqnarray}\n\\hat{Y}_{i}=b_{0}+b_{1} \\hat{X}_{i}+e_{i},\n\\end{eqnarray}\nwhere $b_{0}$ and $b_{1}$ are parameters, often referred to as \"coefficients\", and $\\hat{X}_{i}, \\hat{Y}_{i}$ are data for observation $i$, and $e_{i}$ is a residual error term that represents the difference between the model and the data.\n\nWe then find the parameters which best-fit the data. Specifically, our objective function is\n\\begin{eqnarray}\n\\min_{b_{0}, b_{1}} \\sum_{i=1}^{n} \\left( e_{i} \\right)^2 &=& \\min_{b_{0}, b_{1}} \\sum_{i=1}^{n} \\left( \\hat{Y}_{i} - [b_{0}+b_{1} \\hat{X}_{i}] \\right)^2.\n\\end{eqnarray}\nMinimizing the sum of squared errors then yields two parameter estimates that can be explicitly solved for using math. For the slope coefficient:\n\\begin{eqnarray}\n0 &=& \\sum_{i=1}^{n} 2\\left( \\hat{Y}_{i} - [b_{0}+b_{1} \\hat{X}_{i}] \\right) \\hat{X}_{i} \\\\\n\\Rightarrow \\hat{b}_{1} &=& \\frac{\\sum_{i}^{n}(\\hat{X}_{i}-\\hat{M}_{X})(\\hat{Y}_{i}-\\hat{M}_{Y})}{\\sum_{i}^{}(\\hat{X}_{i}-\\hat{M}_{X})^2} = \\frac{\\hat{C}_{XY}}{\\hat{V}_{X}},\n\\end{eqnarray}\nthe latter term being the estimated covariance between $X$ and $Y$ divided by the variance of $X$. (Recall that $M_{X}$ and $M_{Y}$ are the means.)\nFor the intercept coefficient:\n\\begin{eqnarray}\n0 &=& \\sum_{i=1}^{n} 2\\left( \\hat{Y}_{i} - [b_{0}+b_{1} \\hat{X}_{i}] \\right)\\\\ \n\\Rightarrow \\hat{b}_{0} &=& \\hat{M}_{Y}-\\hat{b}_{1}\\hat{M}_{X} .\n\\end{eqnarray}\nWe could alternatively find the best fitting parameters numerically, by trying out different combinations of $(b_{0}, b_{1})$.\nYou can do that in this [example](https://myshiny.duckdns.org/least-squares/), which can help you understand what is happening.\nThe computer uses math to compute the answer directly, because it is much faster.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run a Simple Regression\nreg <- lm(y~x, dat=xy)\ncoef(reg)\n## (Intercept)           x \n##  6.41594246  0.02093466\n\n# Manual verification\nx <- xy[,'x']\ny <- xy[,'y']\nb1 <- cov( x,y)/var(x)\nb1\n## [1] 0.02093466\nb0 <- mean(y) - b1*mean(x)\nb0\n## [1] 6.415942\n```\n:::\n\n\nOnce we have the coefficient, we can find the predictions\n\\begin{eqnarray}\n\\hat{y}_{i} &=& \\hat{b}_{0}+\\hat{b}_{1}\\hat{X}_{i}\\\\\n\\hat{e}_i &=& \\hat{Y}_{i}-\\hat{y}_{i}\n\\end{eqnarray}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Find predicted values and residuals\npredict(reg)\nresid(reg)\n\n# Manual verification\ny_hat <- b0+b1*x\ny_hat\n\ne <- y - y_hat\ne\n```\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nSuppose we have a dataset with $n = 3$ observations: $\\{(1,2), (2,2.5), (3,4)\\}$.\n\nWe can compute the regression coefficients and model predictions in five steps.\n\nStep 1. Sample Means\n\n\\begin{eqnarray}\n\\hat{M}_X &=& \\frac{1+2+3}{3} = 2 \\\\\n\\hat{M}_Y &=& \\frac{2 + 2.5 + 4}{3} = \\frac{17}{6}.\n\\end{eqnarray}\n\nStep 2. Covariance and Variance \n\n\\begin{array}{c|c|c|c|c|c|c|}\ni & X_i & Y_i & (X_i - \\hat{M}_X) & (Y_i - \\hat{M}_Y) & (X_i - \\hat{M}_X)(Y_i - \\hat{M}_Y) & (X_i - \\hat{M}_X)^2 \\\\ \\hline\n1 & 1 & 2 & -1 & -\\tfrac{5}{6} & \\tfrac{5}{6} & 1 \\\\\n2 & 2 & 2.5 & 0 & -\\tfrac{1}{3} & 0 & 0 \\\\\n3 & 3 & 4 & 1 & \\tfrac{7}{6} & \\tfrac{7}{6} & 1 \\\\\n\\end{array}\n\n\n\\begin{eqnarray}\n\\hat{C}_{XY} &=& \\sum_{i=1}^3 (X_i - \\hat{M}_X)(Y_i - \\hat{M}_Y) /3\n= [\\tfrac{5}{6} + \\tfrac{7}{6}]/3 = 2/3 \\\\\n\\hat{V}_X &=& \\sum_{i=1}^3 (X_i - \\hat{M}_X)^2 = [1 + 1]/3 = 2/3.\n\\end{eqnarray}\n\n\nStep 3. Slope and Intercept\n\n\\begin{eqnarray}\n\\hat{b}_1 = \\frac{\\hat{C}_{XY}}{\\hat{V}_X} = \\frac{2/3}{2/3} = 1,\n\\end{eqnarray}\n\n\\begin{eqnarray}\n\\hat{b}_0 = \\hat{M}_Y - \\hat{b}_1 \\hat{M}_X\n= \\frac{17}{6} - 2\n= \\frac{5}{6}.\n\\end{eqnarray}\n\n\\begin{eqnarray}\n\\hat{y}_i = \\hat{b}_0 + \\hat{b}_1 X_i \n          = \\frac{5}{6} + X_i.\n\\end{eqnarray}\n\n\nStep 4. Fitted Values and Residuals\n\n\\begin{eqnarray}\n\\begin{array}{c|c|c|c|c}\ni & X_i & Y_i & \\hat{y}_i = \\tfrac{5}{6} + X_i & \\hat{e}_i = Y_i - \\hat{y}_i \\\\ \\hline\n1 & 1 & 2   & \\tfrac{11}{6} & \\tfrac{1}{6} \\\\\n2 & 2 & 2.5 & \\tfrac{17}{6} & -\\tfrac{1}{3} \\\\\n3 & 3 & 4   & \\tfrac{23}{6} & \\tfrac{1}{6} \\\\\n\\end{array}\n\\end{eqnarray}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxy0 <- data.frame(\n    x=c(1, 2, 3),\n    y=c(2, 2.5, 4))\nreg0 <- lm(y~x, dat=xy0)\n\ncoef(reg0)\n## (Intercept)           x \n##   0.8333333   1.0000000\npredict(reg0)\n##        1        2        3 \n## 1.833333 2.833333 3.833333\nresid(reg0)\n##          1          2          3 \n##  0.1666667 -0.3333333  0.1666667\n```\n:::\n\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-e1c0433bf40964115075\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-e1c0433bf40964115075\">{\"x\":{\"visdat\":{\"1aca675baf946d\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"1aca675baf946d\",\"attrs\":{\"1aca675baf946d\":{\"x\":{},\"y\":{},\"mode\":\"markers\",\"hoverinfo\":\"text\",\"marker\":{\"color\":\"#00000040\",\"size\":10},\"text\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\"},\"1aca675baf946d.1\":{\"x\":{},\"y\":{},\"hoverinfo\":\"none\",\"mode\":\"lines+markers\",\"type\":\"scatter\",\"color\":[\"black\"],\"line\":{\"width\":0.5},\"marker\":{\"symbol\":134,\"size\":5},\"inherit\":false}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"showlegend\":false,\"title\":\"Crime and Urbanization in America 1975\",\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Percent of People in an Urban Area\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Homicide Arrests per 100,000 People\"},\"hovermode\":\"closest\"},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"x\":[58,48,80,50,91,78,77,72,80,60,83,54,83,65,57,66,52,66,51,67,85,74,66,44,70,53,62,81,56,89,70,86,45,44,75,68,67,72,87,48,45,59,80,80,32,63,73,39,66,60],\"y\":[13.199999999999999,10,8.0999999999999996,8.8000000000000007,9,7.9000000000000004,3.2999999999999998,5.9000000000000004,15.4,17.399999999999999,5.2999999999999998,2.6000000000000001,10.4,7.2000000000000002,2.2000000000000002,6,9.6999999999999993,15.4,2.1000000000000001,11.300000000000001,4.4000000000000004,12.1,2.7000000000000002,16.100000000000001,9,6,4.2999999999999998,12.199999999999999,2.1000000000000001,7.4000000000000004,11.4,11.1,13,0.80000000000000004,7.2999999999999998,6.5999999999999996,4.9000000000000004,6.2999999999999998,3.3999999999999999,14.4,3.7999999999999998,13.199999999999999,12.699999999999999,3.2000000000000002,2.2000000000000002,8.5,4,5.7000000000000002,2.6000000000000001,6.7999999999999998],\"mode\":\"markers\",\"hoverinfo\":[\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\"],\"marker\":{\"color\":\"#00000040\",\"size\":10,\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"text\":[\"<b> Alabama <\\/b> <br>Urban  : 58 <br>Murder : 13.2 <br>Predicted Murder : 7.63 <br>Residual : 5.57\",\"<b> Alaska <\\/b> <br>Urban  : 48 <br>Murder : 10 <br>Predicted Murder : 7.42 <br>Residual : 2.58\",\"<b> Arizona <\\/b> <br>Urban  : 80 <br>Murder : 8.1 <br>Predicted Murder : 8.09 <br>Residual : 0.01\",\"<b> Arkansas <\\/b> <br>Urban  : 50 <br>Murder : 8.8 <br>Predicted Murder : 7.46 <br>Residual : 1.34\",\"<b> California <\\/b> <br>Urban  : 91 <br>Murder : 9 <br>Predicted Murder : 8.32 <br>Residual : 0.68\",\"<b> Colorado <\\/b> <br>Urban  : 78 <br>Murder : 7.9 <br>Predicted Murder : 8.05 <br>Residual : -0.15\",\"<b> Connecticut <\\/b> <br>Urban  : 77 <br>Murder : 3.3 <br>Predicted Murder : 8.03 <br>Residual : -4.73\",\"<b> Delaware <\\/b> <br>Urban  : 72 <br>Murder : 5.9 <br>Predicted Murder : 7.92 <br>Residual : -2.02\",\"<b> Florida <\\/b> <br>Urban  : 80 <br>Murder : 15.4 <br>Predicted Murder : 8.09 <br>Residual : 7.31\",\"<b> Georgia <\\/b> <br>Urban  : 60 <br>Murder : 17.4 <br>Predicted Murder : 7.67 <br>Residual : 9.73\",\"<b> Hawaii <\\/b> <br>Urban  : 83 <br>Murder : 5.3 <br>Predicted Murder : 8.15 <br>Residual : -2.85\",\"<b> Idaho <\\/b> <br>Urban  : 54 <br>Murder : 2.6 <br>Predicted Murder : 7.55 <br>Residual : -4.95\",\"<b> Illinois <\\/b> <br>Urban  : 83 <br>Murder : 10.4 <br>Predicted Murder : 8.15 <br>Residual : 2.25\",\"<b> Indiana <\\/b> <br>Urban  : 65 <br>Murder : 7.2 <br>Predicted Murder : 7.78 <br>Residual : -0.58\",\"<b> Iowa <\\/b> <br>Urban  : 57 <br>Murder : 2.2 <br>Predicted Murder : 7.61 <br>Residual : -5.41\",\"<b> Kansas <\\/b> <br>Urban  : 66 <br>Murder : 6 <br>Predicted Murder : 7.8 <br>Residual : -1.8\",\"<b> Kentucky <\\/b> <br>Urban  : 52 <br>Murder : 9.7 <br>Predicted Murder : 7.5 <br>Residual : 2.2\",\"<b> Louisiana <\\/b> <br>Urban  : 66 <br>Murder : 15.4 <br>Predicted Murder : 7.8 <br>Residual : 7.6\",\"<b> Maine <\\/b> <br>Urban  : 51 <br>Murder : 2.1 <br>Predicted Murder : 7.48 <br>Residual : -5.38\",\"<b> Maryland <\\/b> <br>Urban  : 67 <br>Murder : 11.3 <br>Predicted Murder : 7.82 <br>Residual : 3.48\",\"<b> Massachusetts <\\/b> <br>Urban  : 85 <br>Murder : 4.4 <br>Predicted Murder : 8.2 <br>Residual : -3.8\",\"<b> Michigan <\\/b> <br>Urban  : 74 <br>Murder : 12.1 <br>Predicted Murder : 7.97 <br>Residual : 4.13\",\"<b> Minnesota <\\/b> <br>Urban  : 66 <br>Murder : 2.7 <br>Predicted Murder : 7.8 <br>Residual : -5.1\",\"<b> Mississippi <\\/b> <br>Urban  : 44 <br>Murder : 16.1 <br>Predicted Murder : 7.34 <br>Residual : 8.76\",\"<b> Missouri <\\/b> <br>Urban  : 70 <br>Murder : 9 <br>Predicted Murder : 7.88 <br>Residual : 1.12\",\"<b> Montana <\\/b> <br>Urban  : 53 <br>Murder : 6 <br>Predicted Murder : 7.53 <br>Residual : -1.53\",\"<b> Nebraska <\\/b> <br>Urban  : 62 <br>Murder : 4.3 <br>Predicted Murder : 7.71 <br>Residual : -3.41\",\"<b> Nevada <\\/b> <br>Urban  : 81 <br>Murder : 12.2 <br>Predicted Murder : 8.11 <br>Residual : 4.09\",\"<b> New Hampshire <\\/b> <br>Urban  : 56 <br>Murder : 2.1 <br>Predicted Murder : 7.59 <br>Residual : -5.49\",\"<b> New Jersey <\\/b> <br>Urban  : 89 <br>Murder : 7.4 <br>Predicted Murder : 8.28 <br>Residual : -0.88\",\"<b> New Mexico <\\/b> <br>Urban  : 70 <br>Murder : 11.4 <br>Predicted Murder : 7.88 <br>Residual : 3.52\",\"<b> New York <\\/b> <br>Urban  : 86 <br>Murder : 11.1 <br>Predicted Murder : 8.22 <br>Residual : 2.88\",\"<b> North Carolina <\\/b> <br>Urban  : 45 <br>Murder : 13 <br>Predicted Murder : 7.36 <br>Residual : 5.64\",\"<b> North Dakota <\\/b> <br>Urban  : 44 <br>Murder : 0.8 <br>Predicted Murder : 7.34 <br>Residual : -6.54\",\"<b> Ohio <\\/b> <br>Urban  : 75 <br>Murder : 7.3 <br>Predicted Murder : 7.99 <br>Residual : -0.69\",\"<b> Oklahoma <\\/b> <br>Urban  : 68 <br>Murder : 6.6 <br>Predicted Murder : 7.84 <br>Residual : -1.24\",\"<b> Oregon <\\/b> <br>Urban  : 67 <br>Murder : 4.9 <br>Predicted Murder : 7.82 <br>Residual : -2.92\",\"<b> Pennsylvania <\\/b> <br>Urban  : 72 <br>Murder : 6.3 <br>Predicted Murder : 7.92 <br>Residual : -1.62\",\"<b> Rhode Island <\\/b> <br>Urban  : 87 <br>Murder : 3.4 <br>Predicted Murder : 8.24 <br>Residual : -4.84\",\"<b> South Carolina <\\/b> <br>Urban  : 48 <br>Murder : 14.4 <br>Predicted Murder : 7.42 <br>Residual : 6.98\",\"<b> South Dakota <\\/b> <br>Urban  : 45 <br>Murder : 3.8 <br>Predicted Murder : 7.36 <br>Residual : -3.56\",\"<b> Tennessee <\\/b> <br>Urban  : 59 <br>Murder : 13.2 <br>Predicted Murder : 7.65 <br>Residual : 5.55\",\"<b> Texas <\\/b> <br>Urban  : 80 <br>Murder : 12.7 <br>Predicted Murder : 8.09 <br>Residual : 4.61\",\"<b> Utah <\\/b> <br>Urban  : 80 <br>Murder : 3.2 <br>Predicted Murder : 8.09 <br>Residual : -4.89\",\"<b> Vermont <\\/b> <br>Urban  : 32 <br>Murder : 2.2 <br>Predicted Murder : 7.09 <br>Residual : -4.89\",\"<b> Virginia <\\/b> <br>Urban  : 63 <br>Murder : 8.5 <br>Predicted Murder : 7.73 <br>Residual : 0.77\",\"<b> Washington <\\/b> <br>Urban  : 73 <br>Murder : 4 <br>Predicted Murder : 7.94 <br>Residual : -3.94\",\"<b> West Virginia <\\/b> <br>Urban  : 39 <br>Murder : 5.7 <br>Predicted Murder : 7.23 <br>Residual : -1.53\",\"<b> Wisconsin <\\/b> <br>Urban  : 66 <br>Murder : 2.6 <br>Predicted Murder : 7.8 <br>Residual : -5.2\",\"<b> Wyoming <\\/b> <br>Urban  : 60 <br>Murder : 6.8 <br>Predicted Murder : 7.67 <br>Residual : -0.87\"],\"type\":\"scatter\",\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"line\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[58,48,80,50,91,78,77,72,80,60,83,54,83,65,57,66,52,66,51,67,85,74,66,44,70,53,62,81,56,89,70,86,45,44,75,68,67,72,87,48,45,59,80,80,32,63,73,39,66,60],\"y\":[7.6301526724992739,7.4208060843020256,8.0907151665332204,7.4626754019414747,8.3209964135501941,8.0488458488937713,8.0279111900740467,7.9232378959754222,8.0907151665332204,7.6720219901387239,8.1535191429923959,7.5464140372203747,8.1535191429923959,7.7766952842373476,7.6092180136795484,7.797629943057073,7.5045447195809247,7.797629943057073,7.4836100607611993,7.8185646018767976,8.195388460631845,7.9651072136148713,7.797629943057073,7.3370674490231256,7.8813685783359722,7.5254793784006493,7.713891307778173,8.111649825352945,7.5882833548598239,8.279127095910745,7.8813685783359722,8.2163231194515696,7.3580021078428501,7.3370674490231256,7.9860418724345967,7.8394992606965221,7.8185646018767976,7.9232378959754222,8.2372577782712959,7.4208060843020256,7.3580021078428501,7.6510873313189984,8.0907151665332204,8.0907151665332204,7.0858515431864273,7.7348259665978985,7.9441725547951467,7.232394154924501,7.797629943057073,7.6720219901387239],\"hoverinfo\":[\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\"],\"mode\":\"lines+markers\",\"type\":\"scatter\",\"line\":{\"color\":\"rgba(0,0,0,1)\",\"width\":0.5},\"marker\":{\"color\":\"rgba(0,0,0,1)\",\"symbol\":134,\"size\":5,\"line\":{\"color\":\"rgba(0,0,0,1)\"}},\"textfont\":{\"color\":\"rgba(0,0,0,1)\"},\"error_y\":{\"color\":\"rgba(0,0,0,1)\"},\"error_x\":{\"color\":\"rgba(0,0,0,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n\n#### **Goodness of Fit**. {-}\nFirst, we qualitatively analyze the ''Goodness of fit'' of our model by plotting it against the data.\nFor a quantitative summary, we can also compute the linear correlation between the model predictions and the sample data: $\\hat{R}_{yY} = \\hat{C}_{yY}/[\\hat{S}_{y} \\hat{S}_{Y}]$. \nWith linear models, we typically compute the \"R-squared\" statistic directly, $[\\hat{R}_{yY}]^2$ using the sums of squared errors (Total, Explained, and Residual)\n\\begin{eqnarray}\n\\underbrace{\\sum_{i}(\\hat{Y}_{i}-\\hat{M}_{Y})^2}_\\text{TSS}\n=\\underbrace{\\sum_{i}(\\hat{y}_i-\\hat{M}_{Y})^2}_\\text{ESS}+\\underbrace{\\sum_{i}\\hat{e}_{i}^2}_\\text{RSS}\\\\\n\\hat{R}_{yY}^2 = \\frac{\\hat{ESS}}{\\hat{TSS}}=1-\\frac{\\hat{RSS}}{\\hat{TSS}}\n\\end{eqnarray}\nThis is also known as the \"coefficient of determination\". Intuitively, it is the percentage of all variation explained by our model, and must theorefore fall in $[0,1]$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manually Compute R2\nEhat <- resid(reg)\nRSS  <- sum(Ehat^2)\nY <- xy[,'y']\nTSS  <- sum((Y-mean(Y))^2)\nR2 <- 1 - RSS/TSS\nR2\n## [1] 0.00484035\n\n# Check R2\nsummary(reg)$r.squared\n## [1] 0.00484035\n\n# Double Check R2\nR <- cor(xy[,'y'], predict(reg))\nR^2\n## [1] 0.00484035\n```\n:::\n\n\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nSuppose you have data on education level and wages. Conduct a linear regression. Then summarize the model relationship and how well it fits the data. \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wooldridge)\nxy <- wage1[,c('educ','wage')]\n```\n:::\n\n:::\n\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nWe use the dataset $\\{(1,2), (2,2.5), (3,4)\\}$ to compute Goodness of fit.\n\nFrom before, we know that $\\hat{M}_Y = \\frac{17}{6}$ and the fitted values and residuals from the regression are\n\\begin{eqnarray}\n\\hat{y}_1 = \\frac{11}{6},\\qquad\n\\hat{y}_2 = \\frac{17}{6},\\qquad\n\\hat{y}_3 = \\frac{23}{6}.\\\\\n\\hat{e}_1 = \\frac{1}{6},\\qquad\n\\hat{e}_2 = -\\frac{1}{3},\\qquad\n\\hat{e}_3 = \\frac{1}{6}.\n\\end{eqnarray}\n\nStep 1. Total Sum of Squares (TSS)\n\n\\begin{eqnarray}\n\\begin{aligned}\n\\hat{Y}_1 - \\hat{M}_Y &= 2 - \\frac{17}{6} = -\\frac{5}{6},\\\\\n\\hat{Y}_2 - \\hat{M}_Y &= 2.5 - \\frac{17}{6} = -\\frac{1}{3},\\\\\n\\hat{Y}_3 - \\hat{M}_Y &= 4 - \\frac{17}{6} = \\frac{7}{6}.\n\\end{aligned}\n\\end{eqnarray}\n\n\\begin{eqnarray}\n\\begin{aligned}\n\\hat{TSS} &= \\sum_{i} (\\hat{Y}_i - \\hat{M}_Y)^2 \\\\\n&= \\left(-\\frac{5}{6}\\right)^2\n+ \\left(-\\frac{1}{3}\\right)^2\n+ \\left(\\frac{7}{6}\\right)^2 \\\\\n&= \\frac{25}{36} + \\frac{1}{9} + \\frac{49}{36}\n= \\frac{78}{36}\n= \\frac{13}{6}.\n\\end{aligned}\n\\end{eqnarray}\n\nStep 2. Explained Sum of Squares (ESS)\n\n\\begin{eqnarray}\n\\hat{y}_1 - \\hat{M}_Y = -1,\\qquad\n\\hat{y}_2 - \\hat{M}_Y = 0,\\qquad\n\\hat{y}_3 - \\hat{M}_Y = 1.\n\\end{eqnarray}\n\n\\begin{eqnarray}\n\\hat{ESS}\n= \\sum_i (\\hat{y}_i - \\hat{M}_Y)^2.\n= (-1)^2 + 0^2 + 1^2 = 2.\n\\end{eqnarray}\n\nStep 3. Residual Sum of Squares (RSS)\n\n\\begin{eqnarray}\n\\hat{RSS} \n= \\sum_{i} \\hat{e}_i^2\n= \\left(\\frac{1}{6}\\right)^2\n+ \\left(-\\frac{1}{3}\\right)^2\n+ \\left(\\frac{1}{6}\\right)^2.\n\\end{eqnarray}\n\n\\begin{eqnarray}\n\\hat{RSS}\n= \\frac{1}{36} + \\frac{1}{9} + \\frac{1}{36}\n= \\frac{1}{36} + \\frac{4}{36} + \\frac{1}{36}\n= \\frac{6}{36}\n= \\frac{1}{6}.\n\\end{eqnarray}\n\n\nStep 4. Check the Decomposition\n\n\\begin{eqnarray}\n\\hat{ESS} + \\hat{RSS}\n= 2 + \\frac{1}{6}\n= \\frac{12}{6} + \\frac{1}{6}\n= \\frac{13}{6}\n= \\hat{TSS}.\n\\end{eqnarray}\n\nStep 5. Coefficient of Determination\n\n\\begin{eqnarray}\n\\hat{R}_{yY}^2\n= \\frac{\\hat{ESS}}{\\hat{TSS}}\n= \\frac{2}{13/6}\n= \\frac{12}{13}\n\\approx 0.9\n\\end{eqnarray}\n\n(Verification)\n\\begin{eqnarray}\n1 - \\frac{\\hat{RSS}}{\\hat{TSS}}\n= 1 - \\frac{1/6}{13/6}\n= 1 - \\frac{1}{13}\n= \\frac{12}{13}.\n\\end{eqnarray}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute R2\nEhat0 <- resid(reg0)\nRSS0  <- sum(Ehat0^2)\nY0 <- xy0[,'y']\nTSS0  <- sum((Y0-mean(Y0))^2)\nR2 <- 1 - RSS0/TSS0\n\n# compare with our calculation: 12/13\n\n# compare with our intuitive benchmark\ncor(Y0, predict(reg0))\n## [1] 0.9607689\n```\n:::\n\n\n:::\n\n\n## Variability Estimates\n\nA regression coefficient is a statistic. And, just like all statistics, we can estimate it's variability using a\n\n* *standard error*: variability across different samples (c.f.  *standard deviation*: variability within a single sample.)\n* *confidence interval:* range your statistic varies across different samples.\n\nNote that values reported by your computer do not necessarily satisfy this definition. To calculate these statistics, we will estimate variability using the *data-driven* methods from <https://jadamso.github.io/Rbooks/01_07_Intervals.html>. (For some theoretical background, see also <https://www.sagepub.com/sites/default/files/upm-binaries/21122_Chapter_21.pdf>.) \n\n\n#### **Jackknife**. {-}\nWe first consider the simplest, the jackknife. In this procedure, we loop through each row of the dataset. And, in each iteration of the loop, we drop that observation from the dataset and reestimate the statistic of interest. We then calculate the standard deviation of the statistic across all subsamples to get jackknife standard errors and, assuming the data are approximately normal, use the quantiles to construct a CI. Alternatively, we can just take the percentiles of the jackknife distribution directly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Original OLS coefficient\nslope <-  coef(reg)[2]\n \n# Jackknife Sampling Distribution for OLS Coefficient\njack_coefs <- vector(length=nrow(xy))\nfor(i in seq_along(jack_coefs)){\n    xy_i <- xy[-i,]\n    reg_i <- lm(y~x, dat=xy_i)\n    slope_i <- coef(reg_i)[2]\n    jack_coefs[i] <- slope_i\n}\n\n\n# Plot Jackknife Sampling Distribution\n# + Percentile Confidence Interval\nhist(jack_coefs, breaks=25,\n    main='Jackknife Distribution with 95% CIs',\n    font.main=1, border=NA,\n    freq=F,\n    xlab=expression(hat(b)[-i]))\njack_ci_percentile <- quantile(jack_coefs, probs=c(.025,.975))\nabline(v=jack_ci_percentile, lty=2)\n\n# Normal Approx Confidence Interval\njack_se <- sd(jack_coefs)\njack_ci_normal <- qnorm(c(.025,.975), slope, jack_se)\nabline(v=jack_ci_normal, lty=3)\n```\n\n::: {.cell-output-display}\n![](02_13_SimpleRegression_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n#### **Bootstrap**. {-}\n\nThere are several resampling techniques. The other main one is the bootstrap, which resamples with replacement for an arbitrary number of iterations. When bootstrapping a dataset with $n$ observations, you randomly resample all $n$ rows in your data set $B$ times. We then calculate the standard deviation of the statistic across all boostrap samples to get bootstrap standard errors and, assuming the data are approximately normal, use the quantiles to construct a CI. Alternatively, we can just take the percentiles of the bootstrap distribution.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap Sampling Distribution\nboot_coefs <- vector(length=399)\nfor(i in seq_along(boot_coefs)){\n    b_id <- sample( nrow(xy), replace=T)\n    xy_b <- xy[b_id,]\n    reg_b <- lm(y~x, dat=xy_b)\n    slope_b <- coef(reg_b)[2]\n    boot_coefs[i] <- slope_b\n}\n\n# Plot Bootstrap Sampling Distribution\n# + Percentile CI\nhist(boot_coefs, breaks=25,\n    main='Bootstrap Distribution with 95% CIs',\n    font.main=1, border=NA,\n    freq=F,\n    xlab=expression(hat(b)[b]))\nboot_ci_percentile <- quantile(boot_coefs, probs=c(.025,.975))\nabline(v=boot_ci_percentile, lty=2)\n\n# Normal Approx Confidence Intervals\nboot_se <- sd(boot_coefs)\njack_ci_normal <- qnorm(c(.025,.975), slope, boot_se)\nabline(v=jack_ci_normal, lty=3)\n```\n\n::: {.cell-output-display}\n![](02_13_SimpleRegression_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n## Hypothesis Tests\n\nWe can also bootstrap other statistics, often to test a null hypothesis of \"no relationship\". We are rarely interested in computing standard errors and conducting hypothesis tests for simple linear regressions in practice, but work through the ideas with two variables before moving to analyze multiple variables.\n\n#### **Invert a CI**. {-}\n\nOne main way to conduct hypothesis tests is to examine whether a confidence interval contains a hypothesized value. Does the slope coefficient equal $0$? For reasons we won't go into in this class, we typically normalize the coefficient by its standard error: $\\hat{t} = \\frac{\\hat{b}}{\\hat{s}_{\\hat{b}}}$, where $\\hat{s}_{\\hat{b}}$ is the estimated standard error of the coefficient.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create t-stat with jackknife SE\nt_hat <- coef(reg)['x']/jack_se\n\nboot_t <-  vector(length=399)\nfor(b in seq_along(boot_t)){\n\t# Bootstrap data\n    b_id <- sample( nrow(xy), replace=T)\n    xy_b <- xy[b_id,]\n    # Redo regression\n    reg_b <- lm(y~x, dat=xy_b)\n    slope_b <- coef(reg_b)[2]\n\t# Redo Jackknife SEs\n\tjack_coefs_b <- vector(length=nrow(xy_b))\n\tfor(i in seq_along(jack_coefs_b)){\n\t\txy_b_i <- xy_b[-i,]\n\t\treg_b_i <- lm(y~x, dat=xy_b_i)\n\t\tslope_b_i <- coef(reg_b_i)[2]\n\t\tjack_coefs_b[i] <- slope_b_i\n\t}\n\tjack_se_b <- sd(jack_coefs_b)\n\t# Redo t with Jackknife SE\n    t_hat_b <- slope_b/jack_se_b\n\tboot_t[b] <- t_hat_b\n}\n\nhist(boot_t, breaks=25,\n    main='Bootstrap t with Jackknife SE',\n    font.main=1, border=NA, freq=F,\n    xlab=expression(hat(t)[b]), \n    xlim=range(c(0, boot_t)) )\nabline(v=quantile(boot_t, probs=c(.025,.975)), lty=2)\nabline(v=0, col=\"red\", lwd=2)\n```\n\n::: {.cell-output-display}\n![](02_13_SimpleRegression_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nSuppose you have data on education level and wages. Conduct a linear regression. Then construct a $95\\%$ confidence interval for the slope coefficient and test the hypothesis of no relationship.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wooldridge)\nxy <- wage1[,c('educ','wage')]\n```\n:::\n\n:::\n\n#### **Impose the Null**.{-}\nWe can also compute a null distribution. We focus on the simplest: simulations that each impose the null hypothesis and re-estimate the statistic of interest. Specifically, we compute the distribution of $t$-values on data with randomly reshuffled outcomes (imposing the null), and compare how extreme the observed value is. We can sample with replacement (i.e., the null bootstrap) or without (permutation), just as with the [correlation](https://jadamso.github.io/Rbooks/02_12_Associations.html#cardinal-data) statistic.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Null Distribution for Reg Coef\nnull_t <-  vector(length=399)\nfor(b in seq_along(null_t)){\n\t# Permuted data\n    xy_b <- xy\n    xy_b[,'y'] <- sample( xy_b[,'y'], replace=F) #Bootstrap: replace=T\n    reg_b <- lm(y~x, dat=xy_b)\n    # Redo regression\n    reg_b <- lm(y~x, dat=xy_b)\n    slope_b <- coef(reg_b)[2]\n\t# Redo Jackknife SEs\n\tjack_coefs_b <- vector(length=nrow(xy_b))\n\tfor(i in seq_along(jack_coefs_b)){\n\t\txy_b_i <- xy_b[-i,]\n\t\treg_b_i <- lm(y~x, dat=xy_b_i)\n\t\tslope_b_i <- coef(reg_b_i)[2]\n\t\tjack_coefs_b[i] <- slope_b_i\n\t}\n\t# Redo t with Jackknife SE\n    t_hat_b <- slope_b/ sd(jack_coefs_b)\n\tnull_t[b] <- t_hat_b\n}\n\n# Null Distribution\nhist(null_t, breaks=25,\n    main='Null Distribution',\n    font.main=1, border=NA, freq=F,\n    xlab=expression(hat(t)[b]),\n    xlim=range(null_t))\n# 95% CI\nnull_ci <- quantile(null_t, probs=c(.025,.975))\nabline(v=null_ci, lty=2)\n# Observed value\nabline(v=t_hat, col=\"red\", lwd=2)\n```\n\n::: {.cell-output-display}\n![](02_13_SimpleRegression_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nPermutations are common when testing against \"no association\". From permuted data, we can calculate a *$p$-value*: the probability you would see something as at least as extreme as your statistic under the null (assuming your null hypothesis was true, see <https://jadamso.github.io/Rbooks/01_08_HypothesisTests.html>). We calculate the $p$-value directly from the null distribution. Smaller $p$-values suggest more evidence against the null.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Two Sided Test for P(t > jack_t or t < -jack_t | Null of t=0)\nThat_NullDist2 <- ecdf(abs(null_t))\nPhat2  <-  1-That_NullDist2( abs(t_hat))\nPhat2\n## [1] 0.5839599\nplot(That_NullDist2, xlim=range(null_t, t_hat),\n    xlab=expression( abs(hat(t)[b]) ),\n    main='Null Distribution', font.main=1)\nabline(v=t_hat, col='red')\n```\n\n::: {.cell-output-display}\n![](02_13_SimpleRegression_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nSuppose you have data on grades completed and wages. Conduct a linear regression. Then compute a $p$-value for the null hypothesis of no relationship between education level and wages.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wooldridge)\nxy <- wage1[,c('educ','wage')]\n```\n:::\n\n:::\n\n\n#### **Caveats**.{-}\n\nWe could also use hard decision rules, such as \"$p < 0.05$ is a statistically significant finding\". However, just like confidence intervals, those hard decision rules can be sensitive to somewhat arbitrary choices (Why Jackknife SE's instead of classical ones? Why test at the $95\\%$ level rather than $90\\%$?)\n\n\"No association\" is the most common null hypothesis in practice, but we can also use bootstrapping to test against other specific hypothesis: $\\beta$. To impose the null in this case, you recenter the sampling distribution around the hypothetical value; $\\hat{t} = \\frac{\\hat{b} - \\beta}{\\hat{s}_{\\hat{b}}}$.^[Under some additional assumptions, the null distribution follows a $t$-distribution. (For more on parametric t-testing based on statistical theory, see <https://www.econometrics-with-r.org/4-lrwor.html>.)]  \n\nAlthough two-sided hypothesis tests are most common, you can also test one-sided hypotheses.\n\n## Association is not Causation\n\nThe same caveats about \"correlation is not causation\" extend to regression. You may be tempted to use the term \"the effect\", but that interpretation of a regression coefficient assumes the linear model is true. If you fit a line to a non-linear relationship, then you will still get back a coefficient even though there is no singular *the* effect: the true relationship is non-linear! Also consider a classic example, Anscombe's Quartet, which shows four very different datasets that give the same linear regression coefficient. Notice that you understand the problem because we used scatterplots to visual the data.^[The same principles holds when comparing two groups: <http://www.stat.columbia.edu/~gelman/research/published/causal_quartet_second_revision.pdf>]\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Anscombe's Quartet \n\npar(mfrow=c(2,2))\nfor(i in 1:4){\n    xi <- anscombe[,paste0('x',i)]\n    yi <- anscombe[,paste0('y',i)]\n    plot(xi, yi, ylim=c(4,13), xlim=c(4,20),\n        pch=16, col=grey(0,.6))\n    reg <- lm(yi~xi)\n    b <- round(coef(reg)[2],2)\n    p <- round(summary(reg)$coefficients[2,4],4)\n    abline(reg, col='orange')\n    title(paste0(\"Slope=\", b,', p=',p), font.main=1)\n}\n```\n\n::: {.cell-output-display}\n![](02_13_SimpleRegression_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n## For an even better example, see `Datasaurus Dozen'F\n#browseURL(\n#'https://bookdown.org/paul/applied-data-visualization/\n#why-look-the-datasaurus-dozen.html')\n```\n:::\n\n\nIt is true that linear regression \"is the best linear predictor of the nonlinear regression function if the mean-squared error is used as the loss function.\" \\parencite[p.92]{camerontrivedi2005} But this is not a carte-blanche justification for OLS, as the best of the bad predictors is still a bad predictor. For many economic applications, it is more helpful to think and speak of \"dose response curves\" instead of \"the effect\".\n\nWhile adding interaction terms or squared terms allows one incorporate heterogeneity and non-linearity, they change several features of the model (most of which are not intended). Often, there are nonsensical predicted values. For example, if the most of your age data are between $[23,65]$, a quadratic term can imply silly things for people aged $10$ or $90$. \n\nNonetheless, linear regression provides an important piece of quantitative information that is understood by many. All models are an approximation, and sometimes only unimportant nuances are missing from a vanilla linear model. Other times, that model can be seriously misleading. (This is especially true if your making policy recommendations based on a universal \"the effect\".) As an exploratory tool, linear regession is a good guess but one whose point estimates should not be taken too seriously (in which case, the standard errors are also much less important).  Before trying to find a regression specification that makes sense for the entire dataset, explore local relationships.\n",
    "supporting": [
      "02_13_SimpleRegression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/htmltools-fill-0.5.9/fill.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"site_libs/plotly-binding-4.12.0/plotly.js\"></script>\n<script src=\"site_libs/typedarray-0.1/typedarray.min.js\"></script>\n<script src=\"site_libs/jquery-3.5.1/jquery.min.js\"></script>\n<link href=\"site_libs/crosstalk-1.2.2/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/crosstalk-1.2.2/js/crosstalk.min.js\"></script>\n<link href=\"site_libs/plotly-htmlwidgets-css-2.25.2/plotly-htmlwidgets.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/plotly-main-2.25.2/plotly-latest.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}