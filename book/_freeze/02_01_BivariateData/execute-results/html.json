{
  "hash": "a5e130c5c7f7cc36f41d1195ff64ff00",
  "result": {
    "engine": "knitr",
    "markdown": "\n# Bivariate Data\n***\n\nWe will now study them in more detail. Suppose we have two discrete variables $X_{i}$ and $Y_{i}$. The data for each observation data can be grouped together as a vector $(X_{i}, Y_{i})$.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bivariate Data from USArrests\nxy <- USArrests[,c('Murder','UrbanPop')]\nxy[1,]\n##         Murder UrbanPop\n## Alabama   13.2       58\n```\n:::\n\n\n## Types of Distributions\n\n#### **Joint Distributions**.{-}\n\nScatterplots are used frequently to summarize the joint relationship between two variables, multiple observations of $(X_{i}, Y_{i})$. They can be enhanced in several ways. As a default, use semi-transparent points so as not to hide any points (and perhaps see if your observations are concentrated anywhere). You can also add other features that help summarize the relationship, although I will defer this until later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(Murder~UrbanPop, USArrests, pch=16, col=grey(0.,.5))\n```\n\n::: {.cell-output-display}\n![](02_01_BivariateData_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nIf you have many points, you can also use a 2D histogram instead. <https://plotly.com/r/2D-Histogram/>.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(plotly)\nfig <- plot_ly(\n    USArrests, x = ~UrbanPop, y = ~Assault)\nfig <- add_histogram2d(fig, nbinsx=25, nbinsy=25)\nfig\n```\n:::\n\n\n#### **Marginal Distributions**.{-}\nYou can also show the distributions of each variable along each axis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setup Plot\nlayout( matrix(c(2,0,1,3), ncol=2, byrow=TRUE),\n    widths=c(9/10,1/10), heights=c(1/10,9/10))\n\n# Scatterplot\npar(mar=c(4,4,1,1))\nplot(Murder~UrbanPop, USArrests, pch=16, col=rgb(0,0,0,.5))\n\n# Add Marginals\npar(mar=c(0,4,1,1))\nxhist <- hist(USArrests[,'UrbanPop'], plot=FALSE)\nbarplot(xhist[['counts']], axes=FALSE, space=0, border=NA)\n\npar(mar=c(4,0,1,1))\nyhist <- hist(USArrests[,'Murder'], plot=FALSE)\nbarplot(yhist[['counts']], axes=FALSE, space=0, horiz=TRUE, border=NA)\n```\n\n::: {.cell-output-display}\n![](02_01_BivariateData_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n#### **Conditional Distributions**.{-}\n\nWe can show how distributions and densities change according to a second (or even third) variable using data splits. E.g., \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tailored Histogram \nylim <- c(0,8)\nxbks <-  seq(min(USArrests[,'Murder'])-1, max(USArrests[,'Murder'])+1, by=1)\n\n# Also show more information\n# Split Data by Urban Population above/below mean\npop_mean <- mean(USArrests[,'UrbanPop'])\npop_cut <- USArrests[,'UrbanPop']< pop_mean\nmurder_lowpop <- USArrests[pop_cut,'Murder']\nmurder_highpop <- USArrests[!pop_cut,'Murder']\ncols <- c(low=rgb(0,0,1,.75), high=rgb(1,0,0,.75))\n\npar(mfrow=c(1,2))\nhist(murder_lowpop,\n    breaks=xbks, col=cols[1],\n    main='Urban Pop >= Mean', font.main=1,\n    xlab='Murder Arrests',\n    border=NA, ylim=ylim)\n\nhist(murder_highpop,\n    breaks=xbks, col=cols[2],\n    main='Urban Pop < Mean', font.main=1,\n    xlab='Murder Arrests',\n    border=NA, ylim=ylim)\n```\n\n::: {.cell-output-display}\n![](02_01_BivariateData_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nIt is sometimes it is preferable to show the ECDF instead. And you can glue various combinations together to convey more information all at once\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\n# Full Sample Density\nhist(USArrests[,'Murder'], \n    main='Density Function Estimate', font.main=1,\n    xlab='Murder Arrests',\n    breaks=xbks, freq=F, border=NA)\n\n# Split Sample Distribution Comparison\nF_lowpop <- ecdf(murder_lowpop)\nplot(F_lowpop, col=cols[1],\n    pch=16, xlab='Murder Arrests',\n    main='Distribution Function Estimates',\n    font.main=1, bty='n')\nF_highpop <- ecdf(murder_highpop)\nplot(F_highpop, add=T, col=cols[2], pch=16)\n\nlegend('bottomright', col=cols,\n    pch=16, bty='n', inset=c(0,.1),\n    title='% Urban Pop.',\n    legend=c('Low (<= Mean)','High (>= Mean)'))\n```\n\n::: {.cell-output-display}\n![](02_01_BivariateData_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple Interactive Scatter Plot\n# plot(Assault~UrbanPop, USArrests, col=grey(0,.5), pch=16,\n#    cex=USArrests[,'Murder']/diff(range(USArrests[,'Murder']))*2,\n#    main='US Murder arrests (per 100,000)')\n```\n:::\n\n\n\nYou can also split data into grouped boxplots in the same way\n\n::: {.cell}\n\n```{.r .cell-code}\nlayout( t(c(1,2,2)))\nboxplot(USArrests[,'Murder'], main='',\n    xlab='All Data', ylab='Murder Arrests')\n\n# K Groups with even spacing\nK <- 3\nUSArrests[,'UrbanPop_Kcut'] <- cut(USArrests[,'UrbanPop'],K)\nKcols <- hcl.colors(K,alpha=.5)\nboxplot(Murder~UrbanPop_Kcut, USArrests,\n    main='', col=Kcols,\n    xlab='Urban Population', ylab='')\n```\n\n::: {.cell-output-display}\n![](02_01_BivariateData_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# 4 Groups with equal numbers of observations\n#Qcuts <- c(\n#    '0%'=min(USArrests[,'UrbanPop'])-10*.Machine[['double.eps']],\n#    quantile(USArrests[,'UrbanPop'], probs=c(.25,.5,.75,1)))\n#USArrests[,'UrbanPop']_cut <- cut(USArrests[,'UrbanPop'], Qcuts)\n#boxplot(Murder~UrbanPop_cut, USArrests, col=hcl.colors(4,alpha=.5))\n```\n:::\n\n\n\n## Probability Theory\n\n#### **Definitions for Discrete Data**. {-}\nThe *joint distribution* is defined as\n\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y)\n\\end{eqnarray}\nVariables are *statistically independent* if $Prob(X_{i} = x, Y_{i} = y)= Prob(X_{i} = x) Prob(Y_{i} = y)$ for all $x, y$. Independance is sometimes assumed for mathematical simplicity, not because it generally fits data well.^[The same can be said about assuming normally distributed errors, although at least that can be motivated by the Central Limit Theorems.]\n\n\nThe *conditional distributions* are defined as\n\\begin{eqnarray}\nProb(X_{i} = x | Y_{i} = y) = \\frac{ Prob(X_{i} = x, Y_{i} = y)}{ Prob( Y_{i} = y )}\\\\\nProb(Y_{i} = y | X_{i} = x) = \\frac{ Prob(X_{i} = x, Y_{i} = y)}{ Prob( X_{i} = x )}\n\\end{eqnarray}\nThe *marginal distributions* are then defined as\n\\begin{eqnarray}\nProb(X_{i} = x) = \\sum_{y} Prob(X_{i} = x | Y_{i} = y) Prob( Y_{i} = y ) \\\\\nProb(Y_{i} = y) = \\sum_{x} Prob(Y_{i} = y | X_{i} = x) Prob( X_{i} = x ),\n\\end{eqnarray}\nwhich is also known as the *law of total probability*.\n\n#### **Fair Coin Flips Example**. {-}\n\nFor one example, Consider flipping two coins. Denoted each coin as $k \\in \\{1, 2\\}$, and mark whether \"heads\" is face up; $X_{ki}=1$ if Heads and $=0$ if Tails. Suppose both coins are \"fair\": $Prob(X_{i}=1)= 1/2$ and $Prob(Y_{i}=1|X_{i})=1/2$, then the four potential outcomes have equal probabilities. The joint distribution is \n\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y) &=& Prob(X_{i} = x) Prob(Y_{i} = y)\\\\\nProb(X_{i} = 0, Y_{i} = 0) &=& 1/2 \\times 1/2 = 1/4 \\\\\nProb(X_{i} = 0, Y_{i} = 1) &=& 1/4 \\\\\nProb(X_{i} = 1, Y_{i} = 0) &=& 1/4 \\\\\nProb(X_{i} = 1, Y_{i} = 1) &=& 1/4 .\n\\end{eqnarray}\nThe marginal distribution of the second coin is \n\\begin{eqnarray}\nProb(Y_{i} = 0) &=& Prob(Y_{i} = 0 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 0 | X_{i} = 1) Prob(X_{i}=1)\\\\\n&=& 1/2 \\times 1/2 + 1/2 \\times 1/2 = 1/2\\\\\nProb(Y_{i} = 1) &=& Prob(Y_{i} = 1 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 1 | X_{i} = 1) Prob(X_{i}=1)\\\\\n&=& 1/2 \\times 1/2 + 1/2 \\times 1/2 = 1/2\n\\end{eqnarray}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a 2x2 matrix for the joint distribution.\n# Rows correspond to X1 (coin 1), and columns correspond to X2 (coin 2).\nP_fair <- matrix(1/4, nrow = 2, ncol = 2)\nrownames(P_fair) <- c(\"X1=0\", \"X1=1\")\ncolnames(P_fair) <- c(\"X2=0\", \"X2=1\")\nP_fair\n##      X2=0 X2=1\n## X1=0 0.25 0.25\n## X1=1 0.25 0.25\n\n# Compute the marginal distributions.\n# Marginal for X1: sum across columns.\nP_X1 <- rowSums(P_fair)\nP_X1\n## X1=0 X1=1 \n##  0.5  0.5\n# Marginal for X2: sum across rows.\nP_X2 <- colSums(P_fair)\nP_X2\n## X2=0 X2=1 \n##  0.5  0.5\n\n# Compute the conditional probabilities Prob(X2 | X1).\ncond_X2_given_X1 <- matrix(0, nrow = 2, ncol = 2)\nfor (j in 1:2) {\n  cond_X2_given_X1[, j] <- P_fair[, j] / P_X1[j]\n}\nrownames(cond_X2_given_X1) <- c(\"X2=0\", \"X2=1\")\ncolnames(cond_X2_given_X1) <- c(\"given X1=0\", \"given X1=1\")\ncond_X2_given_X1\n##      given X1=0 given X1=1\n## X2=0        0.5        0.5\n## X2=1        0.5        0.5\n```\n:::\n\n\n#### **UnFair Coin Flips Example**. {-}\nConsider a second example, where the second coin is \"Completely Unfair\", so that it is always the same as the first. The outcomes generated with a Completely Unfair coin are the same as if we only flipped one coin.\n\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y) &=& Prob(X_{i} = x) \\mathbf{1}( x=y )\\\\\nProb(X_{i} = 0, Y_{i} = 0) &=& 1/2 \\\\\nProb(X_{i} = 0, Y_{i} = 1) &=& 0 \\\\\nProb(X_{i} = 1, Y_{i} = 0) &=& 0 \\\\\nProb(X_{i} = 1, Y_{i} = 1) &=& 1/2 .\n\\end{eqnarray}\nNote that $\\mathbf{1}(X_{i}=1)$ means $X_{i}= 1$ and $0$ if $X_{i}\\neq0$.\nThe marginal distribution of the second coin is \n\\begin{eqnarray}\nProb(Y_{i} = 0) \n&=& Prob(Y_{i} = 0 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 0 | X_{i} = 1) Prob(X_{i} = 1)\\\\\n&=& 1/2 \\times 1 + 0 \\times 1/2 = 1/2 .\\\\\nProb(Y_{i} = 1)\n&=& Prob(Y_{i} = 1 | X_{i} =0) Prob( X_{i} = 0) + Prob(Y_{i} = 1 | X_{i} = 1) Prob( X_{i} = 1)\\\\\n&=& 0\\times 1/2 + 1 \\times 1/2 = 1/2 .\n\\end{eqnarray}\nwhich is the same as in the first example! Different joint distributions can have the same marginal distributions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the joint distribution matrix for the unfair coin case.\nP_unfair <- matrix(c(0.5, 0, 0, 0.5), nrow = 2, ncol = 2, byrow = TRUE)\nrownames(P_unfair) <- c(\"X1=0\", \"X1=1\")\ncolnames(P_unfair) <- c(\"X2=0\", \"X2=1\")\nP_unfair\n##      X2=0 X2=1\n## X1=0  0.5  0.0\n## X1=1  0.0  0.5\n\n# Compute the marginal distribution for X2 in the unfair case.\nP_X2_unfair <- colSums(P_unfair)\nP_X1_unfair <- rowSums(P_unfair)\n\n# Compute the conditional probabilities Prob(X1 | X2) for the unfair coin.\ncond_X2_given_X1_unfair <- matrix(NA, nrow = 2, ncol = 2)\nfor (j in 1:2) {\n  if (P_X1_unfair[j] > 0) {\n    cond_X2_given_X1_unfair[, j] <- P_unfair[, j] / P_X1_unfair[j]\n  }\n}\nrownames(cond_X2_given_X1_unfair) <- c(\"X2=0\", \"X2=1\")\ncolnames(cond_X2_given_X1_unfair) <- c(\"given X1=0\", \"given X1=1\")\ncond_X2_given_X1_unfair\n##      given X1=0 given X1=1\n## X2=0          1          0\n## X2=1          0          1\n```\n:::\n\n\n\n\n#### **Definitions for Continuous Data**. {-}\nThe *joint distribution* is defined as\n\\begin{eqnarray}\nF(x, y) &=& Prob(X_{i} \\leq x, Y_{i} \\leq y)\n\\end{eqnarray}\nThe *marginal distributions* are then defined as\n\\begin{eqnarray}\n F_{X}(x) &=& F(x, \\infty)\\\\\n F_{Y}(y) &=& F(\\infty, y).\n\\end{eqnarray}\nwhich is also known as the *law of total probability*.\nVariables are statistically independent if $F(x, y) = F_{X}(x)F_{Y}(y)$ for all $x, y$.\n\n\nFor example, suppose $(X_{i},Y_{i})$ is bivariate normal with  means $(\\mu_{X}, \\mu_{Y})$, variances $(\\sigma_{X}, \\sigma_{Y})$ and covariance $\\rho$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate Bivariate Data\nN <- 10000\nMu <- c(2,2) ## Means\n\nSigma1 <- matrix(c(2,-.8,-.8,1),2,2) ## CoVariance Matrix\nMVdat1 <- mvtnorm::rmvnorm(N, Mu, Sigma1)\n\nSigma2 <- matrix(c(2,.4,.4,1),2,2) ## CoVariance Matrix\nMVdat2 <- mvtnorm::rmvnorm(N, Mu, Sigma2)\n\npar(mfrow=c(1,2))\n## Different diagonals\nplot(MVdat2, col=rgb(1,0,0,0.02), pch=16,\n    main='Joint', font.main=1,\n    ylim=c(-4,8), xlim=c(-4,8), xlab='X1', ylab='X2')\npoints(MVdat1,col=rgb(0,0,1,0.02),pch=16)\n## Same marginal distributions\nxbks <- seq(-4,8,by=.2)\nhist(MVdat2[,2], col=rgb(1,0,0,0.5),\n    breaks=xbks, border=NA, xlab='X2',\n    main='Marginal', font.main=1)\nhist(MVdat1[,2], col=rgb(0,0,1,0.5),\n    add=T, breaks=xbks, border=NA)\n```\n\n::: {.cell-output-display}\n![](02_01_BivariateData_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# See that independent data are a special case\nn <- 2e4\n## 2 Indepenant RV\nXYiid <- cbind( rnorm(n),  rnorm(n))\n## As a single Joint Draw\nXYjoint <- mvtnorm::rmvnorm(n, c(0,0))\n## Plot\npar(mfrow=c(1,2))\nplot(XYiid, xlab=\n    col=grey(0,.05), pch=16, xlim=c(-5,5), ylim=c(-5,5))\nplot(XYjoint,\n    col=grey(0,.05), pch=16, xlim=c(-5,5), ylim=c(-5,5))\n\n# Compare densities\n#d1 <- dnorm(XYiid[,1],0)*dnorm(XYiid[,2],0)\n#d2 <- mvtnorm::dmvnorm(XYiid, c(0,0))\n#head(cbind(d1,d2))\n```\n:::\n\n\n\nThe multivariate normal is a workhorse for analytical work on multivariate random variables, but there are many more. See e.g., <https://cran.r-project.org/web/packages/NonNorMvtDist/NonNorMvtDist.pdf>\n\n\n#### **Important Applications**. {-}\n\nNote *Simpson's Paradox*:\n\nAlso note *Bayes' Theorem*:\n\\begin{eqnarray}\nProb(X_{i} = x | Y_{i} = y)  Prob( Y_{i} = y) \n    &=& Prob(X_{i} = x, Y_{i} = y) = Prob(Y_{i} = y | X_{i} = x) Prob(X_{i} = x).\\\\\nProb(X_{i} = x | Y_{i} = y)\n    &=& \\frac{ Prob(Y_{i} = y | X_{i} = x) Prob(X_{i}=x) }{ Prob( Y_{i} = y) }.\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Verify Bayes' theorem for the unfair coin case:\n# Compute Prob(X1=1 | X2=1) using the formula:\n#   Prob(X1=1 | X2=1) = [Prob(X2=1 | X1=1) * Prob(X1=1)] / Prob(X2=1)\n\nP_X1_1 <- 0.5\nP_X2_1_given_X1_1 <- 1  # Since coin 2 copies coin 1.\nP_X2_1 <- P_X2_unfair[\"X2=1\"]\n\nbayes_result <- (P_X2_1_given_X1_1 * P_X1_1) / P_X2_1\nbayes_result\n## X2=1 \n##    1\n```\n:::\n\n\n\n## Further Reading \n\nFor plotting histograms and marginal distributions, see \n\n* <https://www.r-bloggers.com/2011/06/example-8-41-scatterplot-with-marginal-histograms/>\n* <https://r-graph-gallery.com/histogram.html>\n* <https://r-graph-gallery.com/74-margin-and-oma-cheatsheet.html>\n* <https://jtr13.github.io/cc21fall2/tutorial-for-scatter-plot-with-marginal-distribution.html>\n\nMany introductory econometrics textbooks have a good appendix on probability and statistics. There are many useful statistical texts online too\n\nSee the Further reading about Probability Theory in the Statistics chapter.\n\n* <https://www.r-bloggers.com/2024/03/calculating-conditional-probability-in-r/>\n\n",
    "supporting": [
      "02_01_BivariateData_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}