{
  "hash": "956e0e7336554518edac7f0c9d154b52",
  "result": {
    "engine": "knitr",
    "markdown": "# Local Regression\n***\n\n## Local Averages\n\nScatterplots are a great and simplest plot for bivariate data that simply plots each observation. There are many extensions and similar tools. The example below helps understand how both the central tendency and dispersion change. \n\n::: {.cell}\n\n```{.r .cell-code}\n# Local relationship: wages and education\nlibrary(wooldridge)\n\n## Plot 1\nplot(wage~educ, data=wage1, pch=16, col=grey(0,.1))\neduc_means <- aggregate(wage~educ, data=wage1, mean)\npoints(wage~educ, data=educ_means, pch=17, col='blue', type='b')\ntitle(\"Grouped Means and Scatterplot\", font.main=1)\n```\n\n::: {.cell-output-display}\n![](02_14_LocalRegression_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n## Plot 2 (Alternative for big datasets)\n# boxplot(wage~educ, data=wage1, \n#    pch=16, col=grey(0,.1), varwidth=T)\n#title(\"Boxplots\", font.main=1)\n\n## Plot 3 (Less informative!)\n#barplot(wage~educ, data=educ_means)\n#title(\"Bar Plot of Grouped Means\", font.main=1)\n```\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nExamine the local relationship between 'Murder' and 'Urbanization' in the `USArrests` dataset. Use custom bins.\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\nxy[,'bins'] <- cut(USArrests[,'UrbanPop'], 6) \n```\n:::\n\n:::\n\nTo move beyond descriptive statistics, we will use models. We previously covered linear models, but you could be analyzing data with nonlinear relationships. Below, you will estimate the mean of $Y_{i}$ conditional on $X_{i}=x$ using models that do not assume the relationship is linear. Here we think of a general relationship:\n\\begin{eqnarray}\n\\hat{Y}_{i} = m(\\hat{X}_{i}) + e_{i},\n\\end{eqnarray}\nwhere $m$ is an unknown function and $e_{i}$ is white noise, which we estimate via different models.\n\n## Split Sample Regressions\n\n#### **Regressograms**.{-}\n\nJust as we use histograms to describe the distribution of a random variable, we can use a regressogram for *conditional* relationships. Specifically, we can use dummies for exclusive intervals or \"bins\" to estimate the average value of $Y$ for data inside the bin.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data\nplot(wage~educ, data=wage1, pch=16, col=grey(0,.1))\ndat <- wage1[order(wage1[,'educ']), c('wage','educ')]\n\n## Simple Regression\nreg  <- lm(wage~educ, data=dat) ## OLS\n\n# Regressogram: Course Age Bins\ndat[,'xcc'] <- cut(dat[,'educ'], 2)\nrgram_c  <- lm(wage~xcc, data=dat)\n\n# Regressogram: Fine Age Bins\ndat[,'xcf']   <- cut(dat[,'educ'], 3)\nrgram_f  <- lm(wage~xcf, data=dat)\n\n# Regressogram (Means for each level)\ndat[,'xd']   <- as.factor(dat[,'educ'])\nrgram_d  <- lm(wage~xd, data=dat)\n\n## Compare Models (Only 2 for simplicity)\nlines( dat[,'educ'], predict(reg), lwd=2, col=2)\nlines( dat[,'educ'], predict(rgram_f), lwd=2, col=4, type='s')\nlegend('topleft',\n    legend=c('Linear Regression','Regressogram (3)'),\n    col=c(2,4),\n    lty=1, cex=.8)\n```\n\n::: {.cell-output-display}\n![](02_14_LocalRegression_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nTo conduct a regressogram, first divide $X$ into $1,...L$ exclusive bins of width $h$. Each bin has a midpoint, $x$, and each observation has an associated dummy variable $\\hat{D}_{i}(x,h) = \\mathbf{1}\\left(\\hat{X}_{i} \\in \\left(x-h/2,x+h/2\\right] \\right)$. Then we conduct a regression with this model:\n\\begin{eqnarray}\n\\hat{Y}_{i} &=& \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} b_{0}(x,h) \\hat{D}_{i}(x,h)  + e_{i},\n\\end{eqnarray}\nwhere each bin has a coefficient $b_{0}(x,h)$. The optimal coefficients are denoted as $\\hat{b}_{0}(x,h)$ and the values predicted by the model are found as $\\hat{y}_{i} = \\sum_{x} \\hat{b}_{0}(x,h) \\hat{D}_{i}(x,h)$.\n\nWhen minimizing the sum of squared errors, we can find the optimal coefficients analytically. To do so, notice that each bin has $n(x,h) = \\sum_{i}^{n}\\hat{D}_{i}(x,h)$ observations. This means we can split the dataset into parts associated with each bin\n\\begin{eqnarray}\n\\label{eqn:regressogram1}\n\\sum_{i}^{n}\\left[e_{i}\\right]^2 \n&=& \\sum_{i}^{n}\\left[\\hat{Y}_{i}- \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} b_{0}(x,h) \\hat{D}_{i}(x,h) \\right]^2 \\\\\n&=& \\sum_{i}^{n(x_{1},h)}\\left[\\hat{Y}_{i}- \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} b_{0}(x,h) \\hat{D}_{i}(x,h) \\right]^2 \n\t+ ...  \\sum_{i}^{n(x_{L},h)}\\left[\\hat{Y}_{i}- \\sum_{x \\in \\{x_{1}, ..., x_{L} \\}} b_{0}(x,h) \\hat{D}_{i}(x,h) \\right]^2 \\\\\n&=& \\sum_{i}^{n(x_{1},h)}\\left[\\hat{Y}_{i}- b_{0}\\left(x_1,h\\right) \\right]^2 + ... \\sum_{i}^{n(x_L,h)}\\left[\\hat{Y}_{i}-b_{0}\\left(x_L,h\\right) \\right]^2 % +~ (N-1)\\sum_{i}\\hat{Y}_{i}.\n\\end{eqnarray}\nThis separation allows us to analytically optimize for each bin separately\n\\begin{eqnarray}\n\\label{eqn:regressogram2}\n\\min_{ \\left\\{ b_{0}(x,h) \\right\\} } \\sum_{i}^{n}\\left[e_{i}\\right]^2\n&=& \\min_{ \\left\\{ b_{0}(x,h) \\right\\} } \\sum_{i}^{n(x,h)}\\left[\\hat{Y}_{i}- b_{0}\\left(x,h\\right) \\right]^2,\n\\end{eqnarray}\nIn any case, minimizing yields the optimal coefficient as follows\n\\begin{eqnarray}\n0 &=& -2 \\sum_{i}^{n(x,h)}\\left[ \\hat{Y}_{i} - b_{0}(x,h)  \\right] \\\\\n\\hat{b}_{0}(x,h) &=& \\frac{\\sum_{i}^{n(x,h)} \\hat{Y}_{i}}{ n(x,h) } = \\hat{M}_{Y}(x,h) .\n\\end{eqnarray}\nAs such, the OLS regression yields coefficients that are interpreted as the conditional mean: $\\hat{M}_{Y}(x,h)$. We can directly compute the same statistic directly by simply takes the average value of $\\hat{Y}_{i}$ for all $i$ observations in a particular bin. \n\n:::{.callout-note icon=false collapse=\"true\"}\nConsider this two-bin regressogram example of how age affects wage for people with $\\leq 9$ years of school complete vs $> 9$. \n\\begin{eqnarray}\n\\text{Wage}_{i} &=& b_{0}(x=4.5, h=9) \\mathbf{1}\\left(\\text{Educ}_{i} \\in (0,9]\\right) + b_{0}(x=13.5, h=9) \\mathbf{1}\\left(\\text{Age}_{i} \\in (9,18] \\right) + e_{i}.\n\\end{eqnarray}\n\nHere is a simple example with three data points $(\\hat{X}_{i}, \\hat{Y}_{i}) \\in \\{ (1,2), (4,4), (12,3) \\}$, we can easily compute \n\\begin{eqnarray}\n n(x=4.5,h=9) &=& 2\\\\\n \\hat{b}_{0}(x=4.5, h=9) &=& [2 + 4] / 2 \\\\\n n(x=13.5,h=9) &=& 1\\\\\n \\hat{b}_{0}(x=13.5, h=9) &=& 3 / 1\n\\end{eqnarray}\n\nHere is a simple example with data\n\n::: {.cell}\n\n```{.r .cell-code}\npred_c <- predict(rgram_c)\npred_dat <- data.frame(xcc=dat$xcc, pred=round(pred_c,6))\ntable(pred_dat)\n##             pred\n## xcc          4.107544 6.113475\n##   (-0.018,9]       57        0\n##   (9,18]            0      469\n\n## Compare to simple aggregation \naggregate(wage~xcc, data=dat, mean)\n##          xcc     wage\n## 1 (-0.018,9] 4.107544\n## 2     (9,18] 6.113475\n```\n:::\n\n:::\n\n\n#### **Piecewise Regression**.{-}\nThe regressogram depicts locally constant relationships. We can also included slope terms within each bin to allow for locally linear relationships. This is often called *segmented/piecewise regression*, which runs a separate regression for different subsets of the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data\nplot(wage~educ, data=wage1, pch=16, col=grey(0,.1))\ndat <- wage1[order(wage1[,'educ']), c('wage','educ')]\n\n# Piecewise: Course Age Bins\ndat[,'xcc'] <- cut(dat[,'educ'], 2)\npreg_c  <- lm(wage~xcc*educ, data=dat)\n\n# Piecewise: Fine Age Bins\ndat[,'xcf']   <- cut(dat[,'educ'], 3) \npreg_f  <- lm(wage~xcf*educ, data=dat)\n\n## Compare Models\nlines( dat[,'educ'], predict(preg_c), lwd=2, col=5)\nlines( dat[,'educ'], predict(preg_f), lwd=2, col=6)\nlegend('topleft',\n    legend=c('2 bins','3 bins'),\n    lty=1, col=5:6, cex=.8)\ntitle('Piecewise Regressions', font.main=1)\n```\n\n::: {.cell-output-display}\n![](02_14_LocalRegression_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThe model is\n\\begin{eqnarray}\n\\hat{Y}_{i} &=& \\sum_{x} \\left[b_{0}(x,h) + b_{1}(x,h)\\hat{X}_{i} \\right] \\hat{D}_{i}(x,h) + e_{i}.\n\\end{eqnarray}\nThis same separation as above allows us to analytically optimize for each bin separately. I.e. we run separate regressions on the split samples. From the previous chapter, we know the solutions are\n\\begin{eqnarray}\n \\hat{b}_{0}(x,h) &=& \\hat{M}_{Y}(x,h)-\\hat{b}_{1}\\hat{M}_{X}(x,h)\\\\\n \\hat{b}_{1}(x,h) &=& \\frac{\\sum_{i}^{n(x,h)}(\\hat{X}_{i}-\\hat{M}_{X}(x,h))(\\hat{Y}_{i}-\\hat{M}_{Y}(x,h))}{\\sum_{i}^{n(x,h)}(\\hat{X}_{i}-\\hat{M}_{X}(x,h))^2} = \\frac{\\hat{C}_{XY}(x,h)}{\\hat{V}_{X}(x,h)},\n\\end{eqnarray}\n\n:::{.callout-note icon=false collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# See that two methods give the same predictions\n# Piecewise: Course Age Bins\ndat[,'xcc'] <- cut(dat[,'educ'], 2)\npreg_c  <- lm(wage~xcc*educ, data=dat)\npred2 <- predict(preg_c)\n\n## Split Sample Regressions\ndat2 <- split( dat, dat[,'xcc'])\npred2B <- lapply(dat2, function(d){\n    reg2 <- lm(wage~educ, d)\n    pred_d <- predict(preg_c)\n})\n\n# Any differences?\nall( pred2 - unlist(pred2B) < 1e-10 )\n## [1] TRUE\n```\n:::\n\n:::\n\n:::{.callout-tip icon=false collapse=\"true\"}\nCompare a simple regression to a regressogram and a piecewise regression. Examine the relationship between 'Murder' and 'Urbanization' in the `USArrests` dataset. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\ncolnames(xy) <- c('y','x')\n\n# Globally Linear\nreg <- lm(y~x, data=xy)\n```\n:::\n\n\nTry to do so on your own before looking at the code below, which compares two piecewise regressions to a simple linear regression.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Diagnose Fit\n#plot( fitted(reg), resid(reg), pch=16, col=grey(0,.5))\n#plot( xy[,'x'], resid(reg), pch=16, col=grey(0,.5))\n\n# Linear in 2 Pieces (subsets)\nxcut2 <- cut(xy[,'x'],2)\nxy_list2 <- split(xy, xcut2)\nregs2 <- lapply(xy_list2, function(xy_s){\n    lm(y~x, data=xy_s)\n})\n\n# Linear in 3 Pieces (subsets or bins)\nxcut3 <- cut(xy[,'x'], seq(32,92,by=20)) # Finer Bins\nxy_list3 <- split(xy, xcut3)\nregs3 <- lapply(xy_list3, function(xy_s){\n    lm(y~x, data=xy_s)\n})\n\n## Make Predictions\npred1 <- data.frame(yhat=predict(reg), x=reg[['model']][,'x'])\npred1 <- pred1[order(pred1[,'x']),]\n\npred2 <- lapply(regs2, function(reg){\n    data.frame(yhat=predict(reg), x=reg[['model']][,'x'])\n})\npred2 <- do.call(rbind,pred2)\npred2 <- pred2[order(pred2[,'x']),]\n\npred3 <- lapply(regs3, function(reg){\n    data.frame(yhat=predict(reg), x=reg[['model']][,'x'])\n})\npred3 <- do.call(rbind,pred3)\npred3 <- pred3[order(pred3[,'x']),]\n\n# Compare Predictions\nplot(y ~ x, pch=16, col=grey(0,.5), dat=xy)\nlines(yhat~x, pred1, lwd=2, col=2)\nlines(yhat~x, pred2, lwd=2, col=4)\nlines(yhat~x, pred3, lwd=2, col=3)\nlegend('topleft',\n    legend=c('Globally Linear', 'Piecewise Linear (2)','Piecewise Linear (3)'),\n    lty=1, col=c(2,4,3), cex=.8)\n```\n:::\n\n:::\n\nFor many things, a simple linear regression, regressograms, or piecewise regression is \"good enough\". Simple linear regressions struggle with nonlinear relationships but are very easy to run with a computer. Regressograms and piecewise regressions are intuitive ways to capture nonlinear relationships that are computationally efficient but have obvious problems where the bins change. Sometimes we want smoother predictions or to estimate derivatives (gradients). To cover more advanced regression methods that do those things, we will need to first learn about kernel density estimation.\n\n\n#### **Weighted Regression**.{-}\nInterestingly, we can obtain the same statistics from weighted least squares regression. For some specific design point, $x$, we can find $\\hat{b}(x, h)$ by minimizing\n\\begin{eqnarray}\n\\sum_{i}^{n}\\left[ e_{i} \\right]^2  \\hat{D}_{i}(x,h) &=& \\sum_{i}^{n}\\left[ \\hat{Y}_{i}- b_{0}(x,h) - b_{1}(x,h) \\hat{X}_{i} \\right]^2  \\hat{D}_{i}(x,h) \\\\\n&=& \\sum_{i}^{n(x_{1},h)}\\left[ \\hat{Y}_{i}- b_{0}(x_{1},h) - b_{1}(x_{1},h) \\hat{X}_{i}  \\right]^2  \\hat{D}_{i}(x_{1},h) + ... \\sum_{i}^{n(x_{L},h)}\\left[ \\hat{Y}_{i}- b_{0}(x_{L},h) - b_{1}(x_{L},h) \\hat{X}_{i}  \\right]^2  \\hat{D}_{i}(x_{L},h) \\\\\n&=& \\sum_{i}^{n(x,h)}\\left[\\hat{Y}_{i}- b_{0}\\left(x,h\\right) - b_{1}(x,h) \\hat{X}_{i}  \\right]^2 \n\\end{eqnarray}\n\n\nWe get nearly identical results if we instead use \"uniform weights\" with half-width $h_{2}=h/2$,\n\\begin{eqnarray}\nk_{U}\\left( \\hat{X}_{i}, x, h \\right) &=& \\mathbf{1}\\left( |\\hat{X}_{i} - x| < h_{2} \\right) / h_{2},\n\\end{eqnarray}\nwith is nearly identical to $\\hat{D}_{i}(x,h)/ h_{2}$, but uses open intervals $()$ instead of $(]$. As such we can see that\n\\begin{eqnarray}\n\\sum_{i}^{n}\\left[ e_{i} \\right]^2 k_{U}\\left( \\hat{X}_{i}, x, h \\right) \n&\\approx& \\sum_{i}^{n}\\left[ e_{i} \\right]^2  \\frac{\\hat{D}_{i}(x,h)}{h_{2}} \\\\\n&=& \\sum_{i}^{n(x,h)}\\left[\\hat{Y}_{i}- b_{0}\\left(x,h\\right) - b_{1}(x,h) \\hat{X}_{i}  \\right]^2 \\frac{1}{h_{2}},\n\\end{eqnarray}\nThe constant term $\\frac{1}{h_{2}}$ is irrelevant to finding the optimal solution (you can check the math yourself).\n\n:::{.callout-tip icon=false collapse=\"true\"}\nHere is an example going into the details of the weights.\n\n::: {.cell}\n\n```{.r .cell-code}\n## Generate Sample Data\nx <- 1:5\ny <- rnorm(length(x))\n## plot(x,y)\n\n## Manually Compute Estimate at x=3\nh2 <- 1\nw3 <- dunif( abs(x-3)/h2 )/ h2 #(x >= 2)*(x <= 4)/ h2\nyhat_3 <- sum(w3*y)\nyhat_3\n## [1] 0.6319501\n```\n:::\n\n\nHere is an example with real data\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(wooldridge) # Data from before\n#dat <- wage1[order(wage1[,'educ']), c('wage','educ')]\n\n## Local Linear\nx <- 4.5\nh2 <- 9.1/2\nk <- abs(dat[,'educ']-x) / h2\nk_weights <- dunif(k) / h2\npreg_k  <- lm(wage~educ, data=dat, weights=k_weights)\npredict(preg_k, newdata=data.frame(educ=x))\n##        1 \n## 3.911792\n\n# Compare to Piecewise\ndat[,'xcc'] <- cut(dat[,'educ'], 2)\npreg_c  <- lm(wage~xcc*educ, data=dat)\npredict(preg_c, newdata=data.frame(educ=x, xcc='(-0.018,9]'))\n##        1 \n## 3.911792\n```\n:::\n\n:::\n\n\n## Local Linear Regression\n\n#### **Locally Constant (Moving Average)**.{-}\n\nConsider a point $x$ and model $\\hat{Y}_{i} = b(x, h_{2}) + e_{i}$ locally around $x$. Then notice a weighted OLS estimator with uniform kernel weights yields\n\\begin{eqnarray} \\label{eqn:lcls}\n& & \\min_{b(x,h)}~ \\sum_{i}^{n}\\left[e_{i} \\right]^2 k_{U}\\left( \\hat{X}_{i}, x, h_{2} \\right) \\\\\n\\Rightarrow & & -2 \\sum_{i}^{n}\\left[\\hat{Y}_{i}- b(x, h_{2}) \\right] k_{U}\\left(\\hat{X}_{i}, x, h_{2}\\right) = 0\\\\\n\\label{eqn:lcls1}\n\\Rightarrow & & \\hat{b}_{U}(x) \n= \\frac{\\sum_{i} \\hat{Y}_{i} k_{U} \\left( \\hat{X}_{i}, x, h_{2} \\right) }{ \\sum_{i} k_{U}\\left( \\hat{X}_{i}, x, h_{2} \\right) } \n= \\sum_{i} \\hat{Y}_{i} \\left[ \\frac{ k_{U} \\left( \\hat{X}_{i}, x, h_{2} \\right) }{ \\sum_{i} k_{U}\\left( \\hat{X}_{i}, x, h_{2} \\right)} \\right]\n= \\sum_{i} \\hat{Y}_{i} w_{i}(x, h_{2}),\n\\end{eqnarray}\nwhere weights are determined from\n\\begin{eqnarray}\n\\sum_{i}^{n} k_{U} \\left( \\hat{X}_{i}, x, h_{2} \\right) &=& \\sum_{i}^{n(x,h)} \\frac{1}{h_{2}} = \\frac{n(x, h_{2})}{h_{2}}\\\\\nw_{i}(x, h_{2}) &=&  \\left[ \\frac{ k_{U} \\left( \\hat{X}_{i}, x, h_{2} \\right) }{ \\sum_{i} k_{U}\\left( \\hat{X}_{i}, x, h_{2} \\right)} \\right] \n= \\frac{\\mathbf{1}\\left( |\\hat{X}_{i} - x| < h_{2} \\right)/h_{2}}{ n(x, h_{2}) / h_{2} } \n= \\frac{\\mathbf{1}\\left( |\\hat{X}_{i} - x| < h_{2} \\right)}{n(x, h_{2})} \n\\end{eqnarray}\nSo locally constant kernel regression recovers the weighted mean of $Y_{i}$ around design point $x$. If we use exclusive bins, then we are running a regressogram, which is more crude but can be estimated easily.\n \n#### **Notes**.{-}\n\nWhen $n$ is small, $\\hat{b}_{U}(x, h_{2})$ is typically estimated for each unique observed value: $x \\in \\{ x_{1},...x_{n} \\}$. For large datasets, you can select a subset or evenly spaced values of $x$ for which to make predictions.\n\nIf $\\hat{X}_{i}$ represents time, then the local constant regressions is also called a *moving average*. With non-uniform kernel weights, we have a *weighted* moving average.\n\nThe basic idea also generalizes other kernels. As such, a kernel regression using uniform weights is often called a \"naive kernel regression\". Typically, kernel regressions use kernels that weight nearby observations more heavily. We can also add a slope term to improve the fit.\n\n\n#### **Locally Linear**.{-}\nA less simple case is a *local linear regression* which conducts a linear regression for each data point using a subsample of data around it. Consider a point $x$ and model $\\hat{Y}_{i} = b_{0}(x,h) + b_{1}(x) \\hat{X}_{i} + e_{i}$ for data near $x$. The weighted OLS estimator with kernel weights is\n\\begin{eqnarray}\n& & \\min_{b_{0}(x, h_{2}),b_{1}(x, h_{2})}~ \\sum_{i}^{n}\\left[\\hat{Y}_{i}- b_{0}(x, h_{2}) - b_{1}(x, h_{2}) \\hat{X}_{i} \\right]^2  k_{U}\\left( \\hat{X}_{i}, x, h_{2}\\right) \n\\end{eqnarray} \nDeriving the optimal values $\\hat{b}_{0}(x, h_{2})$ and  $\\hat{b}_{1}(x,h_{2})$ for $k_{U}$ is left as a homework exercise.^[One benefit of LLLS is that it is theoretically motivated: assuming that $Y_{i}=m(X_{i}) + \\epsilon_{i}$, we can then take a Taylor approximation: $m(X_{i}) + \\epsilon_{i} \\approx m(x) + m'(x)[X_{i}-x] + \\epsilon_{i} = [m(x) - m'(x)x ] + m'(x)X_{i} + \\epsilon_{i} = b_{0}(x,h) + b_{1}(x,h) X_{i}$.]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ``Naive\" Smoother\npred_fun <- function(x0, h){\n    # Assign equal weight to observations within h distance to x0\n    # 0 weight for all other observations\n    ki <- abs(dat[,'educ']-x0)/h\n    ki <- dunif(ki)/h ## Could change, e.g. dnorm(ki)/h\n    wi <- ki/sum(ki, na.rm=T) # always sum to 1\n    # run regression with weighted data\n    llls_i <- lm(wage~educ, data=dat, weights=wi)\n    yhat_i <- predict(llls_i, newdata=data.frame(educ=x0))\n}\n\nX0 <- seq(0,18, by=0.5) # Design points\n# Fine Bins\npred_lo1 <- vector(length=length(X0))\nfor(i in seq_along(X0)){\n\tpred_lo1[i] <- pred_fun(x0=X0[i], h=2)\n}\n# Course Bins\npred_lo2 <- vector(length=length(X0))\nfor(i in seq_along(X0)){\n\tpred_lo2[i] <- pred_fun(x0=X0[i], h=6)\n}\n\n# Plot\nplot(wage~educ, pch=16, data=wage1, col=grey(0,.1),\n    ylab='Murder Rate', xlab='Population Density')\ncols <- c(rgb(.8,0,0,.5), rgb(0,0,.8,.5))\nlines(X0, pred_lo1, col=cols[1], lwd=1, type='o')\nlines(X0, pred_lo2, col=cols[2], lwd=1, type='o')\nlegend('topleft', title='Locally Linear',\n    legend=c('h=2', 'h=6'),\n    lty=1, col=cols, cex=.8)\n```\n\n::: {.cell-output-display}\n![](02_14_LocalRegression_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nCompare local linear and local constant regressions using <https://shinyserv.es/shiny/kreg/>, with degree $0$ and $1$.\nAlso try different kernels and different datasets.\n:::\n\n:::{.callout-tip icon=false collapse=\"true\"}\nExamine the local relationship between 'Murder' and 'Urbanization' in the `USArrests` dataset using LLLS.\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\nX0 <- sort(unique(xy[,'UrbanPop']))\nplot(y~x, pch=16, data=xy, col=grey(0,.5),\n    ylab='Murder Rate', xlab='Population Density')\n\npred_lo1 <- sapply(X0, pred_fun, h=2, xy=xy)\n\npred_lo2 <- sapply(X0, pred_fun, h=20, xy=xy)\n\ncols <- c(rgb(.8,0,0,.5), rgb(0,0,.8,.5))\nlines(X0, pred_lo1, col=cols[1], lwd=1, type='o')\nlines(X0, pred_lo2, col=cols[2], lwd=1, type='o')\nlegend('topleft', title='Locally Linear',\n    legend=c('h=2 ', 'h=20'),\n    lty=1, col=cols, cex=.8)\n```\n:::\n\n:::\n\n\nAn important extension of locally linear regressions is called *loess*, which uses adaptive bandwidths in order to have a similar number of data points around each design point. This is especially useful when $X$ is not uniform.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Adaptive-width subsamples with non-uniform weights\ndat <- wage1[order(wage1[,'educ']), c('wage','educ')]\nplot(wage~educ, pch=16, col=grey(0,.1), data=dat)\n\nreg_lo4 <- loess(wage~educ, data=dat, span=.4)\nreg_lo8 <- loess(wage~educ, data=dat, span=.8)\n\ncols <- hcl.colors(3,alpha=.75)[-3]\nlines(dat[,'educ'], predict(reg_lo4),\n    col=cols[1], type='o', pch=2)\nlines(dat[,'educ'], predict(reg_lo8),\n    col=cols[2], type='o', pch=2)\n\nlegend('topleft', title='Loess',\n    legend=c('span=.4 ', 'span=.8'),\n    lty=1, col=cols, cex=.8)\n```\n\n::: {.cell-output-display}\n![](02_14_LocalRegression_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n\n",
    "supporting": [
      "02_14_LocalRegression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}