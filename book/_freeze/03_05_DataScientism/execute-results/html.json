{
  "hash": "2b524370c6bd3feffac58eeb6c745c60",
  "result": {
    "engine": "knitr",
    "markdown": "# Data Scientism\n***\n\nIn practice, it is hard to find a good natural experiment. For example, suppose we asked \"what is the effect of wages on police demanded?\" and examined a policy which lowered the educational requirements from 4 years to 2 to become an officer. This increases the labour supply, but it also affects the demand curve through \"general equilibrium\": as some of the new officers were potentially criminals and, with fewer criminals, the demand for police shifts down.\n\nIn practice, it is also easy to find a bad instrument. Paradoxically, natural experiments are something you are supposed to find but never search for. As you search for good instruments, for example, sometimes random noise will appear like a good instrument (spurious instruments). In this age of big data, we are getting increasingly more data and, perhaps surprisingly, this makes it easier to make false discoveries. \n\nWe will consider three classical ways for false discoveries to arise. After that, there are examples with the latest and greatest empirical recipes---we don't have so many theoretical results yet but I think you can understand the issue with the numerical example. Although it is difficult to express numerically, you must also know that if you search for a good natural experiment for too long, you can also be led astray from important questions. There are good reasons to be excited about empirical social science, but we would be wise to recall some earlier wisdom from economists on the matter.\n\n> The most reckless and treacherous of all theorists is he who professes to let facts and figures speak for themselves, who keeps in the background the part he has played, perhaps unconsciously, in selecting and grouping them\n>\n> ---  Alfred Marshall, 1885 \n\n\n> The blind transfer of the striving for quantitative measurements to a field where the specific conditions are not present which give it its basic importance in the natural sciences is the result of an entirely unfounded prejudice. It is probably responsible for the worst aberrations and absurdities produced by scientism in the social sciences. It not only leads frequently to the selection for study of the most irrelevant aspects of the phenomena because they happen to be measurable, but also to \"measurements\" and assignments of numerical values which are absolutely meaningless. What a distinguished philosopher recently wrote about psychology is at least equally true of the social sciences, namely that it is only too easy \"to rush off to measure something without considering what it is we are measuring, or what measurement means. In this respect some recent measurements are of the same logical type as Plato's determination that a just ruler is 729 times as happy as an unjust one.\"\n>\n> --- F.A. Hayek, 1943\n\n> if you torture the data long enough, it will confess\n>\n> --- R. Coase (Source Unknown)\n\n<!---\n''torture the data (to) confess'' (Coase,  Essays on economics and economists.  1995, p. 27)\n--->\n\n> the definition of a causal parameter is not always clearly stated, and formal statements of identifying conditions in terms of well-specified economic models are rarely presented. Moreover, the absence of explicit structural frameworks makes it difficult to cumulate knowledge across studies conducted within this framework. Many studies produced by this research program have a `stand alone' feature and neither inform nor are influenced by the general body of empirical knowledge in economics.\n>\n> --- J.J. Heckman, 2000\n\n\n> without explicit prior consideration of the effect of the instrument choice on the parameter being estimated, such a procedure is effectively the opposite of standard statistical practice in which a parameter of interest is defined first, followed by an estimator that delivers that parameter. Instead, we have a procedure in which the choice of the instrument, which is guided by criteria designed for a situation in which there is no heterogeneity, is implicitly allowed to determine the parameter of interest. This goes beyond the old story of looking for an object where the light is strong enough to see; rather, we have at least some control over the light but choose to let it fall where it may and then proclaim that whatever it illuminates is what we were looking for all along.\n>\n> --- A. Deaton, 2010\n\n\n## False Positives\n\n#### **Data Errors**. {-}\nA huge amount of data normally means a huge amount of data cleaning/merging/aggregating. This avoids many copy-paste errors, which are a recipe for [disaster](https://blog.hurree.co/8-of-the-biggest-excel-mistakes-of-all-time), but may also introduce other types of errors. Some spurious results are driven by honest errors in data cleaning. According to one [estimate](https://www.pnas.org/doi/10.1073/pnas.1212247109), this is responsible for around one fifth of all medical science retractions (there is even a whole [book](https://www.amazon.de/Much-Cost-Coding-Errors-Implementation/dp/1543772994) about this!). Although there are not similar meta-analysis in economics, there are some high-profile examples. This includes papers that are highly influential, like [Lott, Levitt](https://scienceblogs.com/deltoid/2005/12/02/lott-levitt-and-coding-errors) and [Reinhart and Rogoff](https://blogs.lse.ac.uk/impactofsocialsciences/2013/04/24/reinhart-rogoff-revisited-why-we-need-open-data-in-economics/) as well as others the top economics journals, like the [RESTUD](https://academic.oup.com/restud/article/90/2/1009/6982752) and [AER](https://www.aeaweb.org/articles?id=10.1257/aer.113.7.2053). There are some reasons to think such errors are more widespread across the social sciences; e.g., in [Census data](https://www2.census.gov/ces/tp/tp-2002-17.pdf) and [Aid data](https://www.sciencedirect.com/science/article/abs/pii/S0305750X11001951). So be careful!\n\nNote: one reason to plot your data is to help spot such errors.\n\n#### **P-Hacking**. {-}\nAnother class of errors pertains to P-hacking (and it's various synonyms: data drudging, star mining,....). While there are cases of fraudulent data manipulation (which can be considered as a dishonest data error), P-hacking need not even be intentional. You can simply be trying different variable transformations to uncover patterns in the data, for example, without accounting for how easy it is to find patterns when transforming  completely random data. P-hacking is [pernicious](https://elephantinthelab.org/a-replication-crisis-in-the-making/) and [widespread](https://www.americanscientist.org/article/the-statistical-crisis-in-science). \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# P-hacking OSLS with different explanatory vars\nset.seed(123)\nn <- 50\nX1 <- runif(n)\n\n# Regression Machine:\n# repeatedly finds covariate, runs regression\n# stops when statistically significant at .1%\np <- 1\ni <- 0\nwhile(p >= .001){ \n    # Get Random Covariate\n    X2 <-  runif(n)\n    # Merge and `Analyze'\n    dat_i <- data.frame(X1,X2)\n    reg_i <- lm(X1~X2, data=dat_i)\n    # update results in global environment\n    p <- summary(reg_i)$coefficients[2,4]\n    i <- i+1\n}\n#summary(reg_i)\n\nplot(X1~X2, data=dat_i,\n    pch=16, col=grey(0,.5), font.main=1,\n    main=paste0('Random Dataset ', i,\":   p=\",\n        formatC(p,digits=2, format='fg')))\nabline(reg_i)\n```\n\n::: {.cell-output-display}\n![](03_05_DataScientism_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# P-hacking 2SLS with different explanatory vars\n# and different instrumental vars\nlibrary(fixest)\np <- 1\nii <- 0\nset.seed(123)\nwhile(p >= .05){\n    # Get Random Covariates\n    X2 <-  runif(n)    \n    X3 <-  runif(n)\n    # Create Treatment Variable based on Cutoff\n    cutoffs <- seq(0,1,length.out=11)[-c(1,11)]\n    for(tau in cutoffs){\n        T3 <- 1*(X3 > tau)\n        # Merge and `Analyze'\n        dat_i <- data.frame(X1,X2,T3)\n        ivreg_i <- feols(X1~1|X2~T3, data=dat_i)\n        # Update results in global environment\n        ptab <- summary(ivreg_i)$coeftable\n        if( nrow(ptab)==2){\n            p <- ptab[2,4]\n            ii <- ii+1\n        }\n    }\n}\nsummary(ivreg_i)\n## TSLS estimation - Dep. Var.: X1\n##                   Endo.    : X2\n##                   Instr.   : T3\n## Second stage: Dep. Var.: X1\n## Observations: 50\n## Standard-errors: IID \n##              Estimate Std. Error   t value  Pr(>|t|)    \n## (Intercept) -9.95e-14      1e-06 -9.95e-08         1    \n## fit_X2       1.00e+00      1e-06  1.00e+06 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## RMSE: 5.81e-14   Adj. R2: -0.006886\n## F-test (1st stage), X2: stat = 0.66488, p = 0.418869, on 1 and 48 DoF.\n##             Wu-Hausman: stat = 0.23218, p = 0.632145, on 1 and 47 DoF.\n```\n:::\n\n\n\n## Spurious Regression \n \nEven without any coding errors or p-hacking, you can sometimes make a false discovery. We begin with a motivating empirical example of \"US Gov't Spending on Science\".\n\n\nFirst, get and inspect some data from https://tylervigen.com/spurious-correlations\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your data is not made up in the computer (hopefully!)\nvigen_csv <- read.csv( paste0(\n'https://raw.githubusercontent.com/the-mad-statter/',\n'whysospurious/master/data-raw/tylervigen.csv') ) \nclass(vigen_csv)\n## [1] \"data.frame\"\nnames(vigen_csv)\n##  [1] \"year\"                         \"science_spending\"            \n##  [3] \"hanging_suicides\"             \"pool_fall_drownings\"         \n##  [5] \"cage_films\"                   \"cheese_percap\"               \n##  [7] \"bed_deaths\"                   \"maine_divorce_rate\"          \n##  [9] \"margarine_percap\"             \"miss_usa_age\"                \n## [11] \"steam_murders\"                \"arcade_revenue\"              \n## [13] \"computer_science_doctorates\"  \"noncom_space_launches\"       \n## [15] \"sociology_doctorates\"         \"mozzarella_percap\"           \n## [17] \"civil_engineering_doctorates\" \"fishing_drownings\"           \n## [19] \"kentucky_marriage_rate\"       \"oil_imports_norway\"          \n## [21] \"chicken_percap\"               \"train_collision_deaths\"      \n## [23] \"oil_imports_total\"            \"pool_drownings\"              \n## [25] \"nuclear_power\"                \"japanese_cars_sold\"          \n## [27] \"motor_vehicle_suicides\"       \"spelling_bee_word_length\"    \n## [29] \"spider_deaths\"                \"math_doctorates\"             \n## [31] \"uranium\"\nvigen_csv[1:5,1:5]\n##   year science_spending hanging_suicides pool_fall_drownings cage_films\n## 1 1996               NA               NA                  NA         NA\n## 2 1997               NA               NA                  NA         NA\n## 3 1998               NA               NA                  NA         NA\n## 4 1999            18079             5427                 109          2\n## 5 2000            18594             5688                 102          2\n```\n:::\n\n\n\nExamine some data\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,2), mar=c(2,2,2,1))\nplot.new()\nplot.window(xlim=c(1999, 2009), ylim=c(5,9)*1000)\nlines(science_spending/3~year, data=vigen_csv, lty=1, col=2, pch=16)\ntext(2003, 8200, 'US spending on science, space, technology (USD/3)', col=2, cex=.6, srt=30)\nlines(hanging_suicides~year, data=vigen_csv, lty=1, col=4, pch=16)\ntext(2004, 6500, 'US Suicides by hanging, strangulation, suffocation (Deaths)', col=4, cex=.6, srt=30)\naxis(1)\naxis(2)\n\n\nplot.new()\nplot.window(xlim=c(2002, 2009), ylim=c(0,5))\nlines(cage_films~year, data=vigen_csv[vigen_csv$year>=2002,], lty=1, col=2, pch=16)\ntext(2006, 0.5, 'Number of films with Nicolas Cage (Films)', col=2, cex=.6, srt=0)\nlines(pool_fall_drownings/25~year, data=vigen_csv[vigen_csv$year>=2002,], lty=1, col=4, pch=16)\ntext(2006, 4.5, 'Number of drownings by falling into pool (US Deaths/25)', col=4, cex=.6, srt=0)\naxis(1)\naxis(2)\n```\n\n::: {.cell-output-display}\n![](03_05_DataScientism_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n```{.r .cell-code}\n# Include an intercept to regression 1\n#reg2 <-  lm(cage_films ~ science_spending, data=vigen_csv)\n#suppressMessages(library(stargazer))\n#stargazer(reg1, reg2, type='html')\n```\n\n#### **Another Example**. {-}\nThe US government spending on science is ruining cinema\n(p<.001)!?\n\n::: {.cell}\n\n```{.r .cell-code}\n# Drop Data before 1999\nvigen_csv <- vigen_csv[vigen_csv$year >= 1999,] \n\n# Run OLS Regression\nreg1 <-  lm(cage_films ~ -1 + science_spending, data=vigen_csv)\nsummary(reg1)\n## \n## Call:\n## lm(formula = cage_films ~ -1 + science_spending, data = vigen_csv)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.7670 -0.7165  0.1447  0.7890  1.4531 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(>|t|)    \n## science_spending 9.978e-05  1.350e-05    7.39 2.34e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.033 on 10 degrees of freedom\n##   (1 observation deleted due to missingness)\n## Multiple R-squared:  0.8452,\tAdjusted R-squared:  0.8297 \n## F-statistic: 54.61 on 1 and 10 DF,  p-value: 2.343e-05\n```\n:::\n\nIt's not all bad, because people in Maine stay married longer?\n\n::: {.cell}\n\n```{.r .cell-code}\nplot.new()\nplot.window(xlim=c(1999, 2009), ylim=c(7,9))\nlines(log(maine_divorce_rate*1000)~year, data=vigen_csv)\nlines(log(science_spending/10)~year, data=vigen_csv, lty=2)\naxis(1)\naxis(2)\nlegend('topright', lty=c(1,2), legend=c(\n    'log(maine_divorce_rate*1000)',\n    'log(science_spending/10)'))\n```\n\n::: {.cell-output-display}\n![](03_05_DataScientism_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nFor more intuition on spurious correlations, try http://shiny.calpoly.sh/Corr_Reg_Game/\nThe same principles apply to more sophisticated methods.\n\n## Spurious Causal Impacts\n\nIn practice, it is *hard to find \"good\" natural experiments*. For example, suppose we asked \"what is the effect of wages on police demanded?\" and examined a policy which lowered the educational requirements from 4 years to 2 to become an officer. This increases the labour supply, but it also affects the demand curve through \"general equilibrium\": as some of the new officers were potentially criminals. With fewer criminals, the demand for likely police shifts down.\n\nIn practice, it is also surprisingly *easy to find \"bad\" natural experiments*. Paradoxically, natural experiments are something you are supposed to find but never search for. As you search for good instruments, for example, sometimes random noise will appear like a good instrument (Spurious instruments). Worse, if you search for a good instrument for too long, you can also be led astray from important questions.\n\n#### **Example: Vigen IV's**. {-}\nWe now run IV regressions for different variable combinations in the dataset of spurious relationships\n\n::: {.cell}\n\n```{.r .cell-code}\nknames <- names(vigen_csv)[2:11] # First 10 Variables\n#knames <- names(vigen_csv)[-1] # Try All Variables\np <- 1\nii <- 1\nivreg_list <- vector(\"list\", factorial(length(knames))/factorial(length(knames)-3))\n\n# Choose 3 variable\nfor( k1 in knames){\nfor( k2 in setdiff(knames,k1)){\nfor( k3 in setdiff(knames,c(k1,k2)) ){   \n    X1 <- vigen_csv[,k1]\n    X2 <- vigen_csv[,k2]\n    X3 <- vigen_csv[,k3]\n    # Merge and `Analyze'        \n    dat_i <- na.omit(data.frame(X1,X2,X3))\n    ivreg_i <- feols(X1~1|X2~X3, data=dat_i)\n    ivreg_list[[ii]] <- list(ivreg_i, c(k1,k2,k3))\n    ii <- ii+1\n}}}\npvals <- sapply(ivreg_list, function(ivreg_i){ivreg_i[[1]]$coeftable[2,4]})\n\nplot(ecdf(pvals), xlab='p-value', ylab='CDF', font.main=1,\n    main='Frequency IV is Statistically Significant')\nabline(v=c(.01,.05), col=c(2,4))\n```\n\n::: {.cell-output-display}\n![](03_05_DataScientism_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Most Significant Spurious Combinations\npvars <- sapply(ivreg_list, function(ivreg_i){ivreg_i[[2]]})\npdat <- data.frame(t(pvars), pvals)\npdat <- pdat[order(pdat$pvals),]\nhead(pdat)\n##                     X1                 X2            X3        pvals\n## 4     science_spending   hanging_suicides    bed_deaths 3.049883e-08\n## 76    hanging_suicides   science_spending    bed_deaths 3.049883e-08\n## 3     science_spending   hanging_suicides cheese_percap 3.344890e-08\n## 75    hanging_suicides   science_spending cheese_percap 3.344890e-08\n## 485 maine_divorce_rate   margarine_percap cheese_percap 3.997738e-08\n## 557   margarine_percap maine_divorce_rate cheese_percap 3.997738e-08\n```\n:::\n\n\n#### **Simulation Study**. {-}\nWe apply the three major credible methods (IV, RDD, DID) to random walks. Each time, we find a result that fits mold and add various extensions that make it appear robust. One could tell a story about how $X_{2}$ affects $X_{1}$ but $X_{1}$ might also affect $X_{2}$, and how they discovered an instrument $X_{3}$ to provide the first causal estimate of $X_{2}$ on $X_{1}$. The analysis looks scientific and the story sounds plausible, so you could probably be convinced *if it were not just random noise.*\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 1000\nn_index <- seq(n)\n\nset.seed(1)\nrandom_walk1 <- cumsum(runif(n,-1,1))\n\nset.seed(2)\nrandom_walk2 <- cumsum(runif(n,-1,1))\n\npar(mfrow=c(1,2))\nplot(random_walk1, pch=16, col=rgb(1,0,0,.25),\n    xlab='Time', ylab='Random Value')\nplot(random_walk2, pch=16, col=rgb(0,0,1,.25),\n    xlab='Time', ylab='Random Value')\n```\n\n::: {.cell-output-display}\n![](03_05_DataScientism_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n**IV**. First, find an instrument that satisfy various statistical criterion to provide a causal estimate of $X_{2}$ on $X_{1}$.\n\n::: {.cell}\n\n```{.r .cell-code}\n# \"Find\" \"valid\" ingredients\nlibrary(fixest)\nrandom_walk3 <- cumsum(runif(n,-1,1))\ndat_i <- data.frame(\n    X1=random_walk1,\n    X2=random_walk2,\n    X3=random_walk3)\nivreg_i <- feols(X1~1|X2~X3, data=dat_i)\nsummary(ivreg_i)\n## TSLS estimation - Dep. Var.: X1\n##                   Endo.    : X2\n##                   Instr.   : X3\n## Second stage: Dep. Var.: X1\n## Observations: 1,000\n## Standard-errors: IID \n##             Estimate Std. Error t value   Pr(>|t|)    \n## (Intercept)  8.53309   1.644285 5.18954 2.5533e-07 ***\n## fit_X2       1.79901   0.472285 3.80916 1.4796e-04 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## RMSE: 6.25733   Adj. R2: 0.032314\n## F-test (1st stage), X2: stat = 10.804, p = 0.001048, on 1 and 998 DoF.\n##             Wu-Hausman: stat = 23.407, p = 1.518e-6, on 1 and 997 DoF.\n\n# After experimenting with different instruments\n# you can find even stronger results!\n```\n:::\n\n\n**RDD**. Second, find a large discrete change in the data that you can associate with a policy. You can use this as an instrument too, also providing a causal estimate of $X_{2}$ on $X_{1}$.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Let the data take shape\n# (around the large differences before and after)\nn1 <- 290\nwind1 <- c(n1-300,n1+300)\ndat1 <- data.frame(t=n_index, y=random_walk1, d=1*(n_index > n1))\ndat1_sub <- dat1[ n_index>wind1[1] & n_index < wind1[2],]\n\n# Then find your big break\nreg0 <- lm(y~t, data=dat1_sub[dat1_sub$d==0,])\nreg1 <- lm(y~t, data=dat1_sub[dat1_sub$d==1,])\n\n# The evidence should show openly (it's just science)\nplot(random_walk1, pch=16, col=rgb(0,0,1,.25),\n    xlim=wind1, xlab='Time', ylab='Random Value')\nabline(v=n1, lty=2)\nlines(reg0$model$t, reg0$fitted.values, col=1)\nlines(reg1$model$t, reg1$fitted.values, col=1)\n```\n\n::: {.cell-output-display}\n![](03_05_DataScientism_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n```{.r .cell-code}\n# Dress with some statistics for added credibility\nrdd_sub <- lm(y~d+t+d*t, data=dat1_sub)\nrdd_full <- lm(y~d+t+d*t, data=dat1)\nstargazer::stargazer(rdd_sub, rdd_full, \n    type='html',\n    title='Recipe RDD',\n    header=F,\n    omit=c('Constant'),\n    notes=c('First column uses a dataset around the discontinuity.',\n    'Smaller windows are more causal, and where the effect is bigger.'))\n```\n\n\n<table style=\"text-align:center\"><caption><strong>Recipe RDD</strong></caption>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><td colspan=\"2\"><em>Dependent variable:</em></td></tr>\n<tr><td></td><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n<tr><td style=\"text-align:left\"></td><td colspan=\"2\">y</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(1)</td><td>(2)</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">d</td><td>-13.169<sup>***</sup></td><td>-9.639<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.569)</td><td>(0.527)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">t</td><td>0.011<sup>***</sup></td><td>0.011<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.001)</td><td>(0.002)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">d:t</td><td>0.009<sup>***</sup></td><td>0.004<sup>*</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.002)</td><td>(0.002)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">Observations</td><td>589</td><td>1,000</td></tr>\n<tr><td style=\"text-align:left\">R<sup>2</sup></td><td>0.771</td><td>0.447</td></tr>\n<tr><td style=\"text-align:left\">Adjusted R<sup>2</sup></td><td>0.770</td><td>0.446</td></tr>\n<tr><td style=\"text-align:left\">Residual Std. Error</td><td>1.764 (df = 585)</td><td>3.081 (df = 996)</td></tr>\n<tr><td style=\"text-align:left\">F Statistic</td><td>658.281<sup>***</sup> (df = 3; 585)</td><td>268.763<sup>***</sup> (df = 3; 996)</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"><em>Note:</em></td><td colspan=\"2\" style=\"text-align:right\"><sup>*</sup>p<0.1; <sup>**</sup>p<0.05; <sup>***</sup>p<0.01</td></tr>\n<tr><td style=\"text-align:left\"></td><td colspan=\"2\" style=\"text-align:right\">First column uses a dataset around the discontinuity.</td></tr>\n<tr><td style=\"text-align:left\"></td><td colspan=\"2\" style=\"text-align:right\">Smaller windows are more causal, and where the effect is bigger.</td></tr>\n</table>\n\n**DID**. Third, find a change in the data that you can associate with a policy where the control group has parallel trends. This also provides a causal estimate of $X_{2}$ on $X_{1}$.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Find a reversal of fortune\n# (A good story always goes well with a nice pre-trend)\nn2 <- 318\nwind2 <- c(n2-20,n2+20)\nplot(random_walk2, pch=16, col=rgb(0,0,1,.5),\n    xlim=wind2, ylim=c(-15,15), xlab='Time', ylab='Random Value')\npoints(random_walk1, pch=16, col=rgb(1,0,0,.5))\nabline(v=n2, lty=2)\n```\n\n::: {.cell-output-display}\n![](03_05_DataScientism_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n```{.r .cell-code}\n# Knead out any effects that are non-causal (aka correlation)\ndat2A <- data.frame(t=n_index, y=random_walk1, d=1*(n_index > n2), RWid=1)\ndat2B <- data.frame(t=n_index, y=random_walk2, d=0, RWid=2)\ndat2  <- rbind(dat2A, dat2B)\ndat2$RWid <- as.factor(dat2$RWid)\ndat2$tid <- as.factor(dat2$t)\ndat2_sub <- dat2[ dat2$t>wind2[1] & dat2$t < wind2[2],]\n\n# Report the stars for all to enjoy\n# (what about the intercept?)\n# (stable coefficients are the good ones?)\ndid_fe1 <- lm(y~d+tid, data=dat2_sub)\ndid_fe2 <- lm(y~d+RWid, data=dat2_sub)\ndid_fe3 <- lm(y~d*RWid+tid, data=dat2_sub)\nstargazer::stargazer(did_fe1, did_fe2, did_fe3,\n    type='html',\n    title='Recipe DID',\n    header=F,\n    omit=c('tid','RWid', 'Constant'),\n    notes=c(\n     'Fixed effects for time in column 1, for id in column 2, and both in column 3.',\n     'Fixed effects control for most of your concerns.',\n     'Anything else creates a bias in the opposite direction.'))\n```\n\n\n<table style=\"text-align:center\"><caption><strong>Recipe DID</strong></caption>\n<tr><td colspan=\"4\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><td colspan=\"3\"><em>Dependent variable:</em></td></tr>\n<tr><td></td><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr>\n<tr><td style=\"text-align:left\"></td><td colspan=\"3\">y</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(1)</td><td>(2)</td><td>(3)</td></tr>\n<tr><td colspan=\"4\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">d</td><td>1.804<sup>*</sup></td><td>1.847<sup>***</sup></td><td>5.851<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.892)</td><td>(0.652)</td><td>(0.828)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td><td></td></tr>\n<tr><td colspan=\"4\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">Observations</td><td>78</td><td>78</td><td>78</td></tr>\n<tr><td style=\"text-align:left\">R<sup>2</sup></td><td>0.227</td><td>0.164</td><td>0.668</td></tr>\n<tr><td style=\"text-align:left\">Adjusted R<sup>2</sup></td><td>-0.566</td><td>0.142</td><td>0.309</td></tr>\n<tr><td style=\"text-align:left\">Residual Std. Error</td><td>2.750 (df = 38)</td><td>2.035 (df = 75)</td><td>1.827 (df = 37)</td></tr>\n<tr><td style=\"text-align:left\">F Statistic</td><td>0.287 (df = 39; 38)</td><td>7.379<sup>***</sup> (df = 2; 75)</td><td>1.860<sup>**</sup> (df = 40; 37)</td></tr>\n<tr><td colspan=\"4\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"><em>Note:</em></td><td colspan=\"3\" style=\"text-align:right\"><sup>*</sup>p<0.1; <sup>**</sup>p<0.05; <sup>***</sup>p<0.01</td></tr>\n<tr><td style=\"text-align:left\"></td><td colspan=\"3\" style=\"text-align:right\">Fixed effects for time in column 1, for id in column 2, and both in column 3.</td></tr>\n<tr><td style=\"text-align:left\"></td><td colspan=\"3\" style=\"text-align:right\">Fixed effects control for most of your concerns.</td></tr>\n<tr><td style=\"text-align:left\"></td><td colspan=\"3\" style=\"text-align:right\">Anything else creates a bias in the opposite direction.</td></tr>\n</table>\n\n\n\n\n",
    "supporting": [
      "03_05_DataScientism_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}