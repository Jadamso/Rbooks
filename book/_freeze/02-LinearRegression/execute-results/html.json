{
  "hash": "14c9effe521c843a8b95c554aa84bb8c",
  "result": {
    "engine": "knitr",
    "markdown": "# Introduction to Linear Regression\n\n# Basic Regression\n***\n\nSuppose we have some bivariate data. First, we inspect it as in Part I.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bivariate Data from USArrests\nxy <- USArrests[,c('Murder','UrbanPop')]\ncolnames(xy) <- c('y','x')\n\n# Inspect Dataset\n# head(xy)\n# summary(xy)\nplot(y~x, xy, col=grey(0,.5), pch=16)\ntitle('Murder and Urbanization in America 1975', font.main=1)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nNow we will assess the association between variables by fitting a line through the data points using a \"regression\".\n\n## Simple Linear Regression\nThis refers to fitting a linear model to bivariate data. Specifically, our model is \n$$\ny_i=\\beta_{0}+\\beta_{1} x_i+\\epsilon_{i}\n$$\nand our objective function is\n$$\nmin_{\\beta_{0}, \\beta_{1}} \\sum_{i=1}^{N} \\left( \\epsilon_{i} \\right)^2 =  min_{\\beta_{0}, \\beta_{1}} \\sum_{i=1} \\left( y_i - [\\beta_{0}+\\beta_{1} x_i] \\right).\n$$\nMinimizing the sum of squared errors yields parameter estimates\n$$\n\\hat{\\beta_{0}}=\\bar{Y}-\\hat{\\beta_{1}}\\bar{X} \\\\\n\\hat{\\beta_{1}}=\\frac{\\sum_{i}^{}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i}^{}(x_i-\\bar{x})^2} = \\frac{C_{XY}}{V_{X}}\n$$\nand predictions\n$$\n\\hat{y}_i=\\hat{\\beta_{0}}+\\hat{\\beta}x_i\\\\\n\\hat{\\epsilon}_i=y_i-\\hat{y}_i\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run a Regression Coefficients\nreg <- lm(y~x, dat=xy)\n# predict(reg)\n# resid(reg)\n# coef(reg)\n```\n:::\n\n\n#### **Goodness of Fit**. {-}\nFirst, we qualitatively analyze the ''Goodness of fit'' of our model, we plot our predictions for a qualitative analysis\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot Data and Predictions\nlibrary(plotly)\nxy$ID <- rownames(USArrests)\nxy$pred <- predict(reg)\nxy$resid <- resid(reg)\nfig <- plotly::plot_ly(\n  xy, x=~x, y=~y,\n  mode='markers',\n  type='scatter',\n  hoverinfo='text',\n  marker=list(color=grey(0,.25), size=10),\n  text=~paste('<b>', ID, '</b>',\n              '<br>Urban  :', x,\n              '<br>Murder :', y,\n              '<br>Predicted Murder :', round(pred,2),\n              '<br>Residual :', round(resid,2)))              \n# Add Legend\nfig <- plotly::layout(fig,\n          showlegend=F,\n          title='Crime and Urbanization in America 1975',\n          xaxis = list(title='Percent of People in an Urban Area'),\n          yaxis = list(title='Homicide Arrests per 100,000 People'))\n# Plot Model Predictions\nadd_trace(fig, x=~x, y=~pred,\n    inherit=F, hoverinfo='none',\n    mode='lines+markers', type='scatter',\n    color=I('black'),\n    line=list(width=1/2),\n    marker=list(symbol=134, size=5))\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-ad9096ca0cd6f340aece\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-ad9096ca0cd6f340aece\">{\"x\":{\"visdat\":{\"56979f20d79\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"56979f20d79\",\"attrs\":{\"56979f20d79\":{\"x\":{},\"y\":{},\"mode\":\"markers\",\"hoverinfo\":\"text\",\"marker\":{\"color\":\"#00000040\",\"size\":10},\"text\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\"},\"56979f20d79.1\":{\"x\":{},\"y\":{},\"hoverinfo\":\"none\",\"mode\":\"lines+markers\",\"type\":\"scatter\",\"color\":[\"black\"],\"line\":{\"width\":0.5},\"marker\":{\"symbol\":134,\"size\":5},\"inherit\":false}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"showlegend\":false,\"title\":\"Crime and Urbanization in America 1975\",\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Percent of People in an Urban Area\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Homicide Arrests per 100,000 People\"},\"hovermode\":\"closest\"},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"x\":[58,48,80,50,91,78,77,72,80,60,83,54,83,65,57,66,52,66,51,67,85,74,66,44,70,53,62,81,56,89,70,86,45,44,75,68,67,72,87,48,45,59,80,80,32,63,73,39,66,60],\"y\":[13.199999999999999,10,8.0999999999999996,8.8000000000000007,9,7.9000000000000004,3.2999999999999998,5.9000000000000004,15.4,17.399999999999999,5.2999999999999998,2.6000000000000001,10.4,7.2000000000000002,2.2000000000000002,6,9.6999999999999993,15.4,2.1000000000000001,11.300000000000001,4.4000000000000004,12.1,2.7000000000000002,16.100000000000001,9,6,4.2999999999999998,12.199999999999999,2.1000000000000001,7.4000000000000004,11.4,11.1,13,0.80000000000000004,7.2999999999999998,6.5999999999999996,4.9000000000000004,6.2999999999999998,3.3999999999999999,14.4,3.7999999999999998,13.199999999999999,12.699999999999999,3.2000000000000002,2.2000000000000002,8.5,4,5.7000000000000002,2.6000000000000001,6.7999999999999998],\"mode\":\"markers\",\"hoverinfo\":[\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\"],\"marker\":{\"color\":\"#00000040\",\"size\":10,\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"text\":[\"<b> Alabama <\\/b> <br>Urban  : 58 <br>Murder : 13.2 <br>Predicted Murder : 7.63 <br>Residual : 5.57\",\"<b> Alaska <\\/b> <br>Urban  : 48 <br>Murder : 10 <br>Predicted Murder : 7.42 <br>Residual : 2.58\",\"<b> Arizona <\\/b> <br>Urban  : 80 <br>Murder : 8.1 <br>Predicted Murder : 8.09 <br>Residual : 0.01\",\"<b> Arkansas <\\/b> <br>Urban  : 50 <br>Murder : 8.8 <br>Predicted Murder : 7.46 <br>Residual : 1.34\",\"<b> California <\\/b> <br>Urban  : 91 <br>Murder : 9 <br>Predicted Murder : 8.32 <br>Residual : 0.68\",\"<b> Colorado <\\/b> <br>Urban  : 78 <br>Murder : 7.9 <br>Predicted Murder : 8.05 <br>Residual : -0.15\",\"<b> Connecticut <\\/b> <br>Urban  : 77 <br>Murder : 3.3 <br>Predicted Murder : 8.03 <br>Residual : -4.73\",\"<b> Delaware <\\/b> <br>Urban  : 72 <br>Murder : 5.9 <br>Predicted Murder : 7.92 <br>Residual : -2.02\",\"<b> Florida <\\/b> <br>Urban  : 80 <br>Murder : 15.4 <br>Predicted Murder : 8.09 <br>Residual : 7.31\",\"<b> Georgia <\\/b> <br>Urban  : 60 <br>Murder : 17.4 <br>Predicted Murder : 7.67 <br>Residual : 9.73\",\"<b> Hawaii <\\/b> <br>Urban  : 83 <br>Murder : 5.3 <br>Predicted Murder : 8.15 <br>Residual : -2.85\",\"<b> Idaho <\\/b> <br>Urban  : 54 <br>Murder : 2.6 <br>Predicted Murder : 7.55 <br>Residual : -4.95\",\"<b> Illinois <\\/b> <br>Urban  : 83 <br>Murder : 10.4 <br>Predicted Murder : 8.15 <br>Residual : 2.25\",\"<b> Indiana <\\/b> <br>Urban  : 65 <br>Murder : 7.2 <br>Predicted Murder : 7.78 <br>Residual : -0.58\",\"<b> Iowa <\\/b> <br>Urban  : 57 <br>Murder : 2.2 <br>Predicted Murder : 7.61 <br>Residual : -5.41\",\"<b> Kansas <\\/b> <br>Urban  : 66 <br>Murder : 6 <br>Predicted Murder : 7.8 <br>Residual : -1.8\",\"<b> Kentucky <\\/b> <br>Urban  : 52 <br>Murder : 9.7 <br>Predicted Murder : 7.5 <br>Residual : 2.2\",\"<b> Louisiana <\\/b> <br>Urban  : 66 <br>Murder : 15.4 <br>Predicted Murder : 7.8 <br>Residual : 7.6\",\"<b> Maine <\\/b> <br>Urban  : 51 <br>Murder : 2.1 <br>Predicted Murder : 7.48 <br>Residual : -5.38\",\"<b> Maryland <\\/b> <br>Urban  : 67 <br>Murder : 11.3 <br>Predicted Murder : 7.82 <br>Residual : 3.48\",\"<b> Massachusetts <\\/b> <br>Urban  : 85 <br>Murder : 4.4 <br>Predicted Murder : 8.2 <br>Residual : -3.8\",\"<b> Michigan <\\/b> <br>Urban  : 74 <br>Murder : 12.1 <br>Predicted Murder : 7.97 <br>Residual : 4.13\",\"<b> Minnesota <\\/b> <br>Urban  : 66 <br>Murder : 2.7 <br>Predicted Murder : 7.8 <br>Residual : -5.1\",\"<b> Mississippi <\\/b> <br>Urban  : 44 <br>Murder : 16.1 <br>Predicted Murder : 7.34 <br>Residual : 8.76\",\"<b> Missouri <\\/b> <br>Urban  : 70 <br>Murder : 9 <br>Predicted Murder : 7.88 <br>Residual : 1.12\",\"<b> Montana <\\/b> <br>Urban  : 53 <br>Murder : 6 <br>Predicted Murder : 7.53 <br>Residual : -1.53\",\"<b> Nebraska <\\/b> <br>Urban  : 62 <br>Murder : 4.3 <br>Predicted Murder : 7.71 <br>Residual : -3.41\",\"<b> Nevada <\\/b> <br>Urban  : 81 <br>Murder : 12.2 <br>Predicted Murder : 8.11 <br>Residual : 4.09\",\"<b> New Hampshire <\\/b> <br>Urban  : 56 <br>Murder : 2.1 <br>Predicted Murder : 7.59 <br>Residual : -5.49\",\"<b> New Jersey <\\/b> <br>Urban  : 89 <br>Murder : 7.4 <br>Predicted Murder : 8.28 <br>Residual : -0.88\",\"<b> New Mexico <\\/b> <br>Urban  : 70 <br>Murder : 11.4 <br>Predicted Murder : 7.88 <br>Residual : 3.52\",\"<b> New York <\\/b> <br>Urban  : 86 <br>Murder : 11.1 <br>Predicted Murder : 8.22 <br>Residual : 2.88\",\"<b> North Carolina <\\/b> <br>Urban  : 45 <br>Murder : 13 <br>Predicted Murder : 7.36 <br>Residual : 5.64\",\"<b> North Dakota <\\/b> <br>Urban  : 44 <br>Murder : 0.8 <br>Predicted Murder : 7.34 <br>Residual : -6.54\",\"<b> Ohio <\\/b> <br>Urban  : 75 <br>Murder : 7.3 <br>Predicted Murder : 7.99 <br>Residual : -0.69\",\"<b> Oklahoma <\\/b> <br>Urban  : 68 <br>Murder : 6.6 <br>Predicted Murder : 7.84 <br>Residual : -1.24\",\"<b> Oregon <\\/b> <br>Urban  : 67 <br>Murder : 4.9 <br>Predicted Murder : 7.82 <br>Residual : -2.92\",\"<b> Pennsylvania <\\/b> <br>Urban  : 72 <br>Murder : 6.3 <br>Predicted Murder : 7.92 <br>Residual : -1.62\",\"<b> Rhode Island <\\/b> <br>Urban  : 87 <br>Murder : 3.4 <br>Predicted Murder : 8.24 <br>Residual : -4.84\",\"<b> South Carolina <\\/b> <br>Urban  : 48 <br>Murder : 14.4 <br>Predicted Murder : 7.42 <br>Residual : 6.98\",\"<b> South Dakota <\\/b> <br>Urban  : 45 <br>Murder : 3.8 <br>Predicted Murder : 7.36 <br>Residual : -3.56\",\"<b> Tennessee <\\/b> <br>Urban  : 59 <br>Murder : 13.2 <br>Predicted Murder : 7.65 <br>Residual : 5.55\",\"<b> Texas <\\/b> <br>Urban  : 80 <br>Murder : 12.7 <br>Predicted Murder : 8.09 <br>Residual : 4.61\",\"<b> Utah <\\/b> <br>Urban  : 80 <br>Murder : 3.2 <br>Predicted Murder : 8.09 <br>Residual : -4.89\",\"<b> Vermont <\\/b> <br>Urban  : 32 <br>Murder : 2.2 <br>Predicted Murder : 7.09 <br>Residual : -4.89\",\"<b> Virginia <\\/b> <br>Urban  : 63 <br>Murder : 8.5 <br>Predicted Murder : 7.73 <br>Residual : 0.77\",\"<b> Washington <\\/b> <br>Urban  : 73 <br>Murder : 4 <br>Predicted Murder : 7.94 <br>Residual : -3.94\",\"<b> West Virginia <\\/b> <br>Urban  : 39 <br>Murder : 5.7 <br>Predicted Murder : 7.23 <br>Residual : -1.53\",\"<b> Wisconsin <\\/b> <br>Urban  : 66 <br>Murder : 2.6 <br>Predicted Murder : 7.8 <br>Residual : -5.2\",\"<b> Wyoming <\\/b> <br>Urban  : 60 <br>Murder : 6.8 <br>Predicted Murder : 7.67 <br>Residual : -0.87\"],\"type\":\"scatter\",\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"line\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[58,48,80,50,91,78,77,72,80,60,83,54,83,65,57,66,52,66,51,67,85,74,66,44,70,53,62,81,56,89,70,86,45,44,75,68,67,72,87,48,45,59,80,80,32,63,73,39,66,60],\"y\":[7.630152672499273,7.4208060843020238,8.0907151665332222,7.4626754019414738,8.3209964135501959,8.0488458488937713,8.0279111900740467,7.9232378959754222,8.0907151665332222,7.672021990138723,8.1535191429923959,7.546414037220373,8.1535191429923959,7.7766952842373476,7.6092180136795484,7.7976299430570721,7.5045447195809238,7.7976299430570721,7.4836100607611984,7.8185646018767976,8.1953884606318468,7.9651072136148722,7.7976299430570721,7.3370674490231238,7.8813685783359722,7.5254793784006484,7.713891307778173,8.1116498253529468,7.588283354859823,8.279127095910745,7.8813685783359722,8.2163231194515713,7.3580021078428492,7.3370674490231238,7.9860418724345967,7.8394992606965221,7.8185646018767976,7.9232378959754222,8.2372577782712959,7.4208060843020238,7.3580021078428492,7.6510873313189975,8.0907151665332222,8.0907151665332222,7.0858515431864246,7.7348259665978976,7.9441725547951467,7.2323941549244992,7.7976299430570721,7.672021990138723],\"hoverinfo\":[\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\",\"none\"],\"mode\":\"lines+markers\",\"type\":\"scatter\",\"line\":{\"color\":\"rgba(0,0,0,1)\",\"width\":0.5},\"marker\":{\"color\":\"rgba(0,0,0,1)\",\"symbol\":134,\"size\":5,\"line\":{\"color\":\"rgba(0,0,0,1)\"}},\"textfont\":{\"color\":\"rgba(0,0,0,1)\"},\"error_y\":{\"color\":\"rgba(0,0,0,1)\"},\"error_x\":{\"color\":\"rgba(0,0,0,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\nFor a quantitative summary, we can also compute the linear correlation between the predictions and the data \n$$\nR = Cor( \\hat{y}_i, y)\n$$\nWith linear models, we typically compute $R^2$, known as the \"coefficient of determination\", using the sums of squared errors (Total, Explained, and Residual)\n$$\n\\underbrace{\\sum_{i}(y_i-\\bar{y})^2}_\\text{TSS}=\\underbrace{\\sum_{i}(\\hat{y}_i-\\bar{y})^2}_\\text{ESS}+\\underbrace{\\sum_{i}\\hat{\\epsilon_{i}}^2}_\\text{RSS}\\\\\nR^2 = \\frac{ESS}{TSS}=1-\\frac{RSS}{TSS}\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manually Compute R2\nEhat <- resid(reg)\nRSS  <- sum(Ehat^2)\nY <- xy$y\nTSS  <- sum((Y-mean(Y))^2)\nR2 <- 1 - RSS/TSS\nR2\n## [1] 0.00484035\n\n# Check R2\nsummary(reg)$r.squared\n## [1] 0.00484035\n\n# Double Check R2\nR <- cor(xy$y, predict(reg))\nR^2\n## [1] 0.00484035\n```\n:::\n\n\n\n## Variability Estimates\n\nA regression coefficient is a statistic. And, just like all statistics, we can calculate \n\n* *standard deviation*: variability within a single sample.\n* *standard error*: variability across different samples.\n* *confidence interval:* range your statistic varies across different samples.\n\n\nNote that values reported by your computer do not necessarily satisfy this definition. To calculate these statistics, we will estimate variability using *data-driven* methods. (For some theoretical background, see, e.g., https://www.sagepub.com/sites/default/files/upm-binaries/21122_Chapter_21.pdf.)\n\n#### **Jackknife**. {-}\nWe first consider the simplest, the jackknife. In this procedure, we loop through each row of the dataset. And, in each iteration of the loop, we drop that observation from the dataset and reestimate the statistic of interest. We then calculate the standard deviation of the statistic across all ``subsamples''.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Jackknife Standard Errors for OLS Coefficient\njack_regs <- lapply(1:nrow(xy), function(i){\n    xy_i <- xy[-i,]\n    reg_i <- lm(y~x, dat=xy_i)\n})\njack_coefs <- sapply(jack_regs, coef)['x',]\njack_se <- sd(jack_coefs)\n# classic_se <- sqrt(diag(vcov(reg)))[['x']]\n\n\n# Jackknife Sampling Distribution\nhist(jack_coefs, breaks=25,\n    main=paste0('SE est. = ', round(jack_se,4)),\n    font.main=1, border=NA,\n    xlab=expression(beta[-i]))\n# Original Estimate\nabline(v=coef(reg)['x'], lwd=2)\n# Jackknife Confidence Intervals\njack_ci_percentile <- quantile(jack_coefs, probs=c(.025,.975))\nabline(v=jack_ci_percentile, lty=2)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n\n# Plot Normal Approximation\n# jack_ci_normal <- jack_mean+c(-1.96, +1.96)*jack_se\n# abline(v=jack_ci_normal, col=\"red\", lty=3)\n```\n:::\n\n\n#### **Bootstrap**. {-}\n\nThere are several resampling techniques. The other main one is the bootstrap, which resamples with *replacement* for an *arbitrary* number of iterations. When bootstrapping a dataset with $n$ observations, you randomly resample all $n$ rows in your data set $B$ times. Random subsampling is one of many hybrid approaches that tries to combine the best of both worlds.\n\n| | Sample Size per Iteration | Number of Iterations | Resample |\n| -------- | ------- | ------- | ------- |\nBootstrap | $n$     | $B$  | With Replacement |\nJackknife | $n-1$   | $n$  | Without Replacement |\nRandom Subsample | $m < n$ | $B$  | Without Replacement |\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap\nboot_regs <- lapply(1:399, function(b){\n    b_id <- sample( nrow(xy), replace=T)\n    xy_b <- xy[b_id,]\n    reg_b <- lm(y~x, dat=xy_b)\n})\nboot_coefs <- sapply(boot_regs, coef)['x',]\nboot_se <- sd(boot_coefs)\n\nhist(boot_coefs, breaks=25,\n    main=paste0('SE est. = ', round(boot_se,4)),\n    font.main=1, border=NA,\n    xlab=expression(beta[b]))\nboot_ci_percentile <- quantile(boot_coefs, probs=c(.025,.975))\nabline(v=boot_ci_percentile, lty=2)\nabline(v=coef(reg)['x'], lwd=2)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random Subsamples\nrs_regs <- lapply(1:399, function(b){\n    b_id <- sample( nrow(xy), nrow(xy)-10, replace=F)\n    xy_b <- xy[b_id,]\n    reg_b <- lm(y~x, dat=xy_b)\n})\nrs_coefs <- sapply(rs_regs, coef)['x',]\nrs_se <- sd(rs_coefs)\n\nhist(rs_coefs, breaks=25,\n    main=paste0('SE est. = ', round(rs_se,4)),\n    font.main=1, border=NA,\n    xlab=expression(beta[b]))\nabline(v=coef(reg)['x'], lwd=2)\nrs_ci_percentile <- quantile(rs_coefs, probs=c(.025,.975))\nabline(v=rs_ci_percentile, lty=2)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nWe can also bootstrap other statistics, such as a t-statistic or $R^2$. We do such things to test a null hypothesis, which is often ``no relationship''. We are rarely interested in computing standard errors and conducting hypothesis tests for two variables. However, we work through the ideas in the two-variable case to better understand the multi-variable case.\n\n## Hypothesis Tests\n\n#### **Invert a CI**. {-}\n\nOne main way to conduct hypothesis tests is to examine whether a confidence interval contains a hypothesized value. Does the slope coefficient equal $0$? For reasons we won't go into in this class, we typically normalize the coefficient by its standard error: $$ \\hat{t} = \\frac{\\hat{\\beta}}{\\hat{\\sigma}_{\\hat{\\beta}}} $$\n\n::: {.cell}\n\n```{.r .cell-code}\ntvalue <- coef(reg)['x']/jack_se\n\njack_t <- sapply(jack_regs, function(reg_b){\n    # Data\n    xy_b <- reg_b$model\n    # Coefficient\n    beta_b <- coef(reg_b)[['x']]\n    t_hat_b <- beta_b/jack_se\n    return(t_hat_b)\n})\n\nhist(jack_t, breaks=25,\n    main='Jackknife t Density',\n    font.main=1, border=NA,\n    xlab=expression(hat(t)[b]), \n    xlim=range(c(0, jack_t)) )\nabline(v=quantile(jack_t, probs=c(.025,.975)), lty=2)\nabline(v=0, col=\"red\", lwd=2)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n#### **Impose the Null**.{-}\nWe can also compute a null distribution. We focus on the simplest: bootstrap simulations that each impose the null hypothesis and re-estimate the statistic of interest. Specifically, we compute the distribution of t-values on data with randomly reshuffled outcomes (imposing the null), and compare how extreme the observed value is.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Null Distribution for Beta\nboot_t0 <- sapply( 1:399, function(b){\n    xy_b <- xy\n    xy_b$y <- sample( xy_b$y, replace=T)\n    reg_b <- lm(y~x, dat=xy_b)\n    beta_b <- coef(reg_b)[['x']]\n    t_hat_b <- beta_b/jack_se\n    return(t_hat_b)\n})\n\n# Null Bootstrap Distribution\nboot_ci_percentile0 <- quantile(boot_t0, probs=c(.025,.975))\nhist(boot_t0, breaks=25,\n    main='Null Bootstrap Density',\n    font.main=1, border=NA,\n    xlab=expression(hat(t)[b]),\n    xlim=range(boot_t0))\nabline(v=boot_ci_percentile0, lty=2)\nabline(v=tvalue, col=\"red\", lwd=2)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nAlternatively, you can impose the null by recentering the sampling distribution around the theoretical value; $$\\hat{t} = \\frac{\\hat{\\beta} - \\beta_{0} }{\\hat{\\sigma}_{\\hat{\\beta}}}.$$ Under some assumptions, the null distribution follows a t-distribution. (For more on parametric t-testing based on statistical theory, see https://www.econometrics-with-r.org/4-lrwor.html.)\n\n\nIn any case, we can calculate a *p-value*: the probability you would see something as extreme as your statistic under the null (assuming your null hypothesis was true). We can always calculate a p-value from an explicit null distribution.\n\n::: {.cell}\n\n```{.r .cell-code}\n# One Sided Test for P(t > boot_t | Null) = 1 - P(t < boot_t | Null)\nThat_NullDist1 <- ecdf(boot_t0)\nPhat1  <- 1-That_NullDist1(jack_t)\n\n# Two Sided Test for P(t > jack_t or t < -jack_t | Null)\nThat_NullDist2 <- ecdf(abs(boot_t0))\nplot(That_NullDist2, xlim=range(boot_t0, jack_t),\n    xlab=expression( abs(hat(t)[b]) ),\n    main='Null Bootstrap Distribution', font.main=1)\nabline(v=tvalue, col='red')\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\nPhat2  <-  1-That_NullDist2( abs(tvalue))\nPhat2\n## [1] 0.6265664\n```\n:::\n\n\n## Local Linear Regression\n\nIt is generally safe to assume that you could be analyzing data with nonlinear relationships. Here, our model can be represented as\n\\begin{eqnarray}\ny_{i} = m(x_{i}) + e_{i},\n\\end{eqnarray}\nwith $m$ being some unknown but smooth function. In such cases, linear regressions can still be useful.\n\n#### **Piecewise Regression**.{-}\nThe simplest case is *segmented/piecewise regression*, which runs a separate regression for different subsets of the data.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Globally Linear\nreg <- lm(y~x, data=xy)\n\n# Diagnose Fit\n#plot( fitted(reg), resid(reg), pch=16, col=grey(0,.5))\n#plot( xy$x, resid(reg), pch=16, col=grey(0,.5))\n\n# Linear in 2 Pieces (subsets)\nxcut2 <- cut(xy$x,2)\nxy_list2 <- split(xy, xcut2)\nregs2 <- lapply(xy_list2, function(xy_s){\n    lm(y~x, data=xy_s)\n})\nsapply(regs2, coef)\n##             (31.9,61.5] (61.5,91.1]\n## (Intercept)  -0.2836303  4.15337509\n## x             0.1628157  0.04760783\n\n# Linear in 3 Pieces (subsets or bins)\nxcut3 <- cut(xy$x, seq(32,92,by=20)) # Finer Bins\nxy_list3 <- split(xy, xcut3)\nregs3 <- lapply(xy_list3, function(xy_s){\n    lm(y~x, data=xy_s)\n})\nsapply(regs3, coef)\n##                (32,52]    (52,72]      (72,92]\n## (Intercept) 4.60313390 2.36291848  8.653829140\n## x           0.08233618 0.08132841 -0.007174454\n```\n:::\n\n\nCompare Predictions\n\n::: {.cell}\n\n```{.r .cell-code}\npred1 <- data.frame(yhat=predict(reg), x=reg$model$x)\npred1 <- pred1[order(pred1$x),]\n\npred2 <- lapply(regs2, function(reg){\n    data.frame(yhat=predict(reg), x=reg$model$x)\n})\npred2 <- do.call(rbind,pred2)\npred2 <- pred2[order(pred2$x),]\n\npred3 <- lapply(regs3, function(reg){\n    data.frame(yhat=predict(reg), x=reg$model$x)\n})\npred3 <- do.call(rbind,pred3)\npred3 <- pred3[order(pred3$x),]\n\n# Compare Predictions\nplot(y ~ x, pch=16, col=grey(0,.5), dat=xy)\nlines(yhat~x, pred1, lwd=2, col=2)\nlines(yhat~x, pred2, lwd=2, col=4)\nlines(yhat~x, pred3, lwd=2, col=3)\nlegend('topleft',\n    legend=c('Globally Linear', 'Peicewise Linear (2)','Peicewise Linear (3)'),\n    lty=1, col=c(2,4,3), cex=.8)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n#### **Locally Linear**.{-}\nA less simple case is a **local linear regression** which conducts a linear regression for each data point using a subsample of data around it. \n\n::: {.cell}\n\n```{.r .cell-code}\n# ``Naive\" Smoother\npred_fun <- function(x0, h, xy){\n    # Assign equal weight to observations within h distance to x0\n    # 0 weight for all other observations\n    ki   <- dunif(xy$x, x0-h, x0+h) \n    llls <- lm(y~x, data=xy, weights=ki)\n    yhat_i <- predict(llls, newdata=data.frame(x=x0))\n}\n\nX0 <- sort(unique(xy$x))\npred_lo1 <- sapply(X0, pred_fun, h=2, xy=xy)\npred_lo2 <- sapply(X0, pred_fun, h=20, xy=xy)\n\nplot(y~x, pch=16, data=xy, col=grey(0,.5),\n    ylab='Murder Rate', xlab='Population Density')\ncols <- c(rgb(.8,0,0,.5), rgb(0,0,.8,.5))\nlines(X0, pred_lo1, col=cols[1], lwd=1, type='o')\nlines(X0, pred_lo2, col=cols[2], lwd=1, type='o')\nlegend('topleft', title='Locally Linear',\n    legend=c('h=2 ', 'h=20'),\n    lty=1, col=cols, cex=.8)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nNote that there are more complex versions of local linear regressions (see https://shinyserv.es/shiny/kreg/ for a nice illustration.) An even more complex (and more powerful) version is **loess**, which uses adaptive bandwidths in order to have a similar number of data points in each subsample (especially useful when $X$ is not uniform.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Adaptive-width subsamples with non-uniform weights\nxy0 <- xy[order(xy$x),]\nplot(y~x, pch=16, col=grey(0,.5), dat=xy0)\n\nreg_lo4 <- loess(y~x, data=xy0, span=.4)\nreg_lo8 <- loess(y~x, data=xy0, span=.8)\n\ncols <- hcl.colors(3,alpha=.75)[-3]\nlines(xy0$x, predict(reg_lo4),\n    col=cols[1], type='o', pch=2)\nlines(xy0$x, predict(reg_lo8),\n    col=cols[2], type='o', pch=2)\n\nlegend('topleft', title='Loess',\n    legend=c('span=.4 ', 'span=.8'),\n    lty=1, col=cols, cex=.8)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n#### **Confidence Bands**. {-}\nThe smoothed predicted values estimate the local means. So we can also construct confidence bands\n\n::: {.cell}\n\n```{.r .cell-code}\n# Loess\nxy0 <- xy[order(xy$x),]\nX0 <- unique(xy0$x)\nreg_lo <- loess(y~x, data=xy0, span=.8)\n\n# Jackknife CI\njack_lo <- sapply(1:nrow(xy), function(i){\n    xy_i <- xy[-i,]\n    reg_i <- loess(y~x, dat=xy_i, span=.8)\n    predict(reg_i, newdata=data.frame(x=X0))\n})\njack_cb <- apply(jack_lo,1, quantile,\n    probs=c(.025,.975), na.rm=T)\n\n# Plot\nplot(y~x, pch=16, col=grey(0,.5), dat=xy0)\npreds_lo <- predict(reg_lo, newdata=data.frame(x=X0))\nlines(X0, preds_lo,\n    col=hcl.colors(3,alpha=.75)[2],\n    type='o', pch=2)\n# Plot CI\npolygon(\n    c(X0, rev(X0)),\n    c(jack_cb[1,], rev(jack_cb[2,])),\n    col=hcl.colors(3,alpha=.25)[2],\n    border=NA)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n# Intermediate Regression\n***\n\nFirst, note that you can summarize a dataset with multiple variables using the previous tools.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Inspect Dataset on police arrests for the USA in 1973\nhead(USArrests)\n##            Murder Assault UrbanPop Rape\n## Alabama      13.2     236       58 21.2\n## Alaska       10.0     263       48 44.5\n## Arizona       8.1     294       80 31.0\n## Arkansas      8.8     190       50 19.5\n## California    9.0     276       91 40.6\n## Colorado      7.9     204       78 38.7\n\nlibrary(psych)\npairs.panels( USArrests[,c('Murder','Assault','UrbanPop')],\n    hist.col=grey(0,.25), breaks=30, density=F, hist.border=NA, # Diagonal\n    ellipses=F, rug=F, smoother=F, pch=16, col='red' # Lower Triangle\n    )\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n## Multiple Linear Regression\nWith $K$ variables, the linear model is\n$$\ny_i=\\beta_0+\\beta_1 x_{i1}+\\beta_2 x_{i2}+\\ldots+\\beta_K x_{iK}+\\epsilon_i = [1~~  x_{i1} ~~...~~ x_{iK}] \\beta + \\epsilon_i\n$$\nand our objective is\n$$\nmin_{\\beta} \\sum_{i=1}^{N} (\\epsilon_i)^2.\n$$\n\nDenoting \n$$\ny= \\begin{pmatrix} \ny_{1} \\\\ \\vdots \\\\ y_{N}\n\\end{pmatrix} \\quad\n\\textbf{X} = \\begin{pmatrix} \n1 & x_{11} & ... & x_{1K} \\\\\n& \\vdots & & \\\\\n1 & x_{N1} & ... & x_{NK} \n\\end{pmatrix},\n$$\nwe can also write the model and objective in matrix form\n$$\ny=\\textbf{X}\\beta+\\epsilon\\\\\nmin_{\\beta} (\\epsilon' \\epsilon)\n$$\n\nMinimizing the squared errors yields coefficient estimates\n$$\n\\hat{\\beta}=(\\textbf{X}'\\textbf{X})^{-1}\\textbf{X}'y\n$$\nand predictions \n$$\n\\hat{y}=\\textbf{X} \\hat{\\beta} \\\\\n\\hat{\\epsilon}=y - \\hat{y} \\\\\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manually Compute\nY <- USArrests[,'Murder']\nX <- USArrests[,c('Assault','UrbanPop')]\nX <- as.matrix(cbind(1,X))\n\nXtXi <- solve(t(X)%*%X)\nBhat <- XtXi %*% (t(X)%*%Y)\nc(Bhat)\n## [1]  3.20715340  0.04390995 -0.04451047\n\n# Check\nreg <- lm(Murder~Assault+UrbanPop, data=USArrests)\ncoef(reg)\n## (Intercept)     Assault    UrbanPop \n##  3.20715340  0.04390995 -0.04451047\n```\n:::\n\n\nTo measure the ``Goodness of fit'' of the model, we can again plot our predictions.\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(USArrests$Murder, predict(reg), pch=16, col=grey(0,.5))\nabline(a=0,b=1, lty=2)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nWe can also again compute sums of squared errors. Adding random data may sometimes improve the fit, however, so we adjust the $R^2$ by the number of covariates $K$.\n$$\nR^2 = \\frac{ESS}{TSS}=1-\\frac{RSS}{TSS}\\\\\nR^2_{\\text{adj.}} = 1-\\frac{N-1}{N-K}(1-R^2)\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nksims <- 1:30\nfor(k in ksims){ \n    USArrests[,paste0('R',k)] <- runif(nrow(USArrests),0,20)\n}\nreg_sim <- lapply(ksims, function(k){\n    rvars <- c('Assault','UrbanPop', paste0('R',1:k))\n    rvars2 <- paste0(rvars, collapse='+')\n    reg_k <- lm( paste0('Murder~',rvars2), data=USArrests)\n})\nR2_sim <- sapply(reg_sim, function(reg_k){  summary(reg_k)$r.squared })\nR2adj_sim <- sapply(reg_sim, function(reg_k){  summary(reg_k)$adj.r.squared })\n\nplot.new()\nplot.window(xlim=c(0,30), ylim=c(0,1))\npoints(ksims, R2_sim)\npoints(ksims, R2adj_sim, pch=16)\naxis(1)\naxis(2)\nmtext(expression(R^2),2, line=3)\nmtext('Additional Random Covariates', 1, line=3)\nlegend('topleft', horiz=T,\n    legend=c('Undjusted', 'Adjusted'), pch=c(1,16))\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n\n## Factor Variables\n\nSo far, we have discussed cardinal data where the difference between units always means the same thing: e.g., $4-3=2-1$. There are also factor variables\n\n* Ordered: refers to Ordinal data. The difference between units means something, but not always the same thing. For example, $4th - 3rd \\neq 2nd - 1st$.\n* Unordered: refers to Categorical data. The difference between units is meaningless. For example, $B-A=?$\n\nTo analyze either factor, we often convert them into indicator variables or dummies; $D_{c}=\\mathbf{1}( Factor = c)$. One common case is if you have observations of individuals over time periods, then you may have two factor variables. An unordered factor that indicates who an individual is; for example $D_{i}=\\mathbf{1}( Individual = i)$, and an order factor that indicates the time period; for example $D_{t}=\\mathbf{1}( Time \\in [month~ t, month~ t+1) )$. There are many other cases you see factor variables, including spatial ID's in purely cross sectional data.\n\nBe careful not to handle categorical data as if they were cardinal. E.g., generate city data with Leipzig=1, Lausanne=2, LosAngeles=3, ... and then include city as if it were a cardinal number (that's a big no-no). The same applied to ordinal data; PopulationLeipzig=2, PopulationLausanne=3, PopulationLosAngeles=1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 1000\nx <- runif(N,3,8)\ne <- rnorm(N,0,0.4)\nfo <- factor(rbinom(N,4,.5), ordered=T)\nfu <- factor(rep(c('A','B'),N/2), ordered=F)\ndA <- 1*(fu=='A')\ny <- (2^as.integer(fo)*dA )*sqrt(x)+ 2*as.integer(fo)*e\ndat_f <- data.frame(y,x,fo,fu)\n```\n:::\n\n\nWith factors, you can still include them in the design matrix of an OLS regression\n$$\ny_{it} = x_{it} \\beta_{x} + d_{t}\\beta_{t}\n$$\nWhen, as commonly done, the factors are modeled as being additively seperable, they are modeled \"fixed effects\".^[There are also *random effects*: the factor variable comes from a distribution that is uncorrelated with the regressors. This is rarely used in economics today, however, and are mostly included for historical reasons and special cases where fixed effects cannot be estimated due to data limitations.]\nSimply including the factors into the OLS regression yields a \"dummy variable\" fixed effects estimator.\n**Hansen Econometrics, Theorem 17.1:** *The fixed effects estimator of $\\beta$ algebraically equals the dummy variable estimator of $\\beta$. The two estimators have the same residuals.*\n<!--\nIn fact, if the fixed effect is ``fully unstructured then the only way to consistently estimate the coefficient $\\beta$ is by an estimator which is invariant'' (Hansen Econometrics, p). \n-->\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fixest)\nfe_reg1 <- feols(y~x|fo+fu, dat_f)\ncoef(fe_reg1)\n##         x \n## 0.9646375\nfixef(fe_reg1)[1:2]\n## $fo\n##         0         1         2         3         4 \n##  8.225787 10.365889 14.745716 23.826433 47.178798 \n## \n## $fu\n##         A         B \n##   0.00000 -21.75485\n\n# Compare Coefficients\nfe_reg0 <- lm(y~-1+x+fo+fu, dat_f)\ncoef( fe_reg0 )\n##           x         fo0         fo1         fo2         fo3         fo4 \n##   0.9646375   8.2257871  10.3658894  14.7457164  23.8264332  47.1787979 \n##         fuB \n## -21.7548516\n```\n:::\n\n\nWith fixed effects, we can also compute averages for each group and construct a *between estimator*: $\\bar{y}_i = \\alpha + \\bar{x}_i \\beta$. Or we can subtract the average from each group to construct a *within estimator*: $(y_{it} - \\bar{y}_i) = (x_{it}-\\bar{x}_i)\\beta$. \n\nBut note that many factors are not additively separable. This is easy to check with an F-test;\n\n::: {.cell}\n\n```{.r .cell-code}\nreg0 <- lm(y~-1+x+fo+fu, dat_f)\nreg1 <- lm(y~-1+x+fo*fu, dat_f)\nreg2 <- lm(y~-1+x*fo*fu, dat_f)\n\nanova(reg0, reg2)\n## Analysis of Variance Table\n## \n## Model 1: y ~ -1 + x + fo + fu\n## Model 2: y ~ -1 + x * fo * fu\n##   Res.Df   RSS Df Sum of Sq      F    Pr(>F)    \n## 1    993 71748                                  \n## 2    980  5986 13     65762 828.14 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(reg0, reg1, reg2)\n## Analysis of Variance Table\n## \n## Model 1: y ~ -1 + x + fo + fu\n## Model 2: y ~ -1 + x + fo * fu\n## Model 3: y ~ -1 + x * fo * fu\n##   Res.Df   RSS Df Sum of Sq       F    Pr(>F)    \n## 1    993 71748                                   \n## 2    989 12115  4     59633 2440.62 < 2.2e-16 ***\n## 3    980  5986  9      6129  111.48 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n<!-- \n> The labels \"random effects\" and \"fixed effects\" are misleading. These are labels which arose in the early literature and we are stuck with these labels today. In a previous era regressors were viewed as \"fixed\". Viewing the individual effect as an unobserved regressor leads to the label of the individual effect as \"fixed\". Today, we rarely refer to regressors as \"fixed\" when dealing with observational data. We view all variables as random. Consequently describing u i as \"fixed\" does not make much sense and it is hardly a contrast with the \"random effect\" label since under either assumption u i is treated as random. Once again, the labels are unfortunate but the key difference is whether u i is correlated with the regressors.\n-->\n\n\n\n## Variability Estimates\n\n\nTo estimate the variability of our estimates, we can use the same *data-driven* methods introduced in the last section. As before, we can conduct independent hypothesis tests using t-values.\n\nWe can also conduct *joint* tests that account for interdependancies in our estimates. For example, to test whether two coefficients both equal $0$, we bootstrap the *joint* distribution of coefficients.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap SE's\nboots <- 1:399\nboot_regs <- lapply(boots, function(b){\n    b_id <- sample( nrow(USArrests), replace=T)\n    xy_b <- USArrests[b_id,]\n    reg_b <- lm(Murder~Assault+UrbanPop, dat=xy_b)\n})\nboot_coefs <- sapply(boot_regs, coef)\n\n# Recenter at 0 to impose the null\n#boot_means <- rowMeans(boot_coefs)\n#boot_coefs0 <- sweep(boot_coefs, MARGIN=1, STATS=boot_means)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboot_coef_df <- as.data.frame(cbind(ID=boots, t(boot_coefs)))\nfig <- plotly::plot_ly(boot_coef_df,\n    type = 'scatter', mode = 'markers',\n    x = ~UrbanPop, y = ~Assault,\n    text = ~paste('<b> bootstrap dataset: ', ID, '</b>',\n            '<br>Coef. Urban  :', round(UrbanPop,3),\n            '<br>Coef. Murder :', round(Assault,3),\n            '<br>Coef. Intercept :', round(`(Intercept)`,3)),\n    hoverinfo='text',\n    showlegend=F,\n    marker=list( color='rgba(0, 0, 0, 0.5)'))\nfig <- plotly::layout(fig,\n    showlegend=F,\n    title='Joint Distribution of Coefficients (under the null)',\n    xaxis = list(title='UrbanPop Coefficient'),\n    yaxis = list(title='Assualt Coefficient'))\nfig\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-cd6bcd134f4c2afb8a36\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-cd6bcd134f4c2afb8a36\">{\"x\":{\"visdat\":{\"56972761b76a\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"56972761b76a\",\"attrs\":{\"56972761b76a\":{\"mode\":\"markers\",\"x\":{},\"y\":{},\"text\":{},\"hoverinfo\":\"text\",\"showlegend\":false,\"marker\":{\"color\":\"rgba(0, 0, 0, 0.5)\"},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"showlegend\":false,\"title\":\"Joint Distribution of Coefficients (under the null)\",\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"UrbanPop Coefficient\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Assualt Coefficient\"},\"hovermode\":\"closest\"},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"mode\":\"markers\",\"x\":[-0.023246871569957316,-0.067331250053322064,-0.012823906636590424,-0.066255635800025678,-0.035173639663711717,0.024235163306743315,-0.075119569847100123,-0.0621646071522787,0.0035330606170585997,-0.0036712380998564719,-0.044225602939468117,-0.058849157419885598,-0.01542950714337658,-0.069426758487393186,0.010002326901154263,-0.034626945920711506,-0.077184212883938702,-0.020451707915724295,-0.058978866942045183,-0.072976128468082047,-0.054700390961178687,-0.04986956222210491,-0.067063149897208615,-0.040009073852130921,-0.048559519250706729,-0.029383450888247363,-0.059666602723407416,-0.045112524752162354,-0.06347001246092665,-0.03103944038467266,-0.035952600526211971,-0.057588574622415525,-0.051114963082465846,-0.040409717201980888,-0.055389792231894719,-0.032771079045042714,-0.063956006354920714,0.0070403710124507515,-0.06683039884676982,-0.060036649454459454,-0.018907226801545766,0.01193713018180087,-0.086073540423734413,-0.056626420144193897,-0.015397007744974553,-0.034085211681302269,-0.04435495656051177,-0.089271050168093155,0.010042190584716924,-0.055632638402761878,-0.034336327976010571,-0.046916386929808457,-0.042312191312366715,-0.050785043076225757,-0.036350088512942857,-0.079331122566189904,-0.059635856404474587,-0.035866177016191526,-0.062188172724835997,-0.0078736947123857152,-0.074507926406211247,-0.041267282363707677,-0.18978483277448571,-0.077451565752103532,-0.065848479131214888,-0.045797492130088699,-0.050508438938586635,-0.075309229070289307,-0.024752761396531046,-0.010644187222093949,-0.029311505002106136,-0.048957966679399979,-0.03444674363417205,-0.032006301303960878,-0.024740024343962285,-0.036016604529657707,-0.058569631329735472,-0.024731047450491574,-0.031047310434207727,-0.054611736642406698,-0.039561275712933795,-0.0252985846260258,-0.065635898656566355,-0.076033821915129973,-0.031355280547720576,-0.070105176486574869,-0.00099102708570112264,-0.01647185688805811,-0.0055783852327331617,-0.022418296438712192,-0.083970521114686766,-0.023109558387570575,-0.064766730267510272,-0.044487198519707748,-0.042624710938678638,-0.044759372438227404,-0.096093009548086791,-0.053587915903159859,-0.066409843389195519,-0.077875021496440516,-0.0036597634628278719,-0.017284931299218268,-0.027586133579653255,-0.083728980236314171,-0.037736986750993018,-0.047229377617270314,-0.026098564141458381,-0.033440970337974836,-0.091463106094901159,-0.035454705073657285,-0.049311786128042469,-0.047771722899365987,-0.043557052380552651,-0.07268517535106285,-0.034771604288389567,-0.0097553189838851181,-0.023488321248026377,-0.044176255819392954,-0.028272279645878869,-0.042061109592655618,-0.079729197764422152,-0.068005398151557914,-0.035229171286122865,-0.083876841215723502,-0.040475643487289638,-0.073411302885834334,-0.039933573306297349,-0.0070787987363403766,-0.051420351250371706,-0.063395436550652148,-0.04438045945674704,-0.031170801479808295,-0.045362231256379795,-0.027554868588190808,-0.04129530164652636,-0.013868674897769627,-0.053794524260680182,-0.034771438381792023,-0.039609179483463208,-0.041010956745041759,-0.061188980356092451,-0.088933195706880727,-0.066820294013804432,-0.067834353352179827,-0.06544302113528476,-0.048140016565219025,-0.047648234425003773,-0.0076541229785615696,-0.064832041014834169,-0.099340961280794768,0.0058831724915913378,-0.040228139809383202,-0.040790141435084158,-0.052580299440910805,-0.043940715186717133,-0.071079607144276108,-0.080321229209078943,-0.032533533922503917,-0.078524414415752752,-0.042044602236385425,-0.049949368425242044,-0.035544491702485803,-0.04443666333332727,-0.062381958723980217,-0.0062530262498940049,-0.022939204839621297,-0.073511106641939911,-0.048134439509071356,-0.033943448261283073,-0.052256727943296298,-0.019648395841659061,-0.10232755184791674,-0.069989825755678539,-0.064891031478667024,-0.055061352612373472,-0.063197775652661686,-0.062899352368823352,0.010512102585224715,-0.077760307879166365,-0.047357706445098374,-0.11226474480327361,-0.10483717670032264,-0.087022303595305994,-0.083648937167649873,-0.057735243701800117,-0.060434210668701339,-0.028322256072042473,-0.024927473082395259,-0.015979204961837246,-0.020349147295659877,-0.027434840688989433,-0.036320707069321306,-0.042277227560193864,-0.081314857474223415,-0.059961718460121029,-0.024432505397042583,-0.045602548550306078,-0.048307808232071772,-0.045352280256338966,-0.062288416163462791,-0.0711868267483068,-0.047085842845323578,-0.048022695794469973,-0.012034721166691752,-0.045464192333077837,-0.062228207889235332,-0.045801026255436555,-0.056475806861095303,-0.027000222998186982,-0.076729514520472789,-0.07581634581260889,-0.052408745159967844,-0.073487407744751126,-0.062390224829157216,-0.035709541179258696,-0.038773336730416726,-0.045025244667282437,-0.049791838792014514,-0.057694904634869329,-0.03478852981659751,-0.035489241663861387,-0.0047189493343861326,-0.057658895171265517,-0.0024185001079164476,-0.026713465045321266,-0.016379080854363341,-0.019612024825879596,-0.047635416717888651,-0.051768552289599791,-0.057330525096360115,-0.01369891704598725,-0.0067618232746074237,-0.015914498266776142,-0.050277483198580226,-0.062172651090211827,-0.040566723975697092,-0.073242070328450176,-0.027165867028621223,-0.057174629680751467,-0.086330557265861196,-0.047471271682331596,-0.081898209243788614,-0.042252002342314209,-0.050959758535845119,-0.01969005810720461,-0.094390491958713338,-0.062646846830649208,-0.036126402204772902,-0.036266530918287958,-0.072301918187346881,-0.031531137327794029,-0.021617894064250767,-0.057425496586341558,-0.067728859613261708,-0.03893852058800986,-0.061790637078773719,-0.019261400161231956,-0.052368040482818262,-0.065525491233133615,-0.081735581018874626,-0.02496784573545887,-0.06961850713392867,-0.094504643339899871,-0.082379323629889492,-0.074912644654162835,-0.077399706197891563,-0.016919757171900914,-0.023758906893208236,-0.046576895107423015,-0.08008384273197576,-0.048361077916889504,-0.052114895234443119,-0.026825613855811413,-0.080142929036917415,-0.042835346329738955,-0.036410469241270803,-0.077286166210566998,-0.026867260332063575,-0.10604638607492538,-0.050615065647262404,-0.05457034851672686,-0.0510017154332911,-0.054486282100382609,-0.062632450417566876,-0.018734464997729298,-0.031360511159323104,-0.062272310888991725,-0.033044477904390163,-0.077526706427422806,-0.097613683386523237,-0.06362812620983628,-0.055515890014558804,-0.058504673389952126,-0.075195688509122907,-0.015577431919741185,-0.036129314176129718,-0.03622943172727261,-0.05101269331885417,-0.034611459480328739,-0.0077056759373910566,-0.069925871977601389,-0.06371829572408963,-0.0166567813958798,-0.046330292318211945,-0.056686136539484101,-0.055389260781683686,-0.08415035091605011,-0.045937688041982866,-0.013444693738718138,-0.038482582253123461,-0.11929923323179632,-0.021349059761413963,-0.018161601710739585,-0.03429265495232648,-0.048601655423870863,-0.040166925140425629,-0.071435061779651143,-0.097448971382947475,-0.04244848803407747,-0.081383016474988676,-0.070310644909530387,-0.065391731567117278,-0.06259271261348906,-0.041569754574193991,-0.051068372358674723,-0.077836459515684644,-0.076981349540044888,-0.058343506210671019,-0.045857122737863257,-0.0030801167561822526,-0.07718122202705173,-0.074900174502420341,-0.056795840569736376,-0.068668400819443406,-0.043710212118546705,-0.04374288776660698,-0.038515904268610478,-0.11301444951267597,-0.042091897590923771,-0.035283238466175364,-0.12045408838838723,-0.037558299320448713,-0.043089918764890753,-0.064805798418772959,-0.11304527449512104,-0.026102848205698647,-0.06372176683162839,-0.060257081264854281,-0.0087250084581830013,-0.048623811078963956,-0.064017062746947309,-0.026551067947951788,-0.055866571581156127,-0.064170829987447484,-0.025613148066575324,-0.030513409917126273,-0.10641084384045922,-0.063475533421545757,-0.045383877867430901,-0.11935609085863455,0.0012103434849578131,-0.036845376916169438,-0.031149397183711874,-0.052019324187737803,-0.084050526408118012,-0.081059729888212986,-0.046041177544104771,-0.050041700799388647,-0.039980787966743363,-0.06478307457690162,-0.014775259773938556,-0.078713455644777691,-0.056431091619706587,-0.043297425435355759,-0.038230932195579735,-0.028021583755080237,-0.071466265856289873,-0.017633329811126811,-0.040425970597573427,-0.05341559472193582,-0.057389805752483314,-0.0023268451312609714,-0.044120818640550813,-0.045665608153030487,0.0058935183895323477,-0.049099162115746879,-0.011559048915673404,-0.078335584272509154,-0.06068887075730299,-0.013490949690836158,-0.014928502423711,-0.041003015928948339,-0.060316781050623679,-0.028185274982146555,-0.045104197784802784,-0.088631697567343168,-0.10697815567118718,-0.066681471359633787,-0.063201764562707166],\"y\":[0.038287883137071661,0.043326207767979977,0.045036329362583408,0.040159837929273912,0.039087530550914552,0.035975211016662428,0.046584153480846401,0.050622091609791468,0.038333000839702674,0.040982243818498167,0.042302249705629909,0.047462682877930568,0.037734538122682799,0.04140785974921414,0.040167396088701464,0.041104945609449224,0.046560902158137386,0.036690126685263502,0.042476538874989984,0.040382935249482173,0.050057882269358582,0.050845340613499879,0.049223023717380972,0.036813186940228414,0.045436337490028351,0.040422842252760216,0.040641112079847358,0.044489506492889949,0.054223245201883853,0.050800196451936987,0.043717688611969077,0.046365228788260056,0.052464920408267286,0.038755920139375316,0.047816806112845715,0.04927009377307183,0.047378818061209742,0.039566305407365734,0.043773820631996481,0.040854821142883989,0.046957341855319842,0.04100042602718048,0.046647269319569577,0.039995558606403719,0.040994300431201183,0.038969963845493012,0.042201223635539166,0.044192833483871617,0.038216130418929246,0.04299341755270631,0.041780726287955192,0.038502871032513382,0.043830026043960225,0.042175176297135096,0.04030669012566563,0.053528829670576435,0.044840304399797061,0.044506408553596381,0.04898327303481842,0.033539018959660681,0.0441046125691043,0.054313833995165366,0.056004994382669537,0.048521991670070722,0.050783074121892605,0.042198124460497671,0.047783989646619701,0.045241267220082947,0.043989789453488838,0.038257008174429868,0.03921802113812687,0.051463576943611859,0.042988201405281611,0.038096358588295347,0.044662333977806168,0.043008847595006366,0.050068642296864189,0.050476775685088557,0.042989165076118004,0.043759506817938484,0.043478328793484441,0.044452553986451981,0.046422882999355097,0.045687952674882092,0.039038561575288483,0.048604359370629423,0.04284942860857642,0.042141109962022319,0.039338105244395627,0.038164185733802099,0.038276731910980452,0.047972104083635253,0.048514258812757595,0.050657503345046172,0.038359239226843866,0.044360708933528663,0.045494536126657312,0.047203639003679847,0.039603975432319007,0.048636292168667827,0.039404655639966396,0.035075604275802266,0.040528818235179974,0.049858717659588339,0.041665813244653543,0.043061814029755646,0.047026978791005584,0.031145214734800489,0.04852328803194926,0.040478506892206736,0.041830781321277849,0.045473092969413369,0.040960101208764983,0.051693350377342843,0.041418218877258273,0.036823678197928568,0.038562645188544874,0.042502980183199202,0.04581314654331399,0.045302009124451086,0.052011020234556352,0.049786667456535129,0.039742880607088867,0.054872382232632154,0.043578019991360531,0.046759490124822729,0.039169572661811335,0.039665334148826677,0.04531085211633866,0.043337174021927792,0.034929308767682632,0.039916479186274029,0.051969182961635559,0.050246402575510413,0.047154639252871548,0.042264889687276076,0.049440069150988315,0.042329430886215845,0.037950129511513907,0.041864875383879799,0.058241047922672033,0.047223506729529713,0.043511244477121395,0.044803466128848571,0.039688989159286259,0.037948802643277081,0.044906720095791573,0.043348358821990826,0.043295469757315817,0.050186734552182657,0.044150237815378843,0.034765099530207864,0.042186279928439506,0.037372829642027976,0.043686158752041654,0.046034642547757507,0.056323413264651637,0.042887670859865425,0.054194042731954911,0.045120042644912831,0.045627913271053734,0.042152195914837799,0.036193437960220301,0.041818274897281509,0.035075587836078788,0.04459795734814944,0.045109962218018919,0.044873750293490403,0.045799716609563841,0.039840296532457908,0.047921854357286527,0.043265287538512977,0.044712178953372421,0.049188818214976789,0.043996655327205529,0.042986064353014226,0.046396801942856447,0.036617664128017542,0.040745533955539592,0.040899906250611365,0.056030764880393699,0.047286941993088832,0.040949354909003179,0.04184075354461516,0.0405752676136059,0.047501009691501728,0.042475637030588161,0.041022355665340571,0.041633464205764609,0.03597301634029039,0.044170609451397141,0.043936646898517236,0.042315196316570686,0.050365437190401367,0.04354437007766198,0.040518221043801622,0.044077753043651038,0.054634556692340616,0.04669260906673,0.045023507648644205,0.052528965851368223,0.044563949704549237,0.046873093875715685,0.043208047652426258,0.045566660131656674,0.048575867271091276,0.043134349331034272,0.05408315403275727,0.041116922142734852,0.037306737034300624,0.052989285314124099,0.044955757390202189,0.056391119538565605,0.055820281529305786,0.043008162446256888,0.050000376579012754,0.036895024903463657,0.044468288154396032,0.04322616085783184,0.041818117049497797,0.046594602632658667,0.040708114672071999,0.046028226804620334,0.040201007839891341,0.042386038426993698,0.039467802620542447,0.045575898233393669,0.045689483527995414,0.045207383612153228,0.046329776763990284,0.040714297921627889,0.042601729003318309,0.03534950766109355,0.040198376729318253,0.043708376697339256,0.042429058507925978,0.042567092532759802,0.040720823408807449,0.047456810238251761,0.045934433227729804,0.037740958619035293,0.054984614276764529,0.044392801395137592,0.042970475766377511,0.041573458917314877,0.044853475083829482,0.045018108325060882,0.046299324499827092,0.043905088149821331,0.052099060634165847,0.045457030062803334,0.053264170610917705,0.038988881871302755,0.047475116445443701,0.049312501306059867,0.039178426429115586,0.041137946044161468,0.039272598216425786,0.038630896794352804,0.04492102828539362,0.046591686228880692,0.046537690223953057,0.050816623757581988,0.057289940570501616,0.042115393742255897,0.042817141433761245,0.041525215100171486,0.036684104077585503,0.046417621678463833,0.050313376239706567,0.04933511631667644,0.046231539039079388,0.041718595874963044,0.045775345058280986,0.040382983018506964,0.044587856310417229,0.043483428679821051,0.043272499439832073,0.049515449974638248,0.043827663137071052,0.047458983173729152,0.049125924812648269,0.046719403783936131,0.041833979782478624,0.038919463736973775,0.043091820353999166,0.04751836690925549,0.043574664605043507,0.048353204859357915,0.046202704919278172,0.045088834978015788,0.04822102971965616,0.048657378510129662,0.048497596912811476,0.042761638874452307,0.046667946652003037,0.049528089002306576,0.044082823063054152,0.047011458252365301,0.043153254548606142,0.046915038226912779,0.044415465660407917,0.044833948423872193,0.042645365874391959,0.043123651998667453,0.045446664347191273,0.043299871244122379,0.049619439931786441,0.043145644970635652,0.046171815993552842,0.065970244896771793,0.044350602497101765,0.046788770313171808,0.039278334271810843,0.046732642513984179,0.042195883579529386,0.049813271405667527,0.055851218921387134,0.045018882668886442,0.051594195872645479,0.048795839340888644,0.050242816310132117,0.04927346978013266,0.042604981739411948,0.05047820068138148,0.047785515770947515,0.064032811010573204,0.042010923989253143,0.046504548080973596,0.051283347985242721,0.046629693363715648,0.047316190718578663,0.050297135025866128,0.043743156315149981,0.04647448269926991,0.046252637775786948,0.049057993069148313,0.050891646250812717,0.040271845767850881,0.04164978051720538,0.039903782470766577,0.038304277974155497,0.046982326960251661,0.036654651312307461,0.052578464869400592,0.039713028013254929,0.039731970954045799,0.042555545632846588,0.045088458941102499,0.041679091787771627,0.046348416505280515,0.044585673320584274,0.039481113897210672,0.043999463414827931,0.043197961978533761,0.04404903157887833,0.048609068249226235,0.04858250324963917,0.041387981206055337,0.054904306075654094,0.038678130056074303,0.046204327692608953,0.046738267442400956,0.045406388889863251,0.056655109425238021,0.053591642624972222,0.047909243082697561,0.040454388590674545,0.039036480969913731,0.039044034112866749,0.040820371339746711,0.045502543964254447,0.046119491652653964,0.045602213726276375,0.045121973333744486,0.03770117759746671,0.049536806742055114,0.043105973621540769,0.043608679748766414,0.047628375101877299,0.054298830255317017,0.039266343842822585,0.038241653548148472,0.039639451211720564,0.039306082693459046,0.0443475689965728,0.039214230020543188,0.036891858056401342,0.048800413640882466,0.048074768673252451,0.045362018713155136,0.040178615833891415,0.045208636240669092,0.043667061395774344,0.04348442347015543,0.045963461181908569,0.044455797064916792,0.045873612282641155,0.049088842927593336],\"text\":[\"<b> bootstrap dataset:  1 <\\/b> <br>Coef. Urban  : -0.023 <br>Coef. Murder : 0.038 <br>Coef. Intercept : 2.516\",\"<b> bootstrap dataset:  2 <\\/b> <br>Coef. Urban  : -0.067 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 4.004\",\"<b> bootstrap dataset:  3 <\\/b> <br>Coef. Urban  : -0.013 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 1.787\",\"<b> bootstrap dataset:  4 <\\/b> <br>Coef. Urban  : -0.066 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 5.025\",\"<b> bootstrap dataset:  5 <\\/b> <br>Coef. Urban  : -0.035 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 3.52\",\"<b> bootstrap dataset:  6 <\\/b> <br>Coef. Urban  : 0.024 <br>Coef. Murder : 0.036 <br>Coef. Intercept : 0.117\",\"<b> bootstrap dataset:  7 <\\/b> <br>Coef. Urban  : -0.075 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 4.801\",\"<b> bootstrap dataset:  8 <\\/b> <br>Coef. Urban  : -0.062 <br>Coef. Murder : 0.051 <br>Coef. Intercept : 3.721\",\"<b> bootstrap dataset:  9 <\\/b> <br>Coef. Urban  : 0.004 <br>Coef. Murder : 0.038 <br>Coef. Intercept : 0.969\",\"<b> bootstrap dataset:  10 <\\/b> <br>Coef. Urban  : -0.004 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 1.297\",\"<b> bootstrap dataset:  11 <\\/b> <br>Coef. Urban  : -0.044 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 3.77\",\"<b> bootstrap dataset:  12 <\\/b> <br>Coef. Urban  : -0.059 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 3.411\",\"<b> bootstrap dataset:  13 <\\/b> <br>Coef. Urban  : -0.015 <br>Coef. Murder : 0.038 <br>Coef. Intercept : 2.35\",\"<b> bootstrap dataset:  14 <\\/b> <br>Coef. Urban  : -0.069 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 5.264\",\"<b> bootstrap dataset:  15 <\\/b> <br>Coef. Urban  : 0.01 <br>Coef. Murder : 0.04 <br>Coef. Intercept : -0.273\",\"<b> bootstrap dataset:  16 <\\/b> <br>Coef. Urban  : -0.035 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 2.885\",\"<b> bootstrap dataset:  17 <\\/b> <br>Coef. Urban  : -0.077 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 4.414\",\"<b> bootstrap dataset:  18 <\\/b> <br>Coef. Urban  : -0.02 <br>Coef. Murder : 0.037 <br>Coef. Intercept : 2.586\",\"<b> bootstrap dataset:  19 <\\/b> <br>Coef. Urban  : -0.059 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 4.161\",\"<b> bootstrap dataset:  20 <\\/b> <br>Coef. Urban  : -0.073 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 5.472\",\"<b> bootstrap dataset:  21 <\\/b> <br>Coef. Urban  : -0.055 <br>Coef. Murder : 0.05 <br>Coef. Intercept : 2.904\",\"<b> bootstrap dataset:  22 <\\/b> <br>Coef. Urban  : -0.05 <br>Coef. Murder : 0.051 <br>Coef. Intercept : 2.449\",\"<b> bootstrap dataset:  23 <\\/b> <br>Coef. Urban  : -0.067 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 3.197\",\"<b> bootstrap dataset:  24 <\\/b> <br>Coef. Urban  : -0.04 <br>Coef. Murder : 0.037 <br>Coef. Intercept : 4.117\",\"<b> bootstrap dataset:  25 <\\/b> <br>Coef. Urban  : -0.049 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 3.116\",\"<b> bootstrap dataset:  26 <\\/b> <br>Coef. Urban  : -0.029 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 2.882\",\"<b> bootstrap dataset:  27 <\\/b> <br>Coef. Urban  : -0.06 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 3.958\",\"<b> bootstrap dataset:  28 <\\/b> <br>Coef. Urban  : -0.045 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 3.258\",\"<b> bootstrap dataset:  29 <\\/b> <br>Coef. Urban  : -0.063 <br>Coef. Murder : 0.054 <br>Coef. Intercept : 2.543\",\"<b> bootstrap dataset:  30 <\\/b> <br>Coef. Urban  : -0.031 <br>Coef. Murder : 0.051 <br>Coef. Intercept : 1.305\",\"<b> bootstrap dataset:  31 <\\/b> <br>Coef. Urban  : -0.036 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 3.245\",\"<b> bootstrap dataset:  32 <\\/b> <br>Coef. Urban  : -0.058 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 3.274\",\"<b> bootstrap dataset:  33 <\\/b> <br>Coef. Urban  : -0.051 <br>Coef. Murder : 0.052 <br>Coef. Intercept : 2.012\",\"<b> bootstrap dataset:  34 <\\/b> <br>Coef. Urban  : -0.04 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 3.836\",\"<b> bootstrap dataset:  35 <\\/b> <br>Coef. Urban  : -0.055 <br>Coef. Murder : 0.048 <br>Coef. Intercept : 3.412\",\"<b> bootstrap dataset:  36 <\\/b> <br>Coef. Urban  : -0.033 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 1.841\",\"<b> bootstrap dataset:  37 <\\/b> <br>Coef. Urban  : -0.064 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 4.258\",\"<b> bootstrap dataset:  38 <\\/b> <br>Coef. Urban  : 0.007 <br>Coef. Murder : 0.04 <br>Coef. Intercept : -0.058\",\"<b> bootstrap dataset:  39 <\\/b> <br>Coef. Urban  : -0.067 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 4.733\",\"<b> bootstrap dataset:  40 <\\/b> <br>Coef. Urban  : -0.06 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 4.676\",\"<b> bootstrap dataset:  41 <\\/b> <br>Coef. Urban  : -0.019 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 1.379\",\"<b> bootstrap dataset:  42 <\\/b> <br>Coef. Urban  : 0.012 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 0.011\",\"<b> bootstrap dataset:  43 <\\/b> <br>Coef. Urban  : -0.086 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 5.663\",\"<b> bootstrap dataset:  44 <\\/b> <br>Coef. Urban  : -0.057 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 5.151\",\"<b> bootstrap dataset:  45 <\\/b> <br>Coef. Urban  : -0.015 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 1.791\",\"<b> bootstrap dataset:  46 <\\/b> <br>Coef. Urban  : -0.034 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 3.625\",\"<b> bootstrap dataset:  47 <\\/b> <br>Coef. Urban  : -0.044 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 3.317\",\"<b> bootstrap dataset:  48 <\\/b> <br>Coef. Urban  : -0.089 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 6.045\",\"<b> bootstrap dataset:  49 <\\/b> <br>Coef. Urban  : 0.01 <br>Coef. Murder : 0.038 <br>Coef. Intercept : 0.112\",\"<b> bootstrap dataset:  50 <\\/b> <br>Coef. Urban  : -0.056 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 3.612\",\"<b> bootstrap dataset:  51 <\\/b> <br>Coef. Urban  : -0.034 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 2.95\",\"<b> bootstrap dataset:  52 <\\/b> <br>Coef. Urban  : -0.047 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 4.109\",\"<b> bootstrap dataset:  53 <\\/b> <br>Coef. Urban  : -0.042 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 3.493\",\"<b> bootstrap dataset:  54 <\\/b> <br>Coef. Urban  : -0.051 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 4.111\",\"<b> bootstrap dataset:  55 <\\/b> <br>Coef. Urban  : -0.036 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 3.169\",\"<b> bootstrap dataset:  56 <\\/b> <br>Coef. Urban  : -0.079 <br>Coef. Murder : 0.054 <br>Coef. Intercept : 3.229\",\"<b> bootstrap dataset:  57 <\\/b> <br>Coef. Urban  : -0.06 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 3.876\",\"<b> bootstrap dataset:  58 <\\/b> <br>Coef. Urban  : -0.036 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 2.685\",\"<b> bootstrap dataset:  59 <\\/b> <br>Coef. Urban  : -0.062 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 3.452\",\"<b> bootstrap dataset:  60 <\\/b> <br>Coef. Urban  : -0.008 <br>Coef. Murder : 0.034 <br>Coef. Intercept : 2.207\",\"<b> bootstrap dataset:  61 <\\/b> <br>Coef. Urban  : -0.075 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 4.578\",\"<b> bootstrap dataset:  62 <\\/b> <br>Coef. Urban  : -0.041 <br>Coef. Murder : 0.054 <br>Coef. Intercept : 1.676\",\"<b> bootstrap dataset:  63 <\\/b> <br>Coef. Urban  : -0.19 <br>Coef. Murder : 0.056 <br>Coef. Intercept : 11.197\",\"<b> bootstrap dataset:  64 <\\/b> <br>Coef. Urban  : -0.077 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 3.807\",\"<b> bootstrap dataset:  65 <\\/b> <br>Coef. Urban  : -0.066 <br>Coef. Murder : 0.051 <br>Coef. Intercept : 2.93\",\"<b> bootstrap dataset:  66 <\\/b> <br>Coef. Urban  : -0.046 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 3.706\",\"<b> bootstrap dataset:  67 <\\/b> <br>Coef. Urban  : -0.051 <br>Coef. Murder : 0.048 <br>Coef. Intercept : 3.047\",\"<b> bootstrap dataset:  68 <\\/b> <br>Coef. Urban  : -0.075 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 5.448\",\"<b> bootstrap dataset:  69 <\\/b> <br>Coef. Urban  : -0.025 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 1.753\",\"<b> bootstrap dataset:  70 <\\/b> <br>Coef. Urban  : -0.011 <br>Coef. Murder : 0.038 <br>Coef. Intercept : 2.048\",\"<b> bootstrap dataset:  71 <\\/b> <br>Coef. Urban  : -0.029 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 3.346\",\"<b> bootstrap dataset:  72 <\\/b> <br>Coef. Urban  : -0.049 <br>Coef. Murder : 0.051 <br>Coef. Intercept : 2.236\",\"<b> bootstrap dataset:  73 <\\/b> <br>Coef. Urban  : -0.034 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 2.766\",\"<b> bootstrap dataset:  74 <\\/b> <br>Coef. Urban  : -0.032 <br>Coef. Murder : 0.038 <br>Coef. Intercept : 2.705\",\"<b> bootstrap dataset:  75 <\\/b> <br>Coef. Urban  : -0.025 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 1.503\",\"<b> bootstrap dataset:  76 <\\/b> <br>Coef. Urban  : -0.036 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 3.102\",\"<b> bootstrap dataset:  77 <\\/b> <br>Coef. Urban  : -0.059 <br>Coef. Murder : 0.05 <br>Coef. Intercept : 3.668\",\"<b> bootstrap dataset:  78 <\\/b> <br>Coef. Urban  : -0.025 <br>Coef. Murder : 0.05 <br>Coef. Intercept : 0.656\",\"<b> bootstrap dataset:  79 <\\/b> <br>Coef. Urban  : -0.031 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 2.259\",\"<b> bootstrap dataset:  80 <\\/b> <br>Coef. Urban  : -0.055 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 4.414\",\"<b> bootstrap dataset:  81 <\\/b> <br>Coef. Urban  : -0.04 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 2.997\",\"<b> bootstrap dataset:  82 <\\/b> <br>Coef. Urban  : -0.025 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 2.112\",\"<b> bootstrap dataset:  83 <\\/b> <br>Coef. Urban  : -0.066 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 4.029\",\"<b> bootstrap dataset:  84 <\\/b> <br>Coef. Urban  : -0.076 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 5.137\",\"<b> bootstrap dataset:  85 <\\/b> <br>Coef. Urban  : -0.031 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 3.502\",\"<b> bootstrap dataset:  86 <\\/b> <br>Coef. Urban  : -0.07 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 3.506\",\"<b> bootstrap dataset:  87 <\\/b> <br>Coef. Urban  : -0.001 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 0.357\",\"<b> bootstrap dataset:  88 <\\/b> <br>Coef. Urban  : -0.016 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 1.95\",\"<b> bootstrap dataset:  89 <\\/b> <br>Coef. Urban  : -0.006 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 1.356\",\"<b> bootstrap dataset:  90 <\\/b> <br>Coef. Urban  : -0.022 <br>Coef. Murder : 0.038 <br>Coef. Intercept : 2.311\",\"<b> bootstrap dataset:  91 <\\/b> <br>Coef. Urban  : -0.084 <br>Coef. Murder : 0.038 <br>Coef. Intercept : 6.428\",\"<b> bootstrap dataset:  92 <\\/b> <br>Coef. Urban  : -0.023 <br>Coef. Murder : 0.048 <br>Coef. Intercept : 1.566\",\"<b> bootstrap dataset:  93 <\\/b> <br>Coef. Urban  : -0.065 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 4.122\",\"<b> bootstrap dataset:  94 <\\/b> <br>Coef. Urban  : -0.044 <br>Coef. Murder : 0.051 <br>Coef. Intercept : 1.873\",\"<b> bootstrap dataset:  95 <\\/b> <br>Coef. Urban  : -0.043 <br>Coef. Murder : 0.038 <br>Coef. Intercept : 4.177\",\"<b> bootstrap dataset:  96 <\\/b> <br>Coef. Urban  : -0.045 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 2.779\",\"<b> bootstrap dataset:  97 <\\/b> <br>Coef. Urban  : -0.096 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 5.998\",\"<b> bootstrap dataset:  98 <\\/b> <br>Coef. Urban  : -0.054 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 2.697\",\"<b> bootstrap dataset:  99 <\\/b> <br>Coef. Urban  : -0.066 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 5.075\",\"<b> bootstrap dataset:  100 <\\/b> <br>Coef. Urban  : -0.078 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 4.615\",\"<b> bootstrap dataset:  101 <\\/b> <br>Coef. Urban  : -0.004 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 0.875\",\"<b> bootstrap dataset:  102 <\\/b> <br>Coef. Urban  : -0.017 <br>Coef. Murder : 0.035 <br>Coef. Intercept : 2.026\",\"<b> bootstrap dataset:  103 <\\/b> <br>Coef. Urban  : -0.028 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 2.581\",\"<b> bootstrap dataset:  104 <\\/b> <br>Coef. Urban  : -0.084 <br>Coef. Murder : 0.05 <br>Coef. Intercept : 5.108\",\"<b> bootstrap dataset:  105 <\\/b> <br>Coef. Urban  : -0.038 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 3.12\",\"<b> bootstrap dataset:  106 <\\/b> <br>Coef. Urban  : -0.047 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 3.276\",\"<b> bootstrap dataset:  107 <\\/b> <br>Coef. Urban  : -0.026 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 1.778\",\"<b> bootstrap dataset:  108 <\\/b> <br>Coef. Urban  : -0.033 <br>Coef. Murder : 0.031 <br>Coef. Intercept : 4.586\",\"<b> bootstrap dataset:  109 <\\/b> <br>Coef. Urban  : -0.091 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 5.504\",\"<b> bootstrap dataset:  110 <\\/b> <br>Coef. Urban  : -0.035 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 3.391\",\"<b> bootstrap dataset:  111 <\\/b> <br>Coef. Urban  : -0.049 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 3.602\",\"<b> bootstrap dataset:  112 <\\/b> <br>Coef. Urban  : -0.048 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 2.995\",\"<b> bootstrap dataset:  113 <\\/b> <br>Coef. Urban  : -0.044 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 3.518\",\"<b> bootstrap dataset:  114 <\\/b> <br>Coef. Urban  : -0.073 <br>Coef. Murder : 0.052 <br>Coef. Intercept : 3.496\",\"<b> bootstrap dataset:  115 <\\/b> <br>Coef. Urban  : -0.035 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 1.888\",\"<b> bootstrap dataset:  116 <\\/b> <br>Coef. Urban  : -0.01 <br>Coef. Murder : 0.037 <br>Coef. Intercept : 2.247\",\"<b> bootstrap dataset:  117 <\\/b> <br>Coef. Urban  : -0.023 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 2.544\",\"<b> bootstrap dataset:  118 <\\/b> <br>Coef. Urban  : -0.044 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 3.534\",\"<b> bootstrap dataset:  119 <\\/b> <br>Coef. Urban  : -0.028 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 1.895\",\"<b> bootstrap dataset:  120 <\\/b> <br>Coef. Urban  : -0.042 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 2.634\",\"<b> bootstrap dataset:  121 <\\/b> <br>Coef. Urban  : -0.08 <br>Coef. Murder : 0.052 <br>Coef. Intercept : 4.36\",\"<b> bootstrap dataset:  122 <\\/b> <br>Coef. Urban  : -0.068 <br>Coef. Murder : 0.05 <br>Coef. Intercept : 3.028\",\"<b> bootstrap dataset:  123 <\\/b> <br>Coef. Urban  : -0.035 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 2.831\",\"<b> bootstrap dataset:  124 <\\/b> <br>Coef. Urban  : -0.084 <br>Coef. Murder : 0.055 <br>Coef. Intercept : 4.184\",\"<b> bootstrap dataset:  125 <\\/b> <br>Coef. Urban  : -0.04 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 2.952\",\"<b> bootstrap dataset:  126 <\\/b> <br>Coef. Urban  : -0.073 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 3.764\",\"<b> bootstrap dataset:  127 <\\/b> <br>Coef. Urban  : -0.04 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 3.134\",\"<b> bootstrap dataset:  128 <\\/b> <br>Coef. Urban  : -0.007 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 1.01\",\"<b> bootstrap dataset:  129 <\\/b> <br>Coef. Urban  : -0.051 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 4.193\",\"<b> bootstrap dataset:  130 <\\/b> <br>Coef. Urban  : -0.063 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 4.226\",\"<b> bootstrap dataset:  131 <\\/b> <br>Coef. Urban  : -0.044 <br>Coef. Murder : 0.035 <br>Coef. Intercept : 4.483\",\"<b> bootstrap dataset:  132 <\\/b> <br>Coef. Urban  : -0.031 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 3.182\",\"<b> bootstrap dataset:  133 <\\/b> <br>Coef. Urban  : -0.045 <br>Coef. Murder : 0.052 <br>Coef. Intercept : 2.437\",\"<b> bootstrap dataset:  134 <\\/b> <br>Coef. Urban  : -0.028 <br>Coef. Murder : 0.05 <br>Coef. Intercept : 0.926\",\"<b> bootstrap dataset:  135 <\\/b> <br>Coef. Urban  : -0.041 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 2.677\",\"<b> bootstrap dataset:  136 <\\/b> <br>Coef. Urban  : -0.014 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 1.411\",\"<b> bootstrap dataset:  137 <\\/b> <br>Coef. Urban  : -0.054 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 3.894\",\"<b> bootstrap dataset:  138 <\\/b> <br>Coef. Urban  : -0.035 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 2.937\",\"<b> bootstrap dataset:  139 <\\/b> <br>Coef. Urban  : -0.04 <br>Coef. Murder : 0.038 <br>Coef. Intercept : 3.091\",\"<b> bootstrap dataset:  140 <\\/b> <br>Coef. Urban  : -0.041 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 3.608\",\"<b> bootstrap dataset:  141 <\\/b> <br>Coef. Urban  : -0.061 <br>Coef. Murder : 0.058 <br>Coef. Intercept : 2.231\",\"<b> bootstrap dataset:  142 <\\/b> <br>Coef. Urban  : -0.089 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 5.745\",\"<b> bootstrap dataset:  143 <\\/b> <br>Coef. Urban  : -0.067 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 4.585\",\"<b> bootstrap dataset:  144 <\\/b> <br>Coef. Urban  : -0.068 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 3.865\",\"<b> bootstrap dataset:  145 <\\/b> <br>Coef. Urban  : -0.065 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 4.992\",\"<b> bootstrap dataset:  146 <\\/b> <br>Coef. Urban  : -0.048 <br>Coef. Murder : 0.038 <br>Coef. Intercept : 4.08\",\"<b> bootstrap dataset:  147 <\\/b> <br>Coef. Urban  : -0.048 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 3.565\",\"<b> bootstrap dataset:  148 <\\/b> <br>Coef. Urban  : -0.008 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 1.195\",\"<b> bootstrap dataset:  149 <\\/b> <br>Coef. Urban  : -0.065 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 4.721\",\"<b> bootstrap dataset:  150 <\\/b> <br>Coef. Urban  : -0.099 <br>Coef. Murder : 0.05 <br>Coef. Intercept : 5.949\",\"<b> bootstrap dataset:  151 <\\/b> <br>Coef. Urban  : 0.006 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 0.23\",\"<b> bootstrap dataset:  152 <\\/b> <br>Coef. Urban  : -0.04 <br>Coef. Murder : 0.035 <br>Coef. Intercept : 4.845\",\"<b> bootstrap dataset:  153 <\\/b> <br>Coef. Urban  : -0.041 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 3.565\",\"<b> bootstrap dataset:  154 <\\/b> <br>Coef. Urban  : -0.053 <br>Coef. Murder : 0.037 <br>Coef. Intercept : 5.007\",\"<b> bootstrap dataset:  155 <\\/b> <br>Coef. Urban  : -0.044 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 3.455\",\"<b> bootstrap dataset:  156 <\\/b> <br>Coef. Urban  : -0.071 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 4.988\",\"<b> bootstrap dataset:  157 <\\/b> <br>Coef. Urban  : -0.08 <br>Coef. Murder : 0.056 <br>Coef. Intercept : 3.579\",\"<b> bootstrap dataset:  158 <\\/b> <br>Coef. Urban  : -0.033 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 2.649\",\"<b> bootstrap dataset:  159 <\\/b> <br>Coef. Urban  : -0.079 <br>Coef. Murder : 0.054 <br>Coef. Intercept : 3.372\",\"<b> bootstrap dataset:  160 <\\/b> <br>Coef. Urban  : -0.042 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 3.067\",\"<b> bootstrap dataset:  161 <\\/b> <br>Coef. Urban  : -0.05 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 3.162\",\"<b> bootstrap dataset:  162 <\\/b> <br>Coef. Urban  : -0.036 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 2.593\",\"<b> bootstrap dataset:  163 <\\/b> <br>Coef. Urban  : -0.044 <br>Coef. Murder : 0.036 <br>Coef. Intercept : 5.029\",\"<b> bootstrap dataset:  164 <\\/b> <br>Coef. Urban  : -0.062 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 5.194\",\"<b> bootstrap dataset:  165 <\\/b> <br>Coef. Urban  : -0.006 <br>Coef. Murder : 0.035 <br>Coef. Intercept : 1.778\",\"<b> bootstrap dataset:  166 <\\/b> <br>Coef. Urban  : -0.023 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 1.36\",\"<b> bootstrap dataset:  167 <\\/b> <br>Coef. Urban  : -0.074 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 5.142\",\"<b> bootstrap dataset:  168 <\\/b> <br>Coef. Urban  : -0.048 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 3.32\",\"<b> bootstrap dataset:  169 <\\/b> <br>Coef. Urban  : -0.034 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 2.635\",\"<b> bootstrap dataset:  170 <\\/b> <br>Coef. Urban  : -0.052 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 4.306\",\"<b> bootstrap dataset:  171 <\\/b> <br>Coef. Urban  : -0.02 <br>Coef. Murder : 0.048 <br>Coef. Intercept : 1.337\",\"<b> bootstrap dataset:  172 <\\/b> <br>Coef. Urban  : -0.102 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 7.259\",\"<b> bootstrap dataset:  173 <\\/b> <br>Coef. Urban  : -0.07 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 4.296\",\"<b> bootstrap dataset:  174 <\\/b> <br>Coef. Urban  : -0.065 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 3.819\",\"<b> bootstrap dataset:  175 <\\/b> <br>Coef. Urban  : -0.055 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 3.851\",\"<b> bootstrap dataset:  176 <\\/b> <br>Coef. Urban  : -0.063 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 4.773\",\"<b> bootstrap dataset:  177 <\\/b> <br>Coef. Urban  : -0.063 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 3.699\",\"<b> bootstrap dataset:  178 <\\/b> <br>Coef. Urban  : 0.011 <br>Coef. Murder : 0.037 <br>Coef. Intercept : 0.877\",\"<b> bootstrap dataset:  179 <\\/b> <br>Coef. Urban  : -0.078 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 6.257\",\"<b> bootstrap dataset:  180 <\\/b> <br>Coef. Urban  : -0.047 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 4.184\",\"<b> bootstrap dataset:  181 <\\/b> <br>Coef. Urban  : -0.112 <br>Coef. Murder : 0.056 <br>Coef. Intercept : 5.502\",\"<b> bootstrap dataset:  182 <\\/b> <br>Coef. Urban  : -0.105 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 6.332\",\"<b> bootstrap dataset:  183 <\\/b> <br>Coef. Urban  : -0.087 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 6.256\",\"<b> bootstrap dataset:  184 <\\/b> <br>Coef. Urban  : -0.084 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 5.978\",\"<b> bootstrap dataset:  185 <\\/b> <br>Coef. Urban  : -0.058 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 4.66\",\"<b> bootstrap dataset:  186 <\\/b> <br>Coef. Urban  : -0.06 <br>Coef. Murder : 0.048 <br>Coef. Intercept : 3.77\",\"<b> bootstrap dataset:  187 <\\/b> <br>Coef. Urban  : -0.028 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 2.427\",\"<b> bootstrap dataset:  188 <\\/b> <br>Coef. Urban  : -0.025 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 2.856\",\"<b> bootstrap dataset:  189 <\\/b> <br>Coef. Urban  : -0.016 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 2.001\",\"<b> bootstrap dataset:  190 <\\/b> <br>Coef. Urban  : -0.02 <br>Coef. Murder : 0.036 <br>Coef. Intercept : 2.631\",\"<b> bootstrap dataset:  191 <\\/b> <br>Coef. Urban  : -0.027 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 1.915\",\"<b> bootstrap dataset:  192 <\\/b> <br>Coef. Urban  : -0.036 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 2.672\",\"<b> bootstrap dataset:  193 <\\/b> <br>Coef. Urban  : -0.042 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 3.538\",\"<b> bootstrap dataset:  194 <\\/b> <br>Coef. Urban  : -0.081 <br>Coef. Murder : 0.05 <br>Coef. Intercept : 4.263\",\"<b> bootstrap dataset:  195 <\\/b> <br>Coef. Urban  : -0.06 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 4.135\",\"<b> bootstrap dataset:  196 <\\/b> <br>Coef. Urban  : -0.024 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 2.62\",\"<b> bootstrap dataset:  197 <\\/b> <br>Coef. Urban  : -0.046 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 2.689\",\"<b> bootstrap dataset:  198 <\\/b> <br>Coef. Urban  : -0.048 <br>Coef. Murder : 0.055 <br>Coef. Intercept : 2.33\",\"<b> bootstrap dataset:  199 <\\/b> <br>Coef. Urban  : -0.045 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 3.52\",\"<b> bootstrap dataset:  200 <\\/b> <br>Coef. Urban  : -0.062 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 4.329\",\"<b> bootstrap dataset:  201 <\\/b> <br>Coef. Urban  : -0.071 <br>Coef. Murder : 0.053 <br>Coef. Intercept : 3.652\",\"<b> bootstrap dataset:  202 <\\/b> <br>Coef. Urban  : -0.047 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 2.818\",\"<b> bootstrap dataset:  203 <\\/b> <br>Coef. Urban  : -0.048 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 2.515\",\"<b> bootstrap dataset:  204 <\\/b> <br>Coef. Urban  : -0.012 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 0.964\",\"<b> bootstrap dataset:  205 <\\/b> <br>Coef. Urban  : -0.045 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 2.999\",\"<b> bootstrap dataset:  206 <\\/b> <br>Coef. Urban  : -0.062 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 3.58\",\"<b> bootstrap dataset:  207 <\\/b> <br>Coef. Urban  : -0.046 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 2.946\",\"<b> bootstrap dataset:  208 <\\/b> <br>Coef. Urban  : -0.056 <br>Coef. Murder : 0.054 <br>Coef. Intercept : 2.738\",\"<b> bootstrap dataset:  209 <\\/b> <br>Coef. Urban  : -0.027 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 2.471\",\"<b> bootstrap dataset:  210 <\\/b> <br>Coef. Urban  : -0.077 <br>Coef. Murder : 0.037 <br>Coef. Intercept : 6.646\",\"<b> bootstrap dataset:  211 <\\/b> <br>Coef. Urban  : -0.076 <br>Coef. Murder : 0.053 <br>Coef. Intercept : 3.734\",\"<b> bootstrap dataset:  212 <\\/b> <br>Coef. Urban  : -0.052 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 3.066\",\"<b> bootstrap dataset:  213 <\\/b> <br>Coef. Urban  : -0.073 <br>Coef. Murder : 0.056 <br>Coef. Intercept : 3.535\",\"<b> bootstrap dataset:  214 <\\/b> <br>Coef. Urban  : -0.062 <br>Coef. Murder : 0.056 <br>Coef. Intercept : 2.744\",\"<b> bootstrap dataset:  215 <\\/b> <br>Coef. Urban  : -0.036 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 2.856\",\"<b> bootstrap dataset:  216 <\\/b> <br>Coef. Urban  : -0.039 <br>Coef. Murder : 0.05 <br>Coef. Intercept : 1.973\",\"<b> bootstrap dataset:  217 <\\/b> <br>Coef. Urban  : -0.045 <br>Coef. Murder : 0.037 <br>Coef. Intercept : 4.444\",\"<b> bootstrap dataset:  218 <\\/b> <br>Coef. Urban  : -0.05 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 3.052\",\"<b> bootstrap dataset:  219 <\\/b> <br>Coef. Urban  : -0.058 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 4.17\",\"<b> bootstrap dataset:  220 <\\/b> <br>Coef. Urban  : -0.035 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 3.058\",\"<b> bootstrap dataset:  221 <\\/b> <br>Coef. Urban  : -0.035 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 2.459\",\"<b> bootstrap dataset:  222 <\\/b> <br>Coef. Urban  : -0.005 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 0.779\",\"<b> bootstrap dataset:  223 <\\/b> <br>Coef. Urban  : -0.058 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 3.841\",\"<b> bootstrap dataset:  224 <\\/b> <br>Coef. Urban  : -0.002 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 1.226\",\"<b> bootstrap dataset:  225 <\\/b> <br>Coef. Urban  : -0.027 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 2.655\",\"<b> bootstrap dataset:  226 <\\/b> <br>Coef. Urban  : -0.016 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 2.58\",\"<b> bootstrap dataset:  227 <\\/b> <br>Coef. Urban  : -0.02 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 1.993\",\"<b> bootstrap dataset:  228 <\\/b> <br>Coef. Urban  : -0.048 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 3.359\",\"<b> bootstrap dataset:  229 <\\/b> <br>Coef. Urban  : -0.052 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 3.409\",\"<b> bootstrap dataset:  230 <\\/b> <br>Coef. Urban  : -0.057 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 3.334\",\"<b> bootstrap dataset:  231 <\\/b> <br>Coef. Urban  : -0.014 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 0.993\",\"<b> bootstrap dataset:  232 <\\/b> <br>Coef. Urban  : -0.007 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 1.618\",\"<b> bootstrap dataset:  233 <\\/b> <br>Coef. Urban  : -0.016 <br>Coef. Murder : 0.035 <br>Coef. Intercept : 2.245\",\"<b> bootstrap dataset:  234 <\\/b> <br>Coef. Urban  : -0.05 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 4.421\",\"<b> bootstrap dataset:  235 <\\/b> <br>Coef. Urban  : -0.062 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 4.487\",\"<b> bootstrap dataset:  236 <\\/b> <br>Coef. Urban  : -0.041 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 3.275\",\"<b> bootstrap dataset:  237 <\\/b> <br>Coef. Urban  : -0.073 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 5.034\",\"<b> bootstrap dataset:  238 <\\/b> <br>Coef. Urban  : -0.027 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 2.528\",\"<b> bootstrap dataset:  239 <\\/b> <br>Coef. Urban  : -0.057 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 3.518\",\"<b> bootstrap dataset:  240 <\\/b> <br>Coef. Urban  : -0.086 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 6.463\",\"<b> bootstrap dataset:  241 <\\/b> <br>Coef. Urban  : -0.047 <br>Coef. Murder : 0.038 <br>Coef. Intercept : 4.801\",\"<b> bootstrap dataset:  242 <\\/b> <br>Coef. Urban  : -0.082 <br>Coef. Murder : 0.055 <br>Coef. Intercept : 4.019\",\"<b> bootstrap dataset:  243 <\\/b> <br>Coef. Urban  : -0.042 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 2.511\",\"<b> bootstrap dataset:  244 <\\/b> <br>Coef. Urban  : -0.051 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 3.879\",\"<b> bootstrap dataset:  245 <\\/b> <br>Coef. Urban  : -0.02 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 2.14\",\"<b> bootstrap dataset:  246 <\\/b> <br>Coef. Urban  : -0.094 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 6.508\",\"<b> bootstrap dataset:  247 <\\/b> <br>Coef. Urban  : -0.063 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 4.061\",\"<b> bootstrap dataset:  248 <\\/b> <br>Coef. Urban  : -0.036 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 2.217\",\"<b> bootstrap dataset:  249 <\\/b> <br>Coef. Urban  : -0.036 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 2.793\",\"<b> bootstrap dataset:  250 <\\/b> <br>Coef. Urban  : -0.072 <br>Coef. Murder : 0.052 <br>Coef. Intercept : 3.664\",\"<b> bootstrap dataset:  251 <\\/b> <br>Coef. Urban  : -0.032 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 2.273\",\"<b> bootstrap dataset:  252 <\\/b> <br>Coef. Urban  : -0.022 <br>Coef. Murder : 0.053 <br>Coef. Intercept : 0.573\",\"<b> bootstrap dataset:  253 <\\/b> <br>Coef. Urban  : -0.057 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 4.902\",\"<b> bootstrap dataset:  254 <\\/b> <br>Coef. Urban  : -0.068 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 3.796\",\"<b> bootstrap dataset:  255 <\\/b> <br>Coef. Urban  : -0.039 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 2.402\",\"<b> bootstrap dataset:  256 <\\/b> <br>Coef. Urban  : -0.062 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 5.777\",\"<b> bootstrap dataset:  257 <\\/b> <br>Coef. Urban  : -0.019 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 1.662\",\"<b> bootstrap dataset:  258 <\\/b> <br>Coef. Urban  : -0.052 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 4.024\",\"<b> bootstrap dataset:  259 <\\/b> <br>Coef. Urban  : -0.066 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 5.255\",\"<b> bootstrap dataset:  260 <\\/b> <br>Coef. Urban  : -0.082 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 5.373\",\"<b> bootstrap dataset:  261 <\\/b> <br>Coef. Urban  : -0.025 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 1.75\",\"<b> bootstrap dataset:  262 <\\/b> <br>Coef. Urban  : -0.07 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 4.935\",\"<b> bootstrap dataset:  263 <\\/b> <br>Coef. Urban  : -0.095 <br>Coef. Murder : 0.051 <br>Coef. Intercept : 4.699\",\"<b> bootstrap dataset:  264 <\\/b> <br>Coef. Urban  : -0.082 <br>Coef. Murder : 0.057 <br>Coef. Intercept : 3.993\",\"<b> bootstrap dataset:  265 <\\/b> <br>Coef. Urban  : -0.075 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 5.029\",\"<b> bootstrap dataset:  266 <\\/b> <br>Coef. Urban  : -0.077 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 6.172\",\"<b> bootstrap dataset:  267 <\\/b> <br>Coef. Urban  : -0.017 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 1.554\",\"<b> bootstrap dataset:  268 <\\/b> <br>Coef. Urban  : -0.024 <br>Coef. Murder : 0.037 <br>Coef. Intercept : 2.377\",\"<b> bootstrap dataset:  269 <\\/b> <br>Coef. Urban  : -0.047 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 3.863\",\"<b> bootstrap dataset:  270 <\\/b> <br>Coef. Urban  : -0.08 <br>Coef. Murder : 0.05 <br>Coef. Intercept : 4.46\",\"<b> bootstrap dataset:  271 <\\/b> <br>Coef. Urban  : -0.048 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 3.121\",\"<b> bootstrap dataset:  272 <\\/b> <br>Coef. Urban  : -0.052 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 3.224\",\"<b> bootstrap dataset:  273 <\\/b> <br>Coef. Urban  : -0.027 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 2.091\",\"<b> bootstrap dataset:  274 <\\/b> <br>Coef. Urban  : -0.08 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 5.916\",\"<b> bootstrap dataset:  275 <\\/b> <br>Coef. Urban  : -0.043 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 3.553\",\"<b> bootstrap dataset:  276 <\\/b> <br>Coef. Urban  : -0.036 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 3.508\",\"<b> bootstrap dataset:  277 <\\/b> <br>Coef. Urban  : -0.077 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 5.065\",\"<b> bootstrap dataset:  278 <\\/b> <br>Coef. Urban  : -0.027 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 2.933\",\"<b> bootstrap dataset:  279 <\\/b> <br>Coef. Urban  : -0.106 <br>Coef. Murder : 0.05 <br>Coef. Intercept : 6.843\",\"<b> bootstrap dataset:  280 <\\/b> <br>Coef. Urban  : -0.051 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 4.562\",\"<b> bootstrap dataset:  281 <\\/b> <br>Coef. Urban  : -0.055 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 3.418\",\"<b> bootstrap dataset:  282 <\\/b> <br>Coef. Urban  : -0.051 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 2.773\",\"<b> bootstrap dataset:  283 <\\/b> <br>Coef. Urban  : -0.054 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 3.187\",\"<b> bootstrap dataset:  284 <\\/b> <br>Coef. Urban  : -0.063 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 4.449\",\"<b> bootstrap dataset:  285 <\\/b> <br>Coef. Urban  : -0.019 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 2.1\",\"<b> bootstrap dataset:  286 <\\/b> <br>Coef. Urban  : -0.031 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 2.367\",\"<b> bootstrap dataset:  287 <\\/b> <br>Coef. Urban  : -0.062 <br>Coef. Murder : 0.048 <br>Coef. Intercept : 3.338\",\"<b> bootstrap dataset:  288 <\\/b> <br>Coef. Urban  : -0.033 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 2.549\",\"<b> bootstrap dataset:  289 <\\/b> <br>Coef. Urban  : -0.078 <br>Coef. Murder : 0.048 <br>Coef. Intercept : 5.001\",\"<b> bootstrap dataset:  290 <\\/b> <br>Coef. Urban  : -0.098 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 6.167\",\"<b> bootstrap dataset:  291 <\\/b> <br>Coef. Urban  : -0.064 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 3.835\",\"<b> bootstrap dataset:  292 <\\/b> <br>Coef. Urban  : -0.056 <br>Coef. Murder : 0.048 <br>Coef. Intercept : 3.158\",\"<b> bootstrap dataset:  293 <\\/b> <br>Coef. Urban  : -0.059 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 3.267\",\"<b> bootstrap dataset:  294 <\\/b> <br>Coef. Urban  : -0.075 <br>Coef. Murder : 0.048 <br>Coef. Intercept : 4.765\",\"<b> bootstrap dataset:  295 <\\/b> <br>Coef. Urban  : -0.016 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 2.061\",\"<b> bootstrap dataset:  296 <\\/b> <br>Coef. Urban  : -0.036 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 2.626\",\"<b> bootstrap dataset:  297 <\\/b> <br>Coef. Urban  : -0.036 <br>Coef. Murder : 0.05 <br>Coef. Intercept : 2.002\",\"<b> bootstrap dataset:  298 <\\/b> <br>Coef. Urban  : -0.051 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 3.91\",\"<b> bootstrap dataset:  299 <\\/b> <br>Coef. Urban  : -0.035 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 1.78\",\"<b> bootstrap dataset:  300 <\\/b> <br>Coef. Urban  : -0.008 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 0.568\",\"<b> bootstrap dataset:  301 <\\/b> <br>Coef. Urban  : -0.07 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 4.626\",\"<b> bootstrap dataset:  302 <\\/b> <br>Coef. Urban  : -0.064 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 4.28\",\"<b> bootstrap dataset:  303 <\\/b> <br>Coef. Urban  : -0.017 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 1.149\",\"<b> bootstrap dataset:  304 <\\/b> <br>Coef. Urban  : -0.046 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 3.691\",\"<b> bootstrap dataset:  305 <\\/b> <br>Coef. Urban  : -0.057 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 4.269\",\"<b> bootstrap dataset:  306 <\\/b> <br>Coef. Urban  : -0.055 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 4.244\",\"<b> bootstrap dataset:  307 <\\/b> <br>Coef. Urban  : -0.084 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 5.985\",\"<b> bootstrap dataset:  308 <\\/b> <br>Coef. Urban  : -0.046 <br>Coef. Murder : 0.05 <br>Coef. Intercept : 2.579\",\"<b> bootstrap dataset:  309 <\\/b> <br>Coef. Urban  : -0.013 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 0.684\",\"<b> bootstrap dataset:  310 <\\/b> <br>Coef. Urban  : -0.038 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 2.485\",\"<b> bootstrap dataset:  311 <\\/b> <br>Coef. Urban  : -0.119 <br>Coef. Murder : 0.066 <br>Coef. Intercept : 4.884\",\"<b> bootstrap dataset:  312 <\\/b> <br>Coef. Urban  : -0.021 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 1.713\",\"<b> bootstrap dataset:  313 <\\/b> <br>Coef. Urban  : -0.018 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 1.421\",\"<b> bootstrap dataset:  314 <\\/b> <br>Coef. Urban  : -0.034 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 3.445\",\"<b> bootstrap dataset:  315 <\\/b> <br>Coef. Urban  : -0.049 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 3.01\",\"<b> bootstrap dataset:  316 <\\/b> <br>Coef. Urban  : -0.04 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 3.796\",\"<b> bootstrap dataset:  317 <\\/b> <br>Coef. Urban  : -0.071 <br>Coef. Murder : 0.05 <br>Coef. Intercept : 4.171\",\"<b> bootstrap dataset:  318 <\\/b> <br>Coef. Urban  : -0.097 <br>Coef. Murder : 0.056 <br>Coef. Intercept : 5.024\",\"<b> bootstrap dataset:  319 <\\/b> <br>Coef. Urban  : -0.042 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 2.874\",\"<b> bootstrap dataset:  320 <\\/b> <br>Coef. Urban  : -0.081 <br>Coef. Murder : 0.052 <br>Coef. Intercept : 3.864\",\"<b> bootstrap dataset:  321 <\\/b> <br>Coef. Urban  : -0.07 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 3.483\",\"<b> bootstrap dataset:  322 <\\/b> <br>Coef. Urban  : -0.065 <br>Coef. Murder : 0.05 <br>Coef. Intercept : 3.582\",\"<b> bootstrap dataset:  323 <\\/b> <br>Coef. Urban  : -0.063 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 3.662\",\"<b> bootstrap dataset:  324 <\\/b> <br>Coef. Urban  : -0.042 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 3.3\",\"<b> bootstrap dataset:  325 <\\/b> <br>Coef. Urban  : -0.051 <br>Coef. Murder : 0.05 <br>Coef. Intercept : 2.399\",\"<b> bootstrap dataset:  326 <\\/b> <br>Coef. Urban  : -0.078 <br>Coef. Murder : 0.048 <br>Coef. Intercept : 4.455\",\"<b> bootstrap dataset:  327 <\\/b> <br>Coef. Urban  : -0.077 <br>Coef. Murder : 0.064 <br>Coef. Intercept : 2.705\",\"<b> bootstrap dataset:  328 <\\/b> <br>Coef. Urban  : -0.058 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 4.848\",\"<b> bootstrap dataset:  329 <\\/b> <br>Coef. Urban  : -0.046 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 3.2\",\"<b> bootstrap dataset:  330 <\\/b> <br>Coef. Urban  : -0.003 <br>Coef. Murder : 0.051 <br>Coef. Intercept : -0.436\",\"<b> bootstrap dataset:  331 <\\/b> <br>Coef. Urban  : -0.077 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 4.97\",\"<b> bootstrap dataset:  332 <\\/b> <br>Coef. Urban  : -0.075 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 4.577\",\"<b> bootstrap dataset:  333 <\\/b> <br>Coef. Urban  : -0.057 <br>Coef. Murder : 0.05 <br>Coef. Intercept : 3.177\",\"<b> bootstrap dataset:  334 <\\/b> <br>Coef. Urban  : -0.069 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 4.785\",\"<b> bootstrap dataset:  335 <\\/b> <br>Coef. Urban  : -0.044 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 2.222\",\"<b> bootstrap dataset:  336 <\\/b> <br>Coef. Urban  : -0.044 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 2.683\",\"<b> bootstrap dataset:  337 <\\/b> <br>Coef. Urban  : -0.039 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 2.761\",\"<b> bootstrap dataset:  338 <\\/b> <br>Coef. Urban  : -0.113 <br>Coef. Murder : 0.051 <br>Coef. Intercept : 6.82\",\"<b> bootstrap dataset:  339 <\\/b> <br>Coef. Urban  : -0.042 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 2.971\",\"<b> bootstrap dataset:  340 <\\/b> <br>Coef. Urban  : -0.035 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 2.729\",\"<b> bootstrap dataset:  341 <\\/b> <br>Coef. Urban  : -0.12 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 8.611\",\"<b> bootstrap dataset:  342 <\\/b> <br>Coef. Urban  : -0.038 <br>Coef. Murder : 0.038 <br>Coef. Intercept : 3.556\",\"<b> bootstrap dataset:  343 <\\/b> <br>Coef. Urban  : -0.043 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 2.485\",\"<b> bootstrap dataset:  344 <\\/b> <br>Coef. Urban  : -0.065 <br>Coef. Murder : 0.037 <br>Coef. Intercept : 6.308\",\"<b> bootstrap dataset:  345 <\\/b> <br>Coef. Urban  : -0.113 <br>Coef. Murder : 0.053 <br>Coef. Intercept : 6.703\",\"<b> bootstrap dataset:  346 <\\/b> <br>Coef. Urban  : -0.026 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 2.379\",\"<b> bootstrap dataset:  347 <\\/b> <br>Coef. Urban  : -0.064 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 4.647\",\"<b> bootstrap dataset:  348 <\\/b> <br>Coef. Urban  : -0.06 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 4.54\",\"<b> bootstrap dataset:  349 <\\/b> <br>Coef. Urban  : -0.009 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 0.677\",\"<b> bootstrap dataset:  350 <\\/b> <br>Coef. Urban  : -0.049 <br>Coef. Murder : 0.042 <br>Coef. Intercept : 3.406\",\"<b> bootstrap dataset:  351 <\\/b> <br>Coef. Urban  : -0.064 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 4.001\",\"<b> bootstrap dataset:  352 <\\/b> <br>Coef. Urban  : -0.027 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 2.157\",\"<b> bootstrap dataset:  353 <\\/b> <br>Coef. Urban  : -0.056 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 4.493\",\"<b> bootstrap dataset:  354 <\\/b> <br>Coef. Urban  : -0.064 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 4.231\",\"<b> bootstrap dataset:  355 <\\/b> <br>Coef. Urban  : -0.026 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 2.282\",\"<b> bootstrap dataset:  356 <\\/b> <br>Coef. Urban  : -0.031 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 2.041\",\"<b> bootstrap dataset:  357 <\\/b> <br>Coef. Urban  : -0.106 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 6.349\",\"<b> bootstrap dataset:  358 <\\/b> <br>Coef. Urban  : -0.063 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 3.465\",\"<b> bootstrap dataset:  359 <\\/b> <br>Coef. Urban  : -0.045 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 3.463\",\"<b> bootstrap dataset:  360 <\\/b> <br>Coef. Urban  : -0.119 <br>Coef. Murder : 0.055 <br>Coef. Intercept : 6.221\",\"<b> bootstrap dataset:  361 <\\/b> <br>Coef. Urban  : 0.001 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 0.924\",\"<b> bootstrap dataset:  362 <\\/b> <br>Coef. Urban  : -0.037 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 2.603\",\"<b> bootstrap dataset:  363 <\\/b> <br>Coef. Urban  : -0.031 <br>Coef. Murder : 0.047 <br>Coef. Intercept : 2.039\",\"<b> bootstrap dataset:  364 <\\/b> <br>Coef. Urban  : -0.052 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 3.243\",\"<b> bootstrap dataset:  365 <\\/b> <br>Coef. Urban  : -0.084 <br>Coef. Murder : 0.057 <br>Coef. Intercept : 3.835\",\"<b> bootstrap dataset:  366 <\\/b> <br>Coef. Urban  : -0.081 <br>Coef. Murder : 0.054 <br>Coef. Intercept : 4.157\",\"<b> bootstrap dataset:  367 <\\/b> <br>Coef. Urban  : -0.046 <br>Coef. Murder : 0.048 <br>Coef. Intercept : 2.539\",\"<b> bootstrap dataset:  368 <\\/b> <br>Coef. Urban  : -0.05 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 4.108\",\"<b> bootstrap dataset:  369 <\\/b> <br>Coef. Urban  : -0.04 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 3.3\",\"<b> bootstrap dataset:  370 <\\/b> <br>Coef. Urban  : -0.065 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 4.681\",\"<b> bootstrap dataset:  371 <\\/b> <br>Coef. Urban  : -0.015 <br>Coef. Murder : 0.041 <br>Coef. Intercept : 1.273\",\"<b> bootstrap dataset:  372 <\\/b> <br>Coef. Urban  : -0.079 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 5.44\",\"<b> bootstrap dataset:  373 <\\/b> <br>Coef. Urban  : -0.056 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 3.421\",\"<b> bootstrap dataset:  374 <\\/b> <br>Coef. Urban  : -0.043 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 2.52\",\"<b> bootstrap dataset:  375 <\\/b> <br>Coef. Urban  : -0.038 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 1.912\",\"<b> bootstrap dataset:  376 <\\/b> <br>Coef. Urban  : -0.028 <br>Coef. Murder : 0.038 <br>Coef. Intercept : 3.069\",\"<b> bootstrap dataset:  377 <\\/b> <br>Coef. Urban  : -0.071 <br>Coef. Murder : 0.05 <br>Coef. Intercept : 4.258\",\"<b> bootstrap dataset:  378 <\\/b> <br>Coef. Urban  : -0.018 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 1.186\",\"<b> bootstrap dataset:  379 <\\/b> <br>Coef. Urban  : -0.04 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 3.119\",\"<b> bootstrap dataset:  380 <\\/b> <br>Coef. Urban  : -0.053 <br>Coef. Murder : 0.048 <br>Coef. Intercept : 3.163\",\"<b> bootstrap dataset:  381 <\\/b> <br>Coef. Urban  : -0.057 <br>Coef. Murder : 0.054 <br>Coef. Intercept : 2.503\",\"<b> bootstrap dataset:  382 <\\/b> <br>Coef. Urban  : -0.002 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 1.097\",\"<b> bootstrap dataset:  383 <\\/b> <br>Coef. Urban  : -0.044 <br>Coef. Murder : 0.038 <br>Coef. Intercept : 3.841\",\"<b> bootstrap dataset:  384 <\\/b> <br>Coef. Urban  : -0.046 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 4.173\",\"<b> bootstrap dataset:  385 <\\/b> <br>Coef. Urban  : 0.006 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 0.189\",\"<b> bootstrap dataset:  386 <\\/b> <br>Coef. Urban  : -0.049 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 4.238\",\"<b> bootstrap dataset:  387 <\\/b> <br>Coef. Urban  : -0.012 <br>Coef. Murder : 0.039 <br>Coef. Intercept : 1.829\",\"<b> bootstrap dataset:  388 <\\/b> <br>Coef. Urban  : -0.078 <br>Coef. Murder : 0.037 <br>Coef. Intercept : 6.709\",\"<b> bootstrap dataset:  389 <\\/b> <br>Coef. Urban  : -0.061 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 3.711\",\"<b> bootstrap dataset:  390 <\\/b> <br>Coef. Urban  : -0.013 <br>Coef. Murder : 0.048 <br>Coef. Intercept : 1.233\",\"<b> bootstrap dataset:  391 <\\/b> <br>Coef. Urban  : -0.015 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 0.872\",\"<b> bootstrap dataset:  392 <\\/b> <br>Coef. Urban  : -0.041 <br>Coef. Murder : 0.04 <br>Coef. Intercept : 3.542\",\"<b> bootstrap dataset:  393 <\\/b> <br>Coef. Urban  : -0.06 <br>Coef. Murder : 0.045 <br>Coef. Intercept : 4.263\",\"<b> bootstrap dataset:  394 <\\/b> <br>Coef. Urban  : -0.028 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 1.812\",\"<b> bootstrap dataset:  395 <\\/b> <br>Coef. Urban  : -0.045 <br>Coef. Murder : 0.043 <br>Coef. Intercept : 3.582\",\"<b> bootstrap dataset:  396 <\\/b> <br>Coef. Urban  : -0.089 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 5.801\",\"<b> bootstrap dataset:  397 <\\/b> <br>Coef. Urban  : -0.107 <br>Coef. Murder : 0.044 <br>Coef. Intercept : 6.947\",\"<b> bootstrap dataset:  398 <\\/b> <br>Coef. Urban  : -0.067 <br>Coef. Murder : 0.046 <br>Coef. Intercept : 4.183\",\"<b> bootstrap dataset:  399 <\\/b> <br>Coef. Urban  : -0.063 <br>Coef. Murder : 0.049 <br>Coef. Intercept : 3.985\"],\"hoverinfo\":[\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\"],\"showlegend\":false,\"marker\":{\"color\":\"rgba(0, 0, 0, 0.5)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"type\":\"scatter\",\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"line\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n## Hypothesis Tests\n\n#### **F-statistic**. {-}\n\nWe can also use an $F$ test for any $q$ hypotheses. Specifically, when $q$ hypotheses *restrict* a model, the degrees of freedom drop from $k_{u}$ to $k_{r}$ and the residual sum of squares $RSS=\\sum_{i}(y_{i}-\\widehat{y}_{i})^2$ typically increases. We compute the statistic\n$$\nF_{q} = \\frac{(RSS_{r}-RSS_{u})/(k_{u}-k_{r})}{RSS_{u}/(N-k_{u})}\n$$\n\nIf you test whether all $K$ variables are significant, the restricted model is a simple intercept and $RSS_{r}=TSS$, and $F_{q}$ can be written in terms of $R^2$: $F_{K} = \\frac{R^2}{1-R^2} \\frac{N-K}{K-1}$. The first fraction is the relative goodness of fit, and the second fraction is an adjustment for degrees of freedom (similar to how we  adjusted the $R^2$ term before). \n\nTo conduct a hypothesis test, first compute a null distribution by randomly reshuffling the outcomes and recompute the $F$ statistic, and then compare how often random data give something as extreme as your initial statistic. For some intuition on this F test, examine how the adjusted $R^2$ statistic varies with bootstrap samples. \n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap under the null\nboots <- 1:399\nboot_regs0 <- lapply(boots, function(b){\n  # Generate bootstrap sample\n  xy_b <- USArrests\n  b_id <- sample( nrow(USArrests), replace=T)\n  # Impose the null\n  xy_b$Murder <-  xy_b$Murder[b_id]\n  # Run regression\n  reg_b <- lm(Murder~Assault+UrbanPop, dat=xy_b)\n})\n# Get null distribution for adjusted R2\nR2adj_sim0 <- sapply(boot_regs0, function(reg_k){\n    summary(reg_k)$adj.r.squared })\nhist(R2adj_sim0, xlim=c(-.1,1), breaks=25, border=NA,\n    main='', xlab=expression('adj.'~R[b]^2))\n\n# Compare to initial statistic\nabline(v=summary(reg)$adj.r.squared, lwd=2, col=2)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nNote that *hypothesis testing is not to be done routinely*, as additional complications arise when testing multiple hypothesis sequentially.\n\nUnder some additional assumptions $F_{q}$ follows an F-distribution. For more about F-testing, see https://online.stat.psu.edu/stat501/lesson/6/6.2 and https://www.econometrics.blog/post/understanding-the-f-statistic/\n\n#### **ANOVA** {-}\n\n\n\n## Further Reading\n\nFor OLS, see\n\n* https://bookdown.org/josiesmith/qrmbook/linear-estimation-and-minimizing-error.html\n* https://www.econometrics-with-r.org/4-lrwor.html\n* https://www.econometrics-with-r.org/6-rmwmr.html\n* https://www.econometrics-with-r.org/7-htaciimr.html\n* https://bookdown.org/ripberjt/labbook/bivariate-linear-regression.html\n* https://bookdown.org/ripberjt/labbook/multivariable-linear-regression.html\n* https://online.stat.psu.edu/stat462/node/137/\n* https://book.stat420.org/\n* Hill, Griffiths & Lim (2007), Principles of Econometrics, 3rd ed., Wiley, S. 86f.\n* Verbeek (2004), A Guide to Modern Econometrics, 2nd ed., Wiley, S. 51ff.\n* Asteriou & Hall (2011), Applied Econometrics, 2nd ed., Palgrave MacMillan, S. 177ff.\n* https://online.stat.psu.edu/stat485/lesson/11/\n\nTo derive OLS coefficients in Matrix form, see\n\n* https://jrnold.github.io/intro-methods-notes/ols-in-matrix-form.html\n* https://www.fsb.miamioh.edu/lij14/411_note_matrix.pdf\n* https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf\n\nFor fixed effects, see\n\n* https://www.econometrics-with-r.org/10-rwpd.html\n* https://bookdown.org/josiesmith/qrmbook/topics-in-multiple-regression.html\n* https://bookdown.org/ripberjt/labbook/multivariable-linear-regression.html\n* https://www.princeton.edu/~otorres/Panel101.pdf\n* https://www.stata.com/manuals13/xtxtreg.pdf\n\n\n# Intermediate Regression II\n***\n\n\n## Coefficient Interpretation\n\nNotice that we have gotten pretty far without actually trying to meaningfully interpret regression coefficients. That is because the above procedure will always give us number, regardless as to whether the true data generating process is linear or not. So, to be cautious, we have been interpreting the regression outputs while being agnostic as to how the data are generated. We now consider a special situation where we know the data are generated according to a linear process and are only uncertain about the parameter values.\n\n*If* the data generating process is \n$$\ny=X\\beta + \\epsilon\\\\\n\\mathbb{E}[\\epsilon | X]=0,\n$$\nthen we have a famous result that lets us attach a simple interpretation of OLS coefficients as unbiased estimates of the effect of X:\n$$\n\\hat{\\beta} = (X'X)^{-1}X'y = (X'X)^{-1}X'(X\\beta + \\epsilon) = \\beta + (X'X)^{-1}X'\\epsilon\\\\\n\\mathbb{E}\\left[ \\hat{\\beta} \\right] = \\mathbb{E}\\left[ (X'X)^{-1}X'y \\right] = \\beta + (X'X)^{-1}\\mathbb{E}\\left[ X'\\epsilon \\right] = \\beta\n$$\n\n\nGenerate a simulated dataset with 30 observations and two exogenous variables. Assume the following relationship: $y_{i} = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i$ where the variables and the error term are realizations of the following data generating processes (DGP):\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 30\nB <- c(10, 2, -1)\n\nx1 <- runif(N, 0, 5)\nx2 <- rbinom(N,1,.7)\nX <- cbind(1,x1,x2)\ne <- rnorm(N,0,3)\nY <- X%*%B + e\ndat <- data.frame(Y,X)\ncoef(lm(Y~x1+x2, data=dat))\n## (Intercept)          x1          x2 \n##    8.679582    2.681828   -2.005150\n```\n:::\n\n\nSimulate the distribution of coefficients under a correctly specified model. Interpret the average.\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 30\nB <- c(10, 2, -1)\n\nCoefs <- sapply(1:400, function(sim){\n    x1 <- runif(N, 0, 5)\n    x2 <- rbinom(N,1,.7)\n    X <- cbind(1,x1,x2)\n    e <- rnorm(N,0,3)\n    Y <- X%*%B + e\n    dat <- data.frame(Y,x1,x2)\n    coef(lm(Y~x1+x2, data=dat))\n})\n\npar(mfrow=c(1,2))\nfor(i in 2:3){\n    hist(Coefs[i,], xlab=bquote(beta[.(i)]), main='', border=NA)\n    abline(v=mean(Coefs[i,]), lwd=2)\n    abline(v=B[i], col=rgb(1,0,0))\n}\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n\nMany economic phenomena are nonlinear, even when including potential transforms of $Y$ and $X$. Sometimes the linear model may still be a good or even great approximation. But sometimes not, and it is hard to know ex-ante. Examine the distribution of coefficients under this mispecified model and try to interpret the average.\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 30\n\nCoefs <- sapply(1:600, function(sim){\n    x2 <- runif(N, 0, 5)\n    x3 <- rbinom(N,1,.7)\n    e <- rnorm(N,0,3)\n    Y <- 10*x3 + 2*log(x2)^x3 + e\n    dat <- data.frame(Y,x2,x3)\n    coef(lm(Y~x2+x3, data=dat))\n})\n\npar(mfrow=c(1,2))\nfor(i in 2:3){\n    hist(Coefs[i,],  xlab=bquote(beta[.(i)]), main='', border=NA)\n    abline(v=mean(Coefs[i,]), col=1, lwd=2)\n}\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\nIn general, you can interpret your regression coefficients as \"adjusted correlations\". There are (many) tests for whether the relationships in your dataset are actually additively separable and linear.\n\n\n##  Diagnostics\n\nThere's little sense in getting great standard errors for a terrible model. Plotting your regression object a simple and easy step to help diagnose whether your model is in some way bad. We next go through what each of these figures show.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreg <- lm(Murder~Assault+UrbanPop, data=USArrests)\npar(mfrow=c(2,2))\nplot(reg, pch=16, col=grey(0,.5))\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-29-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n#### **Outliers**. {-}\nThe first diagnostic plot examines outliers in terms the outcome $y_i$ being far from its prediction $\\hat{y}_i$. You may be interested in such outliers because they can (but do not have to) unduly influence your estimates. \n\nThe third diagnostic plot examines another type of outlier, where an observation with the explanatory variable $x_i$ is far from the center of mass of the other $x$'s. A point has high *leverage* if the estimates change dramatically when you estimate the model without that data point.\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 40\nx <- c(25, runif(N-1,3,8))\ne <- rnorm(N,0,0.4)\ny <- 3 + 0.6*sqrt(x) + e\nplot(y~x, pch=16, col=grey(0,.5))\npoints(x[1],y[1], pch=16, col=rgb(1,0,0,.5))\n\nabline(lm(y~x), col=2, lty=2)\nabline(lm(y[-1]~x[-1]))\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\nSee [AEJ-leverage](https://www.rwi-essen.de/fileadmin/user_upload/RWI/Publikationen/I4R_Discussion_Paper_Series/032_I4R_Haddad_Kattan_Wochner-updateJune28.pdf) and [NBER-leverage](https://statmodeling.stat.columbia.edu/2025/02/28/the-r-squared-on-this-is-kinda-low-no/) for examples of leverage in economics.\n\nStandardized residuals are\n$$\nr_i=\\frac{\\hat{\\epsilon}_i}{s_{[i]}\\sqrt{1-h_i}},\n$$\nwhere $s_{[i]}$ is the root mean squared error of a regression with the $i$th observation removed and $h_i$ is the leverage of residual $\\hat{\\epsilon_i}$. \n\n::: {.cell}\n\n```{.r .cell-code}\nwhich.max(hatvalues(reg))\nwhich.max(rstandard(reg))\n```\n:::\n\n\n(See https://www.r-bloggers.com/2016/06/leverage-and-influence-in-a-nutshell/ for a good interactive explanation, and https://online.stat.psu.edu/stat462/node/87/ for detail.)\n\nThe fourth plot further assesses outlier $X$ using *Cook's Distance*, which sums of all prediction changes when observation i is removed and scales proportionally to the mean square error $s^2 = \\frac{\\sum_{i} (e_{i})^2 }{n-K}.\n$$\nD_{i} = \\frac{\\sum_{j} \\left( \\hat{y_j} - \\hat{y_j}_{[i]} \\right)^2 }{ p s^2 }\n= \\frac{[e_{i}]^2}{p s^2 } \\frac{h_i}{(1-h_i)^2}$$\n\n::: {.cell}\n\n```{.r .cell-code}\nwhich.max(cooks.distance(reg))\ncar::influencePlot(reg)\n```\n:::\n\n\n#### **Normality**. {-}\nThe second plot examines whether the residuals are normally distributed. Your OLS coefficient estimates do not depend on the normality of the residuals. (Good thing, because there's no reason the residuals of economic phenomena should be so well behaved.) Many hypothesis tests are, however, affected by the distribution of the residuals. For these reasons, you may be interested in assessing normality \n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\nhist(resid(reg), main='Histogram of Residuals',\n    font.main=1, border=NA)\n\nqqnorm(resid(reg), main=\"Normal Q-Q Plot of Residuals\",\n    font.main=1, col=grey(0,.5), pch=16)\nqqline(resid(reg), col=1, lty=2)\n\n#shapiro.test(resid(reg))\n```\n:::\n\n\nHeterskedasticity may also matters for variability estimates. This is not shown in the plot, but you can conduct a simple test\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmtest)\nlmtest::bptest(reg)\n```\n:::\n\n\n\n#### **Collinearity**. {-}\nThis is when one explanatory variable in a multiple linear regression model can be linearly predicted from the others with a substantial degree of accuracy. Coefficient estimates may change erratically in response to small changes in the model or the data. (In the extreme case where there are more variables than observations $K>N$, the inverse of $X'X$ has an infinite number of solutions.) To diagnose collinearity, we can use the *Variance Inflation Factor*\n$$\nVIF_{k}=\\frac{1}{1-R^2_k},\n$$\nwhere $R^2_k$ is the $R^2$ for the regression of $X_k$ on the other covariates $X_{-k}$ (a regression that does not involve the response variable Y)\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::vif(reg) \nsqrt(car::vif(reg)) > 2 # problem?\n```\n:::\n\n\n\n## Transformations\n\nTransforming variables can often improve your model fit while still allowing it estimated via OLS. This is because OLS only requires the model to be linear in the parameters. Under the assumptions of the model is correctly specified, the following table is how we can interpret the coefficients of the transformed data. (Note for small changes, $\\Delta ln(x) \\approx \\Delta x / x = \\Delta x \\% \\cdot 100$.)\n\n| *Specification* | *Regressand* | *Regressor* | *Derivative* | *Interpretation (If True)* |\n| --- | --- | --- | --- | --- |\n| linear--linear | $y$          | $x$   | $\\Delta y = \\beta_1\\cdot\\Delta x$ | Change $x$ by one unit $\\rightarrow$ change $y$ by $\\beta_1$ units.|\n| log--linear | $ln(y)$ | $x$ | $\\Delta y \\% \\cdot 100 \\approx \\beta_1 \\cdot \\Delta x$ | Change $x$ by one unit $\\rightarrow$ change $y$ by $100 \\cdot \\beta_1$ percent. |\n| linear--log | $y$ | $ln(x)$ | $\\Delta y \\approx  \\frac{\\beta_1}{100}\\cdot \\Delta x \\%$ | Change $x$ by one percent $\\rightarrow$ change $y$ by $\\frac{\\beta_1}{100}$ units |\n| log--log | $ln(y)$ | $ln(x)$ | $\\Delta y \\% \\approx \\beta_1\\cdot \\Delta x \\%$ | Change $x$ by one percent $\\rightarrow$ change $y$ by $\\beta_1$ percent|\n\nNow recall from micro theory that an additively seperable and linear production function is referred to as ``perfect substitutes''. With a linear model and untranformed data, you have implicitly modelled the different regressors $X$ as perfect substitutes. Further recall that the ''perfect substitutes'' model is a special case of the constant elasticity of substitution production function. Here, we will build on http://dx.doi.org/10.2139/ssrn.3917397, and consider box-cox transforming both $X$ and $y$. Specifically, apply the box-cox transform of $y$ using parameter $\\lambda$ and apply another box-cox transform to each $x$ using the same parameter $\\rho$ so that\n$$\ny^{(\\lambda)}_{i} = \\sum_{k=1}^{K}\\beta_{k} x^{(\\rho)}_{ik} + \\epsilon_{i}\\\\\ny^{(\\lambda)}_{i} =\n\\begin{cases}\n\\lambda^{-1}[ (y_i+1)^{\\lambda}- 1] & \\lambda \\neq 0 \\\\\nlog(y_i+1) &  \\lambda=0\n\\end{cases}.\\\\\nx^{(\\rho)}_{i} =\n\\begin{cases}\n\\rho^{-1}[ (x_i)^{\\rho}- 1] & \\rho \\neq 0 \\\\\nlog(x_{i}+1) &  \\rho=0\n\\end{cases}.\n$$\n\nNotice that this nests:\n\n * linear-linear $(\\rho=\\lambda=1)$.\n * linear-log $(\\rho=1, \\lambda=0)$.\n * log-linear $(\\rho=0, \\lambda=1)$.\n * log-log  $(\\rho=\\lambda=0)$.\n\n\nIf $\\rho=\\lambda$, we get the CES production function. This nests the ''perfect substitutes'' linear-linear model ($\\rho=\\lambda=1$) , the ''cobb-douglas''  log-log model  ($\\rho=\\lambda=0$), and many others. We can define $\\lambda=\\rho/\\lambda'$ to be clear that this is indeed a CES-type transformation where\n\n* $\\rho \\in (-\\infty,1]$ controls the \"substitutability\" of explanatory variables. E.g., $\\rho <0$ is ''complementary''.\n* $\\lambda$ determines ''returns to scale''. E.g., $\\lambda<1$ is ''decreasing returns''.\n\n\nWe compute the mean squared error in the original scale by inverting the predictions;\n$$\n\\widehat{y}_{i} =\n\\begin{cases}\n[ \\widehat{y}_{i}^{(\\lambda)} \\cdot \\lambda ]^{1/\\lambda} -1 & \\lambda  \\neq 0 \\\\\nexp( \\widehat{y}_{i}^{(\\lambda)}) -1 &  \\lambda=0\n\\end{cases}.\n$$\n\n\nIt is easiest to optimize parameters in a 2-step procedure called  `concentrated optimization'. We first solve for $\\widehat{\\beta}(\\rho,\\lambda)$ and compute the mean squared error $MSE(\\rho,\\lambda)$. We then find the $(\\rho,\\lambda)$ which minimizes $MSE$.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Box-Cox Transformation Function\nbxcx <- function( xy, rho){\n    if (rho == 0L) {\n      log(xy+1)\n    } else if(rho == 1L){\n      xy\n    } else {\n      ((xy+1)^rho - 1)/rho\n    }\n}\nbxcx_inv <- function( xy, rho){\n    if (rho == 0L) {\n      exp(xy) - 1\n    } else if(rho == 1L){\n      xy\n    } else {\n     (xy * rho + 1)^(1/rho) - 1\n    }\n}\n\n# Which Variables\nreg <- lm(Murder~Assault+UrbanPop, data=USArrests)\nX <- USArrests[,c('Assault','UrbanPop')]\nY <- USArrests[,'Murder']\n\n# Simple Grid Search over potential (Rho,Lambda) \nrl_df <- expand.grid(rho=seq(-2,2,by=.5),lambda=seq(-2,2,by=.5))\n\n# Compute Mean Squared Error\n# from OLS on Transformed Data\nerrors <- apply(rl_df,1,function(rl){\n    Xr <- bxcx(X,rl[[1]])\n    Yr <- bxcx(Y,rl[[2]])\n    Datr <- cbind(Murder=Yr,Xr)\n    Regr <- lm(Murder~Assault+UrbanPop, data=Datr)\n    Predr <- bxcx_inv(predict(Regr),rl[[2]])\n    Resr  <- (Y - Predr)\n    return(Resr)\n})\nrl_df$mse <- colMeans(errors^2)\n\n# Want Small MSE and Interpretable\nlayout(matrix(1:2,ncol=2), width=c(3,1), height=c(1,1))\npar(mar=c(4,4,2,0))\nplot(lambda~rho,rl_df, cex=8, pch=15,\n    xlab=expression(rho),\n    ylab=expression(lambda),\n    col=hcl.colors(25)[cut(1/rl_df$mse,25)])\n# Which min\nrl0 <- rl_df[which.min(rl_df$mse),c('rho','lambda')]\npoints(rl0$rho, rl0$lambda, pch=0, col=1, cex=8, lwd=2)\n# Legend\nplot(c(0,2),c(0,1), type='n', axes=F,\n    xlab='',ylab='', cex.main=.8,\n    main=expression(frac(1,'Mean Square Error')))\nrasterImage(as.raster(matrix(hcl.colors(25), ncol=1)), 0, 0, 1,1)\ntext(x=1.5, y=seq(1,0,l=10), cex=.5,\n    labels=levels(cut(1/rl_df$mse,10)))\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-36-1.png){fig-align='center' width=960}\n:::\n:::\n\n\nThe parameters $-1,0,1,2$ are easy to interpret and might be selected instead if there is only a small loss in fit. (In the above example, we might choose $\\lambda=0$ instead of the $\\lambda$ which minimized the mean square error). You can also plot the specific predictions to better understand the effect of data  transformation beyond mean squared error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot for Specific Comparisons\nXr <- bxcx(X,rl0[[1]])\nYr <- bxcx(Y,rl0[[2]])\nDatr <- cbind(Murder=Yr,Xr)\nRegr <- lm(Murder~Assault+UrbanPop, data=Datr)\nPredr <- bxcx_inv(predict(Regr),rl0[[2]])\n\ncols <- c(rgb(1,0,0,.5), col=rgb(0,0,1,.5))\nplot(Y, Predr, pch=16, col=cols[1], ylab='Prediction', \n    ylim=range(Y,Predr))\npoints(Y, predict(reg), pch=16, col=cols[2])\nlegend('topleft', pch=c(16), col=cols,\n    title=expression(rho~', '~lambda),\n    legend=c(  paste0(rl0, collapse=', '),'1, 1') )\nabline(a=0,b=1, lty=2)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\nWhen explicitly transforming data according to $\\lambda$ and $\\rho$, these parameters increase the degrees of freedom by two. The default hypothesis testing procedures do not account for you trying out different transformations, and should be adjusted by the increased degrees of freedom. Specification searches deflate standard errors and are a major source for false discoveries.\n\n## Regressograms\n\n#### **Break Points**. {-}\nKinks and Discontinuities in $X$ can also be modeled using factor variables. As such, $F$-tests can be used to examine whether a breaks is statistically significant.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER); data(CASchools)\nCASchools$score <- (CASchools$read + CASchools$math) / 2\nreg <- lm(score~income, data=CASchools)\n\n# F Test for Break\nreg2 <- lm(score ~ income*I(income>15), data=CASchools)\nanova(reg, reg2)\n\n# Chow Test for Break\ndata_splits <- split(CASchools, CASchools$income <= 15)\nresids <- sapply(data_splits, function(dat){\n    reg <- lm(score ~ income, data=dat)\n    sum( resid(reg)^2)\n})\nNs <-  sapply(data_splits, function(dat){ nrow(dat)})\nRt <- (sum(resid(reg)^2) - sum(resids))/sum(resids)\nRb <- (sum(Ns)-2*reg$rank)/reg$rank\nFt <- Rt*Rb\npf(Ft,reg$rank, sum(Ns)-2*reg$rank,lower.tail=F)\n\n# See also\n# strucchange::sctest(y~x, data=xy, type=\"Chow\", point=.5)\n# strucchange::Fstats(y~x, data=xy)\n\n# To Find Changes\n# segmented::segmented(reg)\n```\n:::\n\n\n#### **Gradient Summaries**. {-}\n\n#### **GoF**. {-}\n\n#### **ANOVA**. {-}\n\n\n\n\n\n\n\n\n## More Literature\n\n\nDiagnostics\n\n* https://book.stat420.org/model-diagnostics.html#leverage\n* https://socialsciences.mcmaster.ca/jfox/Books/RegressionDiagnostics/index.html\n* https://bookdown.org/ripberjt/labbook/diagnosing-and-addressing-problems-in-linear-regression.html\n* Belsley, D. A., Kuh, E., and Welsch, R. E. (1980). Regression Diagnostics: Identifying influential data and sources of collinearity. Wiley. https://doi.org/10.1002/0471725153\n* Fox, J. D. (2020). Regression diagnostics: An introduction (2nd ed.). SAGE. https://dx.doi.org/10.4135/9781071878651\n\n\n# Observational Data\n***\n\n## Temporal Interdependence\n\nMany observational datasets have temporal dependence, meaning that values at one point in time are related to past values. This violates the standard assumption of independence used in many statistical methods.\n\nStock prices are classic examples of temporally dependent processes. If Apple’s stock was high yesterday, it is more likely (but not guaranteed) to be high today.\n\n::: {.cell}\n\n```{.r .cell-code}\n# highest price each day\nlibrary(plotly)\nstock <- read.csv('https://raw.githubusercontent.com/plotly/datasets/master/finance-charts-apple.csv')\nfig <- plot_ly(stock, type = 'scatter', mode = 'lines')%>%\n  add_trace(x = ~Date, y = ~AAPL.High) %>%\n  layout(showlegend = F)\nfig\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-2c2bbe90e255aaab2f51\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-2c2bbe90e255aaab2f51\">{\"x\":{\"visdat\":{\"56977259ad32\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"56977259ad32\",\"attrs\":{\"56977259ad32\":{\"mode\":\"lines\",\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\"},\"56977259ad32.1\":{\"mode\":\"lines\",\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\",\"x\":{},\"y\":{},\"inherit\":true}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"showlegend\":false,\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Date\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"AAPL.High\"},\"hovermode\":\"closest\"},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"mode\":\"lines\",\"type\":\"scatter\",\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"line\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"mode\":\"lines\",\"type\":\"scatter\",\"x\":[\"2015-02-17\",\"2015-02-18\",\"2015-02-19\",\"2015-02-20\",\"2015-02-23\",\"2015-02-24\",\"2015-02-25\",\"2015-02-26\",\"2015-02-27\",\"2015-03-02\",\"2015-03-03\",\"2015-03-04\",\"2015-03-05\",\"2015-03-06\",\"2015-03-09\",\"2015-03-10\",\"2015-03-11\",\"2015-03-12\",\"2015-03-13\",\"2015-03-16\",\"2015-03-17\",\"2015-03-18\",\"2015-03-19\",\"2015-03-20\",\"2015-03-23\",\"2015-03-24\",\"2015-03-25\",\"2015-03-26\",\"2015-03-27\",\"2015-03-30\",\"2015-03-31\",\"2015-04-01\",\"2015-04-02\",\"2015-04-06\",\"2015-04-07\",\"2015-04-08\",\"2015-04-09\",\"2015-04-10\",\"2015-04-13\",\"2015-04-14\",\"2015-04-15\",\"2015-04-16\",\"2015-04-17\",\"2015-04-20\",\"2015-04-21\",\"2015-04-22\",\"2015-04-23\",\"2015-04-24\",\"2015-04-27\",\"2015-04-28\",\"2015-04-29\",\"2015-04-30\",\"2015-05-01\",\"2015-05-04\",\"2015-05-05\",\"2015-05-06\",\"2015-05-07\",\"2015-05-08\",\"2015-05-11\",\"2015-05-12\",\"2015-05-13\",\"2015-05-14\",\"2015-05-15\",\"2015-05-18\",\"2015-05-19\",\"2015-05-20\",\"2015-05-21\",\"2015-05-22\",\"2015-05-26\",\"2015-05-27\",\"2015-05-28\",\"2015-05-29\",\"2015-06-01\",\"2015-06-02\",\"2015-06-03\",\"2015-06-04\",\"2015-06-05\",\"2015-06-08\",\"2015-06-09\",\"2015-06-10\",\"2015-06-11\",\"2015-06-12\",\"2015-06-15\",\"2015-06-16\",\"2015-06-17\",\"2015-06-18\",\"2015-06-19\",\"2015-06-22\",\"2015-06-23\",\"2015-06-24\",\"2015-06-25\",\"2015-06-26\",\"2015-06-29\",\"2015-06-30\",\"2015-07-01\",\"2015-07-02\",\"2015-07-06\",\"2015-07-07\",\"2015-07-08\",\"2015-07-09\",\"2015-07-10\",\"2015-07-13\",\"2015-07-14\",\"2015-07-15\",\"2015-07-16\",\"2015-07-17\",\"2015-07-20\",\"2015-07-21\",\"2015-07-22\",\"2015-07-23\",\"2015-07-24\",\"2015-07-27\",\"2015-07-28\",\"2015-07-29\",\"2015-07-30\",\"2015-07-31\",\"2015-08-03\",\"2015-08-04\",\"2015-08-05\",\"2015-08-06\",\"2015-08-07\",\"2015-08-10\",\"2015-08-11\",\"2015-08-12\",\"2015-08-13\",\"2015-08-14\",\"2015-08-17\",\"2015-08-18\",\"2015-08-19\",\"2015-08-20\",\"2015-08-21\",\"2015-08-24\",\"2015-08-25\",\"2015-08-26\",\"2015-08-27\",\"2015-08-28\",\"2015-08-31\",\"2015-09-01\",\"2015-09-02\",\"2015-09-03\",\"2015-09-04\",\"2015-09-08\",\"2015-09-09\",\"2015-09-10\",\"2015-09-11\",\"2015-09-14\",\"2015-09-15\",\"2015-09-16\",\"2015-09-17\",\"2015-09-18\",\"2015-09-21\",\"2015-09-22\",\"2015-09-23\",\"2015-09-24\",\"2015-09-25\",\"2015-09-28\",\"2015-09-29\",\"2015-09-30\",\"2015-10-01\",\"2015-10-02\",\"2015-10-05\",\"2015-10-06\",\"2015-10-07\",\"2015-10-08\",\"2015-10-09\",\"2015-10-12\",\"2015-10-13\",\"2015-10-14\",\"2015-10-15\",\"2015-10-16\",\"2015-10-19\",\"2015-10-20\",\"2015-10-21\",\"2015-10-22\",\"2015-10-23\",\"2015-10-26\",\"2015-10-27\",\"2015-10-28\",\"2015-10-29\",\"2015-10-30\",\"2015-11-02\",\"2015-11-03\",\"2015-11-04\",\"2015-11-05\",\"2015-11-06\",\"2015-11-09\",\"2015-11-10\",\"2015-11-11\",\"2015-11-12\",\"2015-11-13\",\"2015-11-16\",\"2015-11-17\",\"2015-11-18\",\"2015-11-19\",\"2015-11-20\",\"2015-11-23\",\"2015-11-24\",\"2015-11-25\",\"2015-11-27\",\"2015-11-30\",\"2015-12-01\",\"2015-12-02\",\"2015-12-03\",\"2015-12-04\",\"2015-12-07\",\"2015-12-08\",\"2015-12-09\",\"2015-12-10\",\"2015-12-11\",\"2015-12-14\",\"2015-12-15\",\"2015-12-16\",\"2015-12-17\",\"2015-12-18\",\"2015-12-21\",\"2015-12-22\",\"2015-12-23\",\"2015-12-24\",\"2015-12-28\",\"2015-12-29\",\"2015-12-30\",\"2015-12-31\",\"2016-01-04\",\"2016-01-05\",\"2016-01-06\",\"2016-01-07\",\"2016-01-08\",\"2016-01-11\",\"2016-01-12\",\"2016-01-13\",\"2016-01-14\",\"2016-01-15\",\"2016-01-19\",\"2016-01-20\",\"2016-01-21\",\"2016-01-22\",\"2016-01-25\",\"2016-01-26\",\"2016-01-27\",\"2016-01-28\",\"2016-01-29\",\"2016-02-01\",\"2016-02-02\",\"2016-02-03\",\"2016-02-04\",\"2016-02-05\",\"2016-02-08\",\"2016-02-09\",\"2016-02-10\",\"2016-02-11\",\"2016-02-12\",\"2016-02-16\",\"2016-02-17\",\"2016-02-18\",\"2016-02-19\",\"2016-02-22\",\"2016-02-23\",\"2016-02-24\",\"2016-02-25\",\"2016-02-26\",\"2016-02-29\",\"2016-03-01\",\"2016-03-02\",\"2016-03-03\",\"2016-03-04\",\"2016-03-07\",\"2016-03-08\",\"2016-03-09\",\"2016-03-10\",\"2016-03-11\",\"2016-03-14\",\"2016-03-15\",\"2016-03-16\",\"2016-03-17\",\"2016-03-18\",\"2016-03-21\",\"2016-03-22\",\"2016-03-23\",\"2016-03-24\",\"2016-03-28\",\"2016-03-29\",\"2016-03-30\",\"2016-03-31\",\"2016-04-01\",\"2016-04-04\",\"2016-04-05\",\"2016-04-06\",\"2016-04-07\",\"2016-04-08\",\"2016-04-11\",\"2016-04-12\",\"2016-04-13\",\"2016-04-14\",\"2016-04-15\",\"2016-04-18\",\"2016-04-19\",\"2016-04-20\",\"2016-04-21\",\"2016-04-22\",\"2016-04-25\",\"2016-04-26\",\"2016-04-27\",\"2016-04-28\",\"2016-04-29\",\"2016-05-02\",\"2016-05-03\",\"2016-05-04\",\"2016-05-05\",\"2016-05-06\",\"2016-05-09\",\"2016-05-10\",\"2016-05-11\",\"2016-05-12\",\"2016-05-13\",\"2016-05-16\",\"2016-05-17\",\"2016-05-18\",\"2016-05-19\",\"2016-05-20\",\"2016-05-23\",\"2016-05-24\",\"2016-05-25\",\"2016-05-26\",\"2016-05-27\",\"2016-05-31\",\"2016-06-01\",\"2016-06-02\",\"2016-06-03\",\"2016-06-06\",\"2016-06-07\",\"2016-06-08\",\"2016-06-09\",\"2016-06-10\",\"2016-06-13\",\"2016-06-14\",\"2016-06-15\",\"2016-06-16\",\"2016-06-17\",\"2016-06-20\",\"2016-06-21\",\"2016-06-22\",\"2016-06-23\",\"2016-06-24\",\"2016-06-27\",\"2016-06-28\",\"2016-06-29\",\"2016-06-30\",\"2016-07-01\",\"2016-07-05\",\"2016-07-06\",\"2016-07-07\",\"2016-07-08\",\"2016-07-11\",\"2016-07-12\",\"2016-07-13\",\"2016-07-14\",\"2016-07-15\",\"2016-07-18\",\"2016-07-19\",\"2016-07-20\",\"2016-07-21\",\"2016-07-22\",\"2016-07-25\",\"2016-07-26\",\"2016-07-27\",\"2016-07-28\",\"2016-07-29\",\"2016-08-01\",\"2016-08-02\",\"2016-08-03\",\"2016-08-04\",\"2016-08-05\",\"2016-08-08\",\"2016-08-09\",\"2016-08-10\",\"2016-08-11\",\"2016-08-12\",\"2016-08-15\",\"2016-08-16\",\"2016-08-17\",\"2016-08-18\",\"2016-08-19\",\"2016-08-22\",\"2016-08-23\",\"2016-08-24\",\"2016-08-25\",\"2016-08-26\",\"2016-08-29\",\"2016-08-30\",\"2016-08-31\",\"2016-09-01\",\"2016-09-02\",\"2016-09-06\",\"2016-09-07\",\"2016-09-08\",\"2016-09-09\",\"2016-09-12\",\"2016-09-13\",\"2016-09-14\",\"2016-09-15\",\"2016-09-16\",\"2016-09-19\",\"2016-09-20\",\"2016-09-21\",\"2016-09-22\",\"2016-09-23\",\"2016-09-26\",\"2016-09-27\",\"2016-09-28\",\"2016-09-29\",\"2016-09-30\",\"2016-10-03\",\"2016-10-04\",\"2016-10-05\",\"2016-10-06\",\"2016-10-07\",\"2016-10-10\",\"2016-10-11\",\"2016-10-12\",\"2016-10-13\",\"2016-10-14\",\"2016-10-17\",\"2016-10-18\",\"2016-10-19\",\"2016-10-20\",\"2016-10-21\",\"2016-10-24\",\"2016-10-25\",\"2016-10-26\",\"2016-10-27\",\"2016-10-28\",\"2016-10-31\",\"2016-11-01\",\"2016-11-02\",\"2016-11-03\",\"2016-11-04\",\"2016-11-07\",\"2016-11-08\",\"2016-11-09\",\"2016-11-10\",\"2016-11-11\",\"2016-11-14\",\"2016-11-15\",\"2016-11-16\",\"2016-11-17\",\"2016-11-18\",\"2016-11-21\",\"2016-11-22\",\"2016-11-23\",\"2016-11-25\",\"2016-11-28\",\"2016-11-29\",\"2016-11-30\",\"2016-12-01\",\"2016-12-02\",\"2016-12-05\",\"2016-12-06\",\"2016-12-07\",\"2016-12-08\",\"2016-12-09\",\"2016-12-12\",\"2016-12-13\",\"2016-12-14\",\"2016-12-15\",\"2016-12-16\",\"2016-12-19\",\"2016-12-20\",\"2016-12-21\",\"2016-12-22\",\"2016-12-23\",\"2016-12-27\",\"2016-12-28\",\"2016-12-29\",\"2016-12-30\",\"2017-01-03\",\"2017-01-04\",\"2017-01-05\",\"2017-01-06\",\"2017-01-09\",\"2017-01-10\",\"2017-01-11\",\"2017-01-12\",\"2017-01-13\",\"2017-01-17\",\"2017-01-18\",\"2017-01-19\",\"2017-01-20\",\"2017-01-23\",\"2017-01-24\",\"2017-01-25\",\"2017-01-26\",\"2017-01-27\",\"2017-01-30\",\"2017-01-31\",\"2017-02-01\",\"2017-02-02\",\"2017-02-03\",\"2017-02-06\",\"2017-02-07\",\"2017-02-08\",\"2017-02-09\",\"2017-02-10\",\"2017-02-13\",\"2017-02-14\",\"2017-02-15\",\"2017-02-16\"],\"y\":[128.88000500000001,128.779999,129.029999,129.5,133,133.60000600000001,131.60000600000001,130.86999499999999,130.570007,130.279999,129.520004,129.55999800000001,128.75,129.36999499999999,129.570007,127.220001,124.769997,124.900002,125.400002,124.949997,127.31999999999999,129.16000399999999,129.25,128.39999399999999,127.849998,128.03999300000001,126.81999999999999,124.879997,124.699997,126.400002,126.489998,125.120003,125.55999799999999,127.510002,128.11999499999999,126.400002,126.58000199999999,127.209999,128.570007,127.290001,127.129997,127.099998,126.139999,128.11999499999999,128.199997,128.86999499999999,130.41999799999999,130.63000500000001,133.13000500000001,134.53999300000001,131.58999600000001,128.63999899999999,130.13000500000001,130.570007,128.449997,126.75,126.08000199999999,127.620003,127.55999799999999,126.879997,127.19000200000001,128.949997,129.490005,130.720001,130.88000500000001,130.979996,131.63000500000001,132.970001,132.91000399999999,132.259995,131.949997,131.449997,131.38999899999999,130.66000399999999,130.94000199999999,130.58000200000001,129.69000199999999,129.21000699999999,128.08000200000001,129.33999600000001,130.179993,128.33000200000001,127.239998,127.849998,127.879997,128.30999800000001,127.81999999999999,128.05999800000001,127.610001,129.800003,129.199997,127.989998,126.470001,126.120003,126.94000200000001,126.69000200000001,126.230003,126.150002,124.639999,124.05999799999999,123.849998,125.760002,126.370003,127.150002,128.570007,129.61999499999999,132.970001,132.91999799999999,125.5,127.089996,125.739998,123.610001,123.910004,123.5,122.56999999999999,122.639999,122.56999999999999,117.699997,117.44000200000001,116.5,116.25,119.989998,118.18000000000001,115.41999800000001,116.400002,116.30999799999999,117.650002,117.44000200000001,116.519997,114.349998,111.900002,108.800003,111.110001,109.889999,113.239998,113.30999799999999,114.529999,111.879997,112.339996,112.779999,110.449997,112.55999799999999,114.019997,113.279999,114.209999,116.889999,116.529999,116.540001,116.489998,114.300003,115.370003,114.18000000000001,114.720001,115.5,116.69000200000001,114.56999999999999,113.510002,111.540001,109.620003,111.010002,111.370003,111.739998,111.769997,110.19000200000001,112.279999,112.75,112.449997,111.519997,112.099998,112,111.75,114.16999800000001,115.58000199999999,115.5,119.230003,118.129997,116.540001,119.300003,120.69000200000001,121.220001,121.360001,123.489998,123.81999999999999,122.69000200000001,121.80999799999999,121.80999799999999,118.06999999999999,117.41999800000001,116.81999999999999,115.56999999999999,114.239998,115.050003,117.489998,119.75,119.91999800000001,119.730003,119.349998,119.230003,118.410004,119.410004,118.80999799999999,118.110001,116.790001,119.25,119.860001,118.599998,117.69000200000001,116.94000200000001,115.389999,112.68000000000001,112.800003,111.989998,112.25,109.519997,107.370003,107.720001,108.849998,109,107.69000200000001,109.43000000000001,108.699997,107.029999,105.370003,105.849998,102.370003,100.129997,99.110000999999997,99.059997999999993,100.69000200000001,101.19000200000001,100.480003,97.709998999999996,98.650002000000001,98.190002000000007,97.879997000000003,101.459999,101.529999,100.879997,96.629997000000003,94.519997000000004,97.339995999999999,96.709998999999996,96.040001000000004,96.839995999999999,97.330001999999993,96.919998000000007,95.699996999999996,95.940002000000007,96.349997999999999,94.720000999999996,94.5,96.849997999999999,98.209998999999996,98.889999000000003,96.760002,96.900002000000001,96.5,96.379997000000003,96.760002,98.019997000000004,98.230002999999996,100.769997,100.889999,101.709999,103.75,102.83000199999999,101.760002,101.58000199999999,102.239998,102.279999,102.910004,105.18000000000001,106.30999799999999,106.470001,106.5,107.650002,107.290001,107.06999999999999,106.25,106.19000200000001,107.790001,110.41999800000001,109.900002,110,112.19000200000001,110.730003,110.980003,110.41999800000001,109.769997,110.610001,110.5,112.339996,112.389999,112.300003,108.949997,108,108.089996,106.93000000000001,106.480003,105.650002,105.300003,98.709998999999996,97.879997000000003,94.720000999999996,94.080001999999993,95.739998,95.900002000000001,94.069999999999993,93.449996999999996,93.769997000000004,93.569999999999993,93.569999999999993,92.779999000000004,91.669998000000007,94.389999000000003,94.699996999999996,95.209998999999996,94.639999000000003,95.430000000000007,97.190002000000007,98.089995999999999,99.739998,100.730003,100.470001,100.400002,99.540001000000004,97.839995999999999,98.269997000000004,101.889999,99.870002999999997,99.559997999999993,99.989998,99.349997999999999,99.120002999999997,98.480002999999996,98.410004000000001,97.75,96.650002000000001,96.569999999999993,96.349997999999999,96.889999000000003,96.290001000000004,94.660004000000001,93.050003000000004,93.660004000000001,94.550003000000004,95.769997000000004,96.470000999999996,95.400002000000001,95.660004000000001,96.5,96.889999000000003,97.650002000000001,97.699996999999996,97.669998000000007,98.989998,99.300003000000004,100.129997,100,100.459999,101,99.300003000000004,98.839995999999999,97.970000999999996,104.349998,104.449997,104.550003,106.150002,106.06999999999999,105.839996,106,107.650002,108.370003,108.94000200000001,108.900002,108.93000000000001,108.44000200000001,109.540001,110.230003,109.370003,109.599998,109.69000200000001,109.099998,109.31999999999999,108.75,107.879997,107.949997,107.44000200000001,106.5,106.56999999999999,106.800003,108,108.300003,108.760002,107.269997,105.720001,105.720001,108.790001,113.029999,115.730003,116.129997,116.18000000000001,114.120003,113.989998,114.94000200000001,114.790001,113.389999,113.18000000000001,114.639999,113.800003,113.370003,113.050003,114.30999799999999,113.660004,114.339996,114.55999799999999,116.75,118.69000200000001,117.980003,117.44000200000001,118.16999800000001,117.839996,118.209999,117.760002,117.379997,116.910004,117.739998,118.360001,115.699997,115.860001,115.209999,114.230003,113.769997,112.349998,111.459999,110.25,110.510002,111.720001,111.31999999999999,111.089996,108.870003,107.80999799999999,107.68000000000001,110.230003,110.349998,110.540001,111.989998,112.41999800000001,111.510002,111.870003,112.470001,112.029999,112.199997,110.94000200000001,110.089996,110.029999,110.360001,111.19000200000001,112.43000000000001,114.699997,115,115.91999800000001,116.199997,116.730003,116.5,117.379997,117.5,117.400002,116.510002,116.519997,117.800003,118.019997,117.110001,117.199997,116.33000199999999,116.510002,116.860001,118.160004,119.43000000000001,119.379997,119.93000000000001,119.300003,119.620003,120.239998,120.5,120.089996,120.449997,120.80999799999999,120.099998,122.099998,122.44000200000001,122.349998,121.629997,121.389999,130.490005,129.38999899999999,129.19000199999999,130.5,132.08999600000001,132.220001,132.449997,132.94000199999999,133.820007,135.08999600000001,136.270004,135.89999399999999],\"marker\":{\"color\":\"rgba(255,127,14,1)\",\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"error_y\":{\"color\":\"rgba(255,127,14,1)\"},\"error_x\":{\"color\":\"rgba(255,127,14,1)\"},\"line\":{\"color\":\"rgba(255,127,14,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\nA random walk is the simplest mathematical model of temporal dependence. Each new value is just the previous value plus a random shock (white noise).\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate Random Walk\ntN <- 200\ny <- numeric(tN)\ny[1] <- stock$AAPL.High[1]\nfor (ti in 2:tN) {\n    y[ti] <- y[ti-1] + runif(1, -10, 10)\n}\n#x <- runif(tN, -1,1) White Noise\n\ny_dat <- data.frame(Date=1:tN, RandomWalk=y)\nfig <- plot_ly(y_dat, type = 'scatter', mode = 'lines') %>%\n  add_trace(x=~Date, y=~RandomWalk) %>%\n  layout(showlegend = F)\nfig\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-65477dff4542cdcf1594\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-65477dff4542cdcf1594\">{\"x\":{\"visdat\":{\"569717ea63d\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"569717ea63d\",\"attrs\":{\"569717ea63d\":{\"mode\":\"lines\",\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\"},\"569717ea63d.1\":{\"mode\":\"lines\",\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\",\"x\":{},\"y\":{},\"inherit\":true}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"showlegend\":false,\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Date\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"RandomWalk\"},\"hovermode\":\"closest\"},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"mode\":\"lines\",\"type\":\"scatter\",\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"line\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"mode\":\"lines\",\"type\":\"scatter\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200],\"y\":[128.88000500000001,120.52637643258007,114.72742819304497,116.48277212477208,112.26788268967451,110.8939817182991,107.90429096746178,113.71866853053601,111.91988498515369,117.84797870165588,114.17817309110345,115.99779906759889,110.58853605373682,109.08025657369674,103.73063242605807,96.200546627705108,91.307656546895515,92.202638071802568,85.507542519671034,77.449884536516976,69.755405356017064,78.387086857882451,77.385371169847559,74.2902785919878,69.311171607886564,61.043288480331313,61.325030350898516,64.216147471060168,57.047584290457081,57.641700651337516,56.058103536096525,58.69748783840717,53.888911121365737,44.694930939776015,43.304424743008923,51.024808404808056,54.284997406256508,47.501737862666261,49.76471159684391,53.965303479534697,44.191678264950525,41.705926839103114,47.649222114895593,40.204028427441727,38.34588146380068,42.238245253292035,43.685026034553658,47.247832455513191,40.742960330871057,36.521717613403808,36.773615438331973,34.932867871438276,35.137790633206379,44.874921795752954,38.893472720709752,33.974318756562781,35.38437710686506,27.444779831756961,34.803145333034109,28.73512010167272,29.350644057457458,31.851448012356769,41.815924812880468,33.399831560281228,29.145151698802721,23.533530995090615,30.626596087430727,39.071771796633612,40.43863202522428,40.237388998110902,43.977180450064253,38.234568071392488,36.725559053783428,44.776283391592216,44.743472645831417,37.694524997686159,36.652435474869918,31.529304278855335,39.505628557716619,42.17930142379285,35.224426296380472,38.18420807442547,38.990178304565262,46.408227203254711,47.507819161591243,46.27810972354294,50.270821145576548,59.230771233168554,66.514623747979414,73.381768181760918,82.597502107572865,74.009887274195563,82.649259037179064,85.559893539947581,83.960651323040139,88.055459815849673,95.299737733629655,93.980303593752097,97.174921227459919,103.84873914609582,98.19542285132141,90.265565417547833,83.240591130060267,80.395781216983806,86.262937016626012,82.300162008804392,85.793803064924788,79.173641452406656,80.567674977545749,79.75518378971995,84.156260989447247,79.205792686742853,70.55684087409557,66.792313028899144,63.50971205702902,62.841410917070817,72.601910387938034,78.133107745860826,72.786933366050135,69.613898830329191,73.875658452351701,73.268739805397701,73.550944017087232,73.770688921985339,79.082226213839959,82.529740571816575,85.939960215081584,81.626000274744939,90.123533658524167,82.916968703110825,78.074243637574028,84.360162886631798,92.524569305432152,91.623875760314178,91.665928224873852,101.46750440898271,96.216125526306342,101.15707640147687,94.378678832044017,104.28249526846142,96.280683875856113,94.732065266770434,95.35387881643058,97.290254212756764,90.984862219383132,82.681616755043279,77.867426235069644,87.731777197522234,84.649821697575163,74.791543959480833,67.831028225672554,59.100350202833425,65.54741732853384,65.128149670389604,67.792663062763523,74.715260319252621,75.006881263879251,81.250990429734003,84.575018068616401,92.230076976892661,97.432767395702314,96.025691564035725,86.470425739725243,78.457685188231181,74.927481269304764,67.010912188169669,75.472967034046661,70.651062331934583,61.231107461107086,58.176361514096271,63.922496061612975,71.62015673316003,77.142387932007324,84.37057361549617,93.487620149579953,94.192252099354874,93.794324324307155,93.266811911788892,90.373815815736663,86.373757837181103,89.773290330384981,80.589042888795149,89.420346786108922,96.922281734486234,89.998831814927172,90.281465766792309,88.3986206500131,84.914555964832317,91.775986338806462,84.229302933280479,86.190972035591614,86.747073564519297,78.700669920844149,69.525578311485361,67.219384066601407,62.588049388331484,65.853245589723002,68.281751755465876,77.372808764529537,75.101022674542975],\"marker\":{\"color\":\"rgba(255,127,14,1)\",\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"error_y\":{\"color\":\"rgba(255,127,14,1)\"},\"error_x\":{\"color\":\"rgba(255,127,14,1)\"},\"line\":{\"color\":\"rgba(255,127,14,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\nIn both plots, we see that today's value is not independent of past values. In contrast to cross-sectional data (e.g. individual incomes), time series often require special methods to account for memory and nonstationarity.\n\n#### **Stationary**. {-}\nA stationary time series is one whose statistical properties --- mean, variance, and autocovariance --- do not change over time. Formally\n\n* Stationary Means: $E[y_{t}]=E[y_{t'}]$ for all periods $t, t'$\n* Stationary Vars: $V[y_{t}]=V[y_{t'}]$ for all periods $t, t'$\n\nE.g., \\( y_t = \\beta t + u_t, \\quad u_t \\sim \\text{N}(0, \\sigma + \\alpha t) \\)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntN <- 200\nsimulate_series <- function(beta, alpha, sigma=.2){\n    y <- numeric(tN)\n    for (ti in 1:tN) {\n        mean_ti <- beta*ti\n        sd_ti <- (.2 + alpha*ti)\n        y[ti] <- mean_ti + rnorm(1, sd=sd_ti)\n    }\n    return(y)\n}\n\n# Plotting Functions\nplot_setup <- function(alpha, beta){\n    plot.new()\n    plot.window(xlim=c(1,tN), ylim=c(-5,20))\n    axis(1)\n    axis(2)\n    mtext(expression(y[t]),2, line=2.5)\n    mtext(\"Time (t)\", 1, line=2.5)\n}\nplot_title <- function(alpha, beta){\n    beta_name <- ifelse(beta==0, 'Mean Stationary', 'Mean Nonstationary')\n    alpha_name <- ifelse(alpha==0, 'Var Stationary', 'Var Nonstationary')\n    title(paste0(beta_name,', ', alpha_name), font.main=1, adj=0)\n}\n\npar(mfrow = c(2, 2))\nfor(alpha in c(0,.015)){\nfor(beta in c(0,.05)){\n    plot_setup(alpha=alpha, beta=beta)\n    for( sim in c('red','blue')){\n        y_sim <- simulate_series(beta=beta, alpha=alpha)\n        lines(y_sim, col=adjustcolor(sim ,alpha.f=0.5), lwd=2)\n    }\n    plot_title(alpha=alpha, beta=beta)\n}}\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\n#### **Measures of temporal association**. {-}\nTime series often exhibit serial dependence—values today are related to past values, and potentially to other processes evolving over time. We can visualize this using correlation-based diagnostics.\n\nThe Autocorrelation Function (AFC) measures correlation between a time series and its own lagged values:\n\n$ACF_{Y}(k) = \\frac{Cov(Y_{t},Y_{t-k})}{ \\sqrt{Var(Y_{t})Var(Y_{t-k})}}$\n\nThis helps detect temporal persistence (memory). For stationary processes, the ACF typically decays quickly, whereas for nonstationary processes, it typically decays slowly or persists.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nfor(alpha in c(0,.015)){\nfor(beta in c(0,.05)){\n    y_sim <- simulate_series(beta=beta, alpha=alpha)\n    acf(y_sim, main='')\n    plot_title(alpha=alpha, beta=beta)\n}}\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\nThe Cross-Correlation Function (CCF) measures correlation between two time series at different lags:\n\n$CCF_{YX}(k) = \\frac{Cov(Y_{t},X_{t-k})}{ \\sqrt{Var(Y_t)Var(X_{t-k})}}$\n\nThis is useful for detecting lagged relationships between two series, such as leading indicators or external drivers. (If $X$ is white noise, any visible structure in the CCF likely reflects nonstationarity in $Y$.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_sim <- runif(tN, -1,1) # White Noise\npar(mfrow = c(2, 2))\nfor(alpha in c(0,.015)){\nfor(beta in c(0,.05)){\n    y_sim <- simulate_series(beta=beta, alpha=alpha)\n    ccf(y_sim, x_sim, main='')\n    plot_title(alpha=alpha, beta=beta)\n}}\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n:::\n\n\n## Spatial Interdependence\n\nMany observational datasets exhibit spatial dependence, meaning that values at one location tend to be related to values at nearby locations. This violates the standard assumption of independent observations used in many classical statistical methods.\n\nFor example, elevation is spatially dependent: if one location is at high elevation, nearby locations are also likely (though not guaranteed) to be high. Similarly, socioeconomic outcomes like disease rates or income often cluster geographically due to shared environmental or social factors.\n\nJust as stock prices today depend on yesterday, spatial variables often depend on neighboring regions, creating a need for specialized statistical methods that account for spatial autocorrelation.\n\n#### **Raster vs. Vector Data**. {-}\nSpatial data typically comes in two formats, each suited to different types of information:\n\n* Vector data uses geometric shapes (points, lines, polygons) to store data. E.g., a census tract map that stores data on population demographics.\n* Raster data uses grid cells (typically squares, but sometimes hexagons) to store data. E.g., an image that stores data on elevation above seawater.\n \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Vector Data\nlibrary(sf)\nnorthcarolina_vector <- st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\n## Reading layer `nc' from data source `/home/Jadamso/R-Libs/sf/shape/nc.shp' using driver `ESRI Shapefile'\n## Simple feature collection with 100 features and 14 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\n## Geodetic CRS:  NAD27\nplot(northcarolina_vector['BIR74'], main='Number of Live Births in 1974')\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-44-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# https://r-spatial.github.io/spdep/articles/sids.html\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Raster Data\nlibrary(terra)\nluxembourg_elevation_raster <- rast(system.file(\"ex/elev.tif\", package=\"terra\"))\nplot(luxembourg_elevation_raster)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-45-1.png){width=672}\n:::\n:::\n\n\n#### **Stationary.** {-}\nJust as with temporal data, stationarity in spatial data means that the statistical properties (like mean, variance, or spatial correlation) are roughly the same across space.\n\n* Stationary Means: $E[y(s)]=E[y(s')]$ for all locations $s,s'$\n* Stationary Vars: $V[y(s)]=V[y(s')]$ for all locations $s,s'$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulated 2D spatial fields\nset.seed(1)\nn <- 20\nx <- y <- seq(0, 1, length.out = n)\ngrid <- expand.grid(x = x, y = y)\n\n# 1. Stationary: Gaussian with constant mean and var\nz_stationary <- matrix(rnorm(n^2, 0, 1), n, n)\n\n# 2. Nonstationary: Mean increases with x and y\nz_nonstationary <- outer(x, y, function(x, y) 3*x*y) + rnorm(n^2, 0, 1)\n\npar(mfrow = c(1, 2))\n# Stationary field\nimage(x, y, z_stationary,\n      main = \"Stationary Field\",\n      col = terrain.colors(100),\n      xlab = \"x\", ylab = \"y\")\n# Nonstationary field\nimage(x, y, z_nonstationary,\n      main = \"Nonstationary Field\",\n      col = terrain.colors(100),\n      xlab = \"x\", ylab = \"y\")\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-46-1.png){width=672}\n:::\n:::\n\n\n#### **Measures of spatial association**. {-}\nJust like temporal data may exhibit autocorrelation, spatial data may show spatial autocorrelation or spatial cross-correlation—meaning that observations located near each other are more (or less) similar than we would expect under spatial independence.\n\nAutocorrelation. We can measure spatial *autocorrelation* using Moran's I, a standard index of spatial dependence. Global Moran’s I summarizes overall spatial association (just like the ACF)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Raster Data Example\nautocor(luxembourg_elevation_raster, method='moran', global=T)\n## elevation \n## 0.8917057\n```\n:::\n\n\nCross-Correlation. We can also assesses the relationship between two variables at varying distances. \n\n::: {.cell}\n\n```{.r .cell-code}\n# Vector Data Example\ndat <- as.data.frame(northcarolina_vector)[, c('BIR74', 'SID74')]\nmu <- colMeans(dat)\n\n# Format Distances\ndmat <- st_distance( st_centroid(northcarolina_vector) )\ndmat <- units::set_units(dmat, 'km')\n\n# At Which Distances to Compute CCF\n# summary(dmat[,1])\nrdists <- c(-1,seq(0,100,by=25)) # includes 0\nrdists <- units::set_units(rdists , 'km')\n\n# Compute Cross-Covariances\nvarXY <- prod( apply(dat, 2, sd) )\nCCF <- lapply( seq(2, length(rdists)), function(ri){\n    # Which Observations are within (rmin, rmax] distance\n    dmat_r <- dmat\n    d_id <- (dmat_r > rdists[ri-1] & dmat_r <= rdists[ri]) \n    dmat_r[!d_id]  <- NA\n    # Compute All Covariances (Stationary)\n    covs_r <- lapply(1:nrow(dmat_r), function(i){\n        pairsi <- which(!is.na(dmat_r[i,]))        \n        covXiYj <- sapply(pairsi, function(j) {\n            dXi <- dat[i,1] - mu[1]\n            dYj <- dat[j,2] - mu[2]\n            return(dXi*dYj)\n        })\n        return(covXiYj)\n    })\n    corXY <- unlist(covs_r)/varXY\n    return(corXY)\n} )\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot Cross-Covariance Function\nx <- as.numeric(rdists[-1])\n\npar(mfrow=c(1,2))\n\n# Distributional Summary\nboxplot(CCF,\n    outline=F, whisklty=0, staplelty=0,\n    ylim=c(-1,1), #quantile(unlist(CCF), probs=c(.05,.95)),\n    names=x, \n    main='',\n    font.main=1,\n    xlab='Distance [km]',\n    ylab='Cross-Correlation of BIR74 and SID74')\ntitle('Binned Medians and IQRs', font.main=1, adj=0)\nabline(h=0, lty=2)\n\n# Inferential Summary\nCCF_means <- sapply(CCF, mean)\nplot(x, CCF_means,\n    ylim=c(-1,1),\n    type='o', pch=16,\n    main='',\n    xlab='Distance [km]',\n    ylab='Cross-Correlation of BIR74 and SID74')\ntitle('Binned Means + 95% Confidence Band', font.main=1, adj=0)\nabline(h=0, lty=2)    \n# Quick and Dirty Subsampling CI\nCCF_meanCI <- sapply(CCF, function(corXY){\n    ss_size <- floor(length(corXY)*3/4)\n    corXY_boot <- sapply(1:200, function(b){\n        corXY_b <- sample(corXY, ss_size, replace=F)\n        mean(corXY_b, na.rm=T)\n    })\n    quantile(corXY_boot,  probs=c(.025,.975), na.rm=T)\n})\npolygon( c(x, rev(x)), \n    c(CCF_meanCI[1,], rev(CCF_meanCI[2,])), \n    col=grey(0,.25), border=NA)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-49-1.png){width=672}\n:::\n:::\n\n\n## Variable Interdependence\n\nIn addition to spatial and temporal dependence, many observational datasets exhibit interdependence between variables. Many economic variables are endogenous: meaning that they are an outcome determined (or caused: $\\to$) by some other variable.\n\n * If $Y \\to X$, then we have reverse causality\n * If $Y \\to X$ and $X \\to Y$, then we have simultaneity\n * If $Z\\to Y$ and either $Z\\to X$ or $X \\to Z$, then we have omitted a potentially important variable\n\nThese endogeneity issues imply $X$ and $\\epsilon$ are correlated, which is a barrier to interpreting OLS estimates causally. ($X$ and $\\epsilon$ may be correlated for other reasons too, such as when $X$ is measured with error.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate data with an endogeneity issue\nn <- 300\nz <- rbinom(n,1,.5)\nxy <- sapply(z, function(zi){\n    y <- rnorm(1,zi,1)\n    x <- rnorm(1,zi*2,1)\n    c(x,y)\n})\nxy <- data.frame(x=xy[1,],y=xy[2,])\nplot(y~x, data=xy, pch=16, col=grey(0,.5))\nabline(lm(y~x,data=xy))\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-50-1.png){width=672}\n:::\n:::\n\n\nWith multiple linear regression, endogeneity biases are not just a problem for your main variable of interest. Suppose your interested in how $x_{1}$ affects $y$, conditional on $x_{2}$. Letting $X=[x_{1}, x_{2}]$, you estimate \n\\begin{eqnarray}\n\\hat{\\beta}_{OLS} = [X'X]^{-1}X'y\n\\end{eqnarray}\nYou paid special attention in your research design to find a case where $x_{1}$ is truly exogenous. Unfortunately, $x_{2}$ is correlated with the error term. (How unfair, I know, especially after all that work). Nonetheless,\n\\begin{eqnarray}\n\\mathbb{E}[X'\\epsilon] = \n\\begin{bmatrix}\n0 \\\\ \\rho\n\\end{bmatrix}\\\\\n\\mathbb{E}[ \\hat{\\beta}_{OLS} - \\beta] = [X'X]^{-1} \\begin{bmatrix}\n0 \\\\ \\rho\n\\end{bmatrix} = \n\\begin{bmatrix}\n\\rho_{1} \\\\ \\rho_{2}\n\\end{bmatrix}\n\\end{eqnarray}\nThe magnitude of the bias for $x_{1}$ thus depends on the correlations between $x_{1}$ and $x_{2}$ as well as $x_{2}$ and $\\epsilon$.\n\nI will focus on the seminal economic example to provide some intuition.\n\n#### **Competitive Market Equilibrium**. {-}\nThis model has three structural relationships: (1) market supply is the sum of quantities supplied by individual firms at a given price, (2) market demand is the sum of quantities demanded by individual people at a given price, and (3) market supply equals market demand in equilibrium. Assuming market supply and demand are linear, we can write these three relationships as\n\\begin{eqnarray}\n\\label{eqn:market_supply}\nQ_{S}(P) &=& A_{S} + B_{S} P + E_{S},\\\\\n\\label{eqn:market_demand}\nQ_{D}(P) &=& A_{D} - B_{D} P + E_{D},\\\\\n\\label{eqn:market_eq}\nQ_{D} &=& Q_{S} = Q.\n%%  $Q_{D}(P) = \\sum_{i} q_{D}_{i}(P)$, \n\\end{eqnarray}\nThis last equation implies a simultaneous \"reduced form\" relationship where both the price and the quantity are outcomes. With a linear parametric structure to these equations, we can use algebra to solve for the equilibrium price and quantity analytically as\n\\begin{eqnarray}\nP^{*} &=& \\frac{A_{D}-A_{S}}{B_{D}+B_{S}} + \\frac{E_{D} - E_{S}}{B_{D}+B_{S}}, \\\\\nQ^{*} &=& \\frac{A_{S}B_{D}+ A_{D}B_{S}}{B_{D}+B_{S}} + \\frac{E_{S}B_{D}+ E_{D}B_{S}}{B_{D}+B_{S}}.\n\\end{eqnarray}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demand Curve Simulator\nqd_fun <- function(p, Ad=8, Bd=-.8, Ed_sigma=.25){\n    Qd <- Ad + Bd*p + rnorm(1,0,Ed_sigma)\n    return(Qd)\n}\n\n# Supply Curve Simulator\nqs_fun <- function(p, As=-8, Bs=1, Es_sigma=.25){\n    Qs <- As + Bs*p + rnorm(1,0,Es_sigma)\n    return(Qs)\n}\n\n# Quantity Supplied and Demanded at 3 Prices\ncbind(P=8:10, D=qd_fun(8:10), S=qs_fun(8:10))\n##       P          D          S\n## [1,]  8  1.1925652 0.01120111\n## [2,]  9  0.3925652 1.01120111\n## [3,] 10 -0.4074348 2.01120111\n\n# Market Equilibrium Finder\neq_fun <- function(demand, supply, P){\n    # Compute EQ (what we observe)\n    eq_id <- which.min( abs(demand-supply) )\n    eq <- c(P=P[eq_id], Q=demand[eq_id]) \n    return(eq)\n}\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulations Parameters\nN <- 300 # Number of Market Interactions\nP <- seq(5,10,by=.01) # Price Range to Consider\n\n# Generate Data from Competitive Market  \n# Plot Underlying Process\nplot.new()\nplot.window(xlim=c(0,2), ylim=range(P))\nEQ1 <- sapply(1:N, function(n){\n    # Market Data Generating Process\n    demand <- qd_fun(P)\n    supply <- qs_fun(P)\n    eq <- eq_fun(demand, supply, P)    \n    # Plot Theoretical Supply and Demand\n    lines(demand, P, col=grey(0,.01))\n    lines(supply, P, col=grey(0,.01))\n    points(eq[2], eq[1], col=grey(0,.05), pch=16)\n    # Save Data\n    return(eq)\n})\naxis(1)\naxis(2)\nmtext('Quantity',1, line=2)\nmtext('Price',2, line=2)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-52-1.png){width=672}\n:::\n:::\n\n\nSuppose we ask \"what is the effect of price on quantity?\" You can simply run a regression of quantity (\"Y\") on price (\"X\"): $\\widehat{\\beta}_{OLS} = Cov(Q^{*}, P^{*}) / Var(P^{*})$. You get a number back, but it is hard to interpret meaningfully. \n\n::: {.cell}\n\n```{.r .cell-code}\n# Analyze Market Data\ndat1 <- data.frame(t(EQ1), cost='1', T=1:N)\nreg1 <- lm(Q~P, data=dat1)\nsummary(reg1)\n## \n## Call:\n## lm(formula = Q ~ P, data = dat1)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.57279 -0.11977 -0.00272  0.11959  0.45525 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)  \n## (Intercept) -0.21323    0.43212  -0.493   0.6221  \n## P            0.12355    0.04864   2.540   0.0116 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1674 on 298 degrees of freedom\n## Multiple R-squared:  0.02119,\tAdjusted R-squared:  0.0179 \n## F-statistic: 6.451 on 1 and 298 DF,  p-value: 0.0116\n```\n:::\n\nThis simple derivation has a profound insight: price-quantity data does not generally tell you how price affects quantity (or vice-versa). The reason is simultaneity: price and quantity mutually cause one another in markets.^[Although there are many ways this simultaneity can happen, economic theorists have made great strides in analyzing the simultaneity problem as it arises from equilibrium market relationships. In fact, 2SLS arose to understand agricultural markets.]\n\nMoreover, this example also clarifies that our initial question \"what is the effect of price on quantity?\" is misguided. We could more sensibly ask  \"what is the effect of price on quantity supplied?\" or \"what is the effect of price on quantity demanded?\"\n\n## Further Reading\n\n\n# Experimental Data\n***\n\n## Design Basics\n\n#### **Control and Randomize**. {-}\n\n*Blocking* and *Clustering*\n\n\n#### **Competitive Equilibrium Example**. {-}\nIf you have exogenous variation on one side of the market, you can get information on the other. For example, lower costs shift out supply (more is produced at given price), allowing you to trace out part of a demand curve. \n\nTo see this, consider an experiment where student subjects are recruited to a classroom and randomly assigned to be either buyers or sellers in a market for little red balls. In this case, the classroom environment allows the experimenter to control for various factors (e.g., the temperature of the room is constant for all subjects) and the explicit randomization of subjects means that there are not typically systematic differences in different groups of students.\n\nIn the experiment, sellers are given linear \"cost functions\" that theoretically yield individual supplies like \\eqref{eqn:market_supply} and are paid \"price - cost\". Buyers are given linear \"benefit functions\" that theoretically yield individual demands like \\eqref{eqn:market_demand}, and are paid \"benefit - price\". The theoretical predictions are theorefore given in \\eqref{eqn:market_supply}. Moreover, experimental manipulation of $A_{S}$ leads to \n\\begin{eqnarray}\n\\label{eqn:comp_market_statics}\n\\frac{d P^{*}}{d A_{S}} = \\frac{-1}{B_{D}+B_{S}}, \\\\\n\\frac{d Q^{*}}{d A_{S}} = \\frac{B_{D}}{B_{D}+B_{S}}.\n\\end{eqnarray}\nIn this case, the supply shock has identified the demand slope: $-B_{D}=d Q^{*}/d P^{*}$.\n\n::: {.cell}\n\n```{.r .cell-code}\n# New Observations After Cost Change\nEQ2 <- sapply(1:N, function(n){\n    demand <- qd_fun(P)\n    supply2 <- qs_fun(P, As=-6.5) # More Supplied at Given Price\n    eq <- eq_fun(demand, supply2, P)\n    return(eq)\n    # lines(supply2, P, col=rgb(0,0,1,.01))\n    #points(eq[2], eq[1], col=rgb(0,0,1,.05), pch=16)\n})\ndat2 <- data.frame(t(EQ2), cost='2', T=(1:N) + N)\ndat2 <- rbind(dat1, dat2)\n\n# Plot Simulated Market Data\ncols <- ifelse(as.numeric(dat2$cost)==2, rgb(0,0,1,.2), rgb(0,0,0,.2))\nplot.new()\nplot.window(xlim=c(0,2), ylim=range(P))\npoints(dat2$Q, dat2$P, col=cols, pch=16)\naxis(1)\naxis(2)\nmtext('Quantity',1, line=2)\nmtext('Price',2, line=2)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n:::\n\n\nIf the function forms for supply and demand are different from what we predicted, we can still measure how much the experimental manipulation of production costs affects the equilibrium quantity sold (and compare that to what was predicted).^[Notice that even in this linear model, however, all effects are conditional: *The* effect of a cost change on quantity or price depends on the demand curve. A change in costs affects quantity supplied but not quantity demanded (which then affects equilibrium price) but the demand side of the market still matters! The change in price from a change in costs depends on the elasticity of demand.]\n\n\n\n## Comparisons Over Time\n\n#### **Regression Discontinuities/Kinks**. {-}\n\nThe basic idea of RDD/RKD is to examine how a variable changes just before and just after a treatment. RDD estimates the difference in the levels of an outcome variable, whereas RKD estimates the difference in the slope. Turning to our canonical competitive market example, the RDD estimate is the difference between the lines at $T=300$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Locally Linear Regression \n# (Compare means near break)\n\ncols <- ifelse(as.numeric(dat2$cost)==2, rgb(0,0,1,.5), rgb(0,0,0,.5))\nplot(P~T, dat2, main='Effect of Cost Shock on Price', \n    font.main=1, pch=16, col=cols)\nregP1 <- loess(P~T, dat2[dat2$cost==1,]) \nx1 <- regP1$x\n#lm(): x1 <- regP1$model$T \nlines(x1, predict(regP1), col=rgb(0,0,0), lwd=2)\nregP2 <- loess(P~T, dat2[dat2$cost==2,])\nx2 <- regP2$x #regP1$model$T\nlines(x2, predict(regP2), col=rgb(0,0,1), lwd=2)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-55-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\nplot(Q~T, dat2, main='Effect of Cost Shock on Quantity',\n    font.main=1, pch=16, col=cols)\nregQ1 <- loess(Q~T, dat2[dat2$cost==1,]) \nlines(x1, predict(regQ1), col=rgb(0,0,0), lwd=2)\nregQ2 <- loess(Q~T, dat2[dat2$cost==2,])\nx2 <- regP2$x #regP1$model$T\nlines(x2, predict(regQ2), col=rgb(0,0,1), lwd=2)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-55-2.png){width=672}\n:::\n:::\n\n\n\n```{.r .cell-code}\n# Linear Regression Alternative\nsub_id <- (dat2$cost==1 & dat2$T > 250) | (dat2$cost==2 & dat2$T < 300)\ndat2W <- dat2[sub_id,  ]\nregP <- lm(P~T*cost, dat2)\nregQ <- lm(Q~T*cost, dat2)\nstargazer::stargazer(regP, regQ, \n    type='html',\n    title='Recipe RDD',\n    header=F)\n```\n\n\n<table style=\"text-align:center\"><caption><strong>Recipe RDD</strong></caption>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><td colspan=\"2\"><em>Dependent variable:</em></td></tr>\n<tr><td></td><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n<tr><td style=\"text-align:left\"></td><td>P</td><td>Q</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(1)</td><td>(2)</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">T</td><td>-0.0001</td><td>-0.00005</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.0001)</td><td>(0.0001)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">cost2</td><td>-0.884<sup>***</sup></td><td>0.650<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.065)</td><td>(0.056)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">T:cost2</td><td>0.0002</td><td>0.0001</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.0002)</td><td>(0.0002)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">Constant</td><td>8.898<sup>***</sup></td><td>0.891<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.023)</td><td>(0.020)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">Observations</td><td>600</td><td>600</td></tr>\n<tr><td style=\"text-align:left\">R<sup>2</sup></td><td>0.813</td><td>0.795</td></tr>\n<tr><td style=\"text-align:left\">Adjusted R<sup>2</sup></td><td>0.812</td><td>0.794</td></tr>\n<tr><td style=\"text-align:left\">Residual Std. Error (df = 596)</td><td>0.200</td><td>0.172</td></tr>\n<tr><td style=\"text-align:left\">F Statistic (df = 3; 596)</td><td>862.854<sup>***</sup></td><td>768.728<sup>***</sup></td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"><em>Note:</em></td><td colspan=\"2\" style=\"text-align:right\"><sup>*</sup>p<0.1; <sup>**</sup>p<0.05; <sup>***</sup>p<0.01</td></tr>\n</table>\n\nRemember that this is effect is *local*: different magnitudes of the cost shock or different demand curves generally yield different estimates.\n\nMoreover, note that more than just costs have changed over time: subjects in the later periods have history experience behind them while they do not in earlier periods. So hidden variables like \"beliefs\" are implicitly treated as well. This is one concrete reason to have an explicit control group.\n\n#### **Difference in Differences**. {-}\nThe basic idea of DID is to examine how a variable changes in response to an exogenous shock, *compared to a control group*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEQ3 <- sapply(1:(2*N), function(n){\n\n    # Market Mechanisms\n    demand <- qd_fun(P)\n    supply <- qs_fun(P)\n\n    # Compute EQ (what we observe)\n    eq_id <- which.min( abs(demand-supply) )\n    eq <- c(P=P[eq_id], Q=demand[eq_id]) \n\n    # Return Equilibrium Observations\n    return(eq)\n})\ndat3 <- data.frame(t(EQ3), cost='1', T=1:ncol(EQ3))\ndat3_pre  <- dat3[dat3$T <= N ,]\ndat3_post <- dat3[dat3$T > N ,]\n\n# Plot Price Data\npar(mfrow=c(1,2))\nplot(P~T, dat2, main='Effect of Cost Shock on Price', \n    font.main=1, pch=16, col=cols, cex=.5)\nlines(x1, predict(regP1), col=rgb(0,0,0), lwd=2)\nlines(x2, predict(regP2), col=rgb(0,0,1), lwd=2)\n# W/ Control group\npoints(P~T, dat3, pch=16, col=rgb(1,0,0,.5), cex=.5)\nregP3a <- loess(P~T, dat3_pre)\nx3a <- regP3a$x\nlines(x3a, predict(regP3a), col=rgb(1,0,0), lwd=2)\nregP3b <- loess(P~T, dat3_post)\nx3b <- regP3b$x\nlines(x3b, predict(regP3b), col=rgb(1,0,0), lwd=2)\n\n\n# Plot Quantity Data\nplot(Q~T, dat2, main='Effect of Cost Shock on Quantity',\n    font.main=1, pch=17, col=cols, cex=.5)\nlines(x1, predict(regQ1), col=rgb(0,0,0), lwd=2)\nlines(x2, predict(regQ2), col=rgb(0,0,1), lwd=2)\n# W/ Control group\npoints(Q~T, dat3, pch=16, col=rgb(1,0,0,.5), cex=.5)\nregQ3a <- loess(Q~T, dat3_pre) \nlines(x3a, predict(regQ3a), col=rgb(1,0,0), lwd=2)\nregQ3b <- loess(Q~T, dat3_post) \nlines(x3b, predict(regQ3b), col=rgb(1,0,0), lwd=2)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-57-1.png){width=672}\n:::\n:::\n\n\nLinear Regression Estimates\n\n```{.r .cell-code}\n# Pool Data\ndat_pooled <- rbind(\n    cbind(dat2, EverTreated=1, PostPeriod=(dat2$T > N)),\n    cbind(dat3, EverTreated=0, PostPeriod=(dat3$T > N)))\ndat_pooled$EverTreated <- as.factor(dat_pooled$EverTreated)\ndat_pooled$PostPeriod <- as.factor(dat_pooled$PostPeriod)\n\n# Estimate Level Shift for Different Groups after T=300\nregP <- lm(P~PostPeriod*EverTreated, dat_pooled)\nregQ <- lm(Q~PostPeriod*EverTreated, dat_pooled)\nstargazer::stargazer(regP, regQ, \n    type='html',\n    title='Recipe DiD',\n    header=F)\n```\n\n\n<table style=\"text-align:center\"><caption><strong>Recipe DiD</strong></caption>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><td colspan=\"2\"><em>Dependent variable:</em></td></tr>\n<tr><td></td><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n<tr><td style=\"text-align:left\"></td><td>P</td><td>Q</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(1)</td><td>(2)</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">PostPeriod</td><td>-0.008</td><td>0.001</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.016)</td><td>(0.014)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">EverTreated1</td><td>-0.011</td><td>-0.007</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.016)</td><td>(0.014)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">PostPeriodTRUE:EverTreated1</td><td>-0.822<sup>***</sup></td><td>0.674<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.023)</td><td>(0.020)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">Constant</td><td>8.892<sup>***</sup></td><td>0.891<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.012)</td><td>(0.010)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">Observations</td><td>1,200</td><td>1,200</td></tr>\n<tr><td style=\"text-align:left\">R<sup>2</sup></td><td>0.767</td><td>0.735</td></tr>\n<tr><td style=\"text-align:left\">Adjusted R<sup>2</sup></td><td>0.766</td><td>0.734</td></tr>\n<tr><td style=\"text-align:left\">Residual Std. Error (df = 1196)</td><td>0.200</td><td>0.175</td></tr>\n<tr><td style=\"text-align:left\">F Statistic (df = 3; 1196)</td><td>1,310.456<sup>***</sup></td><td>1,103.303<sup>***</sup></td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"><em>Note:</em></td><td colspan=\"2\" style=\"text-align:right\"><sup>*</sup>p<0.1; <sup>**</sup>p<0.05; <sup>***</sup>p<0.01</td></tr>\n</table>\n\n\n## \"Natural\" Experiments\n\nNatural experiments are historical case studies that remedy the endogeneity issues in observational data. They assume that a historical events is quasi (or psuedo) random. In addition to \"RDD\" and \"DID\" methods discussed above, instrumental variables are used in historical event studies. The elementary versions use linear regression, so I can cover them here using our competitive equilibrium example from before.\n\n#### **Two Stage Least Squares (2SLS)**. {-}\nConsider the market equilibrium example, which contains a cost shock. We can simply run another regression, but there will still be a problem. \n\n::: {.cell}\n\n```{.r .cell-code}\n# Not exactly right, but at least right sign\nreg2 <- lm(Q~P, data=dat2)\nsummary(reg2)\n## \n## Call:\n## lm(formula = Q ~ P, data = dat2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.76712 -0.16070  0.00393  0.15929  0.66060 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  6.69391    0.17588   38.06   <2e-16 ***\n## P           -0.64642    0.02074  -31.16   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2339 on 598 degrees of freedom\n## Multiple R-squared:  0.6189,\tAdjusted R-squared:  0.6182 \n## F-statistic: 971.1 on 1 and 598 DF,  p-value: < 2.2e-16\n```\n:::\n\nIt turns out that rhe noisiness of the process within each group affects our OLS estimate: $\\widehat{\\beta}_{OLS}=Cov(Q^{*}, P^{*}) / Var(P^{*})$. For details, see\n<details>\n<summary><i>Within Group Variance</i></summary>\nYou can experiment with the effect of different variances on both OLS and IV in the code below. And note that if we had multiple supply shifts and recorded their magnitudes, then we could recover more information about demand, perhaps tracing it out entirely.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Examine\nEgrid <- expand.grid(Ed_sigma=c(.001, .25, 1), Es_sigma=c(.001, .25, 1))\n\nEgrid_regs <- lapply(1:nrow(Egrid), function(i){\n    Ed_sigma <- Egrid[i,1]\n    Es_sigma <- Egrid[i,2]    \n    EQ1 <- sapply(1:N, function(n){\n        demand <- qd_fun(P, Ed_sigma=Ed_sigma)\n        supply <- qs_fun(P, Es_sigma=Es_sigma)\n        return(eq_fun(demand, supply, P))\n    })\n    EQ2 <- sapply(1:N, function(n){\n        demand <- qd_fun(P,Ed_sigma=Ed_sigma)\n        supply2 <- qs_fun(P, As=-6.5,Es_sigma=Es_sigma)\n        return(eq_fun(demand, supply2, P))\n    })\n    dat <- rbind(\n        data.frame(t(EQ1), cost='1'),\n        data.frame(t(EQ2), cost='2'))\n    return(dat)\n})\nEgrid_OLS <- sapply(Egrid_regs, function(dat) coef( lm(Q~P, data=dat)))\nEgrid_IV <- sapply(Egrid_regs, function(dat) coef( feols(Q~1|P~cost, data=dat)))\n\n#cbind(Egrid, coef_OLS=t(Egrid_OLS)[,2], coef_IV=t(Egrid_IV)[,2])\nlapply( list(Egrid_OLS, Egrid_IV), function(ei){\n    Emat <- matrix(ei[2,],3,3)\n    rownames(Emat) <- paste0('Ed_sigma.',c(.001, .25, 1))\n    colnames(Emat) <- paste0('Es_sigma.',c(.001, .25, 1))\n    return( round(Emat,2))\n})\n```\n:::\n\n</details>\nTo overcome this issue, we can compute the change in the expected values $d \\mathbb{E}[Q^{*}] / d \\mathbb{E}[P^{*}] =-B_{D}$. Empirically, this is estimated via the change in average value.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Wald (1940) Estimate\ndat_mean <- rbind(\n    colMeans(dat2[dat2$cost==1,1:2]),\n    colMeans(dat2[dat2$cost==2,1:2]))\ndat_mean\n##             P         Q\n## [1,] 8.881133 0.8840162\n## [2,] 8.051233 1.5583621\nB_est <- diff(dat_mean[,2])/diff(dat_mean[,1])\nround(B_est, 2)\n## [1] -0.81\n```\n:::\n\n\n\n\n\n\nWe can also separately recover $d \\mathbb{E}[Q^{*}] / d \\mathbb{E}[A_{S}]$ and $d \\mathbb{E}[P^{*}] / d \\mathbb{E}[A_{S}]$ from separate regressions.^[Mathematically, we can also do this in a single step by exploiting linear algebra: \n$\\frac{\\frac{ Cov(Q^{*},A_{S})}{ V(A_{S}) } }{\\frac{ Cov(P^{*},A_{S})}{ V(A_{S}) }}\n&=& \\frac{Cov(Q^{*},A_{S} )}{ Cov(P^{*},A_{S})}.$]\n\n::: {.cell}\n\n```{.r .cell-code}\n# Heckman (2000, p.58) Estimate\nols_1 <- lm(P~cost, data=dat2)\nols_2 <- lm(Q~cost, data=dat2)\nB_est2 <- coef(ols_2)/coef(ols_1)\nround(B_est2[[2]],2)\n## [1] -0.81\n```\n:::\n\nAlternatively, we can recover the same estimate using an 2SLS regression with two equations:\n\\begin{eqnarray}\nP &=& \\alpha_{1} + A_{S} \\beta_{1} + \\epsilon_{1} \\\\\nQ &=& \\alpha_{2} + \\hat{P} \\beta_{2} + \\epsilon_{2}.\n\\end{eqnarray}\nIn the first regression, we estimate the average effect of the cost shock on prices. In the second equation, we estimate how the average effect of prices *which are exogenous to demand* affect quantity demanded. To see this, first substitute the equilibrium condition into the supply equation: $Q_{D}=Q_{S}=A_{S}+B_{S} P + E_{S}$, lets us rewrite $P$ as a function of $Q_{D}$. This yields two theoretical equations\n\\begin{eqnarray}\n\\label{eqn:linear_supply_iv}\nP &=& -\\frac{A_{S}}{{B_{S}}} + \\frac{Q_{D}}{B_{S}} - \\frac{E_{S}}{B_{S}} \\\\\n\\label{eqn:linear_demand_iv}\nQ_{D} &=&  A_{D} + B_{D} P  + E_{D}.\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Two Stage Least Squares Estimate\nols_1 <- lm(P~cost, data=dat2)\ndat2_new  <- cbind(dat2, Phat=predict(ols_1))\nreg_2sls <- lm(Q~Phat, data=dat2_new)\nsummary(reg_2sls)\n## \n## Call:\n## lm(formula = Q ~ Phat, data = dat2_new)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.57417 -0.11551  0.00219  0.11010  0.48352 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   8.1005     0.1432   56.56   <2e-16 ***\n## Phat         -0.8126     0.0169  -48.09   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1717 on 598 degrees of freedom\n## Multiple R-squared:  0.7945,\tAdjusted R-squared:  0.7942 \n## F-statistic:  2313 on 1 and 598 DF,  p-value: < 2.2e-16\n\n# One Stage Instrumental Variables Estimate\nlibrary(fixest)\nreg2_iv <- feols(Q~1|P~cost, data=dat2)\nsummary(reg2_iv)\n## TSLS estimation - Dep. Var.: Q\n##                   Endo.    : P\n##                   Instr.   : cost\n## Second stage: Dep. Var.: Q\n## Observations: 600\n## Standard-errors: IID \n##              Estimate Std. Error  t value  Pr(>|t|)    \n## (Intercept)  8.100495   0.205264  39.4637 < 2.2e-16 ***\n## fit_P       -0.812563   0.024216 -33.5546 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## RMSE: 0.245726   Adj. R2: 0.577291\n## F-test (1st stage), P: stat = 2,591.5, p < 2.2e-16, on 1 and 598 DoF.\n##            Wu-Hausman: stat =   518.6, p < 2.2e-16, on 1 and 597 DoF.\n```\n:::\n\n\n\n#### **Caveats**. {-}\n2SLS regression analysis can be very insightful, but I also want to stress some caveats about their practical application.\n\nWe always get coefficients back when running `feols`, and sometimes the computed p-values are very small. The interpretation of those numbers rests on many assumptions:\n\n* *Instrument exogeneity (Exclusion Restriction):* The instrument must affect outcomes only through the treatment variable (e.g., only supply is affected directly, not demand).\n* *Instrument relevance:* The instrument must be strongly correlated with the endogenous regressor, implying the shock creates meaningful variation.\n* *Functional form correctness:* Supply and demand are assumed linear and additively separable.\n* *Multiple hypothesis testing risks:* We were not repeatedly testing different instruments, which can artificially produce significant findings by chance.\n\nWe are rarely sure that all of these assumptions hold, and this is one reason why researchers often also report their OLS results. But that is insufficient, as spatial and temporal dependence also complicate inference:\n\n* *Exclusion restriction violations:* Spatial or temporal spillovers may cause instruments to affect the outcome through unintended channels, undermining instrument exogeneity.\n* *Weak instruments:* Spatial clustering, serial correlation, or network interdependencies can reduce instrument variation, causing weak instruments.\n* *Inference and standard errors:* Spatial or temporal interdependence reduces the effective sample size, making conventional standard errors misleadingly small.\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Further Reading\n\n\nYou are directed to the following resources which discusses endogeneity in more detail and how it applies generally.\n\n* Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction\n* https://www.mostlyharmlesseconometrics.com/\n* https://www.econometrics-with-r.org\n* https://bookdown.org/paul/applied-causal-analysis/\n* https://mixtape.scunning.com/\n* https://theeffectbook.net/\n* https://www.r-causal.org/\n* https://matheusfacure.github.io/python-causality-handbook/landing-page.html\n\nFor RDD and DID methods in natural experiments, see\n\n* https://bookdown.org/paul/applied-causal-analysis/rdd-regression-discontinuity-design.html\n* https://mixtape.scunning.com/06-regression_discontinuity\n* https://theeffectbook.net/ch-RegressionDiscontinuity.html\n* https://mixtape.scunning.com/09-difference_in_differences\n* https://theeffectbook.net/ch-DifferenceinDifference.html\n* http://www.urfie.net/read/index.html#page/226\n    \n\nFor IV methods in natural experiments, see\n\n* https://cameron.econ.ucdavis.edu/e240a/ch04iv.pdf\n* https://mru.org/courses/mastering-econometrics/introduction-instrumental-variables-part-one\n* https://www.econometrics-with-r.org/12-ivr.html\n* https://bookdown.org/paul/applied-causal-analysis/estimation-2.html\n* https://mixtape.scunning.com/07-instrumental_variables\n* https://theeffectbook.net/ch-InstrumentalVariables.html\n* http://www.urfie.net/read/index.html#page/247\n\n\n\n# Data Scientism\n***\n\nIn practice, it is hard to find a good natural experiment. For example, suppose we asked \"what is the effect of wages on police demanded?\" and examined a policy which lowered the educational requirements from 4 years to 2 to become an officer. This increases the labour supply, but it also affects the demand curve through \"general equilibrium\": as some of the new officers were potentially criminals and, with fewer criminals, the demand for police shifts down.\n\nIn practice, it is also easy to find a bad instrument. Paradoxically, natural experiments are something you are supposed to find but never search for. As you search for good instruments, for example, sometimes random noise will appear like a good instrument (spurious instruments). In this age of big data, we are getting increasingly more data and, perhaps surprisingly, this makes it easier to make false discoveries. \n\nWe will consider three classical ways for false discoveries to arise. After that, there are examples with the latest and greatest empirical recipes---we don't have so many theoretical results yet but I think you can understand the issue with the numerical example. Although it is difficult to express numerically, you must also know that if you search for a good natural experiment for too long, you can also be led astray from important questions. There are good reasons to be excited about empirical social science, but we would be wise to recall some earlier wisdom from economists on the matter.\n\n> The most reckless and treacherous of all theorists is he who professes to let facts and figures speak for themselves, who keeps in the background the part he has played, perhaps unconsciously, in selecting and grouping them\n>\n> ---  Alfred Marshall, 1885 \n\n\n> The blind transfer of the striving for quantitative measurements to a field where the specific conditions are not present which give it its basic importance in the natural sciences is the result of an entirely unfounded prejudice. It is probably responsible for the worst aberrations and absurdities produced by scientism in the social sciences. It not only leads frequently to the selection for study of the most irrelevant aspects of the phenomena because they happen to be measurable, but also to \"measurements\" and assignments of numerical values which are absolutely meaningless. What a distinguished philosopher recently wrote about psychology is at least equally true of the social sciences, namely that it is only too easy \"to rush off to measure something without considering what it is we are measuring, or what measurement means. In this respect some recent measurements are of the same logical type as Plato's determination that a just ruler is 729 times as happy as an unjust one.\"\n>\n> --- F.A. Hayek, 1943\n\n> if you torture the data long enough, it will confess\n>\n> --- R. Coase (Source Unknown)\n\n<!---\n''torture the data (to) confess'' (Coase,  Essays on economics and economists.  1995, p. 27)\n--->\n\n> the definition of a causal parameter is not always clearly stated, and formal statements of identifying conditions in terms of well-specified economic models are rarely presented. Moreover, the absence of explicit structural frameworks makes it difficult to cumulate knowledge across studies conducted within this framework. Many studies produced by this research program have a `stand alone' feature and neither inform nor are influenced by the general body of empirical knowledge in economics.\n>\n> --- J.J. Heckman, 2000\n\n\n> without explicit prior consideration of the effect of the instrument choice on the parameter being estimated, such a procedure is effectively the opposite of standard statistical practice in which a parameter of interest is defined first, followed by an estimator that delivers that parameter. Instead, we have a procedure in which the choice of the instrument, which is guided by criteria designed for a situation in which there is no heterogeneity, is implicitly allowed to determine the parameter of interest. This goes beyond the old story of looking for an object where the light is strong enough to see; rather, we have at least some control over the light but choose to let it fall where it may and then proclaim that whatever it illuminates is what we were looking for all along.\n>\n> --- A. Deaton, 2010\n\n\n## False Positives\n\n#### **Data Errors**. {-}\nA huge amount of data normally means a huge amount of data cleaning/merging/aggregating. This avoids many copy-paste errors, which are a recipe for [disaster](https://blog.hurree.co/8-of-the-biggest-excel-mistakes-of-all-time), but may also introduce other types of errors. Some spurious results are driven by honest errors in data cleaning. According to one [estimate](https://www.pnas.org/doi/10.1073/pnas.1212247109), this is responsible for around one fifth of all medical science retractions (there is even a whole [book](https://www.amazon.de/Much-Cost-Coding-Errors-Implementation/dp/1543772994) about this!). Although there are not similar meta-analysis in economics, there are some high-profile examples. This includes papers that are highly influential, like [Lott, Levitt](https://scienceblogs.com/deltoid/2005/12/02/lott-levitt-and-coding-errors) and [Reinhart and Rogoff](https://blogs.lse.ac.uk/impactofsocialsciences/2013/04/24/reinhart-rogoff-revisited-why-we-need-open-data-in-economics/) as well as others the top economics journals, like the [RESTUD](https://academic.oup.com/restud/article/90/2/1009/6982752) and [AER](https://www.aeaweb.org/articles?id=10.1257/aer.113.7.2053). There are some reasons to think such errors are more widespread across the social sciences; e.g., in [Census data](https://www2.census.gov/ces/tp/tp-2002-17.pdf) and [Aid data](https://www.sciencedirect.com/science/article/abs/pii/S0305750X11001951). So be careful!\n\nNote: one reason to plot your data is to help spot such errors.\n\n#### **P-Hacking**. {-}\nAnother class of errors pertains to P-hacking (and it's various synonyms: data drudging, star mining,....). While there are cases of fraudulent data manipulation (which can be considered as a dishonest data error), P-hacking need not even be intentional. You can simply be trying different variable transformations to uncover patterns in the data, for example, without accounting for how easy it is to find patterns when transforming  completely random data. P-hacking is [pernicious](https://elephantinthelab.org/a-replication-crisis-in-the-making/) and [widespread](https://www.americanscientist.org/article/the-statistical-crisis-in-science). \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# P-hacking OSLS with different explanatory vars\nset.seed(123)\nn <- 50\nX1 <- runif(n)\n\n# Regression Machine:\n# repeatedly finds covariate, runs regression\n# stops when statistically significant at .1%\np <- 1\ni <- 0\nwhile(p >= .001){ \n    # Get Random Covariate\n    X2 <-  runif(n)\n    # Merge and `Analyze'\n    dat_i <- data.frame(X1,X2)\n    reg_i <- lm(X1~X2, data=dat_i)\n    # update results in global environment\n    p <- summary(reg_i)$coefficients[2,4]\n    i <- i+1\n}\n#summary(reg_i)\n\nplot(X1~X2, data=dat_i,\n    pch=16, col=grey(0,.5), font.main=1,\n    main=paste0('Random Dataset ', i,\":   p=\",\n        formatC(p,digits=2, format='fg')))\nabline(reg_i)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-65-1.png){width=672}\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# P-hacking 2SLS with different explanatory vars\n# and different instrumental vars\nlibrary(fixest)\np <- 1\nii <- 0\nset.seed(123)\nwhile(p >= .05){\n    # Get Random Covariates\n    X2 <-  runif(n)    \n    X3 <-  runif(n)\n    # Create Treatment Variable based on Cutoff\n    cutoffs <- seq(0,1,length.out=11)[-c(1,11)]\n    for(tau in cutoffs){\n        T3 <- 1*(X3 > tau)\n        # Merge and `Analyze'\n        dat_i <- data.frame(X1,X2,T3)\n        ivreg_i <- feols(X1~1|X2~T3, data=dat_i)\n        # Update results in global environment\n        ptab <- summary(ivreg_i)$coeftable\n        if( nrow(ptab)==2){\n            p <- ptab[2,4]\n            ii <- ii+1\n        }\n    }\n}\nsummary(ivreg_i)\n## TSLS estimation - Dep. Var.: X1\n##                   Endo.    : X2\n##                   Instr.   : T3\n## Second stage: Dep. Var.: X1\n## Observations: 50\n## Standard-errors: IID \n##              Estimate Std. Error       t value  Pr(>|t|)    \n## (Intercept) -9.95e-14   1.28e-13 -7.750700e-01    0.4421    \n## fit_X2       1.00e+00   2.46e-13  4.060978e+12 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## RMSE: 5.81e-14   Adj. R2: 1\n## F-test (1st stage), X2: stat = 0.664884, p = 0.418869, on 1 and 48 DoF.\n##             Wu-Hausman: stat = 0.232185, p = 0.632145, on 1 and 47 DoF.\n```\n:::\n\n\n\n## Spurious Regression \n \nEven without any coding errors or p-hacking, you can sometimes make a false discovery. We begin with a motivating empirical example of \"US Gov't Spending on Science\".\n\n\nFirst, get and inspect some data from https://tylervigen.com/spurious-correlations\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your data is not made up in the computer (hopefully!)\nvigen_csv <- read.csv( paste0(\n'https://raw.githubusercontent.com/the-mad-statter/',\n'whysospurious/master/data-raw/tylervigen.csv') ) \nclass(vigen_csv)\n## [1] \"data.frame\"\nnames(vigen_csv)\n##  [1] \"year\"                         \"science_spending\"            \n##  [3] \"hanging_suicides\"             \"pool_fall_drownings\"         \n##  [5] \"cage_films\"                   \"cheese_percap\"               \n##  [7] \"bed_deaths\"                   \"maine_divorce_rate\"          \n##  [9] \"margarine_percap\"             \"miss_usa_age\"                \n## [11] \"steam_murders\"                \"arcade_revenue\"              \n## [13] \"computer_science_doctorates\"  \"noncom_space_launches\"       \n## [15] \"sociology_doctorates\"         \"mozzarella_percap\"           \n## [17] \"civil_engineering_doctorates\" \"fishing_drownings\"           \n## [19] \"kentucky_marriage_rate\"       \"oil_imports_norway\"          \n## [21] \"chicken_percap\"               \"train_collision_deaths\"      \n## [23] \"oil_imports_total\"            \"pool_drownings\"              \n## [25] \"nuclear_power\"                \"japanese_cars_sold\"          \n## [27] \"motor_vehicle_suicides\"       \"spelling_bee_word_length\"    \n## [29] \"spider_deaths\"                \"math_doctorates\"             \n## [31] \"uranium\"\nvigen_csv[1:5,1:5]\n##   year science_spending hanging_suicides pool_fall_drownings cage_films\n## 1 1996               NA               NA                  NA         NA\n## 2 1997               NA               NA                  NA         NA\n## 3 1998               NA               NA                  NA         NA\n## 4 1999            18079             5427                 109          2\n## 5 2000            18594             5688                 102          2\n```\n:::\n\n\n\nExamine some data\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,2), mar=c(2,2,2,1))\nplot.new()\nplot.window(xlim=c(1999, 2009), ylim=c(5,9)*1000)\nlines(science_spending/3~year, data=vigen_csv, lty=1, col=2, pch=16)\ntext(2003, 8200, 'US spending on science, space, technology (USD/3)', col=2, cex=.6, srt=30)\nlines(hanging_suicides~year, data=vigen_csv, lty=1, col=4, pch=16)\ntext(2004, 6500, 'US Suicides by hanging, strangulation, suffocation (Deaths)', col=4, cex=.6, srt=30)\naxis(1)\naxis(2)\n\n\nplot.new()\nplot.window(xlim=c(2002, 2009), ylim=c(0,5))\nlines(cage_films~year, data=vigen_csv[vigen_csv$year>=2002,], lty=1, col=2, pch=16)\ntext(2006, 0.5, 'Number of films with Nicolas Cage (Films)', col=2, cex=.6, srt=0)\nlines(pool_fall_drownings/25~year, data=vigen_csv[vigen_csv$year>=2002,], lty=1, col=4, pch=16)\ntext(2006, 4.5, 'Number of drownings by falling into pool (US Deaths/25)', col=4, cex=.6, srt=0)\naxis(1)\naxis(2)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-68-1.png){width=672}\n:::\n:::\n\n\n\n```{.r .cell-code}\n# Include an intercept to regression 1\n#reg2 <-  lm(cage_films ~ science_spending, data=vigen_csv)\n#suppressMessages(library(stargazer))\n#stargazer(reg1, reg2, type='html')\n```\n\n#### **Another Example**. {-}\nThe US government spending on science is ruining cinema\n(p<.001)!?\n\n::: {.cell}\n\n```{.r .cell-code}\n# Drop Data before 1999\nvigen_csv <- vigen_csv[vigen_csv$year >= 1999,] \n\n# Run OLS Regression\nreg1 <-  lm(cage_films ~ -1 + science_spending, data=vigen_csv)\nsummary(reg1)\n## \n## Call:\n## lm(formula = cage_films ~ -1 + science_spending, data = vigen_csv)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.7670 -0.7165  0.1447  0.7890  1.4531 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(>|t|)    \n## science_spending 9.978e-05  1.350e-05    7.39 2.34e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.033 on 10 degrees of freedom\n##   (1 observation deleted due to missingness)\n## Multiple R-squared:  0.8452,\tAdjusted R-squared:  0.8297 \n## F-statistic: 54.61 on 1 and 10 DF,  p-value: 2.343e-05\n```\n:::\n\nIt's not all bad, because people in Maine stay married longer?\n\n::: {.cell}\n\n```{.r .cell-code}\nplot.new()\nplot.window(xlim=c(1999, 2009), ylim=c(7,9))\nlines(log(maine_divorce_rate*1000)~year, data=vigen_csv)\nlines(log(science_spending/10)~year, data=vigen_csv, lty=2)\naxis(1)\naxis(2)\nlegend('topright', lty=c(1,2), legend=c(\n    'log(maine_divorce_rate*1000)',\n    'log(science_spending/10)'))\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-71-1.png){width=672}\n:::\n:::\n\n\n\nFor more intuition on spurious correlations, try http://shiny.calpoly.sh/Corr_Reg_Game/\nThe same principles apply to more sophisticated methods.\n\n## Spurious Causal Impacts\n\nIn practice, it is *hard to find \"good\" natural experiments*. For example, suppose we asked \"what is the effect of wages on police demanded?\" and examined a policy which lowered the educational requirements from 4 years to 2 to become an officer. This increases the labour supply, but it also affects the demand curve through \"general equilibrium\": as some of the new officers were potentially criminals. With fewer criminals, the demand for likely police shifts down.\n\nIn practice, it is also surprisingly *easy to find \"bad\" natural experiments*. Paradoxically, natural experiments are something you are supposed to find but never search for. As you search for good instruments, for example, sometimes random noise will appear like a good instrument (Spurious instruments). Worse, if you search for a good instrument for too long, you can also be led astray from important questions.\n\n#### **Example: Vigen IV's**. {-}\nWe now run IV regressions for different variable combinations in the dataset of spurious relationships\n\n::: {.cell}\n\n```{.r .cell-code}\nknames <- names(vigen_csv)[2:11] # First 10 Variables\n#knames <- names(vigen_csv)[-1] # Try All Variables\np <- 1\nii <- 1\nivreg_list <- vector(\"list\", factorial(length(knames))/factorial(length(knames)-3))\n\n# Choose 3 variable\nfor( k1 in knames){\nfor( k2 in setdiff(knames,k1)){\nfor( k3 in setdiff(knames,c(k1,k2)) ){   \n    X1 <- vigen_csv[,k1]\n    X2 <- vigen_csv[,k2]\n    X3 <- vigen_csv[,k3]\n    # Merge and `Analyze'        \n    dat_i <- na.omit(data.frame(X1,X2,X3))\n    ivreg_i <- feols(X1~1|X2~X3, data=dat_i)\n    ivreg_list[[ii]] <- list(ivreg_i, c(k1,k2,k3))\n    ii <- ii+1\n}}}\npvals <- sapply(ivreg_list, function(ivreg_i){ivreg_i[[1]]$coeftable[2,4]})\n\nplot(ecdf(pvals), xlab='p-value', ylab='CDF', font.main=1,\n    main='Frequency IV is Statistically Significant')\nabline(v=c(.01,.05), col=c(2,4))\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-72-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Most Significant Spurious Combinations\npvars <- sapply(ivreg_list, function(ivreg_i){ivreg_i[[2]]})\npdat <- data.frame(t(pvars), pvals)\npdat <- pdat[order(pdat$pvals),]\nhead(pdat)\n##                     X1                 X2            X3        pvals\n## 4     science_spending   hanging_suicides    bed_deaths 3.049883e-08\n## 76    hanging_suicides   science_spending    bed_deaths 3.049883e-08\n## 3     science_spending   hanging_suicides cheese_percap 3.344890e-08\n## 75    hanging_suicides   science_spending cheese_percap 3.344890e-08\n## 485 maine_divorce_rate   margarine_percap cheese_percap 3.997738e-08\n## 557   margarine_percap maine_divorce_rate cheese_percap 3.997738e-08\n```\n:::\n\n\n#### **Simulation Study**. {-}\nWe apply the three major credible methods (IV, RDD, DID) to random walks. Each time, we find a result that fits mold and add various extensions that make it appear robust. One could tell a story about how $X_{2}$ affects $X_{1}$ but $X_{1}$ might also affect $X_{2}$, and how they discovered an instrument $X_{3}$ to provide the first causal estimate of $X_{2}$ on $X_{1}$. The analysis looks scientific and the story sounds plausible, so you could probably be convinced *if it were not just random noise.*\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 1000\nn_index <- seq(n)\n\nset.seed(1)\nrandom_walk1 <- cumsum(runif(n,-1,1))\n\nset.seed(2)\nrandom_walk2 <- cumsum(runif(n,-1,1))\n\npar(mfrow=c(1,2))\nplot(random_walk1, pch=16, col=rgb(1,0,0,.25),\n    xlab='Time', ylab='Random Value')\nplot(random_walk2, pch=16, col=rgb(0,0,1,.25),\n    xlab='Time', ylab='Random Value')\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-73-1.png){width=672}\n:::\n:::\n\n\n**IV**. First, find an instrument that satisfy various statistical criterion to provide a causal estimate of $X_{2}$ on $X_{1}$.\n\n::: {.cell}\n\n```{.r .cell-code}\n# \"Find\" \"valid\" ingredients\nlibrary(fixest)\nrandom_walk3 <- cumsum(runif(n,-1,1))\ndat_i <- data.frame(\n    X1=random_walk1,\n    X2=random_walk2,\n    X3=random_walk3)\nivreg_i <- feols(X1~1|X2~X3, data=dat_i)\nsummary(ivreg_i)\n## TSLS estimation - Dep. Var.: X1\n##                   Endo.    : X2\n##                   Instr.   : X3\n## Second stage: Dep. Var.: X1\n## Observations: 1,000\n## Standard-errors: IID \n##             Estimate Std. Error t value   Pr(>|t|)    \n## (Intercept)  8.53309   1.644285 5.18954 2.5533e-07 ***\n## fit_X2       1.79901   0.472285 3.80916 1.4796e-04 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## RMSE: 6.25733   Adj. R2: -1.29152\n## F-test (1st stage), X2: stat = 10.8, p = 0.001048, on 1 and 998 DoF.\n##             Wu-Hausman: stat = 23.4, p = 1.518e-6, on 1 and 997 DoF.\n\n# After experimenting with different instruments\n# you can find even stronger results!\n```\n:::\n\n\n**RDD**. Second, find a large discrete change in the data that you can associate with a policy. You can use this as an instrument too, also providing a causal estimate of $X_{2}$ on $X_{1}$.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Let the data take shape\n# (around the large differences before and after)\nn1 <- 290\nwind1 <- c(n1-300,n1+300)\ndat1 <- data.frame(t=n_index, y=random_walk1, d=1*(n_index > n1))\ndat1_sub <- dat1[ n_index>wind1[1] & n_index < wind1[2],]\n\n# Then find your big break\nreg0 <- lm(y~t, data=dat1_sub[dat1_sub$d==0,])\nreg1 <- lm(y~t, data=dat1_sub[dat1_sub$d==1,])\n\n# The evidence should show openly (it's just science)\nplot(random_walk1, pch=16, col=rgb(0,0,1,.25),\n    xlim=wind1, xlab='Time', ylab='Random Value')\nabline(v=n1, lty=2)\nlines(reg0$model$t, reg0$fitted.values, col=1)\nlines(reg1$model$t, reg1$fitted.values, col=1)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-75-1.png){width=672}\n:::\n:::\n\n\n\n```{.r .cell-code}\n# Dress with some statistics for added credibility\nrdd_sub <- lm(y~d+t+d*t, data=dat1_sub)\nrdd_full <- lm(y~d+t+d*t, data=dat1)\nstargazer::stargazer(rdd_sub, rdd_full, \n    type='html',\n    title='Recipe RDD',\n    header=F,\n    omit=c('Constant'),\n    notes=c('First column uses a dataset around the discontinuity.',\n    'Smaller windows are more causal, and where the effect is bigger.'))\n```\n\n\n<table style=\"text-align:center\"><caption><strong>Recipe RDD</strong></caption>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><td colspan=\"2\"><em>Dependent variable:</em></td></tr>\n<tr><td></td><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n<tr><td style=\"text-align:left\"></td><td colspan=\"2\">y</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(1)</td><td>(2)</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">d</td><td>-13.169<sup>***</sup></td><td>-9.639<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.569)</td><td>(0.527)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">t</td><td>0.011<sup>***</sup></td><td>0.011<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.001)</td><td>(0.002)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">d:t</td><td>0.009<sup>***</sup></td><td>0.004<sup>*</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.002)</td><td>(0.002)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">Observations</td><td>589</td><td>1,000</td></tr>\n<tr><td style=\"text-align:left\">R<sup>2</sup></td><td>0.771</td><td>0.447</td></tr>\n<tr><td style=\"text-align:left\">Adjusted R<sup>2</sup></td><td>0.770</td><td>0.446</td></tr>\n<tr><td style=\"text-align:left\">Residual Std. Error</td><td>1.764 (df = 585)</td><td>3.081 (df = 996)</td></tr>\n<tr><td style=\"text-align:left\">F Statistic</td><td>658.281<sup>***</sup> (df = 3; 585)</td><td>268.763<sup>***</sup> (df = 3; 996)</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"><em>Note:</em></td><td colspan=\"2\" style=\"text-align:right\"><sup>*</sup>p<0.1; <sup>**</sup>p<0.05; <sup>***</sup>p<0.01</td></tr>\n<tr><td style=\"text-align:left\"></td><td colspan=\"2\" style=\"text-align:right\">First column uses a dataset around the discontinuity.</td></tr>\n<tr><td style=\"text-align:left\"></td><td colspan=\"2\" style=\"text-align:right\">Smaller windows are more causal, and where the effect is bigger.</td></tr>\n</table>\n\n**DID**. Third, find a change in the data that you can associate with a policy where the control group has parallel trends. This also provides a causal estimate of $X_{2}$ on $X_{1}$.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Find a reversal of fortune\n# (A good story always goes well with a nice pre-trend)\nn2 <- 318\nwind2 <- c(n2-20,n2+20)\nplot(random_walk2, pch=16, col=rgb(0,0,1,.5),\n    xlim=wind2, ylim=c(-15,15), xlab='Time', ylab='Random Value')\npoints(random_walk1, pch=16, col=rgb(1,0,0,.5))\nabline(v=n2, lty=2)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-77-1.png){width=672}\n:::\n:::\n\n\n\n```{.r .cell-code}\n# Knead out any effects that are non-causal (aka correlation)\ndat2A <- data.frame(t=n_index, y=random_walk1, d=1*(n_index > n2), RWid=1)\ndat2B <- data.frame(t=n_index, y=random_walk2, d=0, RWid=2)\ndat2  <- rbind(dat2A, dat2B)\ndat2$RWid <- as.factor(dat2$RWid)\ndat2$tid <- as.factor(dat2$t)\ndat2_sub <- dat2[ dat2$t>wind2[1] & dat2$t < wind2[2],]\n\n# Report the stars for all to enjoy\n# (what about the intercept?)\n# (stable coefficients are the good ones?)\ndid_fe1 <- lm(y~d+tid, data=dat2_sub)\ndid_fe2 <- lm(y~d+RWid, data=dat2_sub)\ndid_fe3 <- lm(y~d*RWid+tid, data=dat2_sub)\nstargazer::stargazer(did_fe1, did_fe2, did_fe3,\n    type='html',\n    title='Recipe DID',\n    header=F,\n    omit=c('tid','RWid', 'Constant'),\n    notes=c(\n     'Fixed effects for time in column 1, for id in column 2, and both in column 3.',\n     'Fixed effects control for most of your concerns.',\n     'Anything else creates a bias in the opposite direction.'))\n```\n\n\n<table style=\"text-align:center\"><caption><strong>Recipe DID</strong></caption>\n<tr><td colspan=\"4\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><td colspan=\"3\"><em>Dependent variable:</em></td></tr>\n<tr><td></td><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr>\n<tr><td style=\"text-align:left\"></td><td colspan=\"3\">y</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(1)</td><td>(2)</td><td>(3)</td></tr>\n<tr><td colspan=\"4\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">d</td><td>1.804<sup>*</sup></td><td>1.847<sup>***</sup></td><td>5.851<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.892)</td><td>(0.652)</td><td>(0.828)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td><td></td></tr>\n<tr><td colspan=\"4\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">Observations</td><td>78</td><td>78</td><td>78</td></tr>\n<tr><td style=\"text-align:left\">R<sup>2</sup></td><td>0.227</td><td>0.164</td><td>0.668</td></tr>\n<tr><td style=\"text-align:left\">Adjusted R<sup>2</sup></td><td>-0.566</td><td>0.142</td><td>0.309</td></tr>\n<tr><td style=\"text-align:left\">Residual Std. Error</td><td>2.750 (df = 38)</td><td>2.035 (df = 75)</td><td>1.827 (df = 37)</td></tr>\n<tr><td style=\"text-align:left\">F Statistic</td><td>0.287 (df = 39; 38)</td><td>7.379<sup>***</sup> (df = 2; 75)</td><td>1.860<sup>**</sup> (df = 40; 37)</td></tr>\n<tr><td colspan=\"4\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"><em>Note:</em></td><td colspan=\"3\" style=\"text-align:right\"><sup>*</sup>p<0.1; <sup>**</sup>p<0.05; <sup>***</sup>p<0.01</td></tr>\n<tr><td style=\"text-align:left\"></td><td colspan=\"3\" style=\"text-align:right\">Fixed effects for time in column 1, for id in column 2, and both in column 3.</td></tr>\n<tr><td style=\"text-align:left\"></td><td colspan=\"3\" style=\"text-align:right\">Fixed effects control for most of your concerns.</td></tr>\n<tr><td style=\"text-align:left\"></td><td colspan=\"3\" style=\"text-align:right\">Anything else creates a bias in the opposite direction.</td></tr>\n</table>\n\n\n# Misc. Topics\n***\n\n\n## Locally Linear Multiple Regression\n\n\n## Nonparametric Tests\n\n#### **Distributional Comparisons**. {-}\nWe can also examine whether there are any differences between the entire *distributions*\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample Wage Data\nlibrary(wooldridge)\nx1 <- sort( wage1[wage1$educ == 15,  'wage'])  \nx2 <- sort( wage1[wage1$educ == 16,  'wage'] )\nx <- sort(c(x1, x2))\n\n# Compute Quantiles\nquants <- seq(0,1,length.out=101)\nQ1 <- quantile(x1, probs=quants)\nQ2 <- quantile(x2, probs=quants)\n\n# Compare Distributions via Quantiles\nrx <- range(c(x1, x2))\npar(mfrow=c(1,2))\nplot(rx, c(0,1), type='n', font.main=1,\n    main='Distributional Comparison',\n    xlab=expression(Q[s]),\n    ylab=expression(F[s]))\nlines(Q1, quants, col=2)\nlines(Q2, quants, col=4)\nlegend('bottomright', col=c(2,4), lty=1,\nlegend=c('F1', 'F2'))\n\n# Compare Quantiles\nplot(Q1, Q2, xlim=rx, ylim=rx,\n    main='Quantile-Quantile Plot', font.main=1,\npch=16, col=grey(0,.25))\nabline(a=0,b=1,lty=2)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-79-1.png){width=672}\n:::\n:::\n\n\nThe starting point for hypothesis testing is the Kolmogorov-Smirnov Statistic: the maximum absolute difference between two CDF's over all sample data $x \\in \\{X_1\\} \\cup \\{X_2\\}$.\n\\begin{eqnarray}\nKS &=& \\max_{x} |F_{1}(x)- F_{2}(x)|^{p},\n\\end{eqnarray}\nwhere $p$ is an integer (typically 1).\n\nAn intuitive alternative is the Cramer-von Mises Statistic: the sum of absolute differences (raised to a power, typically 2) between two CDF's. \n\\begin{eqnarray}\nCVM=\\sum_{x} |F_{1}(x)- F_{2}(x)|^{p}.\n\\end{eqnarray}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Distributions\nF1 <- ecdf(x1)(x)\nF2 <- ecdf(x2)(x)\n\nlibrary(twosamples)\n\n# Kolmogorov-Smirnov\nKSq <- which.max(abs(F2 - F1))\nKSqv <- round(twosamples::ks_stat(x1, x2),2)\n\n# Cramer-von Mises Statistic (p=2)\nCVMqv <- round(twosamples::cvm_stat(x1, x2, power=2), 2) \n\n# Visualize Differences\nplot(range(x), c(0,1), type=\"n\", xlab='x', ylab='ECDF')\nlines(x, F1, col=2, lwd=2)\nlines(x, F2, col=4, lwd=2)\n# CVM\nsegments(x, F1, x, F2, lwd=.5, col=grey(0,.2))\n# KS\nsegments(x[KSq], F1[KSq], x[KSq], F2[KSq], lwd=1.5, col=grey(0,.75), lty=2)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-80-1.png){width=672}\n:::\n:::\n\n\nJust as before, you use bootstrapping for hypothesis testing.\n\n::: {.cell}\n\n```{.r .cell-code}\ntwosamples::cvm_test(x1, x2)\n## Test Stat   P-Value \n##  2.084253  0.094500\n```\n:::\n\n\n#### **Comparing Multiple Groups**. {-}\nFor multiple groups, we can tests the equality of all distributions (whether at least one group is different). The *Kruskal-Wallis* test examines\n\\[\nH_0:\\; F_1 = F_2 = \\dots = F_G\n\\quad\\text{versus}\\quad\nH_A:\\; \\text{at least one } F_g \\text{ differs},\n\\]\nwhere $F_g$ is the continuous distribution of group $g$. This test does not tell us which group is different.\n\nTo conduct the test, first denote individuals $i=1,...n$ with overall ranks $r_1,....r_{n}$. Each individual belongs to group $g=1,...G$, and each group $g$ has $n_{g}$ individuals with average rank $\\overline{r}_{g} = \\sum_{i} r_{i} /n_{g}$. The Kruskal Wallis statistic is \n\\begin{eqnarray}\nKW &=& (N-1) \\frac{\\sum_{g=1}^{G} n_{g}( \\overline{r}_{g} - \\overline{r}  )^2  }{\\sum_{i=1}^{N} ( r_{i} - \\overline{r}  )^2}, \n\\end{eqnarray}\nwhere  $\\overline{r} = \\frac{N+1}{2}$ is the grand mean rank.\n\nIn the special case with only two groups, $G=2$, the Kruskal Wallis test reduces to the *Mann–Whitney U-test* (also known as the \\textit{Wilcoxon rank-sum test}). In this case, we can write the hypotheses in terms of individual outcomes in each group, $Y_i$ in one group $Y_j$ in the other;\n\\[\nH_0: P(Y_i > Y_j)=P(Y_i > Y_i)\n\\quad\\text{versus}\\quad\nH_A: P(Y_i > Y_j) \\neq P(Y_i > Y_j) \n\\]\nThe corresponding test statistic is\n\\begin{eqnarray}\nU   &=& \\min(U_1,U_2) \\\\\nU_g &=& \\sum_{i\\in g}\\sum_{j\\in -g}\n           \\Bigl[\\mathbf 1(Y_i > Y_j) + \\tfrac12\\mathbf 1(Y_i = Y_j)\\Bigr].\n\\end{eqnarray}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\ndata(CASchools)\nCASchools$stratio <- CASchools$students/CASchools$teachers\n\n# Do student/teacher ratio differ for at least 1 county?\n# Single test of multiple distributions\nkruskal.test(CASchools$stratio, CASchools$county)\n## \n## \tKruskal-Wallis rank sum test\n## \n## data:  CASchools$stratio and CASchools$county\n## Kruskal-Wallis chi-squared = 161.18, df = 44, p-value = 2.831e-15\n\n# Multiple pairwise tests\n# pairwise.wilcox.test(CASchools$stratio, CASchools$county)\n```\n:::\n\n\n\n\n\n\n## Prediction\n\n#### **Prediction Intervals**. {-}\n\nIn addition to confidence intervals, we can also compute a *prediction interval* which estimate the variability of new data rather than a statistic\n\nIn this example, we consider a single variable and compute the frequency each value was covered.\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- runif(1000)\n# Middle 90% of values\nxq0 <- quantile(x, probs=c(.05,.95))\n\nbks <- seq(0,1,by=.01)\nhist(x, breaks=bks, border=NA,\n    main='Prediction Interval', font.main=1)\nabline(v=xq0)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-83-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\npaste0('we are 90% confident that the a future data point will be between ', \n    round(xq0[1],2), ' and ', round(xq0[2],2) )\n## [1] \"we are 90% confident that the a future data point will be between 0.06 and 0.95\"\n```\n:::\n\nIn this example, we consider a range for $y_{i}(x)$ rather than for $m(x)$. These intervals also take into account the residuals --- the variability of individuals around the mean. \n\n::: {.cell}\n\n```{.r .cell-code}\n# Bivariate Data from USArrests\nxy <- USArrests[,c('Murder','UrbanPop')]\ncolnames(xy) <- c('y','x')\nxy0 <- xy[order(xy$x),]\n```\n:::\n\n\nFor a nice overview of different types of intervals, see https://www.jstor.org/stable/2685212. For an in-depth view, see \"Statistical Intervals: A Guide for Practitioners and Researchers\" or \"Statistical Tolerance Regions: Theory, Applications, and Computation\". See https://robjhyndman.com/hyndsight/intervals/ for constructing intervals for future observations in a time-series context. See Davison and Hinkley, chapters 5 and 6 (also Efron and Tibshirani, or Wehrens et al.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboot_regs <- lapply(1:399, function(b){\n    b_id <- sample( nrow(xy), replace=T)\n    xy_b <- xy[b_id,]\n    reg_b <- lm(y~x, dat=xy_b)\n})\n\nplot(y~x, pch=16, col=grey(0,.5),\n    dat=xy0, ylim=c(0, 20))\nlines(X0, preds_lo,\n    col=hcl.colors(3,alpha=.75)[2],\n    type='o', pch=2)\n\n# Estimate Residuals CI at design points\nres_lo <- sapply(1:nrow(xy), function(i){\n    y_i <- xy[i,'y']\n    preds_i <- jack_lo[,i]\n    resids_i <- y_i - preds_i\n})\nres_cb <- apply(res_lo, 1, quantile,\n    probs=c(.025,.975), na.rm=T)\n\n# Plot\nlines( X0, preds_lo +res_cb[1,],\n    col=hcl.colors(3,alpha=.75)[2], lt=2)\nlines( X0, preds_lo +res_cb[2,],\n    col=hcl.colors(3,alpha=.75)[2], lty=2)\n\n\n\n# Smooth estimates \nres_lo <- lapply(1:nrow(xy), function(i){\n    y_i <- xy[i,'y']\n    x_i <- xy[i,'x']\n    preds_i <- jack_lo[,i]\n    resids_i <- y_i - preds_i\n    cbind(e=resids_i, x=x_i)\n})\nres_lo <- as.data.frame(do.call(rbind, res_lo))\n\nres_fun <- function(x0, h, res_lo){\n    # Assign equal weight to observations within h distance to x0\n    # 0 weight for all other observations\n    ki <- dunif(res_lo$x, x0-h, x0+h) \n    ei <- res_lo[ki!=0,'e']\n    res_i <- quantile(ei, probs=c(.025,.975), na.rm=T)\n}\nX0 <- sort(unique(xy$x))\nres_lo2 <- sapply(X0, res_fun, h=15, res_lo=res_lo)\n\nlines( X0, preds_lo +res_lo2[1,],\n    col=hcl.colors(3,alpha=.75)[2], lty=1, lwd=2)\nlines( X0, preds_lo +res_lo2[2,],\n    col=hcl.colors(3,alpha=.75)[2], lty=1, lwd=2)\n```\n\n::: {.cell-output-display}\n![](02-LinearRegression_files/figure-html/unnamed-chunk-85-1.png){width=672}\n:::\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap Prediction Interval\nboot_resids <- lapply(boot_regs, function(reg_b){\n    e_b <- resid(reg_b)\n    x_b <- reg_b$model$x\n    res_b <- cbind(e_b, x_b)\n})\nboot_resids <- as.data.frame(do.call(rbind, boot_resids))\n# Homoskedastic\nehat <- quantile(boot_resids$e_b, probs=c(.025, .975))\nx <- quantile(xy$x,probs=seq(0,1,by=.1))\nboot_pi <- coef(reg)[1] + x*coef(reg)['x']\nboot_pi <- cbind(boot_pi + ehat[1], boot_pi + ehat[2])\n\n# Plot Bootstrap PI\nplot(y~x, dat=xy, pch=16, main='Prediction Intervals',\n    ylim=c(-5,20), font.main=1)\npolygon( c(x, rev(x)), c(boot_pi[,1], rev(boot_pi[,2])),\n    col=grey(0,.2), border=NA)\n\n# Parametric PI (For Comparison)\n#pi <- predict(reg, interval='prediction', newdata=data.frame(x))\n#lines( x, pi[,'lwr'], lty=2)\n#lines( x, pi[,'upr'], lty=2)\n```\n:::\n\n\n#### **Forecasting and TSCV** {-}\n\n## Decision Theory\n\n#### **Statistical Power** {-}\n\n#### **Quality Control** {-}\n\n#### **Optimal Experiment Designs** {-}\n\n",
    "supporting": [
      "02-LinearRegression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"site_libs/plotly-binding-4.11.0/plotly.js\"></script>\n<script src=\"site_libs/typedarray-0.1/typedarray.min.js\"></script>\n<script src=\"site_libs/jquery-3.5.1/jquery.min.js\"></script>\n<link href=\"site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\n<link href=\"site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/plotly-main-2.11.1/plotly-latest.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}