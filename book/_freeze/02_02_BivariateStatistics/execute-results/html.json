{
  "hash": "3ca418478346c571a7425384f60a76a6",
  "result": {
    "engine": "knitr",
    "markdown": "\n# Bivariate Statistics\n***\n\nAll of the univariate statistics we have covered apply to marginal distributions. For joint distributions, there are several ways to statistically describe the relationship between two variables. The major differences surround whether the data are cardinal or an ordered/unordered factor.\n\n## Statistics of Association\n\n#### **Two Cardinals**. {-}\n*Pearson (Linear) Correlation*.\nSuppose you have two vectors, $\\hat{X}$ and $\\hat{Y}$, that are both cardinal data. As such, you can compute the most famous measure of association, the covariance. Letting $\\hat{M}_{X}$ and $\\hat{M}_{Y}$ denote the mean of $X$ and $Y$, we have\n$$\n\\hat{C}_{XY} =  \\sum_{i=1}^{n} [\\hat{X}_{i} - \\hat{M}_{X}] [\\hat{Y}_i - \\hat{M}_{Y}] / n\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\n#plot(xy, pch=16, col=grey(0,.25))\ncov(xy)\n##             Murder   UrbanPop\n## Murder   18.970465   4.386204\n## UrbanPop  4.386204 209.518776\n```\n:::\n\nNote that $\\hat{C}_{XX}=\\hat{V}_{X}$.\nFor ease of interpretation and comparison, we rescale this statistic to always lay between $-1$ and $1$ \n$$\n\\hat{R}_{XY} = \\frac{ \\hat{C}_{XY} }{ \\sqrt{\\hat{V}_X} \\sqrt{\\hat{V}_Y}}\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(xy)[2]\n## [1] 0.06957262\n```\n:::\n\n\n*Falk Codeviance*.\nThe Codeviance, $\\tilde{C}_{XY}$, is a robust alternative to Covariance. Instead of relying on means (which can be sensitive to outliers), it uses medians.^[See also *Theil-Sen Estimator*, which may be seen as a precursor.] We can also scale the Codeviance by the median absolute deviation to compute the median correlation, which typically lies in $[-1,1]$ but not always. Letting $\\tilde{M}_{X}$ and $\\tilde{M}_{Y}$ denote the median of $X$ and $Y$, we have\n\\begin{eqnarray} \n\\tilde{C}_{XY} = \\text{Med}\\left\\{ |\\hat{X}_{i} - \\tilde{M}_{X}| |\\hat{Y}_i - \\tilde{M}_{Y}| \\right\\} \\\\\n\\tilde{R}_{XY} = \\frac{ \\tilde{C}_{XY} }{ \\hat{\\text{MAD}}_{X} \\hat{\\text{MAD}}_{Y}}. \n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\ncodev <- function(xy) {\n  # Compute medians for each column\n  med <- apply(xy, 2, median)\n  # Subtract the medians from each column\n  xm <- sweep(xy, 2, med, \"-\")\n  # Compute CoDev\n  CoDev <- median(xm[, 1] * xm[, 2])\n  # Compute the medians of absolute deviation\n  MadProd <- prod( apply(abs(xm), 2, median) )\n  # Return the robust correlation measure\n  return( CoDev / MadProd)\n}\ncodev(xy)\n## [1] 0.005707763\n```\n:::\n\n\n#### **Two Ordered Factors**. {-}\nSuppose now that $X$ and $Y$ are both *ordered* variables. *Kendall's rank correlation coefficient* measures the strength and direction of association by counting the number of concordant pairs (where the ranks agree) versus discordant pairs (where the ranks disagree). A value of $1$ implies perfect agreement in rankings, a value of $-1$ indicates perfect disagreement, and a value of $0$ suggests no association in the ordering.\n$$\n\\hat{KT} = \\frac{2}{n(n-1)} \\sum_{i} \\sum_{j > i} \\text{sgn} \\Bigl( (\\hat{X}_{i} - \\hat{X}_{j})(\\hat{Y}_i - \\hat{Y}_j) \\Bigr),\n$$\nwhere the sign function is:\n$$\n\\text{sgn}(z) = \n\\begin{cases}\n+1 & \\text{if } z > 0\\\\\n0  & \\text{if } z = 0 \\\\\n-1 & \\text{if} z < 0 \n\\end{cases}.\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\nxy[,1] <- rank(xy[,1] )\nxy[,2] <- rank(xy[,2] )\n# plot(xy, pch=16, col=grey(0,.25))\nKT <- cor(xy[, 1], xy[, 2], method = \"kendall\")\nround(KT, 3)\n## [1] 0.074\n```\n:::\n\n\n\nKendall's rank correlation coefficient can also be used for non-linear relationships, where Pearson's correlation coefficient often falls short.\n\n::: {.cell}\n\n```{.r .cell-code}\n## https://commons.wikimedia.org/wiki/File:Correlation_examples2.svg\n\nMyPlot <- function(xy, xlim = c(-4, 4), ylim = c(-3,3)) {\n   plot(xy, main ='', xlab = \"\", ylab = \"\",\n        col = grey(0,.25), pch=16, cex=.5,\n        xaxt = \"n\", yaxt = \"n\", bty = \"n\",\n        xlim = xlim, ylim = ylim)\n    box(lwd=.1)\n    \n    cor1 <- cor(xy[,1], xy[,2])\n    cor2 <- codev(xy)\n    cor3 <- cor(xy[,1], xy[,2], method='kendall')\n    cors <- c(cor1, cor2, cor3)\n    \n    #cor3 <- abs(generalCorr::gmcmtx0( xy[,1:2])[2])\n    #cor4 <- Rfast::dcor( xy[,1], xy[,2])[[4]]\n    #cor5 <- XICOR::xicor( xy[,1], xy[,2]) ## cor( xy[,1], xy[,2], method='kendall') \n\n    fm <- cors*0 + 1\n    fm[which.max(abs(cors))] <- 2\n    title(paste0('Pearson:  ',  formatC(cor1, digits=2, format='f')),\n        line=3, adj=0, font.main=fm[1])\n    title(paste0('Falk:     ',  formatC(cor2, digits=2, format='f')),\n        line=2, adj=0, font.main=fm[2])    \n    title(paste0('Kendall:  ',  formatC(cor3, digits=2, format='f')),\n        line=1, adj=0, font.main=fm[3])\n}\n\nMvNormal <- function(n = 1000, cor, f) {\n   for (i in cor) {\n      sd = matrix(c(1, i, i, 1), ncol = 2)\n      x = mvtnorm::rmvnorm(n, c(0, 0), sd)\n      x[,2] <- f(x[,2])\n      MyPlot(x)\n   }\n}\n\noutput <- function() {\n   par(mfrow = c(3, 2), oma = c(0,0,0,0), mar=c(3,2,6,1))\n   cor <- c(0.99, 0.9)\n   MvNormal(800, cor, function(y){y});\n   MvNormal(800, cor, function(y){(y/2)^3});\n   MvNormal(800, cor, function(y){log( (y-min(y)+.Machine$double.eps)/(max(y)-min(y)) ) });\n}\noutput()\n```\n\n::: {.cell-output-display}\n![](02_02_BivariateStatistics_files/figure-html/unnamed-chunk-5-1.png){width=768}\n:::\n:::\n\n\n#### **Two Unordered Factors**. {-}\nSuppose $X$ and $Y$ are both *categorical* variables; the value of $X$ is one of $1...K$ categories and the value of $Y$ is one of $1...J$ categories. *Cramer's V* quantifies the strength of association by adjusting a \"chi-squared\" statistic to provide a measure that ranges from $0$ to $1$; $0$ indicates no association while a value closer to $1$ signifies a strong association. \n\nFirst, consider a contingency table for $X$ and $Y$ with $I$ rows and $J$ columns. The chi-square statistic is then defined as:\n\n$$\n\\hat{\\chi}^2 = \\sum_{k=1}^{K} \\sum_{j=1}^{J} \\frac{(\\hat{O}_{kj} - \\hat{E}_{kj})^2}{\\hat{E}_{kj}}.\n$$\n\nwhere\n\n- $\\hat{O}_{kj}$ denote the observed frequency in cell $(k, j)$,\n- $\\hat{E}_{kj} = \\hat{RF}_{k} \\cdot \\hat{CF}_j / n$ is the expected frequency for each cell if $\\hat{X}$ and $\\hat{Y}$ are independent\n- $\\hat{RF}_{k}$ denotes the total frequency for row $k$ (i.e., $\\hat{RF}_i = \\sum_{j=1}^{J} \\hat{O}_{kj}$),\n- $\\hat{CF}_{j}$ denotes the total frequency for column $j$ (i.e., $\\hat{CF}_{j} = \\sum_{k=1}^{K} \\hat{O}_{kj}$),\n\n\nSecond, normalize the chi-square statistic with the sample size and the degrees of freedom to compute Cramer's V. Recalling that $I$ is the number of categories for $X$, and $J$ is the number of categories for $Y$, the statistic is\n$$\n\\hat{CV} = \\sqrt{\\frac{\\hat{\\chi}^2 / n}{\\min(J - 1, \\, K - 1)}},\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\nxy[,1] <- cut(xy[,1],3)\nxy[,2] <- cut(xy[,2],4)\ntable(xy)\n##               UrbanPop\n## Murder         (31.9,46.8] (46.8,61.5] (61.5,76.2] (76.2,91.1]\n##   (0.783,6.33]           4           5           8           5\n##   (6.33,11.9]            0           4           7           6\n##   (11.9,17.4]            2           4           2           3\n\nCV <- function(xy){\n    # Create a contingency table from the categorical variables\n    tbl <- table(xy)\n    # Compute the chi-square statistic (without Yates' continuity correction)\n    chi2 <- chisq.test(tbl, correct=FALSE)[['statistic']]\n    # Total sample size\n    n <- sum(tbl)\n    # Compute the minimum degrees of freedom (min(rows-1, columns-1))\n    df_min <- min(nrow(tbl) - 1, ncol(tbl) - 1)\n    # Calculate Cramer's V\n    V <- sqrt((chi2 / n) / df_min)\n    return(V)\n}\nCV(xy)\n## X-squared \n## 0.2307071\n\n# DescTools::CramerV( table(xy) )\n```\n:::\n\n\n\n\n## Mixed Data\n\nFor mixed data, $\\hat{Y}_{i}$ is a cardinal variable and $\\hat{X}_{i}$ is a factor variable (typically unordered). For such data, we analyze associations via group comparisons. The basic idea is seen in a comparison of two samples, which corresponds to an $\\hat{X}_{i}$ with two categories.\n\nSuppose we have two samples of data. For example, the heights of men and women in Canada. For another example, homicide rates in two different American states.  For another example, the wages for people with and without completing a degree.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wooldridge)\nx1 <- wage1[wage1$educ == 15, 'wage']\nx2 <- wage1[wage1$educ == 16, 'wage']\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample 1 (e.g., males)\nn1 <- 100\nx1 <- rnorm(n1, 0, 2)\n# Sample 2 (e.g., females)\nn2 <- 80\nx2 <- rnorm(n2, 1, 1)\n\npar(mfrow=c(1,2))\nbks <- seq(-7,7, by=.5)\nhist(x1, border=NA, breaks=bks,\n    main='Sample 1', font.main=1)\n\nhist(x2, border=NA, breaks=bks, \n    main='Sample 2', font.main=1)\n```\n\n::: {.cell-output-display}\n![](02_02_BivariateStatistics_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\nThere may be several differences between these samples. Often, the first summary statistic we investigate is the difference in means. \n\n#### **Mean Differences**. {-}\n\nWe often want to know if the means of different sample are different in . To test this hypothesis, we compute the means separately for each sample and then examine the differences term\n\\begin{eqnarray} \n\\hat{D} = \\hat{M}_{X1} - \\hat{M}_{X2},\n\\end{eqnarray}\nwith a null hypothesis of $D=0$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Differences between means\nm1 <- mean(x1)\nm2 <- mean(x2)\nd <- m1-m2\n    \n# Bootstrap Distribution\nbootstrap_diff <- vector(length=9999)\nfor(b in seq(bootstrap_diff) ){\n    x1_b <- sample(x1, replace=T)\n    x2_b <- sample(x2, replace=T)\n    m1_b <- mean(x1_b)\n    m2_b <- mean(x2_b)\n    d_b <- m1_b - m2_b\n    bootstrap_diff[b] <- d_b\n}\nhist(bootstrap_diff,\n    border=NA, font.main=1,\n    main='Difference in Means')\n\n# 2-Sided Test\nboot_ci <- quantile(bootstrap_diff, probs=c(.025, .975))\nabline(v=boot_ci, lwd=2)\nabline(v=0, lwd=2, col=2)\n```\n\n::: {.cell-output-display}\n![](02_02_BivariateStatistics_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# p-value\n1 - ecdf(bootstrap_diff)(0)\n## [1] 0\n```\n:::\n\n\n\nJust as with one sample tests, we can compute a *standardized differences*, where $D$ is converted into a $t$ statistic.  Note, however, that we have to compute the standard error for the difference statistic, which is a bit more complicated. However, this allows us to easily conduct one or two sided hypothesis tests using a standard normal approximation.\n\n::: {.cell}\n\n```{.r .cell-code}\nse_hat <- sqrt(var(x1)/n1 + var(x2)/n2);\nt_obs <- d/se_hat\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n#### **Other Differences**. {-}\nThe above procedure generalized from differences in means to other statistics like quantiles or standard deviations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap Distribution Function\nboot_fun <- function( fun, B=9999, ...){\n    bootstrap_diff <- vector(length=B)\n    for(b in seq(bootstrap_diff)){\n        x1_b <- sample(x1, replace=T)\n        x2_b <- sample(x2, replace=T)\n        f1_b <- fun(x1_b, ...)\n        f2_b <- fun(x2_b, ...)\n        d_b <- f1_b - f2_b\n        bootstrap_diff[b] <- d_b\n    }\n    return(bootstrap_diff)\n}\n\n# 2-Sided Test for Median Differences\n# d <- median(x2) - median(x1)\nboot_d <- boot_fun(median)\nhist(boot_d, border=NA, font.main=1,\n    main='Difference in Medians')\nabline(v=quantile(boot_d, probs=c(.025, .975)), lwd=2)\nabline(v=0, lwd=2, col=2)\n```\n\n::: {.cell-output-display}\n![](02_02_BivariateStatistics_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\n1 - ecdf(boot_d)(0)\n## [1] 0.00070007\n```\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 2-Sided Test for SD Differences\n#d <- sd(x2) - sd(x1)\nboot_d <- boot_fun(sd)\nhist(boot_d, border=NA, font.main=1,\n    main='Difference in Standard Deviations')\nabline(v=quantile(boot_d, probs=c(.025, .975)), lwd=2)\nabline(v=0, lwd=2, col=2)\n1 - ecdf(boot_d)(0)\n\n\n# Try any function!\n# boot_fun( function(xs) { IQR(xs)/median(xs) } )\n```\n:::\n\n\nNote that these estimates suffer from a finite-sample bias, which we can correct for. Also note that bootstrap tests can perform poorly with highly unequal variances or skewed data.\n:::\n\n#### **Distributional Comparisons**. {-}\nWe can also examine whether there are any differences between the entire *distributions*\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample Wage Data\nlibrary(wooldridge)\nx1 <- sort( wage1[wage1$educ == 15,  'wage'])  \nx2 <- sort( wage1[wage1$educ == 16,  'wage'] )\nx <- sort(c(x1, x2))\n\n# Compute Quantiles\nquants <- seq(0,1,length.out=101)\nQ1 <- quantile(x1, probs=quants)\nQ2 <- quantile(x2, probs=quants)\n\n# Compare Distributions via Quantiles\nrx <- range(c(x1, x2))\npar(mfrow=c(1,2))\nplot(rx, c(0,1), type='n', font.main=1,\n    main='Distributional Comparison',\n    xlab=expression(Q[s]),\n    ylab=expression(F[s]))\nlines(Q1, quants, col=2)\nlines(Q2, quants, col=4)\nlegend('bottomright', col=c(2,4), lty=1,\nlegend=c('F1', 'F2'))\n\n# Compare Quantiles\nplot(Q1, Q2, xlim=rx, ylim=rx,\n    main='Quantile-Quantile Plot', font.main=1,\npch=16, col=grey(0,.25))\nabline(a=0,b=1,lty=2)\n```\n\n::: {.cell-output-display}\n![](02_02_BivariateStatistics_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThe starting point for hypothesis testing is the Kolmogorov-Smirnov Statistic: the maximum absolute difference between two CDF's over all sample data $x \\in \\{X_1\\} \\cup \\{X_2\\}$.\n\\begin{eqnarray}\n\\hat{KS} &=& \\max_{x} |\\hat{F}_{1}(x)- \\hat{F}_{2}(x)|^{p},\n\\end{eqnarray}\nwhere $p$ is an integer (typically 1). An intuitive alternative is the Cramer-von Mises Statistic: the sum of absolute differences (raised to an integer, typically 2) between two CDF's. \n\\begin{eqnarray}\n\\hat{CVM} &=& \\sum_{x} | \\hat{F}_{1}(x)- \\hat{F}_{2}(x)|^{p}.\n\\end{eqnarray}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Distributions\nF1 <- ecdf(x1)(x)\nF2 <- ecdf(x2)(x)\n\nlibrary(twosamples)\n\n# Kolmogorov-Smirnov\nKSq <- which.max(abs(F2 - F1))\nKSqv <- round(twosamples::ks_stat(x1, x2),2)\n\n# Cramer-von Mises Statistic (p=2)\nCVMqv <- round(twosamples::cvm_stat(x1, x2, power=2), 2) \n\n# Visualize Differences\nplot(range(x), c(0,1), type=\"n\", xlab='x', ylab='ECDF')\nlines(x, F1, col=2, lwd=2)\nlines(x, F2, col=4, lwd=2)\n# CVM\nsegments(x, F1, x, F2, lwd=.5, col=grey(0,.2))\n# KS\nsegments(x[KSq], F1[KSq], x[KSq], F2[KSq], lwd=1.5, col=grey(0,.75), lty=2)\n```\n\n::: {.cell-output-display}\n![](02_02_BivariateStatistics_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nJust as before, you use bootstrapping for hypothesis testing.\n\n::: {.cell}\n\n```{.r .cell-code}\ntwosamples::cvm_test(x1, x2)\n## Test Stat   P-Value \n##  2.084253  0.078000\n```\n:::\n\n\n#### **Comparing Multiple Groups**. {-}\nFor multiple groups, we can tests the equality of all distributions (whether at least one group is different). The *Kruskal-Wallis* test examines $H_0:\\; F_1 = F_2 = \\dots = F_G$ versus $H_A:\\; \\text{at least one } F_g \\text{ differs}$, where $F_g$ is the continuous distribution of group $g=1,...G$. This test does not tell us which group is different.\n\nTo conduct the test, first denote individuals $i=1,...n$ with overall ranks $\\hat{r}_1,....\\hat{r}_{n}$. Each individual belongs to group $g=1,...G$, and each group $g$ has $n_{g}$ individuals with average rank $\\bar{r}_{g} = \\sum_{i} \\hat{r}_{i} /n_{g}$. The Kruskal Wallis statistic is \n\\begin{eqnarray}\n\\hat{KW} &=& (N-1) \\frac{\\sum_{g=1}^{G} n_{g}( \\bar{r}_{g} - \\bar{r}  )^2  }{\\sum_{i=1}^{N} ( \\hat{r}_{i} - \\bar{r}  )^2}, \n\\end{eqnarray}\nwhere  $\\bar{r} = \\frac{n+1}{2}$ is the grand mean rank.\n\nIn the special case with only two groups, $G=2$, the Kruskal Wallis test reduces to the *Mann–Whitney U-test* (also known as the \\textit{Wilcoxon rank-sum test}). In this case, we can write the hypotheses in terms of individual outcomes in each group, $Y_i$ in one group $Y_j$ in the other; $H_0: Prob(Y_i > Y_j)=Prob(Y_i > Y_i)$  versus $H_A: Prob(Y_i > Y_j) \\neq Prob(Y_i > Y_j)$. The corresponding test statistic is\n\\begin{eqnarray}\n\\hat{U}   &=& \\min(\\hat{U}_1, \\hat{U}_2) \\\\\n\\hat{U}_g &=& \\sum_{i\\in g}\\sum_{j\\in -g}\n           \\Bigl[\\mathbf 1( \\hat{Y}_{i} > \\hat{Y}_{j}) + \\tfrac12\\mathbf 1(\\hat{Y}_{i} = \\hat{Y}_{j})\\Bigr].\n\\end{eqnarray}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\ndata(CASchools)\nCASchools$stratio <- CASchools$students/CASchools$teachers\n\n# Do student/teacher ratio differ for at least 1 county?\n# Single test of multiple distributions\nkruskal.test(CASchools$stratio, CASchools$county)\n## \n## \tKruskal-Wallis rank sum test\n## \n## data:  CASchools$stratio and CASchools$county\n## Kruskal-Wallis chi-squared = 161.18, df = 44, p-value = 2.831e-15\n\n# Multiple pairwise tests\n# pairwise.wilcox.test(CASchools$stratio, CASchools$county)\n```\n:::\n\n\n\n\n\n## Further Reading \n\nOther Statistics \n\n* <https://cran.r-project.org/web/packages/qualvar/vignettes/wilcox1973.html>\n\n\n",
    "supporting": [
      "02_02_BivariateStatistics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}