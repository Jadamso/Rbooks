{
  "hash": "7d82f5b652f58df85db7b4c4767fd6a8",
  "result": {
    "engine": "knitr",
    "markdown": "# Numerical Statistics\n***\n\nWe often summarize distributions with *statistics*: functions of data. We will go through some specific examples mathematically below, but can intuitively understand that they are generally computed as `statistic <- function(x){ .... }`.\n\n:::{.callout-tip icon=false collapse=\"true\"}\nNote that functions can take functions as arguments, meaning we can also program statistics generally as \n\n::: {.cell}\n\n```{.r .cell-code}\nstatistic <- function(X, f){\n    y <- f(X)\n    return(y)\n}\n\nX <- c(0,1,3,10,6) # Data\nstatistic(X, sum)\n## [1] 20\n```\n:::\n\n:::\n\n\nThe most basic way to compute statistics is with `summary`, which reports multiple values that can all be calculated individually. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A random sample (real data)\nX <- USArrests[,'Murder']\nsummary(X)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   0.800   4.075   7.250   7.788  11.250  17.400\n\n# A random sample (computer simulation)\nX <-  runif(1000)\nsummary(X)\n##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n## 0.001442 0.250637 0.509871 0.501544 0.742488 0.999849\n\n# Another random sample (computer simulation)\nX <- rnorm(1000) \nsummary(X)\n##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n## -3.39150 -0.69232  0.01046 -0.01267  0.63230  3.09302\n```\n:::\n\n\nTogether, the sample mean and variance statistics summarize the central tendency and dispersion of a distribution. In some special cases, such as with the normal distribution, they completely describe the distribution. Other distributions are better described with other statistics, either as an alternative or in addition to the mean and variance. After discussing those other statistics, we will return to the two most basic statistics in theoretical detail.\n\n## Mean and Variance\n\nThe mean and variance are the two most basic statistics that summarize the center and how spread apart the values are for data in your sample. As before, we represent data as vector $\\hat{X}=(\\hat{X}_{1}, \\hat{X}_{2}, ....\\hat{X}_{n})$, where there are $n$ observations and $\\hat{X}_{i}$ is the value of the $i$th one.\n\n\n#### **Mean**. {-}\nPerhaps the most common statistic is the empirical mean, also known as the sample mean, which is the [sum of all values] divided by [number of values];\n\\begin{eqnarray}\n\\hat{M} &=& \\frac{\\sum_{i=1}^{n}\\hat{X}_{i}}{n},\n\\end{eqnarray}\nwhere $\\hat{X}_{i}$ denotes the value of the $i$th observation.\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, a dataset of $\\{1,4,10\\}$ has a mean of $[1+4+10]/3=5$.\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- c(1,4,10)\nsum(X)/length(X)\n## [1] 5\nmean(X)\n## [1] 5\n```\n:::\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute the mean of a random sample\nX1 <- USArrests[,'Murder']\nX1_mean <- mean(X1)\nX1_mean\n## [1] 7.788\n\n# visualize on a histogram\nhist(X1, border=NA, main=NA, freq=F,)\nabline(v=X1_mean, col=2, lwd=2)\ntitle(paste0('mean= ', round(X1_mean,2)), font.main=1)\n```\n\n::: {.cell-output-display}\n![](01_03_DescriptiveStatistics_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n#### **Variance**.{-}\nPerhaps the second most common statistic is the empirical variance: the average squared deviation from the mean\n\\begin{eqnarray}\n\\hat{V} &=&\\frac{\\sum_{i=1}^{n} [\\hat{X}_{i} - \\hat{M}]^2}{n}.\n\\end{eqnarray}\nThe empirical standard deviation is simply $\\hat{S} = \\sqrt{\\hat{V} }$.\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, a dataset of $\\{1,4,10\\}$ has a mean of $[1+4+10]/3=5$. The variance is $[(1-5)^2+(4-5)^2+(10-5)^2]/3=[16+1+25]/3=42/3=14$ and the standard deviation is $\\sqrt{14}$.\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- c(1,4,10)\nX_mean <- mean(X)\nX_var <- mean( (X - X_mean)^2 )\nsqrt(X_var)\n## [1] 3.741657\n```\n:::\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX1_s <- sd(X1) # sqrt(var(X))\nhist(X1, border=NA, main=NA, freq=F)\nX1_s_lh <- c(X1_mean - X1_s,  X1_mean + X1_s)\nabline(v=X1_s_lh, col=4)\ntext(X1_s_lh, -.02,\n    c( expression(bar(X)-s[X]), expression(bar(X)+s[X])),\n    col=4, adj=0)\ntitle(paste0('sd= ', round(X1_s,2)), font.main=1)\n```\n\n::: {.cell-output-display}\n![](01_03_DescriptiveStatistics_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nNote that a \"unbiased version\" of the empirical variance is used by R and many statisticians: $\\hat{V}' =\\frac{\\sum_{i=1}^{n} [\\hat{X}_{i} - \\hat{M}]^2}{n-1}$ and $\\hat{S}' = \\sqrt{\\hat{V}'}$. In this class, we use the version defined previously because we do not yet know about \"bias/unbiased\" statistics. Do not be concerned, as there is hardly any difference when $n$ is large: e.g., $\\frac{1}{n}\\approx \\frac{1}{n-1}$ for $n=100,000$.\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- c(1,4,10)\n\nX_mean <- mean(X)\nX_var <- mean( (X - X_mean)^2 )\nX_var\n## [1] 14\n\n# Corrected Version\nn <- length(X)  \nX_var2 <- sum( (X - X_mean)^2 )/(n-1)\nX_var2\n## [1] 21\n\nvar(X) # R-Version\n## [1] 21\n```\n:::\n\n:::\n\n## Other Center/Spread Statistics\n\nA general rule of applied statistics is that there are multiple ways to measure something. Mean and Variance are measurements of Center and Spread, but there are others that have different theoretical properties and may be better suited for your dataset.\n\n#### **Medians and Absolute Deviations**. {-}\nWe can use the empirical *Median* as a \"robust alternative\" to means that is especially useful for data with asymmetric distributions and extreme values. Recall that the $q$th quantile is the value where $q$ percent of the data are below and ($1-q$) percent are above. The median ($q=.5$) is the point where half of the data is lower values and the other half is higher. This implies that median is not sensitive to extreme values (whereas the mean is).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX1 <- USArrests[,'Murder']\nmedian(X1)\n## [1] 7.25\nquantile(X1, prob=0.5)\n##  50% \n## 7.25\n```\n:::\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nExamine robustness to an extreme value\n\n::: {.cell}\n\n```{.r .cell-code}\nX1_extreme <- c(X1, 1000) # add one extreme value\n#par(mfrow=c(1,2)) # visualize side-by-side\n#hist(X1)\n#hist(X1_extreme)\n\n# Which measures of central tendency are robust\n# to a single extreme value?\nmean( X1)\n## [1] 7.788\nmean( X1_extreme )\n## [1] 27.24314\n\nquantile(X1, prob=0.5)\n##  50% \n## 7.25\nquantile(X1_extreme, prob=0.5)\n## 50% \n## 7.3\n```\n:::\n\n:::\n\n\n\nWe can also use the sample *Interquartile Range* or *Median Absolute Deviation* as an alternative to variance. The difference between the first and third quartiles (quantiles $q=.25$ and $q=.75$) measure is range of the middle $50%$ of the data, which is how the boxplot measures \"spread\". The median absolute deviation is another statistic that also measures \"spread\".\n\\begin{eqnarray}\n\\tilde{M} &=& \\text{Med}( \\hat{X}_{i}) \\\\\n\\hat{\\text{MAD}} &=& \\text{Med}\\left( | \\hat{X}_{i} - \\tilde{M} | \\right).\n\\end{eqnarray}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nIQR(X1)\n## [1] 7.175\nmad(X1)\n## [1] 5.41149\n```\n:::\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nCompute the $IQR$ statistic for the dataset $\\{-100,1,4,10,10\\}$. \n\n$IQR =$ Upper quartile $-$ Lower quartile $= 10 - 1 = 9$. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- c(1,4,10)\n# An intuitive alternative to sd(X), used in the boxplot\nquants <- quantile(X, probs=c(.25,.75))\nquants[2]-quants[1]\n## 75% \n## 4.5\nIQR(X)\n## [1] 4.5\n```\n:::\n\nCompute the $\\text{MAD}$ statistic for the dataset $\\{1,4,10\\}$. First compute the median, $\\text{Med}(1,4,10)=4$. Then compute $\\text{Med}( |1-4|,~ |4-4|,~ |10-4| )= \\text{Med}( 3, 0, 6 ) = 3$.\n\n::: {.cell}\n\n```{.r .cell-code}\n#Another alternative to sd(X)\nmad(X, constant=1)\n## [1] 3\n\n# Computationally equivalent\n# x_med <- quantile(X, probs=.5)\n# x_absdev <- abs(X -x_med)\n# quantile(x_absdev, probs=.5)\n```\n:::\n\n:::\n\n:::{.callout-note icon=false collapse=\"true\"}\nCompare the robustness of various \"spread\" metrics to an extreme value\n\n::: {.cell}\n\n```{.r .cell-code}\nsd(X1)\n## [1] 4.35551\nsd(X1_extreme)\n## [1] 139.0044\n\nIQR(X1)\n## [1] 7.175\nIQR(X1_extreme)\n## [1] 7.2\n\nmad(X1, constant=1)\n## [1] 3.65\nmad(X1_extreme, constant=1)\n## [1] 3.8\n```\n:::\n\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nNote that there other \"absolute deviation\" statistics\n\n::: {.cell}\n\n```{.r .cell-code}\n# sometimes seen elsewhere\nmean( abs(X1 - mean(X1)) )\nmean( abs(X1 - median(X1)) )\n```\n:::\n\n:::\n\n\n#### **Weighted Statistics**. {-}\n\nThe mean generalizes to a *weighted mean*: an average where different values contribute to the final result with varying levels of importance. For each outcome $x$ we have a weight $W_{x}$ and compute\n\\begin{eqnarray}\n\\hat{M} &=& \\frac{\\sum_{x} x W_{x}}{\\sum_{x} W_{x}} = \\sum_{x} x w_{x},\n\\end{eqnarray}\nwhere $w_{x}=\\frac{W_{x}}{\\sum_{x'} W_{x'}}$ is normalized version of $W_{x}$ that implies $\\sum_{x}w_{x}=1$.\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor another example, suppose a student has these scores\n\n::: {.cell}\n\n```{.r .cell-code}\nHomework1 <- c(score=88, weight=0.25)\nHomework2 <- c(score=92, weight=0.25)\nExam1 <- c(score=67, weight=0.2)\nExam2 <- c(score=90, weight=0.3)\n\nGrades <- rbind(Homework1, Homework2, Exam1, Exam2)\nGrades\n##           score weight\n## Homework1    88   0.25\n## Homework2    92   0.25\n## Exam1        67   0.20\n## Exam2        90   0.30\n```\n:::\n\n\nWe can compute the final grade as a weighted mean\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manual Way\n88*0.25 + 92*0.25 + 67*0.2 + 90*0.3\n## [1] 85.4\n\n# Computerized Way\nValues <- Grades[,'score'] * Grades[,'weight']\nFinalGrade <- sum(Values)\nFinalGrade\n## [1] 85.4\n```\n:::\n\n:::\n\n\nWhen data are discrete, we can also compute the mean using \"probability weights\" $w_{x} = \\hat{p}_{x}=\\sum_{i=1}^{n}\\mathbf{1}\\left(\\hat{X}_{i}=x\\right)/n$.\n\n:::{.callout-note icon=false collapse=\"true\"}\nE.g., the dataset $\\{1,2,1,3,1\\}$ has $\\hat{p}_{1}=\\frac{3}{5}$, $\\hat{p}_{2}=\\frac{1}{5}$, $\\hat{p}_{3}=\\frac{1}{5}$.\nThen, sorting the dataset as \\{1,1,1,2,3\\}, we can see that $\\hat{M} = [1+1+1+2+2+3]/5 =  [1+1+1]/5 + [2+2]/5+ 3/5 = 1 \\frac{3}{5} + 2 \\frac{3}{5} + 3 \\frac{1}{5} = 1 \\hat{p}_{1} + 2 \\hat{p}_{2} + 3 \\hat{p}_{3}$. In either case, we end up with $\\frac{3+6+3}{5}=12/5=1.6$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- c(1,2,1,3,1)\nmean(X)\n## [1] 1.6\n\nproportions <- table(X)/length(X)\nproportions\n## X\n##   1   2   3 \n## 0.6 0.2 0.2\nvals <- sort(unique(X))\nsum(vals*proportions)\n## [1] 1.6\n```\n:::\n\n:::\n\n\nIn principle, we can also compute other weighted statistics such as a weighted median.\n\n:::{.callout-tip icon=false collapse=\"false\"}\nSee that we can also compute weighted quantiles\n\n::: {.cell}\n\n```{.r .cell-code}\nweighted.quantile <- function(x, w, probs){\n    #See spatstat.univar::weighted.quantile\n    oo <- order(x)\n    x <- x[oo]\n    w <- w[oo]\n    Fx <- cumsum(w)/sum(w)\n    quantile_id <- max(which(Fx <= probs))+1\n    xq <- x[quantile_id] \n    return(xq)\n}\n\n## Unweighted\nquantile(X, probs=.5)\n## 50% \n##   1\nweights <- rep(1, length(X))\nweighted.quantile(x=X, w=weights, probs=.5)\n## [1] 1\n\n## Weighted\nweights <- seq(X)\nweights <- weights/sum(weights) # normalize\nweighted.quantile(x=X, w=weights, probs=.5)\n## [1] 1\n```\n:::\n\n:::\n\n#### **Mode and Share Concentration**. {-}\nSometimes, none of the above work well. With categorical data, for example, distributions are easier to describe with other statistics. The sample *mode* is the most common observation: the value with the highest observed frequency. We can also measure the spread of the frequencies or concentration at the mode vs elsewhere.\n\n:::{.callout-tip icon=false collapse=\"true\"}\nHere is an example of the mode. Notice it is not the middle letter.\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- c('A', 'B', 'A', 'C', 'C', 'A')\nproportions <- table(X)/length(X)\nplot(proportions, col=grey(0,0.5))\n```\n\n::: {.cell-output-display}\n![](01_03_DescriptiveStatistics_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# mode(s)\nmode_id <- which(proportions==max(proportions))\nnames(proportions)[ mode_id ]\n## [1] \"A\"\n```\n:::\n\n\nHere is an example of spread for categorical data.\n\n::: {.cell}\n\n```{.r .cell-code}\n# freq. spreads\nsd(proportions)\n## [1] 0.1666667\nsum(proportions^2)\n## [1] 0.3888889\n\n# freq. concentration at mode\nmax(proportions)/mean(proportions)\n## [1] 1.5\n```\n:::\n\n:::\n\n## Shape Statistics\n\nCentral tendency and dispersion are often insufficient to describe a distribution. To further describe shape, we can compute sample skew and kurtosis to measure asymmetry and extreme values. \n\nThere are many other statistics we could compute on an ad-hoc basis. However, shape is often best understood with graphical descriptions: histogram, ECDF, Boxplot. These should be made before numerical descriptions: skewness and kurtosis statistics.\n\n#### **Skewness**. {-}\nThe skewness statistic captures how asymmetric the distribution is by measuring the average cubed deviation from the mean, normalized the standard deviation cubed\n\\begin{eqnarray}\n\\frac{\\sum_{i=1}^{n} [\\hat{X}_{i} - \\hat{M}]^3 / n}{ \\hat{S}^3 }\n\\end{eqnarray}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- USArrests[,'Murder']\nhist( X^2, border=NA, \n\txlab='[Murder Rate]^2',\n\tmain=NA, freq=F, breaks=20)\n```\n\n::: {.cell-output-display}\n![](01_03_DescriptiveStatistics_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\nskewness <-  function(X){\n X_mean <- mean(X)\n m3 <- mean((X - X_mean)^3)\n s3 <- sd(X)^3\n skew <- m3/s3\n return(skew)\n}\n\nskewness( X^2 )\n## [1] 1.078254\nskewness( X^3 )\n## [1] 1.658012\n```\n:::\n\n\nWe can automatically compare against the normal distribution, which has a skew of 0.\n\n#### **Kurtosis**. {-}\nThis statistic captures how many \"outliers\" there are like skew but looking at quartic deviations instead of cubed ones.\n\\begin{eqnarray}\n\\frac{\\sum_{i=1}^{n} [\\hat{X}_{i} - \\hat{M}]^4 / n}{ \\hat{S}^4 }.\n\\end{eqnarray}\nSome authors further subtract $3$ to explicitly compare against the normal distribution (the normal distribution has a kurtosis of $3$).\n\nBoxplot whiskers are a great way to examine kurtosis, with more circle ``outliers'' indicating more kurtosis. You can also see skew in the boxplot when one quartile is much further from the median than the other. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nX1 <- X^1/mean(X^1)\nX2 <- X^2/mean(X^2)\nX3 <- X^3/mean(X^3)\nX4 <- X^4/mean(X^4)\nboxplot(X1, X2, X3, X4,\n\tnames=c(1,2,3,4), xlab='Data Transformations',\n\tmain=NA)\n```\n\n::: {.cell-output-display}\n![](01_03_DescriptiveStatistics_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n\nkurtosis <- function(X){  \n X_mean <- mean(X)\n m4 <- mean((X - X_mean)^4) \n s4 <- sd(X)^4\n kurt <- m4/s4\n return(kurt)\n # use instead to compare against normal\n # excess_kurt <- kurt - 3 \n}\n\nkurtosis( X1 )\n## [1] 2.05077\nkurtosis( X2 )\n## [1] 3.247877\nkurtosis( X3 )\n## [1] 5.166225\nkurtosis( X4 )\n## [1] 7.511867\n```\n:::\n\n\n\n#### **Clusters/Gaps**. {-}\nYou can also describe distributions in terms of how clustered the values are, including the number of modes, bunching, and many other statistics. But remember that \"a picture is worth a thousand words\".\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_03_DescriptiveStatistics_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_03_DescriptiveStatistics_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n\n\n## Data Transformations\n\nTransformations can stabilize variance, reduce skewness, and make model errors closer to Gaussian.\n\nPerhaps the most common examples are *power transformations*: $y= x^\\lambda$, which includes $\\sqrt{x}$ and $x^2$.\n\nOther examples include the *exponential transformation*: $y=\\exp(x)$ for any $x\\in (-\\infty, \\infty)$ and *logarithmic transformation*: $y=\\log(x)$ for any $x>0$.\n\n:::{.callout-tip icon=false collapse=\"true\"}\nThe exponential function is $e^{x}=1+x/1+2^2/2+x^3/6....=\\sum_{k=0}^{\\infty} x^k/k!$ and the $log$ function is it's inverse.\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 1:4\n\ny <- exp(x)\ny\n## [1]  2.718282  7.389056 20.085537 54.598150\n\nz <- log(y)\nz\n## [1] 1 2 3 4\n```\n:::\n\n:::\n\n\nThe *Box–Cox Transform* nests many cases used by statisticians. For $x>0$ and parameter $\\lambda$,\n\\begin{eqnarray}\ny=\\begin{cases}\n\\dfrac{x^\\lambda-1}{\\lambda}, & \\lambda\\neq 0,\\\\\n\\log\\left(x\\right) & \\lambda=0.\n\\end{cases}\n\\end{eqnarray}\nThis function is continuous over $\\lambda$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Box–Cox transform and inverse\nbc_transform <- function(x, lambda) {\n  if (any(x <= 0)) stop(\"Box-Cox requires x > 0\")\n  if (abs(lambda) < 1e-8) log(x) else (x^lambda - 1)/lambda\n}\nbc_inverse <- function(t, lambda) {\n  if (abs(lambda) < 1e-8) exp(t) else (lambda*t + 1)^(1/lambda)\n}\n\nX <- USArrests[,'Murder']\nhist(X, main='', border=NA, freq=F)\n```\n\n::: {.cell-output-display}\n![](01_03_DescriptiveStatistics_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\npar(mfrow=c(1,3))\nfor(lambda in c(-1,0,1)){\n    Y <- bc_transform(X, lambda)\n    hist(Y, \n        main=bquote(paste(lambda,'=',.(lambda))),\n        border=NA, freq=F)\n}\n```\n\n::: {.cell-output-display}\n![](01_03_DescriptiveStatistics_files/figure-html/unnamed-chunk-26-2.png){width=672}\n:::\n:::\n\n\nBe careful about transforming your data, as the interpretation can be harder.\n\nThe mean of transformed data is not equivalent to transforming the mean, for example, which is one reason why you want to first summarize your data before transforming it. \n\n#### **Jensen’s inequality**. {-}\n\nWe can actually say more about how the mean of transformed data not equivalent to transforming the mean. To do that, we define two types of functions:\n\n* Concave functions curve inwards, like the inside of a cave.\n* Convex functions curve outward, the opposite of concave.\n\nLet $\\hat{Y}_{i}=g( \\hat{X}_{i})$, and denote the mean as $\\hat{M}_{Y}$.\n\nIf $g$ is a *concave* function, then $g( \\hat{M}_{X} ) \\geq \\hat{M}_{Y}$.\n\n:::{.callout-tip icon=false collapse=\"true\"}\nHere are some examples.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Continuous Example 1\nx <- c(0, 1.123, 2.987, 3.654)\nmean( sqrt(x) )\n## [1] 1.174889\nsqrt( mean(x) ) \n## [1] 1.393198\n\n# Continuous Example 2\nmean( log(x) )\n## [1] -Inf\nlog( mean(x) ) \n## [1] 0.6632033\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Discrete Example\nx  <- c(1,2,3)\npx <- c(0.2,0.5,0.3)\nMX <- sum(x * px)\nMX\n## [1] 2.1\n\ng  <- sqrt\ng(MX)\n## [1] 1.449138\n\nMY <- sum(g(x) * px)\nMY\n## [1] 1.426722\n```\n:::\n\n:::\n\nIf $g$ is a *convex* function, then the inequality reverses: $g( \\hat{M}_{X}) \\leq \\hat{M}_{Y}$.\n\n::: {.cell}\n\n```{.r .cell-code}\nmean( exp(x) )\n## [1] 10.06429\nexp( mean(x) )  \n## [1] 7.389056\n```\n:::\n\n\n",
    "supporting": [
      "01_03_DescriptiveStatistics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}