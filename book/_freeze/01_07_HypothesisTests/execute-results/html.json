{
  "hash": "92c313d1aa6f1bacb4a0058f1b0f8754",
  "result": {
    "engine": "knitr",
    "markdown": "# Hypothesis Tests\n***\n\n## Basic Ideas\n\nIn this section, we test hypotheses using *data-driven* methods that assume much less about the data generating process. There are two main ways to conduct a hypothesis test to do so: inverting a confidence interval and imposing the null. The first treats the distribution of estimates directly; the second explicitly enforces the null hypothesis to evaluate how unusual the observed statistic is. Both approaches rely on the bootstrap: resampling the data to approximate sampling variability. The most typical case is hypothesizing about about the mean, and the bootstrap idea here is to approximate $M-\\mu$, the difference between the sample mean $M$ and the unknown theoretical mean $\\mu$, with the difference between the bootstrap mean $M^{\\text{boot}}$ and the sample mean, $M^{\\text{boot}}-M$.\n\n#### **Invert a CI**.{-}\n\nOne main way to conduct hypothesis tests is to examine whether a confidence interval contains a hypothesized value. We then use this decision rule\n\n* reject the null if value falls outside of the interval\n* fail to reject the null if value falls inside of the interval\n\nWe typically use a 95% confidence interval to create a *rejection region*. \n\nE.g., suppose you hypothesize the mean is $9$. You then construct a bootstrap distribution with $95\\%$ confidence interval, and find your hypothesized value falls outside of the confidence interval. Then, after accounting for sampling variability (which you estimate), it still seems extremely unlikely that the theoretical mean actually equals $9$, so you reject that that hypothesis. (If the theoretical value landed in the interval, you would \"fail to reject\" the theoretical mean equals $9$.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests[,'Murder']\nsample_mean <- mean(sample_dat)\n\nset.seed(1) # to be replicable\nbootstrap_means <- vector(length=999)\nfor(b in seq_along(bootstrap_means)){\n    dat_b <- sample(sample_dat, replace=T) \n    mean_b <- mean(dat_b)\n    bootstrap_means[b] <- mean_b\n}\nhist(bootstrap_means, breaks=25,\n    border=NA,\n    main='',\n    xlab='Bootstrap Samples')\n# CI\nci_95 <- quantile(bootstrap_means, probs=c(.025, .975))\nabline(v=ci_95, lwd=2)\n# H0: mean=9\nabline(v=9, col=2, lwd=2)\n```\n\n::: {.cell-output-display}\n![](01_07_HypothesisTests_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n#### **Impose the Null**. {-}\nWe can also compute a *null distribution*: the sampling distribution of the statistic under the null hypothesis (assuming your null hypothesis was true). We use the bootstrap to loop through a large number of \"resamples\". In each iteration of the loop, we impose the null hypothesis and re-estimate the statistic of interest. We then calculate the range of the statistic across all resamples and compare how extreme the original value we observed is.\n\nE.g., suppose you hypothesize the mean is $9$. You then construct a 95% confidence interval around the *null* bootstrap distribution (resamples centered around $9$). If your sample mean falls outside of that interval, then even after accounting for sampling variability (which you estimate), it seems extremely unlikely that the theoretical mean actually equals $9$, so you reject that that hypothesis. (If the sample mean landed in the interval, you would \"fail to reject\" the theoretical mean equals $9$.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests[,'Murder']\nsample_mean <- mean(sample_dat)\n\n# Bootstrap NULL: mean=9\n# Bootstrap shift: center each bootstrap resample so that the distribution satisfies the null hypothesis on average.\nset.seed(1)\nmu <- 9\nbootstrap_means_null <- vector(length=999)\nfor(b in seq_along(bootstrap_means_null)){\n    dat_b <- sample(sample_dat, replace=T) \n    mean_b <- mean(dat_b) + (mu - sample_mean) # impose the null via Bootstrap shift\n    bootstrap_means_null[b] <- mean_b\n}\nhist(bootstrap_means_null, breaks=25, border=NA,\n    main='',\n    xlab='Null Bootstrap Samples')\nci_95 <- quantile(bootstrap_means_null, probs=c(.025, .975)) # critical region\nabline(v=ci_95, lwd=2)\nabline(v=sample_mean, lwd=2, col=4)\n```\n\n::: {.cell-output-display}\n![](01_07_HypothesisTests_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n## p-values\n\nA *p-value* is the frequency you see something as extreme as your statistic when sampling from the null distribution. There are three tests associated with p-values: the two-sided test (observed statistic is either extremely high or low) or one of the one-sided tests (observed statistic is extremely low, observed statistic is extremely high). \n\nFor a concrete example, consider whether the mean statistic, $M$, is centered on a theoretical value of $\\mu=9$ for the population. If your null hypothesis is that the theoretical mean is eight, $H_{0}: \\mu =9$, and you calculated the mean for your sample as $\\hat{M}$, then you can consider any one of these three alternative hypotheses:\n\n* $H_{A​}: \\mu > 9$, a right-tail test, $Prob( M > \\hat{M} \\mid \\mu = 9 )$.\n* $H_{A}: \\mu < 9$, a left-tail test, $Prob( M < \\hat{M} \\mid \\mu = 9 )$.\n* $H_{A}​: \\mu \\neq 9$, a two-tail test, depicted in the previous section.\n\n\nA one-sided test is straightforward to implement via a bootstrap null distribution. For a left-tail test, we examine \\begin{eqnarray}\nProb( M < \\hat{M} \\mid \\mu = 9 )\n    &\\approx& Prob( M^{\\text{boot}} < \\hat{M} \\mid  \\mu^{\\text{boot}} = 9 ) = \\hat{F}^{\\text{boot}}_{0}(\\hat{M}),\n\\end{eqnarray}\nwhere $\\hat{F}^{\\text{boot}}_{0}$ is the ECDF of the bootstrap null distribution. For a right-tail test, we examine $Prob( M > \\hat{M} \\mid \\mu = 9 ) \\approx 1-\\hat{F}^{\\text{boot}}_{0}(\\hat{M})$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# One-Sided Test, ALTERNATIVE: mean > 9\npar(mfrow=c(1,2))\n# Visualize One Sided Prob. & reject region boundary\nhist(bootstrap_means_null, border=NA,\n    freq=F, main=NA, xlab='Null Bootstrap')\nabline(v=sample_mean, col=4)\n# Equivalent Visualization\nFhat0 <- ecdf(bootstrap_means_null) # Look at right tail\nplot(Fhat0,\n    main='',\n    xlab='Null Bootstrap')\nabline(v=sample_mean, col=4)\n```\n\n::: {.cell-output-display}\n![](01_07_HypothesisTests_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Numerically Compute Two Sided Probability\np1 <- 1- Fhat0(sample_mean) #Compute right Tail\np1\n## [1] 0.986987\n```\n:::\n\n\nA two sided test is slightly more complicated to compute. We want the probability mass in both tails, for the random variable $M$ that is at least as far from the null mean of $9$ as our observed sample mean $\\hat{M}$.\n\\begin{eqnarray}\nProb( |M - \\mu| \\geq |\\hat{M} - \\mu| \\mid \\mu = 9 )\n&\\approx& Prob( |M^{\\text{boot}}- \\mu^{\\text{boot}}| \\geq |\\hat{M}- \\mu^{\\text{boot}}|  \\mid \\mu^{\\text{boot}} = 9) \\\\\n&=& 1-\\hat{F}^{|\\text{boot}|}_{0}(|\\hat{M}-9|),\n\\end{eqnarray}\nwhere $\\hat{F}^{|\\text{boot}|}_{0}$ is the ECDF of $|M^{\\text{boot}}- \\mu^{\\text{boot}}|$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Two-Sided Test, ALTERNATIVE: mean < 9 or mean >9\nmu <- 9\n# Visualize Two Sided Prob. & reject region boundary\npar(mfrow=c(1,2))\nhist(abs(bootstrap_means_null-mu),\n    freq=F, breaks=20,\n    border=NA, main='', xlab='Null Bootstrap')\nabline(v=abs(sample_mean-mu), col=4)\n\n# Equivalent Visualization\nFhat_abs0 <- ecdf( abs(bootstrap_means_null-mu) )\nplot(Fhat_abs0,\n    main='',\n    xlab='Null Bootstrap')\nabline(v=abs(sample_mean-mu), col=4)\n```\n\n::: {.cell-output-display}\n![](01_07_HypothesisTests_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n\n# Numerically Compute Two Sided Probability\np2 <- 1 - Fhat_abs0( abs(sample_mean-mu) )\np2\n## [1] 0.03303303\n```\n:::\n\n\n\n\n#### **Statistical significance**. {-}\n\nOften, one may see or hear \"p<.05: statistically significant\" and \"p>.05: not statistically significant\". That is decision making on purely statistical grounds, and it may or may not be suitable for your context. You simply need to know that whoever says those things is using $5\\%$ as a critical value to reject an alternative hypothesis. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Purely-Statistical Decision Making Examples.\n\n# One Sided Test\nif(p1 >.05){\n    print('fail to reject the null that sample_mean=9, at the 5% level')\n} else {\n    print('reject the null that sample_mean=9 in favor of >9, at the 5% level')\n}\n## [1] \"fail to reject the null that sample_mean=9, at the 5% level\"\n\n# Two Sided Test\nif(p2 >.05){\n    print('fail to reject the null that sample_mean=9, at the 5% level')\n} else {\n    print('reject the null that sample_mean=9 in favor of either <9 or >9, at the 5% level')\n}\n## [1] \"reject the null that sample_mean=9 in favor of either <9 or >9, at the 5% level\"\n```\n:::\n\n\nBeware that a common misreading of the p-value as \"the probability the null is true\". That is false.\n\n\n#### **Caveat**. {-}\n\nAlso note that the *p-value* is itself a function of data, and hence a random variable that changes from sample to sample. Given that the $5\\%$ level is somewhat arbitrary, and that the p-value both varies from sample to sample and is often misunderstood, it makes sense to give p-values a limited role in decision making. \n\n\n::: {.cell}\n\n```{.r .cell-code}\np_values <- vector(length=300)\nfor(b2 in seq(p_values)){\n    bootstrap_means_null <- vector(length=999)\n    for(b in seq_along(bootstrap_means_null)){\n        dat_b <- sample(sample_dat, replace=T) \n        mean_b <- mean(dat_b) + (mu - sample_mean) # impose the null\n        bootstrap_means_null[b] <- mean_b\n    }\n    Fhat_abs0 <- ecdf( abs(bootstrap_means_null-mu) )\n    p2 <- 1- Fhat_abs0( abs(sample_mean-mu) )\n    p_values[b2] <- p2\n}\n\nhist(p_values, freq=F,\n    border=NA, main='')\n```\n\n::: {.cell-output-display}\n![](01_07_HypothesisTests_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## Other Statistics\n\n#### **t-values**. {-}\nA *t-value* standardizes the approach for hypothesis tests of the mean. For any specific sample, we compute the estimate\n\\begin{eqnarray}\n\\hat{t}=(\\hat{M}-\\mu)/\\hat{S},\n\\end{eqnarray}\nwhich corresponds to the estimator $t = (M - \\mu) / \\mathbb{s}(M)$, which varies from sample to sample.\n\n::: {.cell}\n\n```{.r .cell-code}\n# t statistic estimate\njackknife_means <- vector(length=length(sample_dat))\nfor(i in seq_along(jackknife_means)){\n    jackknife_means[i] <- mean(dat_b[-i])\n}\nmu <- 9\nsample_t <- (sample_mean - mu)/sd(jackknife_means)\n```\n:::\n\n\nThere are several benefits to this:\n\n* uses the same statistic for different hypothesis tests\n* makes the statistic comparable across different studies\n* removes dependence on unknown parameters by normalizing with a standard error\n* makes the null distribution theoretically known asymptotically (approximately)\n\nFor the first point, notice that the recentering adjustment affects two-sided tests (because they depend on distance from the null mean) but not one-sided tests (because adding a constant does not change rank order).\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nbootstrap_means_null <- vector(length=999)\nfor(b in seq_along(bootstrap_means_null)){\n    dat_b <- sample(sample_dat, replace=T) \n    mean_b <- mean(dat_b) + (mu - sample_mean) # impose the null\n    bootstrap_means_null[b] <- mean_b\n}\n\n# See that the \"recentering\" matters for two-sided tests\necdf( abs(bootstrap_means_null-mu) )( abs(sample_mean-mu) )\n## [1] 0.966967\necdf( abs(bootstrap_means_null) )( abs(sample_mean) )\n## [1] 0.01301301\n\n# See that the \"recentering\" doesn't matter for one-sided ones\necdf( bootstrap_means_null-mu)( sample_mean-mu)\n## [1] 0.01301301\necdf( bootstrap_means_null )( sample_mean)\n## [1] 0.01301301\n```\n:::\n\n\nThe last point implies we are typically dealing with a normal distribution that is well-studied, or another well-studied distribution derived from it.^[In another statistics class, you will learn the math behind the null t-distribution. In this class, we skip this because we can simply bootstrap the t-statistic too.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Boostrap Null Distribution\nbootstrap_t_null <- vector(length=999)\nfor(b in seq_along(bootstrap_t_null)){\n    dat_b <- sample(sample_dat, replace=T) \n    mean_b <- mean(dat_b) + (mu - sample_mean) # impose the null by recentering\n    # Compute t-stat using jackknife ses (same as above)\n    jackknife_means_b <- vector(length=length(dat_b))\n    for(i in seq_along(jackknife_means_b)){\n        jackknife_means_b[i] <- mean(dat_b[-i])\n    }\n    jackknife_se_b <- sd( jackknife_means_b )\n    jackknife_t_b <- (mean_b - mu)/jackknife_se_b\n    bootstrap_t_null[b] <- jackknife_t_b\n}\n\n# Two Sided Test\nFhat0 <- ecdf(abs(bootstrap_t_null))\nplot(Fhat0, \n    xlim=range(bootstrap_t_null, sample_t),\n    xlab='Null Bootstrap Distribution for |t|',\n    main='')\nabline(v=abs(sample_t), col=4)\n```\n\n::: {.cell-output-display}\n![](01_07_HypothesisTests_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\np <- 1 - Fhat0( abs(sample_t) ) \np\n## [1] 0.04204204\n```\n:::\n\n\n#### **Quantiles and Shape Statistics**. {-}\n\nBootstrap allows hypothesis tests for any statistic, not just the mean, without relying on parametric theory. For example, the above procedures generalize from differences in *means* to statistics like *medians* and other quantiles.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Test for Median Differences (Impose the Null)\n# Bootstrap Null Distribution for the median\n# Each Bootstrap shifts medians so that median = q_null\n\nq_obs <- quantile(sample_dat, probs=.5)\nq_null <- 7.8\nbootstrap_quantile_null <- vector(length=999)\nfor(b in seq_along(bootstrap_quantile_null)){\n    x_b <- sample(sample_dat, replace=T) #bootstrap sample\n    q_b <- quantile(x_b, probs=.5) # median\n    d_b <- q_b - (q_obs-q_null) #impose the null\n    bootstrap_quantile_null[b] <- d_b \n}\n\n# 2-Sided Test for Median Difference\nhist(bootstrap_quantile_null-q_null, \n    border=NA, freq=F, xlab='Null Bootstrap',\n    font.main=1, main='Medians (Impose Null)')\nmedian_ci <- quantile(bootstrap_quantile_null-q_null, probs=c(.025, .975))\nabline(v=median_ci, lwd=2)\nabline(v=q_obs-q_null, lwd=2, col=4)\n```\n\n::: {.cell-output-display}\n![](01_07_HypothesisTests_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# 2-Sided Test for Median Difference\n## Null: No Median Difference\n1 - ecdf( abs(bootstrap_quantile_null-q_null))( abs(q_obs-q_null) ) \n## [1] 0.5485485\n```\n:::\n\n\nThe above procedure generalizes to differences in many other statistics. Perhaps the most informative are differences in shape. E.g., you can test for differences in *spread*, *skew*, or *kurtosis*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Test for SD Differences (Invert CI)\nsd_obs <- sd(sample_dat)\nsd_null <- 3.6\nbootstrap_sd <- vector(length=999)\nfor(b in seq_along(bootstrap_sd)){\n    x_b <- sample(sample_dat, replace=T)\n    sd_b <- sd(x_b)\n    bootstrap_sd[b] <- sd_b\n}\n\nhist(bootstrap_sd, freq=F,\n    border=NA, xlab='Bootstrap', font.main=1,\n    main='Standard Deviations (Invert CI)')\nsd_ci <- quantile(bootstrap_sd, probs=c(0.25,.975) )\nabline(v=sd_ci, lwd=2)\nabline(v=sd_null, lwd=2, col=2)\n```\n\n::: {.cell-output-display}\n![](01_07_HypothesisTests_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n\n# Try any function!\n# IQR(x_b)/median(x_b)\n```\n:::\n\n\n## Further Reading\n\n* <https://learningstatisticswithr.com/book/hypothesistesting.html>\n* <https://okanbulut.github.io/rbook/part5.html>\n",
    "supporting": [
      "01_07_HypothesisTests_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}