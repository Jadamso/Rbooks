{
  "hash": "5d34d19d1e95fcf1861a668ff85060b3",
  "result": {
    "engine": "knitr",
    "markdown": "# Misc. Multivariate Topics\n***\n\n#### **Filtering**.{-}\nIn some cases, we may want to smooth time series data instead of predict into the future. We can distinguish two types of smoothing, incorporating future observations or not. When only weighting two other observations, the differences can be expressed as trying to estimate the average with different available data:\n\n* Filtering: $\\mathbb{E}[Y_{t} | X_{t-1}, X_{t-2}]$\n* Smoothing: $\\mathbb{E}[Y_{t} | X_{t-1}, X_{t+1}]$\n\nOne example of filtering is Exponential Filtering (sometimes confusingly referred to as \"Exponential Smoothing\") which weights only previous observations using an exponential kernel.\n\n::: {.cell}\n\n```{.r .cell-code}\n##################\n# Time series data\n##################\n\nset.seed(1)\n## Underlying Trend\nx0 <- cumsum(rnorm(500,0,1))\n## Observed Datapoints\nx <- x0 + runif(length(x0),-10,10)\ndat <- data.frame(t=seq(x), x0=x0, x=x)\n\n## Asymmetric Kernel\n#bw <- c(2/3,1/3)\n#s1 <- filter(x, bw/sum(bw), sides=1)\n\n## Symmetric Kernel\n#bw <- c(1/6,2/3,1/6)\n#s2 <- filter(x,  bw/sum(bw), sides=2)\n```\n:::\n\n\nThere are several cross-validation procedures for filtering time series data \\parencite{OpsomerEtAl2001}. One is called time series cross-validation (TSCV), which is useful for temporally dependent data \\parencite{Hart1991, HaerdlePhilippe1992, Bergmeir2018}.\n\n::: {.cell}\n\n```{.r .cell-code}\n## Plot Simulated Data\nx <- dat$x\nx0 <- dat$x0\npar(fig = c(0,1,0,1), new=F)\nplot(x, pch=16, col=grey(0,.25))\nlines(x0, col=1, lty=1, lwd=2)\n\n## Work with differenced data?\n#n       <- length(Yt)\n#plot(Yt[1:(n-1)], Yt[2:n],\n#    xlab='d Y (t-1)', ylab='d Y (t)', \n#    col=grey(0,.5), pch=16)\n#Yt <- diff(Yt)\n\n## TSCV One Sided Moving Average\nfilter_bws <- seq(1,20,by=1)\nfilter_mape_bws <- sapply(filter_bws, function(h){\n    bw <- c(0,rep(1/h,h)) ## Leave current one out\n    s2 <- filter(x, bw, sides=1)\n    pe <- s2 - x\n    mape <- mean( abs(pe)^2, na.rm=T)\n})\nfilter_mape_star <- filter_mape_bws[which.min(filter_mape_bws)]\nfilter_h_star <- filter_bws[which.min(filter_mape_bws)]\nfilter_tscv <- filter(x,  c(rep(1/filter_h_star,filter_h_star)), sides=1)\n# Plot Optimization Results\n#par(fig = c(0.07, 0.35, 0.07, 0.35), new=T) \n#plot(filter_bws, filter_mape_bws, type='o', ylab='mape', pch=16)\n#points(filter_h_star, filter_mape_star, pch=19, col=2, cex=1.5)\n\n## TSCV for LLLS\nlibrary(np)\nllls_bws <- seq(8,28,by=1)\nllls_burnin <- 10\nllls_mape_bws <- sapply(llls_bws, function(h){ # cat(h,'\\n')\n    pe <- sapply(llls_burnin:nrow(dat), function(t_end){\n        dat_t <- dat[dat$t<t_end, ]\n        reg <- npreg(x~t, data=dat_t, bws=h,\n            ckertype='epanechnikov',\n            bandwidth.compute=F, regtype='ll')\n        edat <- dat[dat$t==t_end,]\n        pred <- predict(reg, newdata=edat)\n        pe <- edat$x - pred\n        return(pe)    \n    })\n    mape <- mean( abs(pe)^2, na.rm=T)\n})\nllls_mape_star <- llls_mape_bws[which.min(llls_mape_bws)]\nllls_h_star <- llls_bws[which.min(llls_mape_bws)]\n#llls_tscv <- predict( npreg(x~t, data=dat, bws=llls_h_star,\n#    bandwidth.compute=F, regtype='ll', ckertype='epanechnikov'))\nllls_tscv <- sapply(llls_burnin:nrow(dat), function(t_end, h=llls_h_star){\n    dat_t <- dat[dat$t<t_end, ]\n    reg <- npreg(x~t, data=dat_t, bws=h,\n        ckertype='epanechnikov',\n        bandwidth.compute=F, regtype='ll')\n    edat <- dat[dat$t==t_end,]\n    pred <- predict(reg, newdata=edat)\n    return(pred)    \n})\n\n## Compare Fits Qualitatively\nlines(filter_tscv, col=2, lty=1, lwd=1)\nlines(llls_burnin:nrow(dat), llls_tscv, col=4, lty=1, lwd=1)\nlegend('topleft', lty=1, col=c(1,2,4), bty='n',\n    c('Underlying Trend', 'MA-1sided + TSCV', 'LLLS-1sided + TSCV'))\n\n## Compare Fits Quantitatively\ncbind(\n    bandwidth=c(LLLS=llls_h_star, MA=filter_h_star),\n    mape=round(c(LLLS=llls_mape_star, MA=filter_mape_star),2) )\n\n## See also https://cran.r-project.org/web/packages/smoots/smoots.pdf\n#https://otexts.com/fpp3/tscv.html\n#https://robjhyndman.com/hyndsight/tscvexample/\n```\n:::\n\n\n\n#### **Cross Validation**.{-}\n\nPerhaps the most common approach to selecting a bandwidth is to minimize \\textit{prediction} error. *Leave-one-out Cross-validation* minimizes the average \"leave-one-out\" mean square prediction errors:\n\\begin{eqnarray}\n\\min_{\\mathbf{H}} \\quad \\frac{1}{n} \\sum_{i=1}^{n} \\left[ \\hat{Y}_{i} - \\hat{y_{[i]}}(\\mathbf{X},\\mathbf{H}) \\right]^2,\n% \\hat{Y_{[i]}}(\\mathbf{X},\\mathbf{H}) &=& \\sum_{j\\neq i} k(\\mathbf{X}_{j},\\mathbf{X}_{i},\\mathbf{H}) \\left[ \\hat{\\alpha}(\\mathbf{X}_{j}) + \\hat{\\beta}(\\mathbf{X}_{j}) \\mathbf{X}_{i} \\right]\n\\end{eqnarray}\nwhere $\\hat{y}_{[i]}(\\mathbf{X},\\mathbf{H})$ is the model predicted value at $\\mathbf{X}_{i}$ based on a dataset that excludes $\\mathbf{X}_{i}$, and $\\mathbf{H}$ is matrix of bandwidths. With a weighted least squares regression on three explanatory variables, for example, the matrix is\n\\begin{eqnarray}\n\\mathbf{H}=\\begin{pmatrix}\nh_{1} & 0 & 0  \\\\ \n 0    & h_{2} & 0 \\\\\n 0    & 0 & h_{3} \\\\  \n\\end{pmatrix},\n\\end{eqnarray}\nwhere each $h_{k}$ is the bandwidth for variable $X_{k}$.\n\nThere are many types of cross-validation \\parencite{ArlotCelisse2010, BatesEtAl2023}. For example, one extension is *k-fold cross validation*, which splits $N$ datapoints into $k=1...K$ groups, each sized $B$, and predicts values for the left-out group. \\textit{Generalized cross-validation} adjusts for the degrees of freedom, whereas the \\texttt{npreg} function in R uses \\textit{least-squares cross-validation} \\parencite[p.74]{racine2019} by default. You can refer to extensions on a case by case basis.\n\n\n#### **Subsampling**. {-}\nRandom subsampling is one of many hybrid approaches that tries to combine the best of the core methods: Bootstrap and Jacknife.\n\n| | Sample Size per Iteration | Number of Iterations | Resample |\n| -------- | ------- | ------- | ------- |\nBootstrap | $n$     | $B$  | With Replacement |\nJackknife | $n-1$   | $n$  | Without Replacement |\nRandom Subsample | $m < n$ | $B$  | Without Replacement |\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\ncolnames(xy) <- c('y','x')\nreg <-  lm(y~x, dat=xy)\ncoef(reg)\n## (Intercept)           x \n##  6.41594246  0.02093466\n\n# Random Subsamples\nrs_regs <- lapply(1:399, function(b){\n    b_id <- sample( nrow(xy), nrow(xy)-10, replace=F)\n    xy_b <- xy[b_id,]\n    reg_b <- lm(y~x, dat=xy_b)\n})\nrs_coefs <- sapply(rs_regs, coef)['x',]\nrs_se <- sd(rs_coefs)\n\nhist(rs_coefs, breaks=25,\n    main=paste0('SE est. = ', round(rs_se,4)),\n    font.main=1, border=NA,\n    xlab=expression(hat(b)[b]))\nabline(v=coef(reg)['x'], lwd=2)\nrs_ci_percentile <- quantile(rs_coefs, probs=c(.025,.975))\nabline(v=rs_ci_percentile, lty=2)\n```\n\n::: {.cell-output-display}\n![](03_22_MiscTopics_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n#### **Matrix Calculations**. {-}\n\nFirst, not that you can use matrix algebra in R\n\n::: {.cell}\n\n```{.r .cell-code}\nx_mat1 <- matrix( seq(2,7), 2, 3)\nx_mat1\n##      [,1] [,2] [,3]\n## [1,]    2    4    6\n## [2,]    3    5    7\n\nx_mat2 <- matrix( seq(4,-1), 2, 3)\nx_mat2\n##      [,1] [,2] [,3]\n## [1,]    4    2    0\n## [2,]    3    1   -1\n\nx_mat1*x_mat2 #NOT classical matrix multiplication\n##      [,1] [,2] [,3]\n## [1,]    8    8    0\n## [2,]    9    5   -7\nx_mat1^x_mat2\n##      [,1] [,2]      [,3]\n## [1,]   16   16 1.0000000\n## [2,]   27    5 0.1428571\n\ntcrossprod(x_mat1, x_mat2) #x_mat1 %*% t(x_mat2)\n##      [,1] [,2]\n## [1,]   16    4\n## [2,]   22    7\n\ncrossprod(x_mat1, x_mat2)\n##      [,1] [,2] [,3]\n## [1,]   17    7   -3\n## [2,]   31   13   -5\n## [3,]   45   19   -7\n```\n:::\n\n\nSecond, note that you can apply functions to matrices\n\n::: {.cell}\n\n```{.r .cell-code}\nsum_squared <- function(x1, x2) {\n    y <- (x1 + x2)^2\n    return(y)\n}\nsum_squared(x_mat1, x_mat2)\n##      [,1] [,2] [,3]\n## [1,]   36   36   36\n## [2,]   36   36   36\n\n\nsolve(x_mat1[1:2,1:2])\n##      [,1] [,2]\n## [1,] -2.5    2\n## [2,]  1.5   -1\n```\n:::\n\n\n\nThird, note that we can conduct OLS regressions efficiently using matrix algebra. Grouping the coefficients as a vector $B=(b_0 ~~ b_1 ~~... ~~ b_{K})$, we want to find coefficients that minimize the sum of squared errors. The linear model has a fairly simple solution for $\\hat{B}^{*}$ if you know linear algebra. Denoting $\\hat{\\mathbf{X}}_{i} = [1~~  \\hat{X}_{i1} ~~...~~ \\hat{X}_{iK}]$ as a row vector, we can write the model as $\\hat{Y}_{i} = \\hat{\\mathbf{X}}_{i}B + e_{i}$. We can then write the model in matrix form\n\\begin{eqnarray}\n\\hat{Y} &=& \\hat{\\textbf{X}}B + E \\\\\n\\hat{Y} &=& \\begin{pmatrix} \n\\hat{Y}_{1} \\\\ \\vdots \\\\ \\hat{Y}_{N}\n\\end{pmatrix} \\quad\n\\hat{\\textbf{X}} = \\begin{pmatrix} \n1 & \\hat{X}_{11} & ... & \\hat{X}_{1K} \\\\\n& \\vdots & & \\\\\n1 & \\hat{X}_{n1} & ... & \\hat{X}_{nK} \n\\end{pmatrix}\n\\end{eqnarray}\nMinimizing the squared errors: $\\min_{B} e_{i}^2 = \\min_{B} (E' E)$, yields coefficient estimates and predictions \n\\begin{eqnarray}\n\\hat{B^{*}} &=& (\\hat{\\textbf{X}}'\\hat{\\textbf{X}})^{-1}\\hat{\\textbf{X}}'\\hat{Y}\\\\\n\\hat{y} &=& \\hat{\\textbf{X}} \\hat{B^{*}} \\\\\n\\hat{E} &=& \\hat{Y} - \\hat{y} \\\\\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manually Compute\nY <- USArrests[,'Murder']\nX <- USArrests[,c('Assault','UrbanPop')]\nX <- as.matrix(cbind(1,X))\n\nXtXi <- solve(t(X)%*%X)\nBhat <- XtXi %*% (t(X)%*%Y)\nc(Bhat)\n## [1]  3.20715340  0.04390995 -0.04451047\n```\n:::\n\n",
    "supporting": [
      "03_22_MiscTopics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}