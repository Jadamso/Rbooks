{
  "hash": "116ee50bc025a219948581cda9bd594c",
  "result": {
    "engine": "knitr",
    "markdown": "# Advanced Probability\n\n## Drawing Samples\n\nTo generate a random variable from known distributions, you can use some type of physical machine. E.g., you can roll a fair die to generate Discrete Uniform data or you can roll weighted die to generate Categorical data.\n\nThere are also several ways to computationally generate random variables from a probability distribution. Perhaps the most common one is \"inverse sampling\". To generate a random variable using inverse sampling, first sample $p$ from a uniform distribution and then find the associated quantile  quantile function $\\hat{F}^{-1}(p)$.^[Drawing random uniform samples with computers is actually quite complex and beyond the scope of this course.]\n\n\n#### **Using Data**. {-}\n\nYou can generate a random variable from a known empirical distribution. Inverse sampling randomly selects observations from the dataset with equal probabilities. To implement this, we \n\n* order the data and associate each observation with an ECDF value\n* draw $p \\in [0,1]$ as a uniform random variable\n* find the associated quantile via the ECDF\n\n:::{.callout-tip icon=false collapse=\"true\"}\nHere is an example of generating random murder rates for US states.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Empirical Distribution\nX <- USArrests[,'Murder']\nFX_hat <- ecdf(X)\nplot(FX_hat, lwd=2, xlim=c(0,20),\n    pch=16, col=grey(0,.5), main='')\n```\n\n::: {.cell-output-display}\n![](01_09_MiscTopics_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Generating random variables via inverse ECDF\np <- runif(3000) ## Multiple Draws\nQX_hat <- quantile(FX_hat, p, type=1)\nQX_hat[c(1,2,3)]\n## 89.41162406% 45.22195803% 11.42358119% \n##         13.2          6.6          2.6\n\n## Can also do directly from the data\nQX_hat <- quantile(X, p, type=1)\nQX_hat[c(1,2,3)]\n## 89.41162406% 45.22195803% 11.42358119% \n##         13.2          6.6          2.6\n```\n:::\n\n:::\n\n#### **Using Math**. {-}\n\nIf you know the distribution function that generates the data, then you can derive the quantile function and do inverse sampling. \nThat is how computers generate random data from a distribution.\n\n::: {.cell}\n\n```{.r .cell-code}\n# 4 random data points from 3 different distributions\nqunif(4)\n## [1] NaN\nqexp(4)\n## [1] NaN\nqnorm(4)\n## [1] NaN\n```\n:::\n\n\nHere is an in-depth example of the [Dagum distribution](https://en.wikipedia.org/wiki/Dagum_distribution). The distribution function is $F(x)=(1+(x/b)^{-a})^{-c}$. For a given probability $p$, we can then solve for the quantile as $F^{-1}(p)=\\frac{ b p^{\\frac{1}{ac}} }{(1-p^{1/c})^{1/a}}$. Afterwhich, we sample $p$ from a uniform distribution and then find the associated quantile.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Theoretical Quantile Function (from VGAM::qdagum)\nqdagum <- function(p, scale.b=1, shape1.a, shape2.c) {\n  # Quantile function (theoretically derived from the CDF)\n  ans <- scale.b * (expm1(-log(p) / shape2.c))^(-1 / shape1.a)\n  # Special known cases\n  ans[p == 0] <- 0\n  ans[p == 1] <- Inf\n  # Safety Checks\n  ans[p < 0] <- NaN\n  ans[p > 1] <- NaN\n  if(scale.b <= 0 | shape1.a <= 0 | shape2.c <= 0){ ans <- ans*NaN }\n  # Return\n  return(ans)\n}\n\n# Generate Random Variables (VGAM::rdagum)\nrdagum <-function(n, scale.b=1, shape1.a, shape2.c){\n    p <- runif(n) # generate random probabilities\n    x <- qdagum(p, scale.b=scale.b, shape1.a=shape1.a, shape2.c=shape2.c) #find the inverses\n    return(x)\n}\n\n# Example\nset.seed(123)\nX <- rdagum(3000,1,3,1)\nX[c(1,2,3)]\n## [1] 0.7390476 1.5499868 0.8845006\n```\n:::\n\n\n\n## Kernel Density Estimation\n\nA kernel density is a \"smooth\" version of a histogram. \n\nThe general idea behind kernel density is to use small windows around each $x$ that potentially overlap, rather than partitioning the range of $X$ into exclusive bins. The \"uniform\" kernel density is \n\\begin{eqnarray}\n\\label{eqn:uniform}\n\\hat{f}_{U}(x) &=& \\frac{1}{n} \\sum_{i}^{n} \\frac{k_{U}(\\hat{X}_{i}, x, h) }{2h} \\\\\nk_{U}\\left( \\hat{X}_{i}, x, h \\right) &=& \\mathbf{1}\\left(\\frac{|\\hat{X}_{i}-x|}{h}<1\\right)\n= \\mathbf{1}\\left( \\hat{X}_{i} \\in \\left( x-h, x + h\\right) \\right).\n\\end{eqnarray} \nNotice that the uniform kernel is essentially the histogram but without the restriction that $x$ must be a midpoint of *exclusive* bins. Typically, the points $x$ are chosen to be either the unique observations or some equidistant set of \"design points\" (e.g., at $512$ evenly spaced values of $x$ the dataset, not just the midpoints of exclusive bins).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Practical Example\nX <- USArrests[,'Murder']\nhist(X,\n    breaks=seq(0,20,by=2), #bin width=1\n    freq=F,\n    border=NA, \n    main='',\n    xlab='Murder Arrests')\n# Density Estimate\nlines( density(X, bw=3, kernel='rectangular') )\n# Raw Observations\nrug(X, col=grey(0,.5))\n```\n\n::: {.cell-output-display}\n![](01_09_MiscTopics_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nWe can also replace the uniform kernel with a more general kernel function. There are many [kernels](https://en.wikipedia.org/wiki/Kernel_(statistics)#In_non-parametric_statistics), but these are the most intuitive and commonly used.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Kernel Density Functions\n\nX <- seq(-2,2, length.out=1001)\n\nplot.new()\nplot.window(xlim=c(-1.2,1.2), ylim=c(0,1))\n\nh <- 1\nlines( dunif(X,-h,h)~X, col=1, lty=1)\n\nh <- 1/2\nlines( dnorm(X,0,h)~X, col=2, lty=1)\n\ndtricub <- function(X, x=0, h){\n    u <- abs(X-x)/h\n    fu <- 70/81*(1-u^3)^3/h*(u <= 1)\n    return(fu)\n}\nh <- 1\nlines( dtricub(X,0,h)~X, col=3, lty=1)\n\nh <- 1/2\nlines(density(x=0, bw=h, kernel=\"epanechnikov\"), col=4, lty=1)\n## Note that \"density\" defines h slightly differently\n\nrug(0, lwd=2)\naxis(1)\naxis(2)\n\nlegend('topright', lty=1, col=1:4,\n    legend=c('uniform(1)', 'gaussian(1/2)', 'tricubic(1)', 'epanechnikov(1)'))\n```\n\n::: {.cell-output-display}\n![](01_09_MiscTopics_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n\n## Try others:\n## lines(density(x=0, bw=1/2, kernel=\"triangular\"),col=4, lty=1)\n```\n:::\n\n\nOnce we have picked a kernel (which particular one is not particularly important) we can use it to compute density estimates at each design point.\n\n:::{.callout-tip icon=false collapse=\"true\"}\nTry out different kernels\n\n::: {.cell}\n\n```{.r .cell-code}\n# Practical Example\nX <- USArrests[,'Murder']\nhist(X,\n    breaks=seq(0,20,by=2), #bin width=1\n    freq=F,\n    border=NA, \n    main='',\n    xlab='Murder Arrests')\n# Density Estimate\nlines( density(X, bw=2, kernel='epanechnikov') )\n# Raw Observations\nrug(X, col=grey(0,.5))\n```\n\n::: {.cell-output-display}\n![](01_09_MiscTopics_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n:::\n\n\n## Factorial Distributions\n\n#### **Binomial**. {-}\n\nThe sum of $n$ Bernoulli trials (number of successes)\n\n* Discrete, support $\\{0,1,\\ldots,n\\}$\n* Probability Mass Function: $Prob(X_{i}=x)=\\binom{n}{x}p^{x}(1-p)^{n-x}$\n* See <https://en.wikipedia.org/wiki/Binomial_distribution>\n* A common use case: How many heads will I get when I flip a coin twice?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Minimal example\nn <- 15\np <- 0.3\n\n# PMF plot\nx <- seq(0, n)\nf_x <- dbinom(x, n, p)\nplot(x, f_x, type = \"h\", col = \"blue\",\n    main='',  xlab = \"x\", ylab = \"Prob(X = x)\")\ntitle(bquote(paste('Binom(',.(n),', ',.(p), ')' ) ))\npoints(x, f_x, pch = 16, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](01_09_MiscTopics_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Simulate:  Compare empirical vs theoretical\n#X <- rbinom(1e4, size = n, prob = p)\n#c(emp_mean = mean(X), th_mean = n*p)\n#c(emp_var  = var(X),  th_var  = n*p*(1-p))\n```\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nSuppose that employees at a company are $70%$ female and $30%$ male. If we select a random sample of eight employees, what is the probability that more than $2$ in the sample are female?\n:::\n\n:::{.callout-tip icon=false collapse=\"true\"}\nShow that \n$\\mathbb{E}[X_{i}]=np$ and $\\mathbb{V}[X_{i}]=np(1-p)$. Start with the case $n=1$, then the case $n=2$, then case $n=3$, then the general case of any $n$.\n:::\n\nThe Binomial Limit Theorem (de Moivre–Laplace theorem) says that as $n$ grows large, with $p \\in (0,1)$ staying fixed, the Binomial distribution is approximately normal with mean $np$ and variance $np(1-p)$\n\n:::{.callout-tip icon=false collapse=\"true\"}\nThe unemployment rate is $10%$. Suppose that $100$ employable people are selected randomly. What is the probability that this sample contains between $9$ and $12$ unemployed people. Use the normal approximation to binomial probabilities.\n\nParameters are \n$\\mu \\approx n p = 10$\n$\\sigma^2= n p (1−p) = 100 (0.1)(0.9)=9$\n$\\sigma=\\sqrt{9}=3$).\n:::\n\n#### **Poisson**. {-}\n\nThe number of events in a fixed interval\n\n* Discrete, support $\\{0,1,2,\\ldots\\}$\n* Probability Mass Function: $Prob(X_{i}=x)=e^{-\\lambda}\\lambda^x/x!$\n* See <https://en.wikipedia.org/wiki/Poisson_distribution>\n* A common use case: How many cars will show up in the lot tomorrow?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Minimal example\nlambda <- 3.5\n\n# PMF plot\nx <- seq(0, 15)\nf_x <- dpois(x, lambda)\nplot(x, f_x, type=\"h\", col=\"blue\",\n     xlab = \"x\", ylab = \"Prob(X = x)\")\npoints(x, f_x, pch = 16, col = \"blue\")\ntitle(bquote(paste('Pois(',.(lambda), ')')))\n```\n\n::: {.cell-output-display}\n![](01_09_MiscTopics_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n\n# Simulate: Compare empirical vs theoretical\n#X <- rpois(1e4, lambda)\n#c(emp_mean = mean(X), th_mean = lambda)\n#c(emp_var  = var(X),  th_var  = lambda)\n```\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nShow that $\\mathbb{E}[X_{i}] = \\mathbb{V}[X_{i}]= \\lambda$.\n:::\n\n\n#### **Irwin–Hall**. {-}\n\nThe sum of $n$ i.i.d. $\\text{Uniform}(0,1)$.  \n\n* Continuous, support $[0,n]$\n* Probability Density Function: $f(x) = \\dfrac{1}{(n-1)!}\n\\displaystyle\\sum_{k=0}^{\\lfloor x \\rfloor} (-1)^k\n\\binom{n}{k} (x - k)^{n-1}$ for $x \\in [0,n]$, and $0$ otherwise\n* See <https://en.wikipedia.org/wiki/Irwin%E2%80%93Hall_distribution>\n* A common use case: representing the sum of many small independent financial shocks\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Minimal example (base R)\n# Irwin–Hall PDF function\ndirwinhall <- function(x, n) {\n    f_x <- vector(length=length(x))\n    for(i in seq(x)) {\n        xx <- x[i]\n        if(xx < 0 | xx > n){\n            f_x[i] <- 0 \n        } else {\n            k <- seq(0, floor(xx))\n            f_k <- sum((-1)^k*choose(n, k)*(xx-k)^(n-1))/factorial(n-1)\n            f_x[i] <- f_k\n        }\n    }\n    return(f_x)\n}\n\n# Parameters\nn <- 2\nx <- seq(0, n, length.out = 500)\n\n# Compute and plot PDF\nf_x <- dirwinhall(x, n)\nplot(x, f_x, type=\"l\", col=\"blue\",\n    main='', xlab = \"x\", ylab = \"f(x)\")\ntitle(bquote(paste('IrwinHall(',.(n), ')')))\n```\n\n::: {.cell-output-display}\n![](01_09_MiscTopics_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nSee also the [Bates](https://en.wikipedia.org/wiki/Bates_distribution) distribution, which is for the mean  of $n$ i.i.d. $\\text{Uniform}(0,1)$ random variables. It essentially rescales the Irwin-Hall distribution to range between $0$ and $1$.\n\n\n#### **Beta**. {-}\nThe sample space is any number on the unit interval, $X_{i} \\in [0,1]$, but with non-uniform probabilities. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX4 <- rbeta(2000,2,2) ## two shape parameters\nhist(X4, breaks=20, border=NA, main=NA, freq=F)\n\n#See the underlying probabilities\n#f_25 <- dbeta(.25, 2, 2)\n\nx <- seq(0,1,by=.01)\nfx <- dbeta(x, 2, 2)\nlines(x, fx, col='blue')\n```\n\n::: {.cell-output-display}\n![](01_09_MiscTopics_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThe [Beta](https://en.wikipedia.org/wiki/Beta_distribution) distribution is mathematically complicated to write, and so we omit it. However, we can find the probability graphically using either the probability density function or cumulative distribution function. \n\n:::{.callout-tip icon=false collapse=\"true\"}\nSuppose $X_{i}$ is a random variable with a beta distribution. Intuitively depict $Prob(X_{i} \\in [0.2, 0.8])$ by drawing an area under the density function. Numerically estimate that same probability using the CDF.\n\n::: {.cell}\n\n```{.r .cell-code}\nplot( ecdf(X4), main=NA) # Empirical\n\nx <- seq(0,1,by=.01) # Theoretical\nFx <- pbeta(x, 2, 2)\nlines(x, Fx, col='blue')\n\n# Middle Interval Example \nF2 <- pbeta(0.2, 2, 2)\nF8 <- pbeta(0.8, 2, 2)\nF_2_8 <- F8 - F2\nF_2_8\n## [1] 0.792\n\n# Visualize\ntitle('Middle between 0.2 and 0.8')\nsegments( 0.2, F2, -1, F2, col='red')\nsegments( 0.8, F8, -1, F8, col='red')\n```\n\n::: {.cell-output-display}\n![](01_09_MiscTopics_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n:::\n\nThis distribution is often used, as the probability density function has two parameters that allow it to take many different shapes.\n\n:::{.callout-tip icon=false collapse=\"true\"}\nFor each example below, intuitively depict $Prob(X_{i} \\leq 0.5)$ using the PDF. Repeat the exercise using a CDF instead of a PDF to calculate a numerical value.\n\n::: {.cell}\n\n```{.r .cell-code}\nop <- par(no.readonly = TRUE); on.exit(par(op), add = TRUE)\nx <- seq(0,1,by=.01)\npars <- expand.grid( c(.5,1,2), c(.5,1,2) )\npar(mfrow=c(3,3))\napply(pars, 1, function(p){\n    fx <- dbeta( x,p[1], p[2])\n    plot(x, fx, type='l', xlim=c(0,1), ylim=c(0,4), lwd=2, col='blue')\n    #hist(rbeta(2000, p[1], p[2]), breaks=50, border=NA, main=NA, freq=F)\n})\ntitle('Beta densities', outer=T, line=-1)\n```\n\n::: {.cell-output-display}\n![](01_09_MiscTopics_files/figure-html/unnamed-chunk-12-1.png){width=768}\n:::\n:::\n\n:::\n\n\n## Set Theory\n\nLet the sample space contain events $A$ and $B$, with their probability denoted as $Prob(A)$ and $Prob(B)$. Then\n\n- the *union*: $A\\cup B$, refers to either event occurring\n- the *intersection*: $A\\cap B$, refers to both events occurring\n- the events $A$ and $B$ are *mutually exclusive* if and only if $A\\cap B$ is empty\n- the *inclusion–exclusion* rule is: $Prob(A\\cup B)=Prob(A)+Prob(B)-Prob(A\\cap B)$\n- the events $A$ and $B$ are *mutually independent* if and only if $Prob(A\\cap B)=Prob(A) Prob(B)$\n- the *conditional probability* is $Prob(A \\mid B) = Prob(A \\cap B)/ P(B)$\n\nFor example, consider a six sided die where events are the number $>2$ or the number is even.\n\n- The sets are $A=\\{3,4,5,6\\}$ and $B=\\{2,4,6\\}$\n- To check mutually exclusive: notice $A\\cap B=\\{4,6\\}$ which is not empty. So event $A$ and event $B$ are not mutually exclusive\n\nFurther assuming the die are fair, each side has an equal $1/6$ probability. This lets us compute\n\n- $Prob(A)=4/6=2/3$ and $Prob(B)=3/6=1/2$\n- $A\\cup B=\\{2,3,4,5,6\\}$, which implies $Prob(A\\cup B)=5/6$\n- $A\\cap B=\\{4,6\\}$, which implies $Prob(A\\cap B)=2/6=1/3$\n- To check independence: notice $Prob(A\\cap B)=1/3 = (2/3) (1/2) = Prob(A)Prob(B)$. So event $A$ and event $B$ are mutually independent\n- To check inclusion–exclusion: notice $Prob(A) + Prob(B) - Prob(A\\cap B) = 2/3 + 1/2 - 1/3 = 4/6 + 3/6 - 2/6 = 5/6 = Prob(A\\cup B)$\n- Finally, the conditional probability of $Prob(\\text{die is} >2 \\mid \\text{die shows even number}) = Prob(A \\mid B) = \\frac{Prob(A \\cap B)}{P(B)} = \\frac{1/3}{1/2} = 2/3$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulation verification\nset.seed(123)\nx <- seq(1,6)\nrolls <- sample(x, 1e6, replace = TRUE)\nA <- rolls > 2\nB <- rolls %% 2 == 0\nP_AcondB <- mean(A & B) / mean(B)\nround(P_AcondB, 3)\n## [1] 0.667\n```\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nConsider a six sided die with sample space $\\{1,2,3,4,5,6\\}$ where each outcome is each equally likely. What is $Prob(\\text{die is odd or }<5)$?\nDenote the odd events as $A=\\{1,3,5\\}$ and the less than five events as  $B=\\{1,2,3,4\\}$. Then\n\n- $A\\cup B=\\{1,3\\}$, which implies $Prob(A\\cap B)=2/6=1/3$.\n- $Prob(A)=3/6=1/2$ and $Prob(B)=4/6=2/3$.\n- By inclusion–exclusion: $Prob(A\\cup B)=Prob(A)+Prob(B)-Prob(A\\cap B)=1/2+2/3-1/3=5/6$.\n\nNow find $Prob(\\text{die is even and }<5)$. Verify your answer with a computer simulation.\n:::\n\n#### **Law of Total Probability**. {-}\n\nThis law states that $Prob(A)=Prob(A \\mid B) P(B) + Prob(A \\mid \\lnot B) P(\\lnot B)$, where $\\lnot B$ is the event \"not $B$\".\n\nFor example, consider a six sided die where events are the number $>2$ or the number is even.\n- The sets are $A=\\{3,4,5,6\\}$ and $B=\\{2,4,6\\}$ and $\\lnot B = \\{1,3,5\\}$ \nFrom before, we know that $Prob(A)=2/3$, $Prob(B)=1/2$, and $Prob(A \\mid B) = 2/3$.\nThen see that $Prob(A \\mid B) Prob(B) = \\frac{2}{3} \\frac{1}{2} = 2/6$ \n\nWe can also directly compute $Prob(\\lnot B)=1/2$, or infer it from $Prob(\\lnot B)=1-Prob(B)$.\nThe conditional probability of $Prob(\\text{die is} >2 \\mid \\text{die shows odd number}) = Prob(A \\mid \\lnot B) = \\frac{Prob(A \\cap \\lnot B)}{P(\\lnot B)} = 2/3$.\n\nThen we can see that\n$Prob(A \\mid B) Prob(B) + Prob(A \\mid \\lnot B) Prob(\\lnot B) = 2/6 + 2/6 = 2/3 = Prob(A)$.\n\n\n",
    "supporting": [
      "01_09_MiscTopics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}