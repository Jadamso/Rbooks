{
  "hash": "57d61c5282f61f3529a303685fb49ce4",
  "result": {
    "engine": "knitr",
    "markdown": "# Local Relationships\n***\n\n## Regressograms\n\nYou can estimate a nonparametric model with multiple $X$ variables with a multivariate regressogram. Here, we cut the data into exclusive bins along each dimension (called dummy variables), and then run a regression on all dummy variables. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Simulate Data\nN <- 10000\ne <- rnorm(N)\nx1 <- seq(.1,20,length.out=N)\nx2 <- runif(N, 0,1)\ny  <- 3*exp(-2*x2 + 1.5*x1 - .1*x1^2)*x1 + e\ndat <- data.frame(x1, x2, y)\n\n## Create color palette (reused in later examples)\ncol_scale <- seq(min(y)*1.1, max(y)*1.1, length.out=401)\nycol_pal <- hcl.colors(length(col_scale),alpha=.5)\nnames(ycol_pal) <- sort(col_scale)\n\n## Add legend (reused in later examples)\nadd_legend <- function(col_scale,\n    yl=11,\n    colfun=function(x){ hcl.colors(x,alpha=.5) },\n    ...) {\n  opar <- par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), \n              mar=c(0, 0, 0, 0), new=TRUE)\n  on.exit(par(opar))\n  h <- hist(col_scale, plot=F, breaks=yl-1)$mids\n  plot(0, 0, type='n', bty='n', xaxt='n', yaxt='n')\n  legend(...,\n    legend=h,\n    fill=colfun(length(h)),\n    border=NA,\n    bty='n')\n}\n\n\n## Plot Data\npar(oma=c(0,0,0,2))\nplot(x1~x2, dat,\n    col=ycol_pal[cut(y,col_scale)],\n    pch=16, cex=.5, \n    main='Raw Data', font.main=1)\nadd_legend(x='topright', col_scale=col_scale,\n    yl=6, inset=c(0,.05), title='y')\n```\n\n::: {.cell-output-display}\n![](03_20_Regressograms_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## OLS \nreg <- lm(y~x1*x2, data=dat) #(with simple interaction)\nreg <- lm(y~x1+x2, data=dat) #(without interaction)\n\n## Grid Points for Prediction\n# X1 bins\nl1 <- 11\nbks1 <- seq(0,20, length.out=l1)\nh1 <- diff(bks1)[1]/2\nmids1 <- bks1[-1]-h1\n# X2 bins\nl2 <- 11\nbks2 <- seq(0,1, length.out=l2)\nh2 <- diff(bks2)[1]/2\nmids2 <- bks2[-1]-h2\n# Grid\npred_x <- expand.grid(x1=mids1, x2=mids2)\n\n## OLS Predictions\npred_ols <- predict(reg, newdata=pred_x)\npred_df_ols  <- cbind(pred_ols, pred_x)\n\n## Plot Predictions\npar(oma=c(0,0,0,2))\nplot(x1~x2, pred_df_ols,\n    col=ycol_pal[cut(pred_ols,col_scale)],\n    pch=15, cex=2,\n    main='OLS Predictions', font.main=1)\nadd_legend(x='topright', col_scale=col_scale,\n    yl=6, inset=c(0,.05),title='y')\n```\n\n::: {.cell-output-display}\n![](03_20_Regressograms_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##################\n# Multivariate Regressogram\n##################\n\n## Regressogram Bins\ndat$x1c <- cut(dat$x1, bks1)\n#head(dat$x1c,3)\ndat$x2c <- cut(dat$x2, bks2)\n\n## Regressogram\nreg <- lm(y~x1c*x2c, data=dat) #nonlinear w/ complex interactions\n\n## Predicted Values\n## For Points in Middle of Each Bin\npred_df_rgrm <- expand.grid(\n    x1c=levels(dat$x1c),\n    x2c=levels(dat$x2c))\npred_df_rgrm$yhat <- predict(reg, newdata=pred_df_rgrm)\npred_df_rgrm <- cbind(pred_df_rgrm, pred_x)\n\n## Plot Predictions\npar(oma=c(0,0,0,2))\nplot(x1~x2, pred_df_rgrm,\n    col=ycol_pal[cut(pred_df_rgrm$yhat,col_scale)],\n    pch=15, cex=2,\n    main='Regressogram Predictions', font.main=1)\nadd_legend(x='topright', col_scale=col_scale,\n    yl=6, inset=c(0,.05),title='y')\n```\n\n::: {.cell-output-display}\n![](03_20_Regressograms_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n## Local Regressions\nJust like with bivariate data, you can also use split-sample (or peicewise) regressions for multivariate data.\n\n\n::: {.cell}\n\n:::\n\n\n\n#### **Break Points**. {-}\nIncorporating Kinks and Discontinuities in $X$ are a type of transformation that can be modeled using factor variables. As such, $F$-tests can be used to examine whether a breaks is statistically significant.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Ecdat)\ndata(Caschool)\nCaschool$score <- (Caschool$readscr + Caschool$mathscr) / 2\nreg <- lm(score~avginc, data=Caschool)\n\n# F Test for Break\nreg2 <- lm(score ~ avginc*I(avginc>15), data=Caschool)\nanova(reg, reg2)\n\n# Chow Test for Break\ndata_splits <- split(Caschool, Caschool$avginc <= 15)\nresids <- sapply(data_splits, function(dat){\n    reg <- lm(score ~ avginc, data=dat)\n    sum( resid(reg)^2)\n})\nNs <-  sapply(data_splits, function(dat){ nrow(dat)})\nRt <- (sum(resid(reg)^2) - sum(resids))/sum(resids)\nRb <- (sum(Ns)-2*reg$rank)/reg$rank\nFt <- Rt*Rb\npf(Ft,reg$rank, sum(Ns)-2*reg$rank,lower.tail=F)\n\n# See also\n# strucchange::sctest(y~x, data=xy, type=\"Chow\", point=.5)\n# strucchange::Fstats(y~x, data=xy)\n\n# To Find Changes\n# segmented::segmented(reg)\n```\n:::\n\n\n\n\n## Model Selection\n\nOne of the most common approaches to selecting a model or bandwidth is to minimize \\textit{prediction} error. *Leave-one-out Cross-validation* minimizes the average \"leave-one-out\" mean square prediction errors:\n\\begin{eqnarray}\n\\min_{\\mathbf{H}} \\quad \\frac{1}{n} \\sum_{i=1}^{n} \\left[ \\hat{Y}_{i} - \\hat{y_{[i]}}(\\mathbf{X},\\mathbf{H}) \\right]^2,\n\\end{eqnarray}\nwhere $\\hat{y}_{[i]}(\\mathbf{X},\\mathbf{H})$ is the model predicted value at $\\mathbf{X}_{i}$ based on a dataset that excludes $\\mathbf{X}_{i}$, and $\\mathbf{H}$ is matrix of bandwidths. With a weighted least squares regression on three explanatory variables, for example, the matrix is\n\\begin{eqnarray}\n\\mathbf{H}=\\begin{pmatrix}\nh_{1} & 0 & 0  \\\\ \n 0    & h_{2} & 0 \\\\\n 0    & 0 & h_{3} \\\\\n\\end{pmatrix},\n\\end{eqnarray}\nwhere each $h_{k}$ is the bandwidth for variable $X_{k}$.\n\nThere are many types of cross-validation [@ArlotCelisse2010; @BatesEtAl2023]. For example, one extension is *k-fold cross validation*, which splits $N$ datapoints into $k=1...K$ groups, each sized $B$, and predicts values for the left-out group. \\textit{Generalized cross-validation} adjusts for the degrees of freedom, whereas the \\texttt{npreg} function in R uses \\textit{least-squares cross-validation} [@racine2019, p. 74] by default. You can refer to extensions on a case by case basis.\n\n\n\n\n\n## Hypothesis Testing\n\nThere are two main ways to summarize gradients: how $Y$ changes with $X$.\n\n1. For regressograms, you can approximate gradients with small finite differences. For some small $h_{p}$, we can compute\n\\begin{eqnarray}\n\\hat{\\beta}_{p}(\\mathbf{x}) &=& \\frac{ \\hat{y}(x_{1},...,x_{p}+ \\frac{h_{p}}{2}...,x_{P})-\\hat{y}(x_{1},...,x_{p}-\\frac{h_{p}}{2}...,x_{P})}{h_{p}},\n\\end{eqnarray}\n\n2. When using split-sample regressions or local linear regressions, you can use the estimated slope coefficients $\\hat{\\beta}_{p}(\\mathbf{x})$ as gradient estimates in each direction.\n\nAfter computing gradients, you can summarize them in various plots:  Histogram of $\\hat{\\beta}_{p}(\\mathbf{x})$, Scatterplot of $\\hat{\\beta}_{p}(\\mathbf{x})$ vs. $X_{p}$, or the CI of $\\hat{\\beta}_{p}(\\mathbf{x})$ vs $\\hat{\\beta}_{p}(\\mathbf{x})$ after sorting the gradients [@Chaudhuri1999; @HendersonEtAl2012]\n\n\nYou may also be interested in a particular gradient or a single summary statistic.\nFor example, a bivariate regressogram can estimate the marginal effect of $X_{1}$ at the means; $\\hat{\\beta_{1}}(\\bar{\\mathbf{x}}=[\\bar{x_{1}}, \\bar{x_{2}}])$.\nYou may also be interested in the mean of the marginal effects (sometimes said simply as \"average effect\"), which averages the marginal effect over all datapoints in the dataset: $1/n \\sum_{i}^{n} \\hat{\\beta_{1}}(\\mathbf{X}_{i})$, or the median marginal effect.\nSuch statistics are single numbers that can be presented similar to an OLS regression table where each row corresponds a variable and each cell has two elements: \"mean gradient (sd gradient)\".\n",
    "supporting": [
      "03_20_Regressograms_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}