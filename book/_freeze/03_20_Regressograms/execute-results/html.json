{
  "hash": "a830b71ef1183fa6e5f080a39d1c9bfc",
  "result": {
    "engine": "knitr",
    "markdown": "# Local Relationships\n***\n\n## Regressograms\n\nYou can estimate a nonparametric model with multiple $X$ variables with a multivariate regressogram. Here, we cut the data into exclusive bins along each dimension (called dummy variables), and then run a regression on all dummy variables. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Simulate Data\nN <- 10000\ne <- rnorm(N)\nx1 <- seq(.1,20,length.out=N)\nx2 <- runif(N, 0,1)\ny  <- 3*exp(-2*x2 + 1.5*x1 - .1*x1^2)*x1 + e\ndat <- data.frame(x1, x2, y)\n\n## Create color palette (reused in later examples)\ncol_scale <- seq(min(y)*1.1, max(y)*1.1, length.out=401)\nycol_pal <- hcl.colors(length(col_scale),alpha=.5)\nnames(ycol_pal) <- sort(col_scale)\n\n## Add legend (reused in later examples)\nadd_legend <- function(col_scale,\n    yl=11,\n    colfun=function(x){ hcl.colors(x,alpha=.5) },\n    ...) {\n  opar <- par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), \n              mar=c(0, 0, 0, 0), new=TRUE)\n  on.exit(par(opar))\n  h <- hist(col_scale, plot=F, breaks=yl-1)$mids\n  plot(0, 0, type='n', bty='n', xaxt='n', yaxt='n')\n  legend(...,\n    legend=h,\n    fill=colfun(length(h)),\n    border=NA,\n    bty='n')\n}\n\n\n## Plot Data\npar(oma=c(0,0,0,2))\nplot(x1~x2, dat,\n    col=ycol_pal[cut(y,col_scale)],\n    pch=16, cex=.5, \n    main='Raw Data', font.main=1)\nadd_legend(x='topright', col_scale=col_scale,\n    yl=6, inset=c(0,.05), title='y')\n```\n\n::: {.cell-output-display}\n![](03_20_Regressograms_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## OLS \nreg <- lm(y~x1*x2, data=dat) #(with simple interaction)\nreg <- lm(y~x1+x2, data=dat) #(without interaction)\n\n## Grid Points for Prediction\n# X1 bins\nl1 <- 11\nbks1 <- seq(0,20, length.out=l1)\nh1 <- diff(bks1)[1]/2\nmids1 <- bks1[-1]-h1\n# X2 bins\nl2 <- 11\nbks2 <- seq(0,1, length.out=l2)\nh2 <- diff(bks2)[1]/2\nmids2 <- bks2[-1]-h2\n# Grid\npred_x <- expand.grid(x1=mids1, x2=mids2)\n\n## OLS Predictions\npred_ols <- predict(reg, newdata=pred_x)\npred_df_ols  <- cbind(pred_ols, pred_x)\n\n## Plot Predictions\npar(oma=c(0,0,0,2))\nplot(x1~x2, pred_df_ols,\n    col=ycol_pal[cut(pred_ols,col_scale)],\n    pch=15, cex=2,\n    main='OLS Predictions', font.main=1)\nadd_legend(x='topright', col_scale=col_scale,\n    yl=6, inset=c(0,.05),title='y')\n```\n\n::: {.cell-output-display}\n![](03_20_Regressograms_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##################\n# Multivariate Regressogram\n##################\n\n## Regressogram Bins\ndat$x1c <- cut(dat$x1, bks1)\n#head(dat$x1c,3)\ndat$x2c <- cut(dat$x2, bks2)\n\n## Regressogram\nreg <- lm(y~x1c*x2c, data=dat) #nonlinear w/ complex interactions\n\n## Predicted Values\n## For Points in Middle of Each Bin\npred_df_rgrm <- expand.grid(\n    x1c=levels(dat$x1c),\n    x2c=levels(dat$x2c))\npred_df_rgrm$yhat <- predict(reg, newdata=pred_df_rgrm)\npred_df_rgrm <- cbind(pred_df_rgrm, pred_x)\n\n## Plot Predictions\npar(oma=c(0,0,0,2))\nplot(x1~x2, pred_df_rgrm,\n    col=ycol_pal[cut(pred_df_rgrm$yhat,col_scale)],\n    pch=15, cex=2,\n    main='Regressogram Predictions', font.main=1)\nadd_legend(x='topright', col_scale=col_scale,\n    yl=6, inset=c(0,.05),title='y')\n```\n\n::: {.cell-output-display}\n![](03_20_Regressograms_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n## Local Regressions\nJust like with bivariate data, you can also use split-sample (or peicewise) regressions for multivariate data.\n\n\n::: {.cell}\n\n:::\n\n\n\n#### **Break Points**. {-}\nIncorporating Kinks and Discontinuities in $X$ are a type of transformation that can be modeled using factor variables. As such, $F$-tests can be used to examine whether a breaks is statistically significant.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Ecdat)\ndata(Caschool)\nCaschool$score <- (Caschool$readscr + Caschool$mathscr) / 2\nreg <- lm(score~avginc, data=Caschool)\n\n# F Test for Break\nreg2 <- lm(score ~ avginc*I(avginc>15), data=Caschool)\nanova(reg, reg2)\n\n# Chow Test for Break\ndata_splits <- split(Caschool, Caschool$avginc <= 15)\nresids <- sapply(data_splits, function(dat){\n    reg <- lm(score ~ avginc, data=dat)\n    sum( resid(reg)^2)\n})\nNs <-  sapply(data_splits, function(dat){ nrow(dat)})\nRt <- (sum(resid(reg)^2) - sum(resids))/sum(resids)\nRb <- (sum(Ns)-2*reg$rank)/reg$rank\nFt <- Rt*Rb\npf(Ft,reg$rank, sum(Ns)-2*reg$rank,lower.tail=F)\n\n# See also\n# strucchange::sctest(y~x, data=xy, type=\"Chow\", point=.5)\n# strucchange::Fstats(y~x, data=xy)\n\n# To Find Changes\n# segmented::segmented(reg)\n```\n:::\n\n\n\n\n## Model Selection\n\nOne of the most common approaches to selecting a model or bandwidth is to minimize \\textit{prediction} error. *Leave-one-out Cross-validation* minimizes the average \"leave-one-out\" mean square prediction errors:\n\\begin{eqnarray}\n\\min_{\\mathbf{H}} \\quad \\frac{1}{n} \\sum_{i=1}^{n} \\left[ \\hat{Y}_{i} - \\hat{y_{[i]}}(\\mathbf{X},\\mathbf{H}) \\right]^2,\n\\end{eqnarray}\nwhere $\\hat{y}_{[i]}(\\mathbf{X},\\mathbf{H})$ is the model predicted value at $\\mathbf{X}_{i}$ based on a dataset that excludes $\\mathbf{X}_{i}$, and $\\mathbf{H}$ is matrix of bandwidths. With a weighted least squares regression on three explanatory variables, for example, the matrix is\n\\begin{eqnarray}\n\\mathbf{H}=\\begin{pmatrix}\nh_{1} & 0 & 0  \\\\ \n 0    & h_{2} & 0 \\\\\n 0    & 0 & h_{3} \\\\\n\\end{pmatrix},\n\\end{eqnarray}\nwhere each $h_{k}$ is the bandwidth for variable $X_{k}$.\n\nThere are many types of cross-validation \\parencite{ArlotCelisse2010, BatesEtAl2023}. For example, one extension is *k-fold cross validation*, which splits $N$ datapoints into $k=1...K$ groups, each sized $B$, and predicts values for the left-out group. \\textit{Generalized cross-validation} adjusts for the degrees of freedom, whereas the \\texttt{npreg} function in R uses \\textit{least-squares cross-validation} \\parencite[p.74]{racine2019} by default. You can refer to extensions on a case by case basis.\n\n\n\n\n\n## Hypothesis Testing\n\nThere are two main ways to summarize gradients: how $Y$ changes with $X$.\n\n1. For regressograms, you can approximate gradients with small finite differences. For some small $h_{p}$, we can compute\n\\begin{eqnarray}\n\\hat{\\beta}_{p}(\\mathbf{x}) &=& \\frac{ \\hat{y}(x_{1},...,x_{p}+ \\frac{h_{p}}{2}...,x_{P})-\\hat{y}(x_{1},...,x_{p}-\\frac{h_{p}}{2}...,x_{P})}{h_{p}},\n\\end{eqnarray}\n\n2. When using split-sample regressions or local linear regressions, you can use the estimated slope coefficients $\\hat{\\beta}_{p}(\\mathbf{x})$ as gradient estimates in each direction.\n\nAfter computing gradients, you can summarize them in various plots:  Histogram of $\\hat{\\beta}_{p}(\\mathbf{x})$, Scatterplot of $\\hat{\\beta}_{p}(\\mathbf{x})$ vs. $X_{p}$, or the CI of $\\hat{\\beta}_{p}(\\mathbf{x})$ vs $\\hat{\\beta}_{p}(\\mathbf{x})$ after sorting the gradients \\cite{Chaudhuri1999, HendersonEtAl2012}\n\n\nYou may also be interested in a particular gradient or a single summary statistic.\nFor example, a bivariate regressogram can estimate the marginal effect of $X_{1}$ at the means; $\\hat{\\beta_{1}}(\\bar{\\mathbf{x}}=[\\bar{x_{1}}, \\bar{x_{2}}])$.\nYou may also be interested in the mean of the marginal effects (sometimes said simply as \"average effect\"), which averages the marginal effect over all datapoints in the dataset: $1/n \\sum_{i}^{n} \\hat{\\beta_{1}}(\\mathbf{X}_{i})$, or the median marginal effect.\nSuch statistics are single numbers that can be presented similar to an OLS regression table where each row corresponds a variable and each cell has two elements: \"mean gradient (sd gradient)\".\n\n#### **Example with Bivariate Data**.{-}\n\nOften, we are interested in gradients: how $Y$ changes with $X$. The linear model depicts this as a simple constant, $\\hat{b}_{1}$, whereas other models do not. A great first way to assess gradients is to plot the predicted values over the explanatory values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Adaptive-width subsamples with non-uniform weights\nxy <- USArrests[,c('UrbanPop','Murder')]\nxy0 <- xy[order(xy[,'UrbanPop']),]\nnames(xy0) <- c('x','y')\n\n\nplot(y~x, pch=16, col=grey(0,.5), dat=xy0)\nreg_lo <- loess(y~x, data=xy0, span=.6)\n\nred_col <- rgb(1,0,0,.5)\nlines(xy0[,'x'], predict(reg_lo),\n    col=red_col, type='o', pch=2)\n```\n\n::: {.cell-output-display}\n![](03_20_Regressograms_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nMore formally, there are two ways to summarize gradients\n\n1. For all methods, including regressograms, you can approximate gradients with small finite differences. For some small difference $d$, we can manually compute\n\\begin{eqnarray}\n\\hat{b}_{1}(x) &=& \\frac{ \\hat{y}(x+\\frac{d}{2}) - \\hat{y}(x-\\frac{d}{2})}{d},\n\\end{eqnarray}\n\n2. When using split-sample regressions or local linear regressions, you can use the estimated slope coefficients $\\hat{b}_{1}(x)$ as gradient estimates in each direction.\n\nAfter computing gradients, you can summarize them in various plots\n\n* Histograms, Scatterplots\n* Plot of gradients and their CI's, \\cite{Chaudhuri1999, HendersonEtAl2012}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Gradients    \npred_lo <- predict(reg_lo)\ngrad_x  <- xy0[,'x'][-1]\ngrad_dx <- diff(xy0[,'x'])\ngrad_dy <- diff(pred_lo)\ngrad_lo <-grad_dy/grad_dx\n\n## Visual Summary\npar(mfrow=c(1,2))\nhist(grad_lo,  breaks=20,\n    border=NA, freq=F,\n    col=red_col,\n    xlab=expression(d~hat(y)/dx),\n    main='') ## Distributional Summary\nplot(grad_x+grad_dx, grad_lo,\n    xlab='x', ylab=expression(d~hat(y)/dx),\n    col=red_col, pch=16) ## Diminishing Returns?\n```\n\n::: {.cell-output-display}\n![](03_20_Regressograms_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nYou may also be interested in a particular gradient or a single summary statistic. For example, a bivariate regressogram can estimate the \"marginal effect at the mean\"; $\\hat{b}_{1}( x=\\hat{M}_{X} )$. More typically, you would be interested in the \"mean of the gradients\", sometimes said simply as \"average effect\", which averages the gradients over all datapoints in the dataset: $1/n \\sum_{i}^{n} \\hat{b}_{1}(x=\\hat{X}_{i})$. Alternatively, you may be interested in the median of the gradients, or measures of \"effect heterogeneity\": the interquartile range or standard deviation of the gradients. Such statistics are single numbers that can be presented in tabular form: \"mean gradient (sd gradient)\". You can alternative report standard errors: \"mean gradient (estimated se), sd gradient (estimated se)\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Tabular Summary\ntab_stats <- c(\n    M=mean(grad_lo, na.rm=T),\n    S=sd(grad_lo, na.rm=T))\ntab_stats\n##          M          S \n## 0.01554515 0.16296598\n\n## Use bootstrapping to get SE's\nboot_stats <- matrix(NA, nrow=299, ncol=2)\ncolnames(boot_stats) <- c('M se', 'S se')\nfor(b in 1:nrow(boot_stats)){\n    xy_b <- xy0[sample(1:nrow(xy0), replace=T),]\n    reg_lo <- loess(y~x, data=xy_b, span=.6)\n    pred_lo <- predict(reg_lo)\n    grad_lo <- diff(pred_lo)/diff(xy_b[,'x'])\n    dydx_mean <- mean(grad_lo, na.rm=T)\n    dydx_sd <- sd(grad_lo, na.rm=T)\n    boot_stats[b,1] <- dydx_mean\n    boot_stats[b,2] <- dydx_sd\n}\napply(boot_stats, 2, sd)\n##       M se       S se \n## 0.04722890 0.06431175\n```\n:::\n\n\n\n#### **Diminishing Returns**. {-}\n\nJust as before, there are diminishing returns to larger sample sizes for bivariate statistics. For example, the slope coefficient in simple OLS varies less from sample to sample when the samples are larger. Same for the gradients in loess. This decreased variability across samples makes hypothesis testing more accurate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nB <- 300\nNseq <- seq(10,50, by=1)\nSE <- sapply(Nseq, function(n){\n    sample_stat <- sapply(1:B, function(b){\n        x <- rnorm(n)\n        e <- rnorm(n)        \n        y <- x*3 + x + e\n        reg_lo <- loess(y~x)\n        pred_lo <- predict(reg_lo)\n        grad_lo <- diff(pred_lo)/diff(x)\n        dydx_mean <- mean(grad_lo, na.rm=T)\n        #dydx_sd <- sd(grad_lo, na.rm=T)\n        return(dydx_mean)\n    })\n    sd(sample_stat)\n})\n\nplot(Nseq, SE, pch=16, col=grey(0,.5),\n    main='Mean gradient', font.main=1,\n    ylab='standard error',\n    xlab='sample size')\n```\n\n::: {.cell-output-display}\n![](03_20_Regressograms_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\n\n",
    "supporting": [
      "03_20_Regressograms_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}