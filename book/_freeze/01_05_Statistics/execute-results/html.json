{
  "hash": "077f1b43849cfb0dbf821aa32829df5d",
  "result": {
    "engine": "knitr",
    "markdown": "# Statistics\n***\n\nWe often summarize distributions with *statistics*: functions of data. The most basic way to do this is with `summary`, whose values can all be calculated individually. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A random sample (real data)\nX <- USArrests[,'Murder']\nsummary(X)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   0.800   4.075   7.250   7.788  11.250  17.400\n\n# A random sample (computer simulation)\nX <-  runif(1000)\nsummary(X)\n##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n## 0.0005149 0.2460564 0.4984296 0.4958244 0.7629643 0.9989574\n\n# Another random sample (computer simulation)\nX <- rnorm(1000) \nsummary(X)\n##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n## -3.18524 -0.62375  0.10516  0.07157  0.75990  2.95815\n```\n:::\n\n\nTogether, the mean and variance statistics summarize the central tendency and dispersion of a distribution. In some special cases, such as with the normal distribution, they completely describe the distribution. Other distributions are better described with other statistics, either as an alternative or in addition to the mean and variance. After discussing those other statistics, we will return to the two most basic statistics in theoretical detail.\n\n## Mean and Variance\n\nThe mean and variance are the two most basic statistics that summarize the center and how spread apart the values are.\n\n#### **Mean**. {-}\nPerhaps the most common statistic is the mean, which is the [sum of all values] divided by [number of values];\n\\begin{eqnarray}\n\\bar{X}=\\frac{\\sum_{i=1}^{N}X_{i}}{N},\n\\end{eqnarray}\nwhere $X_{i}$ denotes the value of the $i$th observation.\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, a dataset of $\\{1,4,10\\}$ has a mean of $[1+4+10]/3=5$.\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- c(1,4,10)\nmean(X)\n## [1] 5\n```\n:::\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute the mean of a random sample\nX1 <- USArrests[,'Murder']\nX1_bar <- mean(X1)  #sum(x)/length(x)\nX1_bar\n## [1] 7.788\n\n# visualize on a histogram\nhist(X1, border=NA, main=NA)\nabline(v=X1_bar, col=2, lwd=2)\ntitle(paste0('mean= ', round(X1_bar,2)), font.main=1)\n```\n\n::: {.cell-output-display}\n![](01_05_Statistics_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n#### **Variance**.{-}\nPerhaps the second most common statistic is the variance: the average squared deviation from the mean\n$$V_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\bar{X}]^2}{N}.$$\nThe standard deviation is simply $s_{X} = \\sqrt{V_{X}}$, which can be interpreted as the average distance from the mean.\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, a dataset of $\\{1,4,10\\}$ has a mean of $[1+4+10]/3=5$. The variance is $[(1-5)^2+(4-5)^2+(10-5)^2]/3=14$ and the standard deviation is $\\sqrt{14}=3.742$.\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- c(1,4,10)\nx_bar <- mean(X)\nx_var <- mean( (X-x_bar)^2 )\nsqrt(x_var)\n## [1] 3.741657\n```\n:::\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX1_s <- sd(X1) # sqrt(var(X))\nhist(X1, border=NA, main=NA, freq=F)\nX1_s_lh <- c(X1_bar - X1_s,  X1_bar + X1_s)\nabline(v=X1_s_lh, col=4)\ntext(X1_s_lh, -.02,\n    c( expression(bar(X)-s[X]), expression(bar(X)+s[X])),\n    col=4, adj=0)\ntitle(paste0('sd= ', round(X1_s,2)), font.main=1)\n```\n\n::: {.cell-output-display}\n![](01_05_Statistics_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nNote that a \"corrected version\" is used by R and many statisticians: $V_{X}' =\\frac{\\sum_{i=1}^{N} [X_{i} - \\bar{X}]^2}{N-1}$ and $s_{X}' = \\sqrt{V_{X}'}$. \n\n::: {.cell}\n\n```{.r .cell-code}\nX <- c(1,4,10)\n\nx_bar <- mean(X)\nx_var <- mean( (X - x_bar)^2 )\nx_var\n## [1] 14\n\n# Corrected Version\nn <- length(X)  \nx_var2 <- sum((X - x_bar)^2 )/(n-1)\nx_var2\n## [1] 21\n\nvar(X) # R-Version\n## [1] 21\n```\n:::\n\n:::\n\n## Other Center/Spread Statistics\n\nA general rule of applied statistics is that there are multiple ways to measure something. Mean and Variance are measurements of Center and Spread, but there are others that have different theoretical properties and may be better suited for your dataset.\n\n#### **Medians and Absolute Deviations**. {-}\nWe can use the *Median* as a \"robust alternative\" to means that is especially useful for data with asymmetric distributions and extreme values. Recall that the $q$th quantile is the value where $q$ percent of the data are below and ($1-q$) percent are above. The median ($q=.5$) is the point where half of the data is lower values and the other half is higher. This means that median is not sensitive to extreme values (whereas the mean is).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- rgeom(50, .4)\nX\n##  [1] 0 0 1 1 0 2 1 0 3 1 0 0 3 1 0 3 1 3 6 1 1 1 3 1 0 0 1 4 3 0 2 1 0 1 0 0 0 2\n## [39] 1 1 0 0 3 0 0 2 1 3 0 3\n\nproportions <- table(X)/length(X)\nplot(proportions, ylab='proportion')\n```\n\n::: {.cell-output-display}\n![](01_05_Statistics_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nExamine robustness to an extreme value\n\n::: {.cell}\n\n```{.r .cell-code}\n\nX_extreme <- c(X, 1000) # add one extreme value\n#par(mfrow=c(1,2)) # visualize side-by-side\n#hist(X)\n#hist(X_extreme)\n\n\n# Which measures of central tendency are robust\n# to a single extreme value?\nmean(X)\n## [1] 1.22\nmean( X_extreme )\n## [1] 20.80392\n\nquantile(X, prob=0.5)\n## 50% \n##   1\nquantile(X_extreme, prob=0.5)\n## 50% \n##   1\n```\n:::\n\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nJust like weighted means, we can also compute weighted quantiles\n\n::: {.cell}\n\n```{.r .cell-code}\nweighted.quantile <- function(x, w, probs){\n    #See spatstat.univar::weighted.quantile\n    oo <- order(x)\n    x <- x[oo]\n    w <- w[oo]\n    Fx <- cumsum(w)/sum(w)\n    quantile_id <- max(which(Fx <= probs))+1\n    xq <- x[quantile_id] \n    return(xq)\n    \n}\n\n## Unweighted\nquantile(X, probs=.5)\n## 50% \n##   1\nweights <- rep(1, length(X))\nweighted.quantile(x=X, w=weights, probs=.5)\n## [1] 1\n\n## Weighted\nweights <- seq(X)\nweights <- weights/sum(weights) # normalize\nweighted.quantile(x=X, w=weights, probs=.5)\n## [1] 1\n```\n:::\n\n:::\n\nWe can also use the *Interquartile Range* or *Median Absolute Deviation* as an alternative to variance. The difference between the first and third quartiles ($q=.25$ and $q=.75$) measure is range of the middle $50%$ of the data, which is how the boxplot measures \"spread\". The median absolute deviation is another statistic that also measures \"spread\"\n\\begin{eqnarray}\n\\tilde{X} &=& Med(X_{i}) \\\\\nMAD_{X} &=& Med\\left( | X_{i} - \\tilde{X} | \\right).\n\\end{eqnarray}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#An alternative to sd(X)\nmad(X, constant=1) # median( abs(X - median(X)) )\n## [1] 1\n\n# Another alternative, used in the boxplot\nIQR(X) # diff( quantile(x, probs=c(.25,.75)))\n## [1] 2\n```\n:::\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nExamine robustness to an extreme value\n\n::: {.cell}\n\n```{.r .cell-code}\nsd(X)\n## [1] 1.359622\nsd(X_extreme)\n## [1] 139.8637\n\nmad(X, constant=1)\n## [1] 1\nmad(X_extreme, constant=1)\n## [1] 1\n\nIQR(X)\n## [1] 2\nIQR(X_extreme)\n## [1] 2\n```\n:::\n\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nNote that there other \"absolute deviation\" statistics\n\n::: {.cell}\n\n```{.r .cell-code}\n# sometimes seen elsewhere\nmean( abs(X - mean(X)) )\nmean( abs(X - median(X)) )\nmedian( abs(X - mean(X)) )\n```\n:::\n\n:::\n\n#### **Mode and Share Concentration**. {-}\nSometimes, none of the above work well. With categorical data, for example, distributions are easier to describe with other statistics. The mode is the most common observation: the value with the highest observed frequency. We can also measure the spread of the frequencies or concentration at the mode vs elsewhere.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Draw 3 Random Letters\nK <- length(LETTERS)\nX_id <- rmultinom(3, 1, prob=rep(1/K,K))\nX_id\n##       [,1] [,2] [,3]\n##  [1,]    0    0    0\n##  [2,]    0    0    0\n##  [3,]    0    0    0\n##  [4,]    0    0    0\n##  [5,]    0    0    0\n##  [6,]    0    1    0\n##  [7,]    0    0    0\n##  [8,]    0    0    0\n##  [9,]    0    0    1\n## [10,]    1    0    0\n## [11,]    0    0    0\n## [12,]    0    0    0\n## [13,]    0    0    0\n## [14,]    0    0    0\n## [15,]    0    0    0\n## [16,]    0    0    0\n## [17,]    0    0    0\n## [18,]    0    0    0\n## [19,]    0    0    0\n## [20,]    0    0    0\n## [21,]    0    0    0\n## [22,]    0    0    0\n## [23,]    0    0    0\n## [24,]    0    0    0\n## [25,]    0    0    0\n## [26,]    0    0    0\n\n# Draw Random Letters 100 Times\nX_id <- rowSums(rmultinom(100, 1, prob=rep(1/K,K)))\nX <- lapply(1:K, function(k){\n    rep(LETTERS[k], X_id[k])\n})\nX <- factor(unlist(X), levels=LETTERS)\n\nproportions <- table(X)/length(X)\nplot(proportions)\n```\n\n::: {.cell-output-display}\n![](01_05_Statistics_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# mode(s)\nnames(proportions)[proportions==max(proportions)]\n## [1] \"B\" \"C\" \"G\" \"M\" \"U\"\n\n# freq. spreads\nsd(proportions)\n## [1] 0.01641763\nsum(proportions^2)\n## [1] 0.0452\n# freq. concentration at mode\nmax(proportions)/mean(proportions)\n## [1] 1.56\n```\n:::\n\n\n## Shape Statistics\n\nCentral tendency and dispersion are often insufficient to describe a distribution. To further describe shape, we can compute skew and kurtosis to measure asymmetry and extreme values. There are many other statistics we could compute on an ad-hoc basis.\n\n#### **Skewness**. {-}\nThis captures how symmetric the distribution is.\n$$W_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\bar{X}]^3 / N}{ [s_{X}]^3 }$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- rweibull(1000, shape=1)\nhist(X, border=NA, main=NA, freq=F, breaks=20)\n```\n\n::: {.cell-output-display}\n![](01_05_Statistics_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\nskewness <-  function(X){\n X_bar <- mean(X)\n m3 <- mean((X - X_bar)^3)\n s3 <- sd(X)^3\n skew <- m3/s3\n return(skew)\n}\n\nskewness( rweibull(1000, shape=1))\n## [1] 2.090984\nskewness( rweibull(1000, shape=10) )\n## [1] -0.6510462\n```\n:::\n\n\n#### **Kurtosis**. {-}\nThis captures how many \"outliers\" there are.\n$$K_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\bar{X}]^4 / N}{ [s_{X}]^4 }.$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- rweibull(1000, shape=1)\nboxplot(X, main=NA)\n```\n\n::: {.cell-output-display}\n![](01_05_Statistics_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\nkurtosis <- function(X){  \n X_bar <- mean(X)\n m4 <- mean((X - X_bar)^4) \n s4 <- sd(X)^4\n kurt <- m4/s4 - 3\n return(kurt)\n}\n\nkurtosis( rweibull(1000, shape=1) )\n## [1] 4.111567\nkurtosis( rweibull(1000, shape=10) )\n## [1] 0.639655\n```\n:::\n\n\n#### **Clusters/Gaps**. {-}\nYou can also describe distributions in terms of how clustered the values are, including the number of modes and many other statistics. But remember that \"a picture is worth a thousand words\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Number of Modes\nX <- rbeta(1000, .6, .6)\nhist(X, border=NA, main=NA, freq=F, breaks=20)\n```\n\n::: {.cell-output-display}\n![](01_05_Statistics_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random Number Generator \nr_ugly1 <- function(n, theta1=c(-8,-1), theta2=c(-2,2), rho=.25){\n    omega   <- rbinom(n, size=1, rho)\n    epsilon <- omega * runif(n, theta1[1], theta2[1]) +\n        (1-omega) * rnorm(n, theta1[2], theta2[2])\n    return(epsilon)\n}\n# Large Sample\npar(mfrow=c(1,1))\nx <- seq(-12,6,by=.001)\nrX <- r_ugly1(1000000)\nhist(rX, breaks=1000,  freq=F, border=NA,\n    xlab=\"x\", main='')\n```\n\n::: {.cell-output-display}\n![](01_05_Statistics_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Show True Density\nd_ugly1 <- function(x, theta1=c(-8,-1), theta2=c(-2,2), rho=.25){\n    rho     * dunif(x, theta1[1], theta2[1]) +\n    (1-rho) * dnorm(x, theta1[2], theta2[2]) }\ndx <- d_ugly1(x)\nlines(x, dx, col=1)\n```\n:::\n\n\n\n\n## Probability Theory\n\nYou were already introduced to this with (random variables)[https://jadamso.github.io/Rbooks/random-variables.html] and probability distributions. In this section, we will dig a little deeper theoretically into the statistics we are most likely to use in practice.\n\nThe mean and variance are probably the two most basic statistics we might compute, and are often used. To understand them theoretically, we separately analyze how they are computed for discrete and continuous random variables.\n\n#### **Discrete Random Variables**. {-} \nIf the sample space is discrete, we can compute the theoretical mean (or expected value) as\n$$\n\\mathbb{E}[X_{i}] = \\sum_{x} x Prob(X_{i}=x),\n$$\nwhere $Prob(X_{i}=x)$ is the probability the random variable $X_{i}$ takes the particular value $x$. Similarly, we can compute the theoretical variance as\n$$\n\\mathbb{V}[X_{i}] = \\sum_{x} \\left(x - \\mathbb{E}[X_{i}] \\right)^2 Prob(X_{i}=x).\n$$\nThe standard deviation is $\\sqrt{\\mathbb{V}[X_{i}]}$.\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, consider an unfair coin with a $.75$ probability of heads ($x=1$) and a $.25$ probability of tails ($x=0$) has a theoretical mean of \n$$\n\\mathbb{E}[X_{i}] = 1\\times.75 + 0 \\times .25 = .75\n$$\nand a theoretical variance of \n$$\n\\mathbb{V}[X_{i}] = [1 - .75]^2 \\times.75 + [0 - .75]^2 \\times.25 = 0.1875\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A simulation of many coin flips\nx <- c(0,1)\nx_probs <- c(1/2, 1/2)\nX <- sample(x, 400, prob=x_probs, replace=T)\n\nround( mean(X), 4)\n## [1] 0.475\n\nround( var(X), 4)\n## [1] 0.25\n```\n:::\n\n:::\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nConsider a six-sided die with equal probability of landing on each side. What is the mean?\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manual Way\n1*1/6 + 2*1/6 + 3*1/6 + 4*1/6 + 5*1/6 + 6*1/6\n## [1] 3.5\n\n# Computerized Way\nx <- c(1,2,3,4,5,6)\nx_probs <- c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6)\nx_bar <- sum(x*x_probs)\nx_bar\n## [1] 3.5\n\n# Verified by simulation\nXsim <- sample(x, prob=x_probs,\n          size=10000, replace=T)\nmean(Xsim)\n## [1] 3.5133\n```\n:::\n\n\nWhat is the standard deviation?\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manual Way\nXvar <- (1-x_bar)^2*1/6 +\n  (2-x_bar)^2*1/6 +\n  (3-x_bar)^2*1/6 +\n  (4-x_bar)^2*1/6 +\n  (5-x_bar)^2*1/6 + \n  (6-x_bar)^2*1/6\nXsd <- sqrt(Xvar)\nXsd\n## [1] 1.707825\n\n# Verified by simulation\nsd(Xsim)\n## [1] 1.709303\n```\n:::\n\n:::\n\n#### **Weighted Data**. {-} \nSometimes, you may have a dataset of values and probability weights. Othertimes, you can calculate them yourself. In either case, you can explicitly do the computations for discrete data. Given data on unique outcome $x$ and their frequency $\\widehat{p}_{x}=\\sum_{i=1}^{N}\\mathbf{1}\\left(X_{i}=x\\right)/N$, we compute \n\\begin{eqnarray}\n\\bar{X} &=& \\sum_{x} x \\widehat{p}_{x}.\n\\end{eqnarray}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute probability weights for unique values\nP  <- table(X) #table of counts\np <- c(P)/length(X) #frequencies (must sum to 1)\nx <- as.numeric(names(p)) #unique values\ncbind(x,p)\n##   x     p\n## 0 0 0.525\n## 1 1 0.475\n\n# Compute Mean\nX_mean <- sum(x*p)\nX_mean\n## [1] 0.475\n```\n:::\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nTry computing the mean both ways for another random sample\n\n::: {.cell}\n\n```{.r .cell-code}\nX  <-  sample(c(0,1,2), 1000, replace=T)\n\n# First Way (Computerized)\nmean(X)\n## [1] 0.955\n\n# Second Way (Explicit Math)\n# start with a table of counts like the previous example\n```\n:::\n\n:::\n\nThis idea generalizes to a *weighted mean*: an average where different values contribute to the final result with varying levels of importance. For each outcome $x$ we have a weight $W_{x}$ and compute\n\\begin{eqnarray}\n\\bar{X} &=& \\frac{\\sum_{x} x W_{x}}{\\sum_{x} W_{x}} = \\sum_{x} x w_{x},\n\\end{eqnarray}\nwhere $w_{x}=\\frac{W_{x}}{\\sum_{x'} W_{x'}}$ is normalized version of $W_{x}$ that implies $\\sum_{x}w_{x}=1$.\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, suppose a student has these scores\n\n::: {.cell}\n\n```{.r .cell-code}\nHomework1 <- c(score=88, weight=0.25)\nHomework2 <- c(score=92, weight=0.25)\nExam1 <- c(score=67, weight=0.2)\nExam2 <- c(score=90, weight=0.3)\n\nGrades <- rbind(Homework1, Homework2, Exam1, Exam2)\nGrades\n##           score weight\n## Homework1    88   0.25\n## Homework2    92   0.25\n## Exam1        67   0.20\n## Exam2        90   0.30\n```\n:::\n\n\nWe can compute the final grade as a Weighted Mean\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manual Way\n88*0.25 + 92*0.25 + 67*0.2 + 90*0.3\n## [1] 85.4\n\n# Computerized Way\nValues <- Grades[,'score'] * Grades[,'weight']\nFinalGrade <- sum(Values)\nFinalGrade\n## [1] 85.4\n```\n:::\n\n:::\n\nNote that if there are $K$ unique outcomes and $W_{x}=1$ then $\\sum_{x}W_{x}=K$ and $w_{x}=1/K$. This means $\\bar{X} = \\sum_{x} x w_{x} = \\sum_{x} x /K$, which is just a simple mean.\n\n:::{.callout-tip icon=false collapse=\"false\"}\nBonus: provide an example of computing a weighted variance building on this code below\n\n::: {.cell}\n\n```{.r .cell-code}\nx_diff <- (x - x_mean)^2\np <- P/sum(P)\nx_var <- sum(p * x_diff)\n```\n:::\n\n:::\n\n\n#### **Continuous Random Variables**. {-}\n<details>\n<summary> advanced and optional </summary>\n  <p>\nIf the sample space is continuous, we can compute the theoretical mean (or expected value) as\n$$\n\\mathbb{E}[X] = \\int x f(x) d x,\n$$\nwhere $f(x)$ is the probability the random variable takes the particular value $x$. Similarly, we can compute the theoretical variance as\n$$\n\\mathbb{V}[X_{i}]= \\int \\left(x - \\mathbb{E}[X_{i}] \\right)^2 f(x) d x,\n$$\n\nFor example, consider a random variable with a continuous uniform distribution over [-1, 1]. In this case, $f(x)=1/[1 - (-1)]=1/2$ for each $x$ in  [-1, 1] and \n$$\n\\mathbb{E}[X_{i}] = \\int_{-1}^{1} \\frac{x}{2} d x = \\int_{-1}^{0} \\frac{x}{2} d x + \\int_{0}^{1} \\frac{x}{2} d x = 0\n$$\nand \n$$\n\\mathbb{V}[X_{i}]= \\int_{-1}^{1} x^2 \\frac{1}{2} d x = \\frac{1}{2} \\frac{x^3}{3}|_{-1}^{1} = \\frac{1}{6}[1 - (-1)] = 2/6 =1/3\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- USArrests[,'Murder']\nround( mean(X), 4)\n## [1] 7.788\nround( var(X), 4)\n## [1] 18.9705\n```\n:::\n\n:::{.callout-tip icon=false collapse=\"false\"}\nYou can again explicitly do the computations with weighted data, but here we have an additional approximation error\n\n::: {.cell}\n\n```{.r .cell-code}\n# values and probabilities\nh  <- hist(X, plot=F)\nwt <- h[['counts']]/length(X) \nxt <- h[['mids']]\n# Weighted mean\nX_mean <- sum(wt*xt)\nX_mean\n## [1] 7.8\n\n# Compare to \"mean(x)\"\n```\n:::\n\n\nTry it yourself with \n\n::: {.cell}\n\n```{.r .cell-code}\nX <- runif(2000, -1, 1)\n```\n:::\n\n:::\n\n  </p>\n</details>\n\n\n## Further Reading\n\nProbability Theory\n\n* [Refresher] <https://www.khanacademy.org/math/statistics-probability/probability-library/basic-theoretical-probability/a/probability-the-basics>\n* <https://book.stat420.org/probability-and-statistics-in-r.html>\n* <https://bookdown.org/speegled/foundations-of-statistics/>\n* <https://math.dartmouth.edu/~prob/prob/prob.pdf>\n* <https://bookdown.org/probability/beta/discrete-random-variables.html>\n* <https://www.econometrics-with-r.org/2.1-random-variables-and-probability-distributions.html>\n* <https://probability4datascience.com/ch02.html>\n* <https://statsthinking21.github.io/statsthinking21-R-site/probability-in-r-with-lucy-king.html>\n* <https://bookdown.org/probability/statistics/>\n* <https://www.atmos.albany.edu/facstaff/timm/ATM315spring14/R/IPSUR.pdf>\n* <https://rc2e.com/probability>\n* <https://bookdown.org/probability/beta/>\n* <https://bookdown.org/a_shaker/STM1001_Topic_3/>\n* <https://bookdown.org/fsancier/bookdown-demo/>\n* <https://bookdown.org/kevin_davisross/probsim-book/>\n* <https://bookdown.org/machar1991/ITER/2-pt.html>\n* <https://www.atmos.albany.edu/facstaff/timm/ATM315spring14/R/IPSUR.pdf>\n* <https://math.dartmouth.edu/~prob/prob/prob.pdf>\n\nFor weighted statistics, see\n\n* <https://seismo.berkeley.edu/~kirchner/Toolkits/Toolkit_12.pdf>\n* <https://www.bookdown.org/rwnahhas/RMPH/survey-desc.html>\n\n",
    "supporting": [
      "01_05_Statistics_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}