{
  "hash": "3907b3feb86ac7aec78f26786082a7d7",
  "result": {
    "engine": "knitr",
    "markdown": "\n# Statistics\n***\n\nWe often summarize distributions with *statistics*: functions of data. The most basic way to do this is with `summary`, whose values can all be calculated individually. (E.g., the \"mean\" computes the [sum of all values] divided by [number of values].) There are *many* other statistics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary( runif(1000))\n##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n## 0.0001567 0.2566212 0.5050232 0.5043039 0.7467155 0.9995065\nsummary( rnorm(1000) )\n##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n## -2.785726 -0.636147  0.005909  0.005797  0.632699  3.001677\n```\n:::\n\n\n## Mean and Variance\n\nThe most basic statistics summarize the center of a distribution and how far apart the values are spread.\n\n#### **Mean**. {-}\nPerhaps the most common statistic is the mean;\n$$\\overline{X}=\\frac{\\sum_{i=1}^{N}X_{i}}{N},$$ where $X_{i}$ denotes the value of the $i$th observation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute the mean of a random sample\nx <- runif(100)\nhist(x, border=NA, main=NA)\nm <- mean(x)  #sum(x)/length(x)\nabline(v=m, col=2, lwd=2)\ntitle(paste0('mean= ', round(m,2)), font.main=1)\n```\n\n::: {.cell-output-display}\n![](01_05_Statistics_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n#### **Variance**.{-}\nPerhaps the second most common statistic is the variance: the average squared deviation from the mean\n$$V_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\overline{X}]^2}{N}.$$\nThe standard deviation is simply $s_{X} = \\sqrt{V_{X}}$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns <- sd(x) # sqrt(var(x))\nhist(x, border=NA, main=NA, freq=F)\ns_lh <- c(m - s,  m + s)\nabline(v=s_lh, col=4)\ntext(s_lh, -.02,\n    c( expression(bar(X)-s[X]), expression(bar(X)+s[X])),\n    col=4, adj=0)\ntitle(paste0('sd= ', round(s,2)), font.main=1)\n```\n\n::: {.cell-output-display}\n![](01_05_Statistics_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nNote that a \"corrected version\" is used by R and many statisticians: $V_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\overline{X}]^2}{N-1}$.\n\n::: {.cell}\n\n```{.r .cell-code}\nvar(x)\n## [1] 0.0798367\nmean( (x - mean(x))^2 )\n## [1] 0.07903833\n```\n:::\n\n\nTogether, these statistics summarize the central tendency and dispersion of a distribution. In some special cases, such as with the normal distribution, they completely describe the distribution. Other distributions are easier to describe with other statistics.\n\n\n## Other Center/Spread Statistics\n\n#### **Absolute Deviations**. {-}\nWe can use the *Median* as a \"robust alternative\" to means. Recall that the $q$th quantile is the value where $q$ percent of the data are below and ($1-q$) percent are above. The median ($q=.5$) is the point where half of the data is lower values and the other half is higher.\n\n\nWe can also use the *Interquartile Range* or *Median Absolute Deviation* as an alternative to variance. The first and third quartiles ($q=.25$ and $q=.75$) together measure is the middle 50 percent of the data. The size of that range (interquartile range: the difference between the quartiles) represents \"spread\" or \"dispersion\" of the data. The median absolute deviation also measures spread\n$$\n\\tilde{X} = Med(X_{i}) \\\\\nMAD_{X} = Med\\left( | X_{i} - \\tilde{X} | \\right).\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rgeom(50, .4)\nx\n##  [1]  4 14  2  0  3  4  0  3  0  0  5  0  0  0  1  0  2  1  2  1  0  1  0 10  4\n## [26]  0  0  1  0  1  0  0  4  0  0  5  3  0  2  1  1  0  0  2  0  0  0  0  2  1\n\nplot(table(x))\n```\n\n::: {.cell-output-display}\n![](01_05_Statistics_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n#mean(x)\nmedian(x)\n## [1] 1\n\n#sd(x)\n#IQR(x) # diff( quantile(x, probs=c(.25,.75)))\nmad(x, constant=1) # median( abs(x - median(x)) )\n## [1] 1\n```\n:::\n\n\nNote that there other absolute deviations:\n\n::: {.cell}\n\n```{.r .cell-code}\nmean( abs(x - mean(x)) )\nmean( abs(x - median(x)) )\nmedian( abs(x - mean(x)) )\n```\n:::\n\n\n#### **Mode and Share Concentration**. {-}\nSometimes, none of the above work well. With categorical data, for example, distributions are easier to describe with other statistics. The mode is the most common observation: the value with the highest observed frequency. We can also measure the spread/dispersion of the frequencies, or compare the highest frequency to the average frequency to measure concentration at the mode.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Draw 3 Random Letters\nK <- length(LETTERS)\nx_id <- rmultinom(3, 1, prob=rep(1/K,K))\nx_id\n##       [,1] [,2] [,3]\n##  [1,]    0    0    0\n##  [2,]    0    0    0\n##  [3,]    0    0    0\n##  [4,]    0    0    0\n##  [5,]    1    0    0\n##  [6,]    0    0    0\n##  [7,]    0    0    0\n##  [8,]    0    0    0\n##  [9,]    0    0    0\n## [10,]    0    1    0\n## [11,]    0    0    0\n## [12,]    0    0    0\n## [13,]    0    0    0\n## [14,]    0    0    0\n## [15,]    0    0    0\n## [16,]    0    0    0\n## [17,]    0    0    0\n## [18,]    0    0    1\n## [19,]    0    0    0\n## [20,]    0    0    0\n## [21,]    0    0    0\n## [22,]    0    0    0\n## [23,]    0    0    0\n## [24,]    0    0    0\n## [25,]    0    0    0\n## [26,]    0    0    0\n\n# Draw Random Letters 100 Times\nx_id <- rowSums(rmultinom(100, 1, prob=rep(1/K,K)))\nx <- lapply(1:K, function(k){\n    rep(LETTERS[k], x_id[k])\n})\nx <- factor(unlist(x), levels=LETTERS)\n\nplot(x)\n```\n\n::: {.cell-output-display}\n![](01_05_Statistics_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\ntx <- table(x)\n# mode(s)\nnames(tx)[tx==max(tx)]\n## [1] \"O\"\n\n# freq. spread\nsx <- tx/sum(tx)\nsd(sx) # mad(sx)\n## [1] 0.02148345\n\n# freq. concentration \nmax(tx)/mean(tx)\n## [1] 2.08\n```\n:::\n\n\n## Shape Statistics\n\nCentral tendency and dispersion are often insufficient to describe a distribution. To further describe shape, we can compute the \"standard moments\" skew and kurtosis, as well as other statistics.\n\n\n#### **Skewness**. {-}\nThis captures how symmetric the distribution is.\n$$W_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\overline{X}]^3 / N}{ [s_{X}]^3 }$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rweibull(1000, shape=1)\nhist(x, border=NA, main=NA, freq=F, breaks=20)\n```\n\n::: {.cell-output-display}\n![](01_05_Statistics_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\nskewness <-  function(x) {\n x_bar <- mean(x)\n m3 <- mean((x - x_bar)^3)\n skew <- m3/(sd(x)^3)\n return(skew)\n}\n\nskewness( rweibull(1000, shape=1))\n## [1] 2.359216\nskewness( rweibull(1000, shape=10) )\n## [1] -0.625527\n```\n:::\n\n\n#### **Kurtosis**. {-}\nThis captures how many \"outliers\" there are.\n$$K_{X} =\\frac{\\sum_{i=1}^{N} [X_{i} - \\overline{X}]^4 / N}{ [s_{X}]^4 }.$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rweibull(1000, shape=1)\nboxplot(x, main=NA)\n```\n\n::: {.cell-output-display}\n![](01_05_Statistics_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\nkurtosis <- function(x) {  \n x_bar <- mean(x)\n m4 <- mean((x - x_bar)^4) \n kurt <- m4/(sd(x)^4) - 3  \n return(kurt)\n}\n\nkurtosis( rweibull(1000, shape=1))\n## [1] 4.868291\nkurtosis( rweibull(1000, shape=10) )\n## [1] 0.7134826\n```\n:::\n\n\n#### **Clusters/Gaps**. {-}\nYou can also describe distributions in terms of how clustered the values are. Remember: *a picture is worth a thousand words*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Number of Modes\nx <- rbeta(1000, .6, .6)\nhist(x, border=NA, main=NA, freq=F, breaks=20)\n```\n\n::: {.cell-output-display}\n![](01_05_Statistics_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Random Number Generator \nr_ugly1 <- function(n, theta1=c(-8,-1), theta2=c(-2,2), rho=.25){\n    omega   <- rbinom(n, size=1, rho)\n    epsilon <- omega * runif(n, theta1[1], theta2[1]) +\n        (1-omega) * rnorm(n, theta1[2], theta2[2])\n    return(epsilon)\n}\n# Large Sample\npar(mfrow=c(1,1))\nX <- seq(-12,6,by=.001)\nrx <- r_ugly1(1000000)\nhist(rx, breaks=1000,  freq=F, border=NA,\n    xlab=\"x\", main='')\n```\n\n::: {.cell-output-display}\n![](01_05_Statistics_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Show True Density\nd_ugly1 <- function(x, theta1=c(-8,-1), theta2=c(-2,2), rho=.25){\n    rho     * dunif(x, theta1[1], theta2[1]) +\n    (1-rho) * dnorm(x, theta1[2], theta2[2]) }\ndx <- d_ugly1(X)\nlines(X, dx, col=1)\n```\n:::\n\n\n## Probability Theory\n\nYou were already introduced to this with [https://jadamso.github.io/Rbooks/random-variables.html](random variables) and probability distributions. In this section, we will dig a little deeper theoretically into the statistics we are most likely to use in practice.\n\nThe mean and variance are probably the two most basic statistics we might compute, and are often used. To understand them theoretically, we separately analyze how they are computed for discrete and continuous random variables.\n\n#### **Discrete**. {-} \nIf the sample space is discrete, we can compute the theoretical mean (or expected value) as\n$$\n\\mu = \\sum_{i} x_{i} Prob(X=x_{i}),\n$$\nwhere $Prob(X=x_{i})$ is the probability the random variable $X$ takes the particular value $x_{i}$. Similarly, we can compute the theoretical variance as\n$$\n\\sigma^2 = \\sum_{i} [x_{i} - \\mu]^2 Prob(X=x_{i}),\n$$\n\n***Example***. Consider an unfair coin with a $.75$ probability of heads ($x_{i}=1$) and a $.25$ probability of tails ($x_{i}=0$) has a theoretical mean of \n$$\n\\mu = 1\\times.75 + 0 \\times .25 = .75\n$$\nand a theoretical variance of \n$$\n\\sigma^2 = [1 - .75]^2 \\times.75 + [0 - .75]^2 \\times.25 = 0.1875\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rbinom(10000, size=1, prob=.75)\n\nround( mean(x), 4)\n## [1] 0.7454\n\nround( var(x), 4)\n## [1] 0.1898\n```\n:::\n\n\n***Weighted Data***.\nSometimes, you may have a dataset of values and probability weights. Othertimes, you can calculate them yourself. In either case, you can explicitly do the computations \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute probability weights for unique values\nh  <- table(x) #table of counts\nwt <- c(h)/length(x) #probabilities (must sum to 1)\nxt <- as.numeric(names(h)) #values\n# Weighted Mean\nxm <- sum(wt*xt)\nxm\n## [1] 0.7454\n```\n:::\n\n\nTry computing the mean both ways for another random sample\n\n::: {.cell}\n\n```{.r .cell-code}\nx  <-  sample(c(0,1,2), 1000, replace=T)\n```\n:::\n\nTry also computing a weighted variance\n\n::: {.cell}\n\n```{.r .cell-code}\n# xv <- sum(wt * (x - xm)^2)/sum(wt)\n```\n:::\n\n\n#### **Continuous**. {-}\nIf the sample space is continuous, we can compute the theoretical mean (or expected value) as\n$$\n\\mu = \\int x f(x) d x,\n$$\nwhere $f(x)$ is the probability the random variable takes the particular value $x$. Similarly, we can compute the theoretical variance as\n$$\n\\sigma^2 = \\int [x - \\mu]^2 f(x) d x,\n$$\n\n***Example***. Consider a random variable with a continuous uniform distribution over [-1, 1]. In this case, $f(x)=1/[1 - (-1)]=1/2$ for each $x$ in  [-1, 1] and \n$$\n\\mu = \\int_{-1}^{1} \\frac{x}{2} d x = \\int_{-1}^{0} \\frac{x}{2} d x + \\int_{0}^{1} \\frac{x}{2} d x = 0\n$$\nand \n$$\n\\sigma^2 = \\int_{-1}^{1} x^2 \\frac{1}{2} d x = \\frac{1}{2} \\frac{x^3}{3}|_{-1}^{1} = \\frac{1}{6}[1 - (-1)] = 2/6 =1/3\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- runif(10000, -1,1)\nround( mean(x), 4)\n## [1] -0.0011\nround( var(x), 4)\n## [1] 0.3276\n```\n:::\n\n\n***Weighted Data***.\nYou can again explicitly do the computations with weighted data, but here we have an additional approximation error\n\n::: {.cell}\n\n```{.r .cell-code}\n# values and probabilities\nh  <- hist(x, plot=F)\nwt <- h$counts/length(x) \nxt <- h$mids\n# Weighted mean\nxm <- sum(wt*xt)\nxm\n## [1] -0.00097\n\n# Compare to \"mean(x)\"\n```\n:::\n\n\n\n## Further Reading\n\nProbability Theory\n\n* [Refresher] https://www.khanacademy.org/math/statistics-probability/probability-library/basic-theoretical-probability/a/probability-the-basics\n* https://book.stat420.org/probability-and-statistics-in-r.html\n* https://bookdown.org/speegled/foundations-of-statistics/\n* https://math.dartmouth.edu/~prob/prob/prob.pdf\n* https://bookdown.org/probability/beta/discrete-random-variables.html\n* https://www.econometrics-with-r.org/2.1-random-variables-and-probability-distributions.html\n* https://probability4datascience.com/ch02.html\n* https://statsthinking21.github.io/statsthinking21-R-site/probability-in-r-with-lucy-king.html\n* https://bookdown.org/probability/statistics/\n* https://www.atmos.albany.edu/facstaff/timm/ATM315spring14/R/IPSUR.pdf\n* https://rc2e.com/probability\n* https://bookdown.org/probability/beta/\n* https://bookdown.org/a_shaker/STM1001_Topic_3/\n* https://bookdown.org/fsancier/bookdown-demo/\n* https://bookdown.org/kevin_davisross/probsim-book/\n* https://bookdown.org/machar1991/ITER/2-pt.html\n* https://www.atmos.albany.edu/facstaff/timm/ATM315spring14/R/IPSUR.pdf\n* https://math.dartmouth.edu/~prob/prob/prob.pdf\n\nFor weighted statistics, see\n\n* https://seismo.berkeley.edu/~kirchner/Toolkits/Toolkit_12.pdf\n* https://www.bookdown.org/rwnahhas/RMPH/survey-desc.html\n\nNote that many random variables are related to each other\n\n* https://en.wikipedia.org/wiki/Relationships_among_probability_distributions\n* https://www.math.wm.edu/~leemis/chart/UDR/UDR.html\n* https://qiangbo-workspace.oss-cn-shanghai.aliyuncs.com/2018-11-11-common-probability-distributions/distab.pdf\n\nAlso note that numbers randomly generated on your computer cannot be truly random, they are \"Pseudorandom\".\n\n\n\n",
    "supporting": [
      "01_05_Statistics_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}