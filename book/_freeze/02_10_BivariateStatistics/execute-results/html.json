{
  "hash": "3968e04d8558441627456d6489a65526",
  "result": {
    "engine": "knitr",
    "markdown": "\n# Associations\n***\n\nWe will now study two variables. The data for each observation data can be grouped together as a vector $(\\hat{X}_{i}, \\hat{Y}_{i})$.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bivariate Data from USArrests\nxy <- USArrests[,c('Murder','UrbanPop')]\nxy[1,]\n##         Murder UrbanPop\n## Alabama   13.2       58\n```\n:::\n\n\n\n## Joint Distributions\n\n\n#### **Discrete Data**.{-}\n\nWe have multiple observations of $(\\hat{X}_{i}, \\hat{Y}_{i})$, each of which corresponds to a unique value $(x,y)$. A frequency table shows how often each combination of values appear. The *joint distribution* counts up the number of pairs with the same values and divides by the number of obersvations, $n$;\n\\begin{eqnarray}\n\\begin{eqnarray}\n\\hat{p}_{xy} = \\sum_{i=1}^{n}\\mathbf{1}\\left( \\hat{X}_{i}=x \\& \\hat{Y}_{i}=y \\right)/n,\n\\end{eqnarray}\nThe *marginal distributions* are just the univariate information, which can be computed independantly as in <https://jadamso.github.io/Rbooks/01_06_PopStatistics.html#estimates> or from the joint distribution\n\\begin{eqnarray}\n\\hat{p}_{x} = \\sum_{y} \\hat{p}_{xy}\n\\end{eqnarray}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wooldridge)\n\n# Highschool Students, Sex and Education Attainment\ndat <- wage1[,c('educ','female')]\ndat <- dat[ dat[,'educ'] >=10,]\n\ntab <- table(dat) ## counts\ntab\n##     female\n## educ   0   1\n##   10  13  17\n##   11  17  12\n##   12  85 113\n##   13  14  25\n##   14  31  22\n##   15  12   9\n##   16  45  23\n##   17  10   2\n##   18  13   6\n\nprop <- tab/sum(tab) ## frequencies\nround(prop,3)\n##     female\n## educ     0     1\n##   10 0.028 0.036\n##   11 0.036 0.026\n##   12 0.181 0.241\n##   13 0.030 0.053\n##   14 0.066 0.047\n##   15 0.026 0.019\n##   16 0.096 0.049\n##   17 0.021 0.004\n##   18 0.028 0.013\n\nprop_full <- addmargins(prop) ## column and row sums\nround(prop_full,3)\n##      female\n## educ      0     1   Sum\n##   10  0.028 0.036 0.064\n##   11  0.036 0.026 0.062\n##   12  0.181 0.241 0.422\n##   13  0.030 0.053 0.083\n##   14  0.066 0.047 0.113\n##   15  0.026 0.019 0.045\n##   16  0.096 0.049 0.145\n##   17  0.021 0.004 0.026\n##   18  0.028 0.013 0.041\n##   Sum 0.512 0.488 1.000\n```\n:::\n\n\n#### **Continuous Data**.{-}\n\nScatterplots are used frequently to summarizes the joint distribution of continuous data. They can be enhanced in several ways. As a default, use semi-transparent points so as not to hide any points (and perhaps see if your observations are concentrated anywhere). You can also add other features that help summarize the relationship, although I will defer this until later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(Murder~UrbanPop, USArrests, pch=16, col=grey(0.,.5))\n```\n\n::: {.cell-output-display}\n![](02_10_BivariateStatistics_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nYou can also show the *marginal distributions* of each variable along each axis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setup Plot\nlayout( matrix(c(2,0,1,3), ncol=2, byrow=TRUE),\n    widths=c(9/10,1/10), heights=c(1/10,9/10))\n\n# Scatterplot\npar(mar=c(4,4,1,1))\nplot(Murder~UrbanPop, USArrests, pch=16, col=rgb(0,0,0,.5))\n\n# Add Marginals\npar(mar=c(0,4,1,1))\nxhist <- hist(USArrests[,'UrbanPop'], plot=FALSE)\nbarplot(xhist[['counts']], axes=FALSE, space=0, border=NA)\n\npar(mar=c(4,0,1,1))\nyhist <- hist(USArrests[,'Murder'], plot=FALSE)\nbarplot(yhist[['counts']], axes=FALSE, space=0, horiz=TRUE, border=NA)\n```\n\n::: {.cell-output-display}\n![](02_10_BivariateStatistics_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n## Statistics of Association\n\n\nAll of the univariate statistics we have covered apply to marginal distributions. For joint distributions, there are several ways to statistically describe the relationship between two variables. The major differences surround whether the data are cardinal or an ordered/unordered factor. \n\n#### **Two Cardinals**. {-}\n*Pearson (Linear) Correlation*.\nSuppose you have two vectors, $\\hat{X}$ and $\\hat{Y}$, that are both cardinal data. As such, you can compute the most famous measure of association, the covariance. Letting $\\hat{M}_{X}$ and $\\hat{M}_{Y}$ denote the mean of $\\hat{X}$ and $\\hat{Y}$, we have\n\\begin{eqnarray}\n\\hat{C}_{XY} =  \\sum_{i=1}^{n} [\\hat{X}_{i} - \\hat{M}_{X}] [\\hat{Y}_i - \\hat{M}_{Y}] / n\n\\end{eqnarray}\n\nNote that covariance of $\\hat{X}$ and $\\hat{X}$ is just the variance of $\\hat{X}$; $\\hat{C}_{XX}=\\hat{V}_{X}$, and recall that the standard deviation is $\\hat{S}_{X}=\\sqrt{\\hat{V}_X}$. For ease of interpretation and comparison, we rescale the correlation statistic to always lay on a scale $-1$ and $+1$. A value close to $-1$ suggests negative association, a value close to $0$ suggests no association, and a value close to $+1$ suggests positive association.\n\\begin{eqnarray}\n\\hat{R}_{XY} = \\frac{ \\hat{C}_{XY} }{ \\hat{S}_{X} \\hat{S}_{Y}}\n\\end{eqnarray}\n\n:::{.callout-note icon=false collapse=\"true\"}\nWhat is the correlation for the dataset $\\{ (0,0.1) , (1, 0.3), (2, 0.2) \\}$? Find the answer both mathematically and computationally.\n\nMathematically, there are five steps.\n\nStep 1: Compute the means\n\\begin{eqnarray}\n\\hat{M}_{X} &=& \\frac{0+1+2}{3} = 1 \\\\\n\\hat{M}_{Y} &=& \\frac{0.1+0.3+0.2}{3} = 0.2\n\\end{eqnarray}\n\nStep 2: Compute the deviances\n\\begin{eqnarray}\n\\begin{array}{c|rrrr}\n\\hat{X}_i   & 0   & 1 & 2 \\\\\n\\hat{X}_i-\\hat{M}_{X} & -1  & 0 & 1 \\\\\n\\hat{Y}_i   & 0.1 & 0.3 & 0.2 \\\\\n\\hat{Y}_i-\\hat{M}_{Y}   & -0.1 & 0.1 & 0 \n\\end{array}\n\\end{eqnarray}\n\nStep 3: Compute the Covariance\n\\begin{eqnarray}\n\\hat{C}_{XY} &=&\n\\sum  (\\hat{X}_i-\\hat{M}_{X})(\\hat{Y}_i-\\hat{M}_{Y})/n \n= \\left[ (-1)(-0.1) + 0(0.1) + 1(0) \\right] \\frac{1}{3} \n=  (-0.1) \\frac{1}{3}\n= 1/30\n\\end{eqnarray}\n\nStep 4: Compute Standard Deviations\n\\begin{eqnarray}\n\\hat{V}_{X} &=& \\sum_{i=1}^n \\left(\\hat{X}_i-\\hat{M}_{X}\\right)^2 / n\n= \\left[(-1)^2+0^2+1^2 \\right]/3\n= 2/3 \\\\\n\\hat{S}_{X} &=& \\sqrt{2/3} \\\\\n\\hat{V}_{Y} &=& \\sum_{i=1}^n \\left( \\hat{Y}_i-\\hat{M}_{Y} \\right)^2 / n\n= \\left[ (-0.1)^2+(0.1)^2+0^2 \\right]/3\n= \\left[0.01+0.01\\right]/3\n= \\frac{2}{100} \\frac{1}{3} = 2/300 \\\\\n\\hat{S}_{Y} &=& \\sqrt{2/300}\n\\end{eqnarray}\n\nStep 5: Compute the Correlation\n\\begin{eqnarray}\n\\frac{\\hat{C}_{XY}}{\\hat{S}_X \\hat{S}_Y}\n&=& \\frac{1/30}{ \\sqrt{2/3}  \\sqrt{2/300}}\n= \\frac{1/30}{ 2 /\\sqrt{900}}\n= \\frac{1/30}{2/30} = 1/2\n\\end{eqnarray}\n\nNote that this value suggests a positive relationship between the variables.\n\nComputationally, we do the same steps\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the Data\nX <- c(0,1,2)\nX\n## [1] 0 1 2\nY <- c(0.1,0.3,0.2)\nY\n## [1] 0.1 0.3 0.2\n\n# Compute the Means\nmX <- mean(X)\nmY <- mean(Y)\n\n# Compute the Deviances\ndev_X <- X - mX\ndev_Y <- Y - mY\n\n# Compute the Covariance\ncov_manual <-  sum(dev_X * dev_Y) / length(X)\n\n# Compute the Standard Deviations\nvar_X <- sum(dev_X^2) / length(X)\nsd_X <- sqrt(var_X)\nvar_Y <- sum(dev_Y^2) / length(Y)\nsd_Y <- sqrt(var_Y)\n\n# Compute the Correlation\ncor_manual <- cov_manual / (sd_X * sd_Y)\ncor_manual\n## [1] 0.5\n\n# Verify with the built-in function\ncor(X,Y)\n## [1] 0.5\n```\n:::\n\n:::\n\n\n\nYou can conduct hypothesis tests for these statistics using the same procedures we learned for univariate data. For example, by inverting a confidence interval.\n\n:::{.callout-note icon=false collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the Data\nxy <- USArrests[,c('Murder','UrbanPop')]\nxy_cor <- cor(xy[, 1], xy[, 2])\n#plot(xy, pch=16, col=grey(0,.25))\n    \n# Bootstrap Distribution of Correlation\nn <- nrow(xy)\nbootstrap_cor <- vector(length=9999)\nfor(b in seq(bootstrap_cor) ){\n    xy_b <- xy[sample(n, replace=T),]\n    xy_cor_b <- cor(xy_b[, 1], xy_b[, 2])\n    bootstrap_cor[b] <- xy_cor_b\n}\nhist(bootstrap_cor, breaks=100,\n    border=NA, font.main=1,\n    xlab='Correlation',\n    main='Bootstrap Distribution')\n\n## Test whether correlation is statistically different from 0\nboot_ci <- quantile(bootstrap_cor, probs=c(0.025, 0.975))\nabline(v=boot_ci)\nabline(v=0, col='red')\n```\n\n::: {.cell-output-display}\n![](02_10_BivariateStatistics_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n:::\n\n\nImportantly, we can also impose the null of hypothesis of no association by reshuffling the data. If we resample without replacement, this is known as a *permutation test*.\n\n:::{.callout-note icon=false collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\nxy_cor <- cor(xy[, 1], xy[, 2])\n#plot(xy, pch=16, col=grey(0,.25))\n    \n# Null Bootstrap Distribution of Correlation\nn <- nrow(xy)\nnull_bootstrap_cor <- vector(length=9999)\nfor(b in seq(null_bootstrap_cor) ){\n    xy_b <- xy\n    xy_b[,'UrbanPop'] <- xy[sample(n, replace=T),'UrbanPop'] ## Reshuffle X\n    xy_cor_b <- cor(xy_b[, 1], xy_b[, 2])\n    null_bootstrap_cor[b] <- xy_cor_b\n}\nhist(null_bootstrap_cor, breaks=100,\n    border=NA, font.main=1,\n    xlab='Correlation',\n    main='Null Bootstrap Distribution')\n\n## Test whether correlation is statistically different from 0\nboot_ci <- quantile(null_bootstrap_cor, probs=c(0.025, 0.975))\nabline(v=boot_ci)\nabline(v=xy_cor, col='blue')\n```\n\n::: {.cell-output-display}\n![](02_10_BivariateStatistics_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\nBecause all dependence resides in the pairing, and permuting one margin destroys the pairing completely.\n\nTo construct a permutation test, we need to use `replace=F`. Rework the above code to make a Permutation Null Distribution and conduct a permutation test.\n:::\n\nSo the bootstrap evaluates sampling variability in the world that generated your data. A permutation test constructs the distribution of the statistic in a world where the null is true. Altogether, we have\n\n| Distribution| Sample Size per Iteration | Number of Iterations | Mechanism | Typical Purpose |\n| -------- | ------- | ------- | ------- |----------------|\nJackknife  | $n-1$   | $n$  | $X,Y$: Deterministically leave-one-out observation | Variance estimate |\nBootstrap  | $n$     | $B$  | $X,Y$: Random resample with replacement | Variance and CI estimate |\nNull Bootstrap | $n$ | $B$  | $X,Y$: Random resample with replacement and shifted | CI under imposed null, $p$-values |\nPermutation    | $n$ | $B$  | $X$: Random resample without replacement | CI under imposed null of no association, $p$-values |\n: Types of resampling\n\n*Falk Codeviance*.\nThe Codeviance, $\\tilde{C}_{XY}$, is a robust alternative to Covariance. Instead of relying on means (which can be sensitive to outliers), it uses medians.^[See also *Theil-Sen Estimator*, which may be seen as a precursor.] We can also scale the Codeviance by the median absolute deviation to compute the median correlation, $\\tilde{R}_{XY}$, which typically lies in $[-1,1]$ but not always. Letting $\\tilde{M}_{X}$ and $\\tilde{M}_{Y}$ denote the median of $\\hat{X}$ and $\\hat{Y}$, we have\n\\begin{eqnarray} \n\\tilde{C}_{XY} = \\text{Med}\\left\\{ |\\hat{X}_{i} - \\tilde{M}_{X}| |\\hat{Y}_i - \\tilde{M}_{Y}| \\right\\} \\\\\n\\tilde{R}_{XY} = \\frac{ \\tilde{C}_{XY} }{ \\hat{\\text{MAD}}_{X} \\hat{\\text{MAD}}_{Y}}. \n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\ncodev <- function(xy) {\n  # Compute medians for each column\n  med <- apply(xy, 2, median)\n  # Subtract the medians from each column\n  xm <- sweep(xy, 2, med, \"-\")\n  # Compute CoDev\n  CoDev <- median(xm[, 1] * xm[, 2])\n  # Compute the medians of absolute deviation\n  MadProd <- prod( apply(abs(xm), 2, median) )\n  # Return the robust correlation measure\n  return( CoDev / MadProd)\n}\nxy_codev <- codev(xy)\nxy_codev\n## [1] 0.005707763\n```\n:::\n\n\nYou construct sampling distributions and conduct hypothesis tests for Falk's Codeviance statistic in the same way you do for Pearson's Correlation statistic.\n\n:::{.callout-tip icon=false collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\nxy_cor <- cor(xy[, 1], xy[, 2])\n#plot(xy, pch=16, col=grey(0,.25))\n    \n# Null Permutation Distribution of Codeviance\nn <- nrow(xy)\nnull_permutation_codev <- vector(length=9999)\nfor(b in seq(null_permutation_codev) ){\n    xy_b <- xy\n    xy_b[,'UrbanPop'] <- xy[sample(n, replace=F),'UrbanPop'] ## Reshuffle X\n    xy_codev_b <- codev(xy_b)\n    null_permutation_codev[b] <- xy_codev_b\n}\nhist(null_permutation_codev, breaks=100,\n    border=NA, font.main=1,\n    xlab='Codeviance',\n    main='Null Permutation Distribution')\n\n## Test whether correlation is statistically different from 0\nabline(v=quantile(null_permutation_codev, probs=c(0.025, 0.975)))\nabline(v=xy_codev, col='blue')\n```\n\n::: {.cell-output-display}\n![](02_10_BivariateStatistics_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n:::\n\n\n#### **Two Ordered Factors**. {-}\nSuppose now that $\\hat{X}$ and $\\hat{Y}$ are both *ordered* variables. *Kendall's rank correlation* statistic measures the strength and direction of association by counting the number of concordant pairs (where the ranks agree) versus discordant pairs (where the ranks disagree). A value closer to $1$ suggests positive association in rankings, a value closer to $-1$ suggests a negative association, and a value of $0$ suggests no association in the ordering.\n\\begin{eqnarray}\n\\hat{KT} = \\frac{2}{n(n-1)} \\sum_{i} \\sum_{j > i} \\text{sgn} \\Bigl( (\\hat{X}_{i} - \\hat{X}_{j})(\\hat{Y}_i - \\hat{Y}_j) \\Bigr),\n\\end{eqnarray}\nwhere the sign function is:\n\\begin{eqnarray}\n\\text{sgn}(z) = \n\\begin{cases}\n    +1 & \\text{if } z > 0\\\\\n    0  & \\text{if } z = 0 \\\\\n    -1 & \\text{if } z < 0 \n\\end{cases}.\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\nxy[,1] <- rank(xy[,1] )\nxy[,2] <- rank(xy[,2] )\n# plot(xy, pch=16, col=grey(0,.25))\nKT <- cor(xy[, 1], xy[, 2], method = \"kendall\")\nround(KT, 3)\n## [1] 0.074\n```\n:::\n\n\n\nYou construct sampling distributions and conduct hypothesis tests for Kendall's rank correlation statistic in the same way you do as for Pearson's Correlation statistic and Falk's Codeviance statistic.\n\n:::{.callout-tip icon=false collapse=\"true\"}\nTest whether Kendal's correlation statistic is statistically different from $0$. Expand on the example below to use bootstrapping.\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\nKT <- cor(xy[, 1], xy[, 2], method=\"kendall\")\n```\n:::\n\n:::\n\nKendall's rank correlation coefficient can also be used for non-linear relationships, where Pearson's correlation coefficient often falls short. It almost always helps to visual your data first before summarizing it with a statistic.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_10_BivariateStatistics_files/figure-html/unnamed-chunk-12-1.png){width=768}\n:::\n:::\n\n\n\n\n#### **Two Unordered Factors**. {-}\nSuppose $\\hat{X}$ and $\\hat{Y}$ are both *categorical* variables; the value of $\\hat{X}$ is one of $1...K$ categories and the value of $\\hat{Y}$ is one of $1...J$ categories. We organize such data as a contingency table with $K$ rows and $J$ columns and use *Cramer's V* to quantify the strength of association by adjusting a *chi-squared* statistic to provide a measure that ranges from $0$ to $1$; $0$ suggests no association while a value closer to $1$ suggests a strong association. \n\nFirst, compute the chi-square statistic:\n\\begin{eqnarray}\n\\hat{\\chi}^2 = \\sum_{k=1}^{K} \\sum_{j=1}^{J} \\frac{(\\hat{O}_{kj} - \\hat{E}_{kj})^2}{\\hat{E}_{kj}}.\n\\end{eqnarray}\n\nwhere\n\n* $\\hat{O}_{kj}$ denote the observed frequency in cell $(k, j)$,\n* $\\hat{E}_{kj} = \\hat{RF}_{k} \\cdot \\hat{CF}_j / n$ is the expected frequency for each cell if $\\hat{X}$ and $\\hat{Y}$ are independent\n* $\\hat{RF}_{k}$ denotes the total frequency for row $k$ (i.e., $\\hat{RF}_i = \\sum_{j=1}^{J} \\hat{O}_{kj}$),\n* $\\hat{CF}_{j}$ denotes the total frequency for column $j$ (i.e., $\\hat{CF}_{j} = \\sum_{k=1}^{K} \\hat{O}_{kj}$),\n\n\nSecond, normalize the chi-square statistic with the sample size and the degrees of freedom to compute Cramer's V. Recalling that $I$ is the number of categories for $\\hat{X}$, and $J$ is the number of categories for $\\hat{Y}$, the statistic is\n\\begin{eqnarray}\n\\hat{CV} = \\sqrt{\\frac{\\hat{\\chi}^2 / n}{\\min(J - 1, \\, K - 1)}}.\n\\end{eqnarray}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\nxy[,1] <- cut(xy[,1],3)\nxy[,2] <- cut(xy[,2],4)\ntable(xy)\n##               UrbanPop\n## Murder         (31.9,46.8] (46.8,61.5] (61.5,76.2] (76.2,91.1]\n##   (0.783,6.33]           4           5           8           5\n##   (6.33,11.9]            0           4           7           6\n##   (11.9,17.4]            2           4           2           3\n\nCV <- function(xy){\n    # Create a contingency table from the categorical variables\n    tbl <- table(xy)\n    # Compute the chi-square statistic (without Yates' continuity correction)\n    chi2 <- chisq.test(tbl, correct=FALSE)[['statistic']]\n    # Total sample size\n    n <- sum(tbl)\n    # Compute the minimum degrees of freedom (min(rows-1, columns-1))\n    df_min <- min(nrow(tbl) - 1, ncol(tbl) - 1)\n    # Calculate Cramer's V\n    V <- sqrt((chi2 / n) / df_min)\n    return(V)\n}\nCV(xy)\n## X-squared \n## 0.2307071\n\n# DescTools::CramerV( table(xy) )\n```\n:::\n\n\nYou construct sampling distributions and conduct hypothesis tests for  Cramer's V statistic in the same way you do as the other statistics.\n\n\n## Association is not Causation\n\nIn all of the above statistics, we measure association not causation. \n\nOne major issue is statistical significance: sometimes there are relationships in the population that do not show up in samples, or non-relationships that appear in samples. This should be familiar at this point.\n\nOther major issues pertain to real relationships averaging out and mechanically inducing relationships with random data. To be concrete about these issue, I will focus on the most-used correlation statistic and examine\n\n* Causation without correlation\n* Correlation without causation\n\n\nNote that these are not the only examples of causation without correlation and correlation without causation. Many real datasets have temporal and spatial interdependence that create additional issues. Many real datasets also have economic interdependence, which also creates additional issues. We will delay covering these issues until much later.\n\n#### **Causation without correlation**. {-}\n\nExamples of this first issue includes *nonlinear effects* and  *heterogeneous effects* averaging out.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nn <- 10000\n\n# X causes Y via Y = X^2 + noise\nX <- runif(n, min = -1, max = 1)\nepsilon <- rnorm(n, mean = 0, sd = 0.1)\nY <- X^2 + epsilon  # clear causal effect of X on Y\nplot(X,Y, pch=16, col=grey(0,.1))\n\n# Correlation over the entire range\ntitle( paste0('Cor: ', round( cor(X, Y), 1) ) ,\n    font.main=1)\n```\n\n::: {.cell-output-display}\n![](02_10_BivariateStatistics_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Heterogeneous Effects\nX <- rnorm(n) # Randomized \"treatment\"\n\n# Heterogeneous effects based on group\ngroup <- rbinom(n, size = 1, prob = 0.5)\nepsilon <- rnorm(n, mean = 0, sd = 1)\nY <- ifelse(group == 1,\n            X + epsilon,   # positive effect\n            -X + epsilon)  # negative effect\nplot(X,Y, pch=16, col=grey(0,.1))\n\n# Correlation in the pooled sample\ntitle( paste0('Cor: ', round( cor(X, Y), 1) ),\n    font.main=1 )\n```\n\n::: {.cell-output-display}\n![](02_10_BivariateStatistics_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n#### **Correlation without causation**. {-}\nExamples of this second issue includes *shared denominators* and  *selection bias* inducing correlations.\n\nConsider three completely random variables. We can induce a mechanical relationship between the first two variables by dividing them both by the third variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nn <- 20000\n\n# Independent components\nA <- runif(n)\nB <- runif(n)\nC <- runif(n)\npar(mfrow=c(1,2))\nplot(A,B, pch=16, col=grey(0,.1))\ntitle('Independent Variables', font.main=1)\n\n# Ratios with a shared denominator\nX <- A / C\nY <- B / C\nplot(X,Y, pch=16, col=grey(0,.1))\ntitle('With Common Divisor', font.main=1)\n```\n\n::: {.cell-output-display}\n![](02_10_BivariateStatistics_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Correlation\ncor(X, Y)\n## [1] 0.8183118\n```\n:::\n\n\nConsider an admissions rule into university: applicants are accepted if they have either high test scores or strong extracurriculars. Even if there is no general relationship between test scores and extracurriculars, you will see one amongst university students.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# Independent traits in the population\ntest_score        <- rnorm(n, mean = 0, sd = 1)\nextracurriculars  <- rnorm(n, mean = 0, sd = 1)\n\n# Selection above thresholds\nthreshold <- 1.0\nadmitted <- (test_score > threshold) | (extracurriculars > threshold)\nmean(admitted)  # admission rate\n## [1] 0.29615\n\npar(mfrow = c(1, 2))\n# Full population\nplot(test_score, extracurriculars,\n     pch=16, col=grey(0,.1))\ntitle('General Sample', font.main=1)\n# Admitted only\nplot(test_score[admitted], extracurriculars[admitted],\n     pch=16, col=grey(0,.1))\ntitle('University Sample', font.main=1)\n```\n\n::: {.cell-output-display}\n![](02_10_BivariateStatistics_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Correlation among admitted applicants only\ncor(test_score[admitted], extracurriculars[admitted])\n## [1] -0.5597409\n```\n:::\n\n\n\n## Further Reading \n\nFor plotting histograms and marginal distributions, see \n\n* <https://www.r-bloggers.com/2011/06/example-8-41-scatterplot-with-marginal-histograms/>\n* <https://r-graph-gallery.com/histogram.html>\n* <https://r-graph-gallery.com/74-margin-and-oma-cheatsheet.html>\n* <https://jtr13.github.io/cc21fall2/tutorial-for-scatter-plot-with-marginal-distribution.html>\n\n\nOther Statistics \n\n* <https://cran.r-project.org/web/packages/qualvar/vignettes/wilcox1973.html>\n",
    "supporting": [
      "02_10_BivariateStatistics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}