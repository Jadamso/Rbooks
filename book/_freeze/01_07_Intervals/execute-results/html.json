{
  "hash": "5fe5e992e42b459fd2b3c57b901f680c",
  "result": {
    "engine": "knitr",
    "markdown": "# Intervals\n***\n\n\n## Confidence Intervals\n\nWe can also estimate variability using a *Confidence Interval:* range your statistic varies across different samples.\n\n#### **Percentile Intervals**.  {-}\nThis type of confidence interval is simply the upper and lower quantiles of the sampling distribution.\n\nFor example, consider the sample mean. We simulate the sampling distribution of the sample mean and construct a $90\\%$ confidence interval by taking the $5^{th}$ and $95^{th}$ percentiles of the simulated means. \n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 300 samples, each with 1000 random uniform variables\nx_samples <- matrix(nrow=300, ncol=1000)\nfor(i in seq(1,nrow(x_samples))){\n    x_samples[i,] <- runif(1000)\n}\nsample_means <- apply(x_samples, 1, mean) # mean for each sample (row)\n\n# Middle 90%\nmq <- quantile(sample_means, probs=c(.05,.95))\npaste0('we are 90% confident that the mean is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\n## [1] \"we are 90% confident that the mean is between 0.49 and 0.51\"\n\nhist(sample_means,\n    breaks=seq(.4,.6, by=.001), \n    border=NA, freq=F,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='90% Confidence Interval for the Mean')\nabline(v=mq)\n```\n\n::: {.cell-output-display}\n![](01_07_Intervals_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor another example, consider the median. We now repeat the above process to estimate the median for each sample, instead of the mean. \n\n::: {.cell}\n\n```{.r .cell-code}\n## Sample Quantiles (medians)\nsample_quants <- apply(x_samples, 1, quantile, probs=0.5) #quantile for each sample (row)\n\n# Middle 90% of estimates\nmq <- quantile(sample_quants, probs=c(.05,.95))\npaste0('we are 90% confident that the median is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\n## [1] \"we are 90% confident that the median is between 0.47 and 0.52\"\n\nhist(sample_quants,\n    breaks=seq(.4,.6, by=.001),\n    border=NA, freq=F,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='90% Confidence Interval for the Median')\nabline(v=mq)\n```\n\n::: {.cell-output-display}\n![](01_07_Intervals_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n:::\n\n\n\nNote that $Z\\%$ confidence intervals do not generally cover $Z\\%$ of the data (those types of intervals are covered later). In the examples above, notice the confidence interval for the mean differs from the confidence interval of the median, and so both cannot cover $90\\%$ of the data. The confidence interval for the mean is roughly $[0.48, 0.52]$, which theoretically covers only a $0.52-0.48=0.04$ proportion of uniform random data, much less than the proportion 0.9.\n\n#### **Interval Size**.  {-}\n\nConfidence intervals shrink with more data, as averaging washes out random fluctuations. Here is the intuition for estimating the weight of an apple:\n\n* With $n=1$ apple, your estimate depends entirely on that one draw. If it happens to be unusually large or small, your estimate can be far off.\n* With $n=2$ apples, the estimate averages out their idiosyncrasies. An unusually heavy apple can be balanced by a lighter one, lowering how far off you can be. You are less likely to get two extreme values than just one.\n* With $n=100$ apples, individual apples barely move the needle. The average becomes stable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- c(18,20,22,24) #student ages\n# six possible samples of size 2\nm1 <- mean( X[c(1,2)] ) #{1,2}\nm2 <- mean( X[c(1,3)] ) #{1,3}\nm3 <- mean( X[c(1,4)] ) #{3,4}\nm4 <- mean( X[c(2,3)] ) #{2,3}\nm5 <- mean( X[c(2,4)] ) #{2,4}\nm6 <- mean( X[c(3,4)] ) #{3,4}\nmeans_2 <- c(m1, m2, m3, m4, m5, m6)\nsd(means_2)\n## [1] 1.414214\n\n# four possible samples of size 3\nm1 <- mean( X[c(1,2,3)] ) \nm2 <- mean( X[c(1,2,4)] ) \nm3 <- mean( X[c(1,3,4)] ) \nm4 <- mean( X[c(2,3,4)] ) \nmeans_3 <- c(m1, m2, m3, m4)\nsd(means_3)\n## [1] 0.860663\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 300 samples, each of size n\nn <- 10000\nx_samples <- matrix(nrow=300, ncol=n)\nfor(i in seq(1,nrow(x_samples))){\n    x_samples[i,] <- runif(1000)\n}\n# Compute means for each row (for each sample)\nsample_means <- apply(x_samples, 1, mean)\nmq <- quantile(sample_means, probs=c(.05,.95))\npaste0('we are 90% confident that the mean is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\nhist(sample_means,\n    breaks=seq(.4,.6, by=.001), \n    border=NA, freq=F,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='90% Confidence Interval for the Mean (larger n)')\nabline(v=mq)\n```\n:::\n\n\nNote that both resampling methods provide imperfect estimates, and can give different numbers. Jackknife resamples are systematically less variable than they should be and sample $n-1$ instead of $n$. Bootstrap resamples have the right $n$ but often have duplicated data. Until you know more, a conservative approach is to take the larger estimate (often the bootstrap). That is also good advice when considering theoretically derived confidence intervals too.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests[,'Murder']\nsample_mean <- mean(sample_dat)\nsample_mean\n## [1] 7.788\n\n# Jackknife Distribution\nn <- length(sample_dat)\njackknife_means <- vector(length=n)\nfor(i in seq_along(jackknife_means)){\n    dat_noti <- sample_dat[-i]\n    mean_noti <- mean(dat_noti)\n    jackknife_means[i] <- mean_noti\n}\n\n# Bootstrap Distribution\nset.seed(1) # to be replicable\nbootstrap_means <- vector(length=9999)\nfor(b in seq_along(bootstrap_means)){\n    dat_id <- seq(1,n)\n    boot_id <- sample(dat_id , replace=T)\n    dat_b  <- sample_dat[boot_id] # c.f. jackknife\n    mean_b <- mean(dat_b)\n    bootstrap_means[b] <-mean_b\n}\n\n# Jack CI\njack_ci <- quantile(jackknife_means, probs=c(.025, .975))\njack_ci\n##     2.5%    97.5% \n## 7.621582 7.904082\n\n# Boot CI\nboot_ci <- quantile(bootstrap_means, probs=c(.025, .975))\nboot_ci\n##   2.5%  97.5% \n## 6.6039 8.9420\n\n# more conservative estimate\nci <- boot_ci\n```\n:::\n\n\n\n\n\n\n## Hypothesis Testing\n\nIn this section, we test hypotheses using *data-driven* methods that assume much less about the data generating process. There are two main ways to conduct a hypothesis test to do so: inverting a confidence interval and imposing the null. The first treats the distribution of estimates directly; the second explicitly enforces the null hypothesis to evaluate how unusual the observed statistic is. Both approaches rely on the bootstrap: resampling the data to approximate sampling variability. The most typical case is hypothesizing about about the mean.\n\n#### **Invert a CI**.{-}\n\nOne main way to conduct hypothesis tests is to examine whether a confidence interval contains a hypothesized value. We then use this decision rule\n\n* reject the null if value falls outside of the interval\n* fail to reject the null if value falls inside of the interval\n\nWe typically use a $95\\%$ confidence interval to create a *rejection region*. \n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, suppose you hypothesize the mean is $9$. You then construct a bootstrap distribution with $95\\%$ confidence interval, and find your hypothesized value falls outside of the confidence interval. Then, after accounting for sampling variability (which you estimate), it still seems extremely unlikely that the theoretical mean actually equals $9$, so you reject that that hypothesis. (If the theoretical value landed in the interval, you would \"fail to reject\" the theoretical mean equals $9$.)\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(bootstrap_means, breaks=25,\n    border=NA,\n    main='',\n    xlab='Bootstrap Samples')\n# CI\nci_95 <- quantile(bootstrap_means, probs=c(.025, .975))\nabline(v=ci_95, lwd=2)\n# H0: mean=9\nabline(v=9, col=2, lwd=2)\n```\n\n::: {.cell-output-display}\n![](01_07_Intervals_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nThe above procedure also generalizes to many other statistics. Perhaps the most informative additional statistics for spread or shape. E.g., you can conduct hypothesis tests for `sd` and `IQR`, or `skew` and `kurtosis`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap Distribution for SD\nsd_obs <- sd(sample_dat)\nbootstrap_sd <- vector(length=999)\nfor(b in seq_along(bootstrap_sd)){\n    x_b <- sample(sample_dat, replace=T)\n    sd_b <- sd(x_b)\n    bootstrap_sd[b] <- sd_b\n}\n\n# Test for SD Differences (Invert CI)\nsd_null <- 3.6\nhist(bootstrap_sd, freq=F,\n    border=NA, xlab='Bootstrap', font.main=1,\n    main='Standard Deviations (Invert CI)')\nsd_ci <- quantile(bootstrap_sd, probs=c(0.25,.975) )\nabline(v=sd_ci, lwd=2)\nabline(v=sd_null, lwd=2, col=2)\n```\n\n::: {.cell-output-display}\n![](01_07_Intervals_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nTo better your understanding, try redoing the above for any function (such as `IQR(x_b)/median(x_b)`)\n:::\n\n:::{.callout-tip icon=false collapse=\"true\"}\nSuppose you scored $83\\%$ on your exam with $50$ questions, but think you are really a $90\\%$ student. Explain how you might test your hypothesis to your professor who insists your claim be supported by evidence. What would be the issue if we could not reject your hypothesis? Provide a computer simulation illustrating the issue.\n:::\n\n\n#### **Impose the Null**. {-}\nWe can also compute a *null distribution*: the sampling distribution of the statistic under the null hypothesis (assuming your null hypothesis was true). We use the bootstrap to loop through a large number of \"resamples\". In each iteration of the loop, we impose the null hypothesis and re-estimate the statistic of interest. We then calculate the range of the statistic across all resamples and compare how extreme the original value we observed is.\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, suppose you hypothesize the mean is $9$. You then construct a $95\\%$ confidence interval around the *null* bootstrap distribution (resamples centered around $9$). If your sample mean falls outside of that interval, then even after accounting for sampling variability (which you estimate), it seems extremely unlikely that the theoretical mean actually equals $9$, so you reject that that hypothesis. (If the sample mean landed in the interval, you would \"fail to reject\" the theoretical mean equals $9$.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests[,'Murder']\nsample_mean <- mean(sample_dat)\n\n# Bootstrap NULL: mean=9\n# Bootstrap shift: center each bootstrap resample so that the distribution satisfies the null hypothesis on average.\nset.seed(1)\nmu <- 9\nbootstrap_means_null <- vector(length=999)\nfor(b in seq_along(bootstrap_means_null)){\n    dat_b <- sample(sample_dat, replace=T) \n    mean_b <- mean(dat_b) + (mu - sample_mean) # impose the null via Bootstrap shift\n    bootstrap_means_null[b] <- mean_b\n}\nhist(bootstrap_means_null, breaks=25, border=NA,\n    main='',\n    xlab='Null Bootstrap Samples')\nci_95 <- quantile(bootstrap_means_null, probs=c(.025, .975)) # critical region\nabline(v=ci_95, lwd=2)\nabline(v=sample_mean, lwd=2, col=4)\n```\n\n::: {.cell-output-display}\n![](01_07_Intervals_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n:::\n\n\n\n\n\n\n## Probability Theory\n\n\n#### **CI Coverage**. {-}\n\nOften, a $Z\\%$ confidence interval means that $Z\\%$ of the intervals we generate will contain the true mean. E.g., suppose that we repeatedly sample data and construct $95\\%$ bootstrap confidence interval for the mean, then we expect that $95\\%$ of our constructed confidence intervals contain the theoretical population mean. Given the sampling distribution is approximately normally, confidence intervals are symmetric. For the sample mean $M$, we can construct the interval $[M - E, M + E]$, where $E$ is some \"margin of error\" on either side of $M$. A coverage level of $1-\\alpha$ means $Prob( M - E < \\mu < M + E)=1-\\alpha$. I.e., if the same sampling procedure were repeated $100$ times from the same population, approximately $95$ of the resulting intervals would be expected to contain the true population mean.^[Notice that $Prob( M - E < \\mu < M + E) = Prob( - E < \\mu - M < + E) = Prob( E + \\mu > M > \\mu - E)$. So if the interval $[\\mu - 10, \\mu + 10]$ contains $95\\%$ of all $M$, then the interval $[M-10, M+10]$ will also contain $\\mu$ in $95\\%$ of the samples because whenever $M$ is within $10$ of $\\mu$, the value $\\mu$ is also within $10$ of $M$. But for any particular sample, the interval $[\\hat{M}-10, \\hat{M}+10]$ either does or does not contain $\\mu$.] Note that a $95\\%$ coverage level does not imply a $95\\%$ probability that the true parameter lies within a particular calculated interval. E.g., if you compute $\\hat{M}=9$ for your particular sample, a coverage level of $1-\\alpha=95\\%$ does not mean $Prob(9 - E < \\mu < 9 + E)=95\\%$. The interval you computed either contains the true mean or it does not.\n\n:::{.callout-tip icon=false collapse=\"true\"}\nGiven the sample size, $n$, is large enough for the sample mean to be approximately normally distributed, what confidence interval satisfies the following: the theoretical mean $\\mu$ is inside of the interval with probability $95%$ (i.e., for $(1 - 0.05)%$ of samples)?\n:::\n\nFor a fixed sample size $n$, there is a trade-off between *precision*: the width of a confidence interval, and *accuracy*: the probability that a confidence interval contains the theoretical value.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Confidence Interval for each sample\nxq <- apply(x_samples, 1, function(r){ #theoretical se's \n    mean(r) + c(-1,1)*sd(r)/sqrt(length(r))\n})\n# First 3 interval estimates\nxq[, c(1,2,3)]\n##           [,1]      [,2]      [,3]\n## [1,] 0.5059216 0.4959424 0.4958189\n## [2,] 0.5243759 0.5144455 0.5140334\n\n# Explicit calculation\nmu_true <- 0.5 # theoretical result for uniform samples\n# Logical vector: whether the true mean is in each CI\ncovered <- mu_true >= xq[1, ] & mu_true <= xq[2, ]\n# Empirical coverage rate\ncoverage_rate <- mean(covered)\ncat(sprintf(\"Estimated coverage probability: %.2f%%\\n\", 100 * coverage_rate))\n## Estimated coverage probability: 71.00%\n\n# Theoretically: [-1 sd, +1 sd] has 2/3 coverage\n# Change to [-2 sd, +2 sd] to see Precision-Accuracy tradeoff.\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize first 100 confidence intervals\nn <- 100\nplot.new()\nplot.window(xlim = range(xq), ylim = c(0, n))\nfor (i in seq(1,n) ) {\n  col_i <- if (covered[i]) rgb(0, 0, 0, 0.3) else rgb(1, 0, 0, 0.5)\n  segments(xq[1, i], i, xq[2, i], i, col = col_i, lwd = 2)\n}\nabline(v = mu_true, col = \"blue\", lwd = 2)\naxis(1)\ntitle(\"Visualizing CI Coverage (Red = Missed)\", font.main=1)\n```\n\n::: {.cell-output-display}\n![](01_07_Intervals_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n## Further Reading\n\nSee \n\n* <https://www.r-bloggers.com/2025/02/bootstrap-vs-standard-error-confidence-intervals/>\n\n\n* <https://learningstatisticswithr.com/book/hypothesistesting.html>\n* <https://okanbulut.github.io/rbook/part5.html>\n",
    "supporting": [
      "01_07_Intervals_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}