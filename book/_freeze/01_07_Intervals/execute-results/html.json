{
  "hash": "0e93824df64b775e7e4014a1738a38e7",
  "result": {
    "engine": "knitr",
    "markdown": "# Confidence Intervals\n***\n\n\n## Confidence Intervals\n\nThe sampling distribution describes how the statistic varies across samples. The *confidence interval* is a way to turn knowledge about that sampling distribution into a statement about the unknown parameter. A $Z\\%$ confidence interval for the mean implies that $Z\\%$ of the intervals we generate will contain the population mean, $\\mu$. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_07_Intervals_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nNote that a $Z\\%$ confidence interval does not imply a $Z\\%$ probability that the true parameter lies within a particular calculated interval. The interval you computed either contains the true mean or it does not.\n\nIn practice, people often interpret confidence intervals informally as \"showing the uncertainty around our estimate\": wider intervals correspond to higher sampling variability and less precise information about $\\mu$. Just as with standard errors, we can estimate confidence intervals using theory-driven or data-driven approaches. We will focus on data-driven approaches first.\n\n\n\n#### **Computation**. {-}\n\nFor example, consider the sample mean. We simulate the sampling distribution of the sample mean and construct a $90\\%$ confidence interval by taking the $5^{th}$ and $95^{th}$ percentiles of the sampling distribution. We then expect that approximately $90\\%$ of our constructed confidence intervals contain the theoretical population mean. \n\nFor example, consider the mean of a uniform random sample with a sample size of $n=1000$.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 300 samples, each with 1000 random uniform variables\nx_samples <- matrix(nrow=300, ncol=1000)\nfor(i in seq(1,nrow(x_samples))){\n    x_samples[i,] <- runif(1000)\n}\nsample_means <- apply(x_samples, 1, mean) # mean for each sample (row)\n\n# Middle 90%\nmq <- quantile(sample_means, probs=c(.05,.95))\npaste0('we are 90% confident that the mean is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\n## [1] \"we are 90% confident that the mean is between 0.49 and 0.51\"\n\nhist(sample_means,\n    breaks=seq(.4,.6, by=.001), \n    border=NA, freq=F,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='90% Confidence Interval for the Mean')\nabline(v=mq)\n```\n\n::: {.cell-output-display}\n![](01_07_Intervals_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nFor another example, consider the median. We now repeat the above process to estimate the median for each sample, instead of the mean. \n\n::: {.cell}\n\n```{.r .cell-code}\n## Sample Quantiles (medians)\nsample_quants <- apply(x_samples, 1, quantile, probs=0.5) #quantile for each sample (row)\n\n# Middle 90% of estimates\nmq <- quantile(sample_quants, probs=c(.05,.95))\npaste0('we are 90% confident that the median is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\n## [1] \"we are 90% confident that the median is between 0.48 and 0.52\"\n\nhist(sample_quants,\n    breaks=seq(.4,.6, by=.001),\n    border=NA, freq=F,\n    col=rgb(0,0,0,.25), font.main=1,\n    main='90% Confidence Interval for the Median')\nabline(v=mq)\n```\n\n::: {.cell-output-display}\n![](01_07_Intervals_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n:::\n\nThe $5^{th}$ and $95^{th}$ percentiles are called the \"critical values\" for the $90\\%$ confidence interval.\nThe $2.5^{th}$ and $97.5^{th}$ percentiles are the critical values for the $95\\%$ confidence interval.\n\n\nThis is a repeated-sampling demonstration; in practice with one sample, we estimate the interval using resampling in the next section.\n\n#### **Interval Size**.  {-}\n\nConfidence intervals shrink with more data, as averaging washes out random fluctuations. Here is the intuition for estimating the weight of an apple:\n\n* With $n=1$ apple, your estimate depends entirely on that one draw. If it happens to be unusually large or small, your estimate can be far off.\n* With $n=2$ apples, the estimate averages out their idiosyncrasies. An unusually heavy apple can be balanced by a lighter one, lowering how far off you can be. You are less likely to get two extreme values than just one.\n* With $n=100$ apples, individual apples barely move the needle. The average becomes stable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 300 samples, each of size n\npar(mfrow=c(1,3))\nfor(n in c(25, 100, 250)){\nx_samples <- matrix(nrow=300, ncol=n)\nfor(i in seq(1,nrow(x_samples))){\n    x_samples[i,] <- runif(n)\n}\n# Compute means for each row (for each sample)\nsample_means <- apply(x_samples, 1, mean)\n# 90% Confidence Interval\nmq <- quantile(sample_means, probs=c(.05,.95))\npaste0('we are 90% confident that the mean is between ', \n    round(mq[1],2), ' and ', round(mq[2],2) )\nhist(sample_means,\n    breaks=seq(.1,.9, by=.005), \n    border=NA, freq=F, \n    col=rgb(0,0,0,.25), font.main=1,\n    main=paste0('n=',n))\nabline(v=mq)\n}\n```\n\n::: {.cell-output-display}\n![](01_07_Intervals_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nHere is an intuitive example, from a small discrete population. Notice the extreme values \n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- c(18,20,22,24) #student ages (population values)\n# six possible samples of size 2\nm1 <- mean( X[c(1,2)] ) #{1,2}\nm2 <- mean( X[c(1,3)] ) #{1,3}\nm3 <- mean( X[c(1,4)] ) #{1,4}\nm4 <- mean( X[c(2,3)] ) #{2,3}\nm5 <- mean( X[c(2,4)] ) #{2,4}\nm6 <- mean( X[c(3,4)] ) #{3,4}\nmeans_2 <- c(m1, m2, m3, m4, m5, m6)\nsort(means_2)\n## [1] 19 20 21 21 22 23\n\n# four possible samples of size 3\nm1 <- mean( X[c(1,2,3)] ) \nm2 <- mean( X[c(1,2,4)] ) \nm3 <- mean( X[c(1,3,4)] ) \nm4 <- mean( X[c(2,3,4)] ) \nmeans_3 <- c(m1, m2, m3, m4)\nsort(means_3)\n## [1] 20.00000 20.66667 21.33333 22.00000\n```\n:::\n\n:::\n\n\n\n\n\nFor a fixed sample size $n$, there is a trade-off between *precision*: the width of a confidence interval, and *accuracy*: the probability that a confidence interval contains the theoretical value.\n\n## Resampling Intervals\n\n\nOften, we have only one sample. In practice, we can use resampling procedures to estimate a confidence interval. E.g., we repeatedly resample data and construct a bootstrap or jackknife sampling distribution. Then we compute the confidence intervals using the upper and lower quantiles of the sampling distribution. \n\n:::{.callout-note icon=false collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests[,'Murder']\nsample_mean <- mean(sample_dat)\nsample_mean\n## [1] 7.788\n\n\n# Bootstrap Distribution\nset.seed(1) # to be replicable\nbootstrap_means <- vector(length=9999)\nfor(b in seq_along(bootstrap_means)){\n    dat_id <- seq_along(sample_dat)\n    boot_id <- sample(dat_id , replace=T)\n    dat_b  <- sample_dat[boot_id] # c.f. jackknife\n    mean_b <- mean(dat_b)\n    bootstrap_means[b] <-mean_b\n}\n\n# Boot CI\nboot_ci <- quantile(bootstrap_means, probs=c(.025, .975), na.rm=T)\nboot_ci\n##   2.5%  97.5% \n## 6.6039 8.9420\n```\n:::\n\n:::\n\n\n## Normal Approximation\n\nGiven the sampling distribution is approximately normal, the usual confidence intervals are symmetric. For the sample mean $M$, we can then construct the interval $[M - E, M + E]$, where $E$ is a \"margin of error\" on either side of $M$. A coverage level of $1-\\alpha$ means $Prob( M - E < \\mu < M + E)=1-\\alpha$. I.e., if the same sampling procedure were repeated $100$ times from the same population, approximately $95$ of the resulting intervals would be expected to contain the true population mean.^[Notice that $Prob( M - E < \\mu < M + E) = Prob( - E < \\mu - M < + E) = Prob( \\mu + E > M > \\mu - E)$. So if the interval $[\\mu - 10, \\mu + 10]$ contains $95\\%$ of all $M$, then the interval $[M-10, M+10]$ will also contain $\\mu$ in $95\\%$ of the samples because whenever $M$ is within $10$ of $\\mu$, the value $\\mu$ is also within $10$ of $M$. But for any particular sample, the interval $[\\hat{M}-10, \\hat{M}+10]$ either does or does not contain $\\mu$. Similarly, if you compute $\\hat{M}=9$ for your particular sample, a coverage level of $1-\\alpha=95\\%$ does not mean $Prob(9 - E < \\mu < 9 + E)=95\\%$.] We can also compute from theory that $\\pm 1.96~ SE(M)$ corresponds to the critical values of the Normal distribution, where $SE(M)$ is estimated using the bootstrap distribution or theory (classical SEs: $\\hat{S}/\\sqrt{n}$).\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, suppose we sample $n=36$ monthly electricity bills (in dollars) and find $\\hat{M}=142$ and $\\hat{S}=30$. The classical standard error is $SE(M) \\approx \\hat{S}/\\sqrt{n} = 30/\\sqrt{36} = 5$. A $95\\%$ confidence interval is\n$$\\hat{M} \\pm 1.96 \\times SE(M) = 142 \\pm 1.96 \\times 5 = [132.2,~ 151.8].$$\nWe are $95\\%$ confident that the population mean electricity bill is between $\\$132.20$ and $\\$151.80$.\n\n::: {.cell}\n\n```{.r .cell-code}\nM_hat <- 142\nS_hat <- 30\nn <- 36\nSE <- S_hat / sqrt(n)\nSE\n## [1] 5\n\n# 95% CI\nM_hat + c(-1.96, 1.96) * SE\n## [1] 132.2 151.8\n```\n:::\n\n:::\n\n\nThe main advantages of the Normal approximation is that 1) it can be computed formulaically as above and b) it works well for estimating extreme probabilities, where resampling methods tend to be worse. The main disadvantage is that the sampling distribution might be far from normal. In the example below, they are all quite similar, but that does not always need to be the case.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap Distribution with Percentile CI\nhist(bootstrap_means, breaks=25,\n    main='Percentile vs Normal 95% CIs',\n    font.main=1, border=NA,\n    freq=F, ylim=c(0,0.7),\n    xlab=expression(hat(b)[b]))\nboot_ci_percentile <- quantile(bootstrap_means, probs=c(.025,.975))\nabline(v=boot_ci_percentile, lty=1)\n\n# Normal Approximation with Bootstrap SEs\nx <- seq(5,10,by=0.01)\nse_boot <- sd(bootstrap_means)\nfx <- dnorm(x,sample_mean,se_boot)\nlines(x, fx, col=\"blue\", lty=1)\nboot_ci_normal <- qnorm(c(.025,.975), sample_mean, se_boot)\nabline(v=boot_ci_normal, col=\"blue\", lty=3)\n\n# Normal Approximation with IID Theory SEs\nclassic_se <- sd(sample_dat)/sqrt(length(sample_dat))\nfx2 <- dnorm(x,sample_mean,classic_se)\nlines(x, fx2, col=\"red\", lty=1)\nci_normal <-  qnorm(c(.025,.975), sample_mean, classic_se) #sample_mean+c(-1.96, +1.96)*classic_se\nabline(v=ci_normal, col=\"red\", lty=3)\n```\n\n::: {.cell-output-display}\n![](01_07_Intervals_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nHere is an example showing what a Normal CI estimates\n\n::: {.cell}\n\n```{.r .cell-code}\n# Normal CI for each sample\nxq <- apply(x_samples, 1, function(r){ #theoretical se's\n    mean(r) + c(-1,1)*sd(r)/sqrt(length(r))\n})\n# First 3 interval estimates\nxq[, c(1,2,3)]\n##           [,1]      [,2]      [,3]\n## [1,] 0.4919691 0.5141185 0.4477037\n## [2,] 0.5277886 0.5500446 0.4833812\n\n# Explicit calculation\nmu_true <- 0.5 # theoretical result for uniform samples\n# Logical vector: whether the true mean is in each CI\ncovered <- mu_true >= xq[1, ] & mu_true <= xq[2, ]\n# Empirical coverage rate\ncoverage_rate <- mean(covered)\ncat(sprintf(\"Estimated coverage probability: %.2f%%\\n\", 100 * coverage_rate))\n## Estimated coverage probability: 66.00%\n\n# Theoretically: [-1 sd, +1 sd] has 2/3 coverage\n# Change to [-2 sd, +2 sd] to see Precision-Accuracy tradeoff.\n```\n:::\n\n:::\n\n\n## Misc. Topics\n\n#### **One-Sided Intervals**. {-}\n\nAbove, our confidence intervals were two-sided: they contained the middle $Z\\%$ of the sampling distribution. We can also construct one-sided intervals that extend to infinity in one direction.\n\nA one-sided interval is shifted to one side, containing one tail rather than the middle. For example, an upper-bounded interval uses $(-\\infty, q_{0.95}]$, where $q_{0.95}$ is the $95^{\\text{th}}$ percentile of the bootstrap distribution: we are $95\\%$ confident the true value is at most $q_{0.95}$. A lower-bounded interval uses $[q_{0.05}, \\infty)$, where $q_{0.05}$ is the $5^{\\text{th}}$ percentile of the bootstrap distribution: we are $95\\%$ confident the true value is at least $q_{0.05}$.\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor the electricity bill example above with $\\hat{M}=142$ and $SE=5$, a one-sided $95\\%$ upper-bounded confidence interval is\n$$(-\\infty,~ \\hat{M} + 1.645 \\times SE] = (-\\infty,~ 142 + 1.645 \\times 5] = (-\\infty,~ 150.2].$$\nWe are $95\\%$ confident the population mean bill is at most $\\$150.20$. Note that $1.645$ is the $95^{\\text{th}}$ percentile of the standard normal (compared to $1.96$ for the two-sided case).\n\n::: {.cell}\n\n```{.r .cell-code}\nM_hat <- 142\nSE <- 5\n# One-sided 95% upper bound\nM_hat + qnorm(0.95) * SE\n## [1] 150.2243\n```\n:::\n\n:::\n\n:::{.callout-tip icon=false collapse=\"true\"}\nHere is a bootstrap right-tail example\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap Distribution\nset.seed(1) # to be replicable\nbootstrap_means <- vector(length=9999)\nfor(b in seq_along(bootstrap_means)){\n    dat_id <- seq(1,length(sample_dat))\n    boot_id <- sample(dat_id, replace=T)\n    dat_b  <- sample_dat[boot_id] # c.f. jackknife\n    mean_b <- mean(dat_b)\n    bootstrap_means[b] <-mean_b\n}\n\n# One-sided 95% lower bound\n# We are 95% confident the mean is at least q_05\nhist(bootstrap_means, border=NA, breaks=50,\n    freq=F, main=NA, xlab='Bootstrap')\nabline(v=sample_mean, col=4)\nci_95 <- quantile(bootstrap_means, probs=c(0.05,1) )\nabline(v=ci_95, lwd=2)\n```\n\n::: {.cell-output-display}\n![](01_07_Intervals_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n:::\n\n\n#### **Prediction Intervals**. {-}\n\nNote that $Z\\%$ confidence intervals do not generally cover $Z\\%$ of the data (those types of intervals are covered later). In the examples above, notice the confidence interval for the mean differs from the confidence interval of the median, and so both cannot cover $90\\%$ of the data. The confidence interval for the mean is roughly $[0.48, 0.52]$, which theoretically covers only a $0.52-0.48=0.04$ proportion of uniform random data, much less than the proportion $0.9$.\n\nIn addition to confidence intervals, we can also compute a *prediction interval* which estimate the variability of new data rather than a statistic. To do so, we compute the lower/upper quantiles of the data.\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- runif(1000)\n# Middle 90% of values\nxq0 <- quantile(x, probs=c(.05,.95))\npaste0('we are 90% confident that a future data point will be between ', \n    round(xq0[1],2), ' and ', round(xq0[2],2) )\n## [1] \"we are 90% confident that a future data point will be between 0.05 and 0.95\"\n\nhist(x,\n    breaks=seq(0,1,by=.01), border=NA,\n    main='Prediction Interval', font.main=1)\nabline(v=xq0)\n```\n\n::: {.cell-output-display}\n![](01_07_Intervals_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n## Further Reading\n\nSee\n\n* <https://www.r-bloggers.com/2025/02/bootstrap-vs-standard-error-confidence-intervals/>\n",
    "supporting": [
      "01_07_Intervals_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}