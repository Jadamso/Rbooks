{
  "hash": "c7d2a904baecff67b3dd8b2d0329e109",
  "result": {
    "engine": "knitr",
    "markdown": "\n# Bivariate Data\n***\n\nWe will now study them in more detail. Suppose we have two discrete variables $X_{i}$ and $Y_{i}$. The data for each observation data can be grouped together as a vector $(X_{i}, Y_{i})$.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bivariate Data from USArrests\nxy <- USArrests[,c('Murder','UrbanPop')]\nxy[1,]\n##         Murder UrbanPop\n## Alabama   13.2       58\n```\n:::\n\n\n## Types of Distributions\n\n#### **Joint Distributions**.{-}\n\nScatterplots are used frequently to summarize the joint relationship between two variables, multiple observations of $(X_{i}, Y_{i})$. They can be enhanced in several ways. As a default, use semi-transparent points so as not to hide any points (and perhaps see if your observations are concentrated anywhere). You can also add other features that help summarize the relationship, although I will defer this until later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(Murder~UrbanPop, USArrests, pch=16, col=grey(0.,.5))\n```\n\n::: {.cell-output-display}\n![](01_09_BivariateData_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nIf you have many points, you can also use a 2D histogram instead. <https://plotly.com/r/2D-Histogram/>.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(plotly)\nfig <- plot_ly(\n    USArrests, x = ~UrbanPop, y = ~Assault)\nfig <- add_histogram2d(fig, nbinsx=25, nbinsy=25)\nfig\n```\n:::\n\n\n#### **Marginal Distributions**.{-}\nYou can also show the distributions of each variable along each axis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setup Plot\nlayout( matrix(c(2,0,1,3), ncol=2, byrow=TRUE),\n    widths=c(9/10,1/10), heights=c(1/10,9/10))\n\n# Scatterplot\npar(mar=c(4,4,1,1))\nplot(Murder~UrbanPop, USArrests, pch=16, col=rgb(0,0,0,.5))\n\n# Add Marginals\npar(mar=c(0,4,1,1))\nxhist <- hist(USArrests[,'UrbanPop'], plot=FALSE)\nbarplot(xhist[['counts']], axes=FALSE, space=0, border=NA)\n\npar(mar=c(4,0,1,1))\nyhist <- hist(USArrests[,'Murder'], plot=FALSE)\nbarplot(yhist[['counts']], axes=FALSE, space=0, horiz=TRUE, border=NA)\n```\n\n::: {.cell-output-display}\n![](01_09_BivariateData_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n#### **Conditional Distributions**.{-}\n\nWe can show how distributions and densities change according to a second (or even third) variable using data splits. E.g., \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tailored Histogram \nylim <- c(0,8)\nxbks <-  seq(min(USArrests[,'Murder'])-1, max(USArrests[,'Murder'])+1, by=1)\n\n# Also show more information\n# Split Data by Urban Population above/below mean\npop_mean <- mean(USArrests[,'UrbanPop'])\npop_cut <- USArrests[,'UrbanPop']< pop_mean\nmurder_lowpop <- USArrests[pop_cut,'Murder']\nmurder_highpop <- USArrests[!pop_cut,'Murder']\ncols <- c(low=rgb(0,0,1,.75), high=rgb(1,0,0,.75))\n\npar(mfrow=c(1,2))\nhist(murder_lowpop,\n    breaks=xbks, col=cols[1],\n    main='Urban Pop >= Mean', font.main=1,\n    xlab='Murder Arrests',\n    border=NA, ylim=ylim)\n\nhist(murder_highpop,\n    breaks=xbks, col=cols[2],\n    main='Urban Pop < Mean', font.main=1,\n    xlab='Murder Arrests',\n    border=NA, ylim=ylim)\n```\n\n::: {.cell-output-display}\n![](01_09_BivariateData_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nIt is sometimes it is preferable to show the ECDF instead. And you can glue various combinations together to convey more information all at once\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\n# Full Sample Density\nhist(USArrests[,'Murder'], \n    main='Density Function Estimate', font.main=1,\n    xlab='Murder Arrests',\n    breaks=xbks, freq=F, border=NA)\n\n# Split Sample Distribution Comparison\nF_lowpop <- ecdf(murder_lowpop)\nplot(F_lowpop, col=cols[1],\n    pch=16, xlab='Murder Arrests',\n    main='Distribution Function Estimates',\n    font.main=1, bty='n')\nF_highpop <- ecdf(murder_highpop)\nplot(F_highpop, add=T, col=cols[2], pch=16)\n\nlegend('bottomright', col=cols,\n    pch=16, bty='n', inset=c(0,.1),\n    title='% Urban Pop.',\n    legend=c('Low (<= Mean)','High (>= Mean)'))\n```\n\n::: {.cell-output-display}\n![](01_09_BivariateData_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple Interactive Scatter Plot\n# plot(Assault~UrbanPop, USArrests, col=grey(0,.5), pch=16,\n#    cex=USArrests[,'Murder']/diff(range(USArrests[,'Murder']))*2,\n#    main='US Murder arrests (per 100,000)')\n```\n:::\n\n\n\nYou can also split data into grouped boxplots in the same way\n\n::: {.cell}\n\n```{.r .cell-code}\nlayout( t(c(1,2,2)))\nboxplot(USArrests[,'Murder'], main='',\n    xlab='All Data', ylab='Murder Arrests')\n\n# K Groups with even spacing\nK <- 3\nUSArrests[,'UrbanPop_Kcut'] <- cut(USArrests[,'UrbanPop'],K)\nKcols <- hcl.colors(K,alpha=.5)\nboxplot(Murder~UrbanPop_Kcut, USArrests,\n    main='', col=Kcols,\n    xlab='Urban Population', ylab='')\n```\n\n::: {.cell-output-display}\n![](01_09_BivariateData_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# 4 Groups with equal numbers of observations\n#Qcuts <- c(\n#    '0%'=min(USArrests[,'UrbanPop'])-10*.Machine[['double.eps']],\n#    quantile(USArrests[,'UrbanPop'], probs=c(.25,.5,.75,1)))\n#USArrests[,'UrbanPop']_cut <- cut(USArrests[,'UrbanPop'], Qcuts)\n#boxplot(Murder~UrbanPop_cut, USArrests, col=hcl.colors(4,alpha=.5))\n```\n:::\n\n\n\n\n\n## Statistics of Association\n\nThere are several ways to statistically describe the relationship between two variables. The major differences surround whether the data are cardinal or an ordered/unordered factor.\n\n#### **Cardinal**. {-}\n*Pearson (Linear) Correlation*.\nSuppose $X$ and $Y$ are both cardinal data. As such, you can compute the most famous measure of association, the covariance:\n$$\nC_{XY} =  \\sum_{i} [X_i - \\overline{X}] [Y_i - \\overline{Y}] / N\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\n#plot(xy, pch=16, col=grey(0,.25))\ncov(xy)\n##             Murder   UrbanPop\n## Murder   18.970465   4.386204\n## UrbanPop  4.386204 209.518776\n```\n:::\n\nNote that $C_{XX}=V_{X}$.\nFor ease of interpretation and comparison, we rescale this statistic to always lay between $-1$ and $1$ \n$$\nr_{XY} = \\frac{ C_{XY} }{ \\sqrt{V_X} \\sqrt{V_Y}}\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(xy)[2]\n## [1] 0.06957262\n```\n:::\n\n\n*Falk Codeviance*.\nThe Codeviance is a robust alternative to Covariance. Instead of relying on means (which can be sensitive to outliers), it uses medians ($\\tilde{X}$) to capture the central tendency.^[See also *Theil-Sen Estimator*, which may be seen as a precursor.] We can also scale the Codeviance by the median absolute deviation to compute the median correlation.\n\\begin{eqnarray}\n\\text{CoDev}(X,Y) = \\text{Med}\\left\\{ |X_i - \\tilde{X}| |Y_i - \\tilde{Y}| \\right\\} \\\\\n\\tilde{r}_{XY} = \\frac{ \\text{CoDev}(X,Y) }{ \\text{MAD}(X) \\text{MAD}(Y) }.\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\ncor_m <- function(xy) {\n  # Compute medians for each column\n  med <- apply(xy, 2, median)\n  # Subtract the medians from each column\n  xm <- sweep(xy, 2, med, \"-\")\n  # Compute CoDev\n  CoDev <- median(xm[, 1] * xm[, 2])\n  # Compute the medians of absolute deviation\n  MadProd <- prod( apply(abs(xm), 2, median) )\n  # Return the robust correlation measure\n  return( CoDev / MadProd)\n}\ncor_m(xy)\n## [1] 0.005707763\n```\n:::\n\n\n#### **Ordered Factor**. {-}\nSuppose $X$ and $Y$ are both *ordered* variables. *Kendall's Tau* measures the strength and direction of association by counting the number of concordant pairs (where the ranks agree) versus discordant pairs (where the ranks disagree). A value of $\\tau = 1$ implies perfect agreement in rankings, $\\tau = -1$ indicates perfect disagreement, and $\\tau = 0$ suggests no association in the ordering.\n$$\n\\tau = \\frac{2}{n(n-1)} \\sum_{i} \\sum_{j > i} \\text{sgn} \\Bigl( (X_i - X_j)(Y_i - Y_j) \\Bigr),\n$$\nwhere the sign function is:\n$$\n\\text{sgn}(z) = \n\\begin{cases}\n+1 & \\text{if } z > 0\\\\\n0  & \\text{if } z = 0 \\\\\n-1 & \\text{if} z < 0 \n\\end{cases}.\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\nxy[,1] <- rank(xy[,1] )\nxy[,2] <- rank(xy[,2] )\n# plot(xy, pch=16, col=grey(0,.25))\ntau <- cor(xy[, 1], xy[, 2], method = \"kendall\")\nround(tau, 3)\n## [1] 0.074\n```\n:::\n\n\n#### **Unordered Factor**. {-}\nSuppose $X$ and $Y$ are both *categorical* variables; the value of $X$ is one of $1...r$ categories and the value of $Y$ is one of $1...k$ categories. *Cramer's V* quantifies the strength of association by adjusting a \"chi-squared\" statistic to provide a measure that ranges from $0$ to $1$; $0$ indicates no association while a value closer to $1$ signifies a strong association. \n\nFirst, consider a contingency table for $X$ and $Y$ with $r$ rows and $k$ columns. The chi-square statistic is then defined as:\n\n$$\n\\chi^2 = \\sum_{i=1}^{r} \\sum_{j=1}^{k} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}.\n$$\n\nwhere\n\n- $O_{ij}$ denote the observed frequency in cell $(i, j)$,\n- $E_{ij} = \\frac{R_i \\cdot C_j}{n}$ is the expected frequency for each cell if $X$ and $Y$ are independent\n- $R_i$ denote the total frequency for row $i$ (i.e., $R_i = \\sum_{j=1}^{k} O_{ij}$),\n- $C_j$ denote the total frequency for column $j$ (i.e., $C_j = \\sum_{i=1}^{r} O_{ij}$),\n- $n$ be the grand total of observations, so that $n = \\sum_{i=1}^{r} \\sum_{j=1}^{k} O_{ij}$.\n\n\nSecond, normalize the chi-square statistic with the sample size and the degrees of freedom to compute Cramer's V. \n\n$$\nV = \\sqrt{\\frac{\\chi^2 / n}{\\min(k - 1, \\, r - 1)}},\n$$\n\nwhere:\n\n- $n$ is the total sample size,\n- $k$ is the number of categories for one variable,\n- $r$ is the number of categories for the other variable.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\nxy[,1] <- cut(xy[,1],3)\nxy[,2] <- cut(xy[,2],4)\ntable(xy)\n##               UrbanPop\n## Murder         (31.9,46.8] (46.8,61.5] (61.5,76.2] (76.2,91.1]\n##   (0.783,6.33]           4           5           8           5\n##   (6.33,11.9]            0           4           7           6\n##   (11.9,17.4]            2           4           2           3\n\ncor_v <- function(xy){\n    # Create a contingency table from the categorical variables\n    tbl <- table(xy)\n    # Compute the chi-square statistic (without Yates' continuity correction)\n    chi2 <- chisq.test(tbl, correct=FALSE)[['statistic']]\n    # Total sample size\n    n <- sum(tbl)\n    # Compute the minimum degrees of freedom (min(rows-1, columns-1))\n    df_min <- min(nrow(tbl) - 1, ncol(tbl) - 1)\n    # Calculate Cramer's V\n    V <- sqrt((chi2 / n) / df_min)\n    return(V)\n}\ncor_v(xy)\n## X-squared \n## 0.2307071\n\n# DescTools::CramerV( table(xy) )\n```\n:::\n\n\n\n#### **Mixed**. {-}\n\nFor mixed data, where $Y_{i}$ is a cardinal variable and $X_{i}$ is an unordered factor variable, we analyze associations via group comparisons. You have already seen via [two-sample difference](https://jadamso.github.io/Rbooks/01_07_HypothesisTests.html#two-sample-differences), which corresponds to an $X_{i}$ with two categories.\n\n\n\n## Probability Theory\n\n#### **Definitions for Discrete Data**. {-}\nThe *joint distribution* is defined as\n\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y)\n\\end{eqnarray}\nVariables are *statistically independent* if $Prob(X_{i} = x, Y_{i} = y)= Prob(X_{i} = x) Prob(Y_{i} = y)$ for all $x, y$. Independance is sometimes assumed for mathematical simplicity, not because it generally fits data well.^[The same can be said about assuming normally distributed errors, although at least that can be motivated by the Central Limit Theorems.]\n\n\nThe *conditional distributions* are defined as\n\\begin{eqnarray}\nProb(X_{i} = x | Y_{i} = y) = \\frac{ Prob(X_{i} = x, Y_{i} = y)}{ Prob( Y_{i} = y )}\\\\\nProb(Y_{i} = y | X_{i} = x) = \\frac{ Prob(X_{i} = x, Y_{i} = y)}{ Prob( X_{i} = x )}\n\\end{eqnarray}\nThe *marginal distributions* are then defined as\n\\begin{eqnarray}\nProb(X_{i} = x) = \\sum_{y} Prob(X_{i} = x | Y_{i} = y) Prob( Y_{i} = y ) \\\\\nProb(Y_{i} = y) = \\sum_{x} Prob(Y_{i} = y | X_{i} = x) Prob( X_{i} = x ),\n\\end{eqnarray}\nwhich is also known as the *law of total probability*.\n\n#### **Fair Coin Flips Example**. {-}\n\nFor one example, Consider flipping two coins. Denoted each coin as $k \\in \\{1, 2\\}$, and mark whether \"heads\" is face up; $X_{ki}=1$ if Heads and $=0$ if Tails. Suppose both coins are \"fair\": $Prob(X_{i}=1)= 1/2$ and $Prob(Y_{i}=1|X_{i})=1/2$, then the four potential outcomes have equal probabilities. The joint distribution is \n\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y) &=& Prob(X_{i} = x) Prob(Y_{i} = y)\\\\\nProb(X_{i} = 0, Y_{i} = 0) &=& 1/2 \\times 1/2 = 1/4 \\\\\nProb(X_{i} = 0, Y_{i} = 1) &=& 1/4 \\\\\nProb(X_{i} = 1, Y_{i} = 0) &=& 1/4 \\\\\nProb(X_{i} = 1, Y_{i} = 1) &=& 1/4 .\n\\end{eqnarray}\nThe marginal distribution of the second coin is \n\\begin{eqnarray}\nProb(Y_{i} = 0) &=& Prob(Y_{i} = 0 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 0 | X_{i} = 1) Prob(X_{i}=1)\\\\\n&=& 1/2 \\times 1/2 + 1/2 \\times 1/2 = 1/2\\\\\nProb(Y_{i} = 1) &=& Prob(Y_{i} = 1 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 1 | X_{i} = 1) Prob(X_{i}=1)\\\\\n&=& 1/2 \\times 1/2 + 1/2 \\times 1/2 = 1/2\n\\end{eqnarray}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a 2x2 matrix for the joint distribution.\n# Rows correspond to X1 (coin 1), and columns correspond to X2 (coin 2).\nP_fair <- matrix(1/4, nrow = 2, ncol = 2)\nrownames(P_fair) <- c(\"X1=0\", \"X1=1\")\ncolnames(P_fair) <- c(\"X2=0\", \"X2=1\")\nP_fair\n##      X2=0 X2=1\n## X1=0 0.25 0.25\n## X1=1 0.25 0.25\n\n# Compute the marginal distributions.\n# Marginal for X1: sum across columns.\nP_X1 <- rowSums(P_fair)\nP_X1\n## X1=0 X1=1 \n##  0.5  0.5\n# Marginal for X2: sum across rows.\nP_X2 <- colSums(P_fair)\nP_X2\n## X2=0 X2=1 \n##  0.5  0.5\n\n# Compute the conditional probabilities Prob(X2 | X1).\ncond_X2_given_X1 <- matrix(0, nrow = 2, ncol = 2)\nfor (j in 1:2) {\n  cond_X2_given_X1[, j] <- P_fair[, j] / P_X1[j]\n}\nrownames(cond_X2_given_X1) <- c(\"X2=0\", \"X2=1\")\ncolnames(cond_X2_given_X1) <- c(\"given X1=0\", \"given X1=1\")\ncond_X2_given_X1\n##      given X1=0 given X1=1\n## X2=0        0.5        0.5\n## X2=1        0.5        0.5\n```\n:::\n\n\n#### **UnFair Coin Flips Example**. {-}\nConsider a second example, where the second coin is \"Completely Unfair\", so that it is always the same as the first. The outcomes generated with a Completely Unfair coin are the same as if we only flipped one coin.\n\\begin{eqnarray}\nProb(X_{i} = x, Y_{i} = y) &=& Prob(X_{i} = x) \\mathbf{1}( x=y )\\\\\nProb(X_{i} = 0, Y_{i} = 0) &=& 1/2 \\\\\nProb(X_{i} = 0, Y_{i} = 1) &=& 0 \\\\\nProb(X_{i} = 1, Y_{i} = 0) &=& 0 \\\\\nProb(X_{i} = 1, Y_{i} = 1) &=& 1/2 .\n\\end{eqnarray}\nNote that $\\mathbf{1}(X_{i}=1)$ means $X_{i}= 1$ and $0$ if $X_{i}\\neq0$.\nThe marginal distribution of the second coin is \n\\begin{eqnarray}\nProb(Y_{i} = 0) \n&=& Prob(Y_{i} = 0 | X_{i} = 0) Prob(X_{i}=0) + Prob(Y_{i} = 0 | X_{i} = 1) Prob(X_{i} = 1)\\\\\n&=& 1/2 \\times 1 + 0 \\times 1/2 = 1/2 .\\\\\nProb(Y_{i} = 1)\n&=& Prob(Y_{i} = 1 | X_{i} =0) Prob( X_{i} = 0) + Prob(Y_{i} = 1 | X_{i} = 1) Prob( X_{i} = 1)\\\\\n&=& 0\\times 1/2 + 1 \\times 1/2 = 1/2 .\n\\end{eqnarray}\nwhich is the same as in the first example! Different joint distributions can have the same marginal distributions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the joint distribution matrix for the unfair coin case.\nP_unfair <- matrix(c(0.5, 0, 0, 0.5), nrow = 2, ncol = 2, byrow = TRUE)\nrownames(P_unfair) <- c(\"X1=0\", \"X1=1\")\ncolnames(P_unfair) <- c(\"X2=0\", \"X2=1\")\nP_unfair\n##      X2=0 X2=1\n## X1=0  0.5  0.0\n## X1=1  0.0  0.5\n\n# Compute the marginal distribution for X2 in the unfair case.\nP_X2_unfair <- colSums(P_unfair)\nP_X1_unfair <- rowSums(P_unfair)\n\n# Compute the conditional probabilities Prob(X1 | X2) for the unfair coin.\ncond_X2_given_X1_unfair <- matrix(NA, nrow = 2, ncol = 2)\nfor (j in 1:2) {\n  if (P_X1_unfair[j] > 0) {\n    cond_X2_given_X1_unfair[, j] <- P_unfair[, j] / P_X1_unfair[j]\n  }\n}\nrownames(cond_X2_given_X1_unfair) <- c(\"X2=0\", \"X2=1\")\ncolnames(cond_X2_given_X1_unfair) <- c(\"given X1=0\", \"given X1=1\")\ncond_X2_given_X1_unfair\n##      given X1=0 given X1=1\n## X2=0          1          0\n## X2=1          0          1\n```\n:::\n\n\n\n\n#### **Definitions for Continuous Data**. {-}\nThe *joint distribution* is defined as\n\\begin{eqnarray}\nF(x, y) &=& Prob(X_{i} \\leq x, Y_{i} \\leq y)\n\\end{eqnarray}\nThe *marginal distributions* are then defined as\n\\begin{eqnarray}\n F_{X}(x) &=& F(x, \\infty)\\\\\n F_{Y}(y) &=& F(\\infty, y).\n\\end{eqnarray}\nwhich is also known as the *law of total probability*.\nVariables are statistically independent if $F(x, y) = F_{X}(x)F_{Y}(y)$ for all $x, y$.\n\n\nFor example, suppose $(X_{i},Y_{i})$ is bivariate normal with  means $(\\mu_{X}, \\mu_{Y})$, variances $(\\sigma_{X}, \\sigma_{Y})$ and covariance $\\rho$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate Bivariate Data\nN <- 10000\nMu <- c(2,2) ## Means\n\nSigma1 <- matrix(c(2,-.8,-.8,1),2,2) ## CoVariance Matrix\nMVdat1 <- mvtnorm::rmvnorm(N, Mu, Sigma1)\n\nSigma2 <- matrix(c(2,.4,.4,1),2,2) ## CoVariance Matrix\nMVdat2 <- mvtnorm::rmvnorm(N, Mu, Sigma2)\n\npar(mfrow=c(1,2))\n## Different diagonals\nplot(MVdat2, col=rgb(1,0,0,0.02), pch=16,\n    main='Joint', font.main=1,\n    ylim=c(-4,8), xlim=c(-4,8), xlab='X1', ylab='X2')\npoints(MVdat1,col=rgb(0,0,1,0.02),pch=16)\n## Same marginal distributions\nxbks <- seq(-4,8,by=.2)\nhist(MVdat2[,2], col=rgb(1,0,0,0.5),\n    breaks=xbks, border=NA, xlab='X2',\n    main='Marginal', font.main=1)\nhist(MVdat1[,2], col=rgb(0,0,1,0.5),\n    add=T, breaks=xbks, border=NA)\n```\n\n::: {.cell-output-display}\n![](01_09_BivariateData_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# See that independent data are a special case\nn <- 2e4\n## 2 Indepenant RV\nXYiid <- cbind( rnorm(n),  rnorm(n))\n## As a single Joint Draw\nXYjoint <- mvtnorm::rmvnorm(n, c(0,0))\n## Plot\npar(mfrow=c(1,2))\nplot(XYiid, xlab=\n    col=grey(0,.05), pch=16, xlim=c(-5,5), ylim=c(-5,5))\nplot(XYjoint,\n    col=grey(0,.05), pch=16, xlim=c(-5,5), ylim=c(-5,5))\n\n# Compare densities\n#d1 <- dnorm(XYiid[,1],0)*dnorm(XYiid[,2],0)\n#d2 <- mvtnorm::dmvnorm(XYiid, c(0,0))\n#head(cbind(d1,d2))\n```\n:::\n\n\n\nThe multivariate normal is a workhorse for analytical work on multivariate random variables, but there are many more. See e.g., <https://cran.r-project.org/web/packages/NonNorMvtDist/NonNorMvtDist.pdf>\n\n\n#### **Important Applications**. {-}\n\nNote *Simpson's Paradox*:\n\nAlso note *Bayes' Theorem*:\n\\begin{eqnarray}\nProb(X_{i} = x | Y_{i} = y)  Prob( Y_{i} = y) \n    &=& Prob(X_{i} = x, Y_{i} = y) = Prob(Y_{i} = y | X_{i} = x) Prob(X_{i} = x).\\\\\nProb(X_{i} = x | Y_{i} = y)\n    &=& \\frac{ Prob(Y_{i} = y | X_{i} = x) Prob(X_{i}=x) }{ Prob( Y_{i} = y) }.\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Verify Bayes' theorem for the unfair coin case:\n# Compute Prob(X1=1 | X2=1) using the formula:\n#   Prob(X1=1 | X2=1) = [Prob(X2=1 | X1=1) * Prob(X1=1)] / Prob(X2=1)\n\nP_X1_1 <- 0.5\nP_X2_1_given_X1_1 <- 1  # Since coin 2 copies coin 1.\nP_X2_1 <- P_X2_unfair[\"X2=1\"]\n\nbayes_result <- (P_X2_1_given_X1_1 * P_X1_1) / P_X2_1\nbayes_result\n## X2=1 \n##    1\n```\n:::\n\n\n\n\n## Further Reading \n\nFor plotting histograms and marginal distributions, see \n\n* <https://www.r-bloggers.com/2011/06/example-8-41-scatterplot-with-marginal-histograms/>\n* <https://r-graph-gallery.com/histogram.html>\n* <https://r-graph-gallery.com/74-margin-and-oma-cheatsheet.html>\n* <https://jtr13.github.io/cc21fall2/tutorial-for-scatter-plot-with-marginal-distribution.html>\n\nMany introductory econometrics textbooks have a good appendix on probability and statistics. There are many useful statistical texts online too\n\nSee the Further reading about Probability Theory in the Statistics chapter.\n\n* <https://www.r-bloggers.com/2024/03/calculating-conditional-probability-in-r/>\n\nOther Statistics \n\n* <https://cran.r-project.org/web/packages/qualvar/vignettes/wilcox1973.html>\n\n\n",
    "supporting": [
      "01_09_BivariateData_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}