{
  "hash": "44da220aa5b98955c0d286e2108b1cda",
  "result": {
    "engine": "knitr",
    "markdown": "\n# Bivariate Data\n***\n\n## Statistics\n\nThere are several ways to quantitatively describe the relationship between two variables, $Y$ and $X$. The major differences surround whether the variables are cardinal, ordinal, or categorical.\n\n#### **Correlation**. {-}\n**Pearson (Linear) Correlation**.\nSuppose $X$ and $Y$ are both cardinal data. As such, you can compute the most famous measure of association, the covariance:\n$$\nC_{XY} =  \\sum_{i} [X_i - \\overline{X}] [Y_i - \\overline{Y}] / N\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bivariate Data from USArrests\nxy <- USArrests[,c('Murder','UrbanPop')]\n#plot(xy, pch=16, col=grey(0,.25))\ncov(xy)\n##             Murder   UrbanPop\n## Murder   18.970465   4.386204\n## UrbanPop  4.386204 209.518776\n```\n:::\n\nNote that $C_{XX}=V_{X}$.\nFor ease of interpretation, we rescale this statistic to always lay between $-1$ and $1$ \n$$\nr_{XY} = \\frac{ C_{XY} }{ \\sqrt{V_X} \\sqrt{V_Y}}\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(xy)[2]\n## [1] 0.06957262\n```\n:::\n\n\n**Falk Codeviance**.\nThe Codeviance is a robust alternative to Covariance. Instead of relying on means (which can be sensitive to outliers), it uses medians ($\\tilde{X}$) to capture the central tendency.^[See also *Theil-Sen Estimator*, which may be seen as a precursor.] We can also scale the Codeviance by the median absolute deviation to compute the median correlation.\n\\[\n\\text{CoDev}(X,Y) = \\text{Med}\\left\\{ |X_i - \\tilde{X}| |Y_i - \\tilde{Y}| \\right\\} \\\\\n\\tilde{r}_{XY} = \\frac{ \\text{CoDev}(X,Y) }{ \\text{MAD}(X) \\text{MAD}(Y) }.\n\\]\n\n::: {.cell}\n\n```{.r .cell-code}\ncor_m <- function(xy) {\n  # Compute medians for each column\n  med <- apply(xy, 2, median)\n  # Subtract the medians from each column\n  xm <- sweep(xy, 2, med, \"-\")\n  # Compute CoDev\n  CoDev <- median(xm[, 1] * xm[, 2])\n  # Compute the medians of absolute deviation\n  MadProd <- prod( apply(abs(xm), 2, median) )\n  # Return the robust correlation measure\n  return( CoDev / MadProd)\n}\ncor_m(xy)\n## [1] 0.005707763\n```\n:::\n\n\n#### **Kendall's Tau**. {-}\nSuppose $X$ and $Y$ are both *ordered* variables. Kendall's Tau measures the strength and direction of association by counting the number of concordant pairs (where the ranks agree) versus discordant pairs (where the ranks disagree). A value of \\(\\tau = 1\\) implies perfect agreement in rankings, \\(\\tau = -1\\) indicates perfect disagreement, and \\(\\tau = 0\\) suggests no association in the ordering.\n\\[\n\\tau = \\frac{2}{n(n-1)} \\sum_{i} \\sum_{j > i} \\text{sgn} \\Bigl( (X_i - X_j)(Y_i - Y_j) \\Bigr),\n\\]\nwhere the sign function is:\n$$\n\\text{sgn}(z) = \n\\begin{cases}\n+1 & \\text{if } z > 0\\\\\n0  & \\text{if } z = 0 \\\\\n-1 & \\text{if} z < 0 \n\\end{cases}.\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\nxy[,1] <- rank(xy[,1] )\nxy[,2] <- rank(xy[,2] )\n# plot(xy, pch=16, col=grey(0,.25))\ntau <- cor(xy[, 1], xy[, 2], method = \"kendall\")\nround(tau, 3)\n## [1] 0.074\n```\n:::\n\n\n#### **Cramer's V**. {-}\nSuppose $X$ and $Y$ are both *categorical* variables; the value of $X$ is one of $1...r$ categories and the value of $Y$ is one of $1...k$ categories. Cramer's V quantifies the strength of association by adjusting a \"chi-squared\" statistic to provide a measure that ranges from 0 to 1; 0 indicates no association while a value closer to 1 signifies a strong association. \n\nFirst, consider a contingency table for $X$ and $Y$ with \\(r\\) rows and \\(k\\) columns. The chi-square statistic is then defined as:\n\n\\[\n\\chi^2 = \\sum_{i=1}^{r} \\sum_{j=1}^{k} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}.\n\\]\n\nwhere\n\n- \\(O_{ij}\\) denote the observed frequency in cell \\((i, j)\\),\n- \\(E_{ij} = \\frac{R_i \\cdot C_j}{n}\\) is the expected frequency for each cell if $X$ and $Y$ are independent\n- \\(R_i\\) denote the total frequency for row \\(i\\) (i.e., \\(R_i = \\sum_{j=1}^{k} O_{ij}\\)),\n- \\(C_j\\) denote the total frequency for column \\(j\\) (i.e., \\(C_j = \\sum_{i=1}^{r} O_{ij}\\)),\n- \\(n\\) be the grand total of observations, so that \\(n = \\sum_{i=1}^{r} \\sum_{j=1}^{k} O_{ij}\\).\n\n\nSecond, normalize the chi-square statistic with the sample size and the degrees of freedom to compute Cramer's V. \n\n\\[\nV = \\sqrt{\\frac{\\chi^2 / n}{\\min(k - 1, \\, r - 1)}},\n\\]\n\nwhere:\n\n- \\(n\\) is the total sample size,\n- \\(k\\) is the number of categories for one variable,\n- \\(r\\) is the number of categories for the other variable.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxy <- USArrests[,c('Murder','UrbanPop')]\nxy[,1] <- cut(xy[,1],3)\nxy[,2] <- cut(xy[,2],4)\ntable(xy)\n##               UrbanPop\n## Murder         (31.9,46.8] (46.8,61.5] (61.5,76.2] (76.2,91.1]\n##   (0.783,6.33]           4           5           8           5\n##   (6.33,11.9]            0           4           7           6\n##   (11.9,17.4]            2           4           2           3\n\ncor_v <- function(xy){\n    # Create a contingency table from the categorical variables\n    tbl <- table(xy)\n    # Compute the chi-square statistic (without Yates' continuity correction)\n    chi2 <- chisq.test(tbl, correct=FALSE)$statistic\n    # Total sample size\n    n <- sum(tbl)\n    # Compute the minimum degrees of freedom (min(rows-1, columns-1))\n    df_min <- min(nrow(tbl) - 1, ncol(tbl) - 1)\n    # Calculate Cramer's V\n    V <- sqrt((chi2 / n) / df_min)\n    return(V)\n}\ncor_v(xy)\n## X-squared \n## 0.2307071\n\n# DescTools::CramerV( table(xy) )\n```\n:::\n\n\n\n## Probability Theory\nSuppose we have two discrete variables $X_{1}$ and $X_{2}$.\n\nTheir **joint distribution** is denoted as\n\\begin{eqnarray}\nProb(X_{1} = x_{1}, X_{2} = x_{2})\n\\end{eqnarray}\nThe **conditional distributions** are defined as\n\\begin{eqnarray}\nProb(X_{1} = x_{1} | X_{2} = x_{2}) = \\frac{ Prob(X_{1} = x_{1}, X_{2} = x_{2})}{ Prob( X_{2} = x_{2} )}\\\\\nProb(X_{2} = x_{2} | X_{1} = x_{1}) = \\frac{ Prob(X_{1} = x_{1}, X_{2} = x_{2})}{ Prob( X_{1} = x_{1} )}\n\\end{eqnarray}\nThe **marginal distributions** are then defined as\n\\begin{eqnarray}\nProb(X_{1} = x_{1}) = \\sum_{x_{2}} Prob(X_{1} = x_{1} | X_{2} = x_{2}) Prob( X_{2} = x_{2} ) \\\\\nProb(X_{2} = x_{2}) = \\sum_{x_{1}} Prob(X_{2} = x_{2} | X_{1} = x_{1}) Prob( X_{1} = x_{1} ),\n\\end{eqnarray}\nwhich is also known as the *law of total probability*.\n\n#### **Example: Fair Coin**. {-}\nFor one example, Consider flipping two coins. Denoted each coin as $i \\in \\{1, 2\\}$, and mark whether \"heads\" is face up; $X_{i}=1$ if Heads and $=0$ if Tails. Suppose both coins are \"fair\": $Prob(X_{1}=1)= 1/2$ and $Prob(X_{2}=1|X_{1})=1/2$, then the four potential outcomes have equal probabilities. The joint distribution is \n\\begin{eqnarray}\nProb(X_{1} = x_{1}, X_{2} = x_{2}) &=& Prob(X_{1} = x_{1}) Prob(X_{2} = x_{2})\\\\\nProb(X_{1} = 0, X_{2} = 0) &=& 1/2 \\times 1/2 = 1/4 \\\\\nProb(X_{1} = 0, X_{2} = 1) &=& 1/4 \\\\\nProb(X_{1} = 1, X_{2} = 0) &=& 1/4 \\\\\nProb(X_{1} = 1, X_{2} = 1) &=& 1/4 .\n\\end{eqnarray}\nThe marginal distribution of the second coin is \n\\begin{eqnarray}\nProb(X_{2} = 0) &=& Prob(X_{2} = 0 | X_{1} = 0) Prob(X_{1}=0) + Prob(X_{2} = 0 | X_{1} = 1) Prob(X_{1}=1)\\\\\n&=& 1/2 \\times 1/2 + 1/2 \\times 1/2 = 1/2\\\\\nProb(X_{2} = 1) &=& Prob(X_{2} = 1 | X_{1} = 0) Prob(X_{1}=0) + Prob(X_{2} = 1 | X_{1} = 1) Prob(X_{1}=1)\\\\\n&=& 1/2 \\times 1/2 + 1/2 \\times 1/2 = 1/2\n\\end{eqnarray}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a 2x2 matrix for the joint distribution.\n# Rows correspond to X1 (coin 1), and columns correspond to X2 (coin 2).\nP_fair <- matrix(1/4, nrow = 2, ncol = 2)\nrownames(P_fair) <- c(\"X1=0\", \"X1=1\")\ncolnames(P_fair) <- c(\"X2=0\", \"X2=1\")\nP_fair\n##      X2=0 X2=1\n## X1=0 0.25 0.25\n## X1=1 0.25 0.25\n\n# Compute the marginal distributions.\n# Marginal for X1: sum across columns.\nP_X1 <- rowSums(P_fair)\nP_X1\n## X1=0 X1=1 \n##  0.5  0.5\n# Marginal for X2: sum across rows.\nP_X2 <- colSums(P_fair)\nP_X2\n## X2=0 X2=1 \n##  0.5  0.5\n\n# Compute the conditional probabilities Prob(X2 | X1).\ncond_X2_given_X1 <- matrix(0, nrow = 2, ncol = 2)\nfor (j in 1:2) {\n  cond_X2_given_X1[, j] <- P_fair[, j] / P_X1[j]\n}\nrownames(cond_X2_given_X1) <- c(\"X2=0\", \"X2=1\")\ncolnames(cond_X2_given_X1) <- c(\"given X1=0\", \"given X1=1\")\ncond_X2_given_X1\n##      given X1=0 given X1=1\n## X2=0        0.5        0.5\n## X2=1        0.5        0.5\n```\n:::\n\n\n#### **Example: Unfair Coin**. {-}\nConsider a second example, where the second coin is \"Completely Unfair\", so that it is always the same as the first. The outcomes generated with a Completely Unfair coin are the same as if we only flipped one coin.\n\\begin{eqnarray}\nProb(X_{1} = x_{1}, X_{2} = x_{2}) &=& Prob(X_{1} = x_{1}) \\mathbf{1}( x_{1}=x_{2} )\\\\\nProb(X_{1} = 0, X_{2} = 0) &=& 1/2 \\\\\nProb(X_{1} = 0, X_{2} = 1) &=& 0 \\\\\nProb(X_{1} = 1, X_{2} = 0) &=& 0 \\\\\nProb(X_{1} = 1, X_{2} = 1) &=& 1/2 .\n\\end{eqnarray}\nNote that $\\mathbf{1}(X_{1}=1)$ means $X_{1}= 1$ and $0$ if $X_{1}\\neq0$.\nThe marginal distribution of the second coin is \n\\begin{eqnarray}\nProb(X_{2} = 0) \n&=& Prob(X_{2} = 0 | X_{1} = 0) Prob(X_{1}=0) + Prob(X_{2} = 0 | X_{1} = 1) Prob(X_{1} = 1)\\\\\n&=& 1/2 \\times 1 + 0 \\times 1/2 = 1/2 .\\\\\nProb(X_{2} = 1)\n&=& Prob(X_{2} = 1 | X_{1} =0) Prob( X_{1} = 0) + Prob(X_{2} = 1 | X_{1} = 1) Prob( X_{1} = 1)\\\\\n&=& 0\\times 1/2 + 1 \\times 1/2 = 1/2 .\n\\end{eqnarray}\nwhich is the same as in the first example! Different joint distributions can have the same marginal distributions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the joint distribution matrix for the unfair coin case.\nP_unfair <- matrix(c(0.5, 0, 0, 0.5), nrow = 2, ncol = 2, byrow = TRUE)\nrownames(P_unfair) <- c(\"X1=0\", \"X1=1\")\ncolnames(P_unfair) <- c(\"X2=0\", \"X2=1\")\nP_unfair\n##      X2=0 X2=1\n## X1=0  0.5  0.0\n## X1=1  0.0  0.5\n\n# Compute the marginal distribution for X2 in the unfair case.\nP_X2_unfair <- colSums(P_unfair)\nP_X1_unfair <- rowSums(P_unfair)\n\n# Compute the conditional probabilities Prob(X1 | X2) for the unfair coin.\ncond_X2_given_X1_unfair <- matrix(NA, nrow = 2, ncol = 2)\nfor (j in 1:2) {\n  if (P_X1_unfair[j] > 0) {\n    cond_X2_given_X1_unfair[, j] <- P_unfair[, j] / P_X1_unfair[j]\n  }\n}\nrownames(cond_X2_given_X1_unfair) <- c(\"X2=0\", \"X2=1\")\ncolnames(cond_X2_given_X1_unfair) <- c(\"given X1=0\", \"given X1=1\")\ncond_X2_given_X1_unfair\n##      given X1=0 given X1=1\n## X2=0          1          0\n## X2=1          0          1\n```\n:::\n\n\n#### **Bayes' Theorem**. {-}\nFinally, note **Bayes' Theorem**:\n\\begin{eqnarray}\nProb(X_{1} = x_{1} | X_{2} = x_{2})  Prob( X_{2} = x_{2}) \n    &=& Prob(X_{1} = x_{1}, X_{2} = x_{2}) = Prob(X_{2} = x_{2} | X_{1} = x_{1}) Prob(X_{1} = x_{1}).\\\\\nProb(X_{1} = x_{1} | X_{2} = x_{2})\n    &=& \\frac{ Prob(X_{2} = x_{2} | X_{1} = x_{1}) Prob(X_{1}=x_{1}) }{ Prob( X_{2} = x_{2}) }.\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Verify Bayes' theorem for the unfair coin case:\n# Compute Prob(X1=1 | X2=1) using the formula:\n#   Prob(X1=1 | X2=1) = [Prob(X2=1 | X1=1) * Prob(X1=1)] / Prob(X2=1)\n\nP_X1_1 <- 0.5\nP_X2_1_given_X1_1 <- 1  # Since coin 2 copies coin 1.\nP_X2_1 <- P_X2_unfair[\"X2=1\"]\n\nbayes_result <- (P_X2_1_given_X1_1 * P_X1_1) / P_X2_1\nbayes_result\n## X2=1 \n##    1\n```\n:::\n\n\n\n\n## Further Reading \n\nMany introductory econometrics textbooks have a good appendix on probability and statistics. There are many useful texts online too\n\nSee the Further reading about Probability Theory in the Statistics chaper.\n\n* https://www.r-bloggers.com/2024/03/calculating-conditional-probability-in-r/\n\n\nOther Statistics \n* https://cran.r-project.org/web/packages/qualvar/vignettes/wilcox1973.html\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}