{
  "hash": "d4c49bbc8447b9f34d4ca5a8f04c4c75",
  "result": {
    "engine": "knitr",
    "markdown": "# Hypothesis Testing\n***\n\nIn this chapter, we test hypotheses using *data-driven* methods that assume much less about the data generating process. There are two main ways to conduct a hypothesis test: inverting a confidence interval and imposing the null. The first treats the distribution of estimates directly; the second explicitly enforces the null hypothesis to evaluate how unusual the observed statistic is. Both approaches rely on the bootstrap: resampling the data to approximate sampling variability. The most typical case is hypothesizing about the mean.\n\n#### **Invert a CI**. {-}\n\nOne main way to conduct hypothesis tests is to examine whether a confidence interval contains a hypothesized value. We then use this decision rule\n\n* reject the null if value falls outside of the interval\n* fail to reject the null if value falls inside of the interval\n\nWe typically use a $95\\%$ confidence interval to create a *rejection region*: the area that falls outside of the interval.\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, suppose you hypothesize the mean is $9$. You then construct a bootstrap distribution with $95\\%$ confidence interval, and find your hypothesized value falls outside of the confidence interval. Then, after accounting for sampling variability (which you estimate), it still seems extremely unlikely that the theoretical mean actually equals $9$, so you reject that hypothesis. (If the theoretical value landed in the interval, you would \"fail to reject\" the theoretical mean equals $9$.)\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests[,'Murder']\nsample_mean <- mean(sample_dat)\n\n# Bootstrap Distribution\nn <- length(sample_dat)\nset.seed(1) # to be replicable\nbootstrap_means <- vector(length=9999)\nfor(b in seq_along(bootstrap_means)){\n    dat_id <- seq(1,n)\n    boot_id <- sample(dat_id , replace=T)\n    dat_b  <- sample_dat[boot_id] # c.f. jackknife\n    mean_b <- mean(dat_b)\n    bootstrap_means[b] <-mean_b\n}\n\nhist(bootstrap_means, breaks=25,\n    border=NA,\n    freq=F,\n    main='',\n    xlab='Bootstrap Samples')\n# CI\nci_95 <- quantile(bootstrap_means, probs=c(.025, .975))\nabline(v=ci_95, lwd=2)\n# H0: mean=9\nabline(v=9, col=2, lwd=2)\n```\n\n::: {.cell-output-display}\n![](01_08_HypothesisTests_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nThe above procedure also generalizes to many other statistics. Perhaps the most informative additional statistics for spread or shape. E.g., you can conduct hypothesis tests for `sd` and `IQR`, or `skew` and `kurtosis`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap Distribution for SD\nsd_obs <- sd(sample_dat)\nbootstrap_sd <- vector(length=999)\nfor(b in seq_along(bootstrap_sd)){\n    x_b <- sample(sample_dat, replace=T)\n    sd_b <- sd(x_b)\n    bootstrap_sd[b] <- sd_b\n}\n\n# Test for SD Differences (Invert CI)\nsd_null <- 3.6\nhist(bootstrap_sd, freq=F,\n    border=NA, xlab='Bootstrap', font.main=1,\n    main='Standard Deviations (Invert CI)')\nsd_ci <- quantile(bootstrap_sd, probs=c(0.025,.975) )\nabline(v=sd_ci, lwd=2)\nabline(v=sd_null, lwd=2, col=2)\n```\n\n::: {.cell-output-display}\n![](01_08_HypothesisTests_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nTo better your understanding, try redoing the above for any function (such as `IQR(x_b)/median(x_b)`)\n:::\n\n:::{.callout-tip icon=false collapse=\"true\"}\nSuppose you scored $83\\%$ on your exam with $50$ questions, but think you are really a $90\\%$ student. Explain how you might test your hypothesis to your professor who insists your claim be supported by evidence. What would be the issue if we could not reject your hypothesis? Provide a computer simulation illustrating the issue.\n:::\n\n\n#### **Impose the Null**. {-}\n\nWe can also compute a *null distribution*: the sampling distribution of the statistic under the null hypothesis (assuming your null hypothesis was true). We use the bootstrap to loop through a large number of \"resamples\". In each iteration of the loop, we impose the null hypothesis and re-estimate the statistic of interest. We then calculate the range of the statistic across all resamples and compare how extreme the original value we observed is.\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, suppose you hypothesize the mean is $9$. You then construct a $95\\%$ confidence interval around the *null* bootstrap distribution (resamples centered around $9$). If your sample mean falls outside of that interval, then even after accounting for sampling variability (which you estimate), it seems extremely unlikely that the theoretical mean actually equals $9$, so you reject that hypothesis. (If the sample mean landed in the interval, you would \"fail to reject\" the theoretical mean equals $9$.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests[,'Murder']\nsample_mean <- mean(sample_dat)\n\n# Bootstrap NULL: mean=9\n# Bootstrap shift: center each bootstrap resample so that the distribution satisfies the null hypothesis on average.\nset.seed(1)\nmu <- 9\nbootstrap_means_null <- vector(length=999)\nfor(b in seq_along(bootstrap_means_null)){\n    dat_b <- sample(sample_dat, replace=T)\n    mean_b <- mean(dat_b) + (mu - sample_mean) # impose the null via Bootstrap shift\n    bootstrap_means_null[b] <- mean_b\n}\nhist(bootstrap_means_null, breaks=25, border=NA,\n    main='',\n    xlab='Null Bootstrap Samples')\nci_95 <- quantile(bootstrap_means_null, probs=c(.025, .975)) # critical region\nabline(v=ci_95, lwd=2)\nabline(v=sample_mean, lwd=2, col=4)\n```\n\n::: {.cell-output-display}\n![](01_08_HypothesisTests_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n:::\n\n\n\n\nThis Normal based interval can also provide an alternative to the Null Bootstrap. While we could also use a Null Jackknife distribution, that is rarely done. Altogether, there are two different types of confidence intervals that \"impose the null\". Until you know more, a conservative rule-of-thumb is to take the larger estimate.\n\n| Interval | Mechanism |\n| -------- | ------- |\nBootstrap Percentile | randomly resample $n$ observations with replacement and shift |\nNormal     | assume observations are i.i.d. and normal distribution is a good approximation (can use bootstrap or classical SE's) |\n: Types of Confidence Interval Estimates that \"impose the null\"\n\n\n\n\n## $p$-values\n\nA $p$-value is the frequency you see something at least as extreme as your statistic under the null hypothesis (when sampling from the null distribution). We want the probability that the random variable $M$ that is at least as extreme (far from the null mean of $9$) as our observed sample mean $\\hat{M}$.\n\nRecall that we used the bootstrap to estimate the distribution of the sample statistic like the mean, and the null-bootstrap shifted the bootstrap to be centered at a hypothesized value. The bootstrap idea here is to approximate $M-\\mu$, the difference between the sample mean $M$ and the unknown theoretical mean $\\mu$, with the null-bootstrap analogue $M^{\\text{boot}}_{0}-\\mu$, where $M^{\\text{boot}}_{0}=M^{\\text{boot}}+(\\mu-\\hat{M})$.\n\\begin{eqnarray}\n& & Prob( |M - \\mu| \\geq |\\hat{M} - \\mu| \\mid \\mu = 9 ) \\\\\n& & \\approx Prob( |M^{\\text{boot}}_{0}- \\mu| \\geq |\\hat{M}- \\mu|  \\mid \\mu = 9) \\\\\n& & = 1-\\hat{F}^{|\\text{boot}|}_{0}(|\\hat{M}-9|),\n\\end{eqnarray}\nwhere $\\hat{F}^{|\\text{boot}|}_{0}$ is the ECDF of $|M^{\\text{boot}}_{0}- \\mu|$.\n\n\n:::{.callout-note icon=false collapse=\"true\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests[,'Murder']\nsample_mean <- mean(sample_dat)\n\nset.seed(1)\n# Bootstrap NULL: mean=9\n# Bootstrap shift: center each bootstrap resample so that the distribution satisfies the null hypothesis on average.\nmu <- 9\nbootstrap_means_null <- vector(length=999)\nfor(b in seq_along(bootstrap_means_null)){\n    dat_b <- sample(sample_dat, replace=T)\n    mean_b <- mean(dat_b) + (mu - sample_mean) # impose the null via Bootstrap shift\n    bootstrap_means_null[b] <- mean_b\n}\nhist(bootstrap_means_null, breaks=25, border=NA,\n    main='',\n    xlab='Null Bootstrap Samples')\nci_95 <- quantile(bootstrap_means_null, probs=c(.025, .975)) # critical region\nabline(v=ci_95, lwd=2)\nabline(v=sample_mean, lwd=2, col=4)\n```\n\n::: {.cell-output-display}\n![](01_08_HypothesisTests_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Two-Sided Test, ALTERNATIVE: mean < 9 or mean >9\n# Visualize Two Sided Prob. & reject region boundary\npar(mfrow=c(1,2))\nhist(bootstrap_means_null-mu,\n    freq=F, breaks=20,\n    border=NA,\n    main='',\n    xlab=expression('Null Bootstrap for M - '~mu))\nabline(v=sample_mean-mu, col=4)\nci_95 <- quantile(bootstrap_means_null-mu, probs=c(0.025,.975))\nabline(v=ci_95, lwd=2)\n\n# Equivalent Visualization\nboot_absval <- abs(bootstrap_means_null-mu)\nFhat_abs0 <- ecdf(boot_absval)\nplot(Fhat_abs0,\n    main='',\n    xlab=expression('Null Bootstrap for |M - '~mu~'|'))\nabline(v=abs(sample_mean-mu), col=4)\n# with Two Sided Probability\np2 <- 1 - Fhat_abs0( abs(sample_mean-mu) )\ntitle( paste0('p=', round(p2,3)))\n```\n\n::: {.cell-output-display}\n![](01_08_HypothesisTests_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n:::\n\n\nYou can conduct hypothesis test using $p$-values instead of confidence intervals. It is common to use this decision rule:\n\n* reject the null at the $5\\%$ level if $p \\leq 0.05$\n* fail to reject the null at the $5\\%$ level if $p > 0.05$\n\n#### **Caveats**. {-}\n\nBeware that a common misreading of the $p$-value as \"the probability the null is true\". That is false. A $p$-value is the frequency you see something at least as extreme as your statistic under the null hypothesis.\n\nOften, one may also see or hear \"$p<.05$: statistically significant\" and \"$p>.05$: not statistically significant\". That is decision making on purely statistical grounds, and it may or may not be suitable for your context. You simply need to know that whoever says those things is using $5\\%$ as a critical value to reject a null hypothesis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Purely-Statistical Decision Making\n# via Two Sided Test\nif(p2 >.05){\n    print('fail to reject the null that mean=9, at the 5% level')\n} else {\n    print('reject the null that mean=9 in favor of either <9 or >9, at the 5% level')\n}\n## [1] \"reject the null that mean=9 in favor of either <9 or >9, at the 5% level\"\n```\n:::\n\n\nAlso note that the $p$-value is itself a function of data, and hence a random variable that changes from sample to sample. Given that the $5\\%$ level is somewhat arbitrary, and that the $p$-value both varies from sample to sample and is often misunderstood, it makes sense to give specific $p$-values a limited role in decision making.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_values <- vector(length=300)\nfor(b2 in seq(p_values)){\n    bootstrap_means_null_p <- vector(length=999)\n    for(b in seq_along(bootstrap_means_null_p)){\n        dat_b <- sample(sample_dat, replace=T)\n        mean_b <- mean(dat_b) + (mu - sample_mean) # impose the null\n        bootstrap_means_null_p[b] <- mean_b\n    }\n    Fhat_abs0 <- ecdf( abs(bootstrap_means_null_p-mu) )\n    p2 <- 1- Fhat_abs0( abs(sample_mean-mu) )\n    p_values[b2] <- p2\n}\n\nhist(p_values, freq=F,\n    border=NA, main='')\n```\n\n::: {.cell-output-display}\n![](01_08_HypothesisTests_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Other Statistics\n\n#### **$t$-values**. {-}\nA *$t$-value* standardizes the approach for hypothesis tests of the mean.\n\\begin{eqnarray}\nt = (M - \\mu) / SE(M),\n\\end{eqnarray}\n\nFor any specific sample, we must approximate the standard error. Using the theory-driven approach, we compute $\\hat{t}=(\\hat{M}-\\mu)/(\\hat{S}/\\sqrt{n})$. Using the data-driven approach, we compute $\\hat{t}=(\\hat{M}-\\mu)/(\\hat{SE}^{\\text{jack}})$ or $\\hat{t}=(\\hat{M}-\\mu)/(\\hat{SE}^{\\text{boot}})$. In any case, we can use bootstrapping to estimate the variability of the $t$ statistic, just like we did with the mean.\n\n:::{.callout-note icon=false collapse=\"true\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#null hypothesis\nmu <- 9\n\n# t statistic\njackknife_means <- vector(length=length(sample_dat))\nfor(i in seq_along(jackknife_means)){\n    dat_i <- sample_dat[-i]\n    jackknife_means[i] <- mean(dat_i)\n}\njackknife_se <- sd(jackknife_means)\nsample_t <- (sample_mean - mu)/jackknife_se\n\n# Boostrap Null Distribution\nbootstrap_t_null <- vector(length=999)\nfor(b in seq_along(bootstrap_t_null)){\n    dat_b <- sample(sample_dat, replace=T)\n    mean_b <- mean(dat_b) + (mu - sample_mean) # impose the null by recentering\n    # Compute t stat using jackknife ses (same as above)\n    jackknife_means_b <- vector(length=length(dat_b))\n    for(i in seq_along(jackknife_means_b)){\n        jackknife_means_b[i] <- mean(dat_b[-i])\n    }\n    jackknife_se_b <- sd(jackknife_means_b)\n    jackknife_t_b <- (mean_b - mu)/jackknife_se_b\n    bootstrap_t_null[b] <- jackknife_t_b\n}\n\n# Plot the null distribution and CI\npar(mfrow=c(1,2))\nhist(bootstrap_t_null, border=NA, breaks=50,\n    freq=F, main=NA, xlab='Null Bootstrap for t')\nabline(v=sample_t, col=4)\nci_95 <- quantile(bootstrap_t_null, probs=c(0.025,0.975) )\nabline(v=ci_95, lwd=2)\n\n# Compute the p-value for two-sided test\nFhat0 <- ecdf(abs(bootstrap_t_null))\nplot(Fhat0,\n    xlim=range(bootstrap_t_null, sample_t),\n    xlab='Null Bootstrap for |t|',\n    main='')\nabline(v=abs(sample_t), col=4)\np <- 1 - Fhat0( abs(sample_t) )\ntitle( paste0('p=', round(p,3)) )\n```\n\n::: {.cell-output-display}\n![](01_08_HypothesisTests_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\nif(p >.05){\n    print('fail to reject the null that mean=9, at the 5% level')\n} else {\n    print('reject the null that mean=9 in favor of either <9 or >9, at the 5% level')\n}\n## [1] \"fail to reject the null that mean=9, at the 5% level\"\n```\n:::\n\n:::\n\n\nThere are several benefits to this statistic:\n\n* uses the same statistic for different hypothesis tests\n* makes the statistic comparable across different studies\n* removes dependence on unknown parameters by normalizing with a standard error\n* makes the null distribution theoretically known asymptotically (approximately)\n\nThe last point implies we are typically dealing with a normal distribution that is well-studied, or another well-studied distribution derived from it. We will discuss this more when [comparing means](https://jadamso.github.io/Rbooks/02_11_ComparingGroups.html).\n\n#### **Quantiles and Shape Statistics**. {-}\n\nBootstrap allows hypothesis tests for any statistic, not just the mean, without relying on parametric theory. For example, the above procedures generalize from means to quantile statistics like medians.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Test for Median Differences (Impose the Null)\n# Bootstrap Null Distribution for the median\n# Each Bootstrap shifts medians so that median = q_null\n\nq_obs <- quantile(sample_dat, probs=.5)\nq_null <- 7.8\nbootstrap_quantile_null <- vector(length=999)\nfor(b in seq_along(bootstrap_quantile_null)){\n    x_b <- sample(sample_dat, replace=T) #bootstrap sample\n    q_b <- quantile(x_b, probs=.5) # median\n    d_b <- q_b - (q_obs-q_null) #impose the null\n    bootstrap_quantile_null[b] <- d_b\n}\n\n    # Note that you could also standardize like the t value. E.g.,\n    # jackknife_quantiles_b <- vector(length=length(dat_b))\n    # se_b <- sd(jackknife_quantiles_b)\n    # d_b <- d_b/se_b\n\n# 2-Sided Test for Medians\nhist(bootstrap_quantile_null-q_null,\n    border=NA, freq=F, xlab='Null Bootstrap',\n    font.main=1, main='Medians (Impose Null)')\nmedian_ci <- quantile(bootstrap_quantile_null-q_null, probs=c(.025, .975))\nabline(v=median_ci, lwd=2)\nabline(v=q_obs-q_null, lwd=2, col=4)\n```\n\n::: {.cell-output-display}\n![](01_08_HypothesisTests_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# 2-Sided Test for Median Difference\n## Null: No Median Difference\n1 - ecdf( abs(bootstrap_quantile_null-q_null))( abs(q_obs-q_null) )\n## [1] 0.5695696\n```\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nConduct a hypothesis test for whether the upper quartile is statistically different from $12$.\n\n::: {.cell}\n\n```{.r .cell-code}\nq_obs <- quantile(sample_dat, probs=.75)\n```\n:::\n\n:::\n\n## One-Sided Tests\n\nAbove, we tested whether the observed statistic is either extremely high or low. This is known as a two-sided test. There are also two one-sided tests (left tail: observed statistic is extremely low, right tail: observed statistic is extremely high). For a concrete example, consider whether the mean statistic, $M$, is centered on a theoretical value of $\\mu=9$ for the population. If your null hypothesis is that the theoretical mean is nine, $H_{0}: \\mu =9$, and you calculated the mean for your sample as $\\hat{M}$, then you can consider any one of these three alternative hypotheses:\n\n* $H_{A}: \\mu \\neq 9$, a two-tail test\n* $H_{A}: \\mu < 9$, a left-tail test\n* $H_{A}: \\mu > 9$, a right-tail test\n\nOne-sided hypothesis tests can be conducted by inverting a one-sided confidence interval (covered in the previous chapter) or by computing a one-sided $p$-value.\n\n#### **One-Sided $p$-values**. {-}\n\nThe $p$-value for a one-sided test is more straightforward to implement via a bootstrap null distribution.\n\nFor a left-tail test, we examine \\begin{eqnarray}\np = Prob( M < \\hat{M} \\mid \\mu = 9 )\n    &\\approx& Prob( M^{\\text{boot}} < \\hat{M} \\mid  \\mu = 9 ) = \\hat{F}^{\\text{boot}}_{0}(\\hat{M}),\n\\end{eqnarray}\nwhere $\\hat{F}^{\\text{boot}}_{0}$ is the ECDF of the bootstrap null distribution. We reject the null if $p < 0.05$ at the $5\\%$ level, and otherwise fail to reject.\n\nFor a right-tail test, we examine $p=Prob( M > \\hat{M} \\mid \\mu = 9 ) \\approx 1-\\hat{F}^{\\text{boot}}_{0}(\\hat{M})$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Right-tail Test, ALTERNATIVE: mean > 9\n# Equivalent Visualization with p-value\nFhat0 <- ecdf(bootstrap_means_null) # Look at right tail\nplot(Fhat0,\n    main='',\n    xlab='Null Bootstrap')\nabline(v=sample_mean, col=4)\np1 <- 1- Fhat0(sample_mean) #Compute right tail prob: 0.987\ntitle( paste0('p=', round(p1,3)))\n```\n\n::: {.cell-output-display}\n![](01_08_HypothesisTests_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\nif(p1 >.05){\n    print('fail to reject the null that mean=9, at the 5% level')\n} else {\n    print('reject the null that mean=9 in favor of >9, at the 5% level')\n}\n## [1] \"fail to reject the null that mean=9, at the 5% level\"\n```\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nNotice that the recentering adjustment affects two-sided tests (because they depend on distance from the null mean) but not one-sided tests (because adding a constant does not change rank order). Specifically, $p = Prob( M < \\hat{M} \\mid \\mu = 9 ) =  Prob( M - \\mu < \\hat{M} - \\mu \\mid \\mu = 9 )$. That is intuitively also why the $t$-value can be used for both one and two-sided hypothesis tests.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# See that the \"recentering\" matters for two-sided tests\necdf( abs(bootstrap_means_null-mu) )( abs(sample_mean-mu) )\n## [1] 0.966967\necdf( abs(bootstrap_means_null) )( abs(sample_mean) )\n## [1] 0.01301301\n\n# See that the \"recentering\" doesn't matter for one-sided ones\necdf( bootstrap_means_null-mu)( sample_mean-mu)\n## [1] 0.01301301\necdf( bootstrap_means_null )( sample_mean)\n## [1] 0.01301301\n```\n:::\n\n:::\n\n## Further Reading\n\n* <https://learningstatisticswithr.com/book/hypothesistesting.html>\n* <https://okanbulut.github.io/rbook/part5.html>\n",
    "supporting": [
      "01_08_HypothesisTests_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}