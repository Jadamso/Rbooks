{
  "hash": "37251bef62894a8d5900c4832b4c8c4b",
  "result": {
    "engine": "knitr",
    "markdown": "# Model Assessment\n***\n\n## Data Exploration\n\nFirst, you can summarize a dataset with multiple variables using the previous tools. This is often the first indication as to whether you should analyze the data using a linear model. Such figures are almost always a good idea to make first, as they can suggest issues that you were not even thinking about.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Inspect Dataset on police arrests for the USA in 1973\nhead(USArrests)\n##            Murder Assault UrbanPop Rape\n## Alabama      13.2     236       58 21.2\n## Alaska       10.0     263       48 44.5\n## Arizona       8.1     294       80 31.0\n## Arkansas      8.8     190       50 19.5\n## California    9.0     276       91 40.6\n## Colorado      7.9     204       78 38.7\n\nlibrary(psych)\npairs.panels( USArrests[,c('Murder','Assault','UrbanPop')],\n    hist.col=grey(0,.25), breaks=30, density=F, hist.border=NA, # Diagonal\n    ellipses=F, rug=F, smoother=F, pch=16, col='red' # Lower Triangle\n    )\n```\n\n::: {.cell-output-display}\n![](03_18_InterprettingRegression_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nYou can also use size, color, and shape to distinguish conditional relationships.\n\n::: {.cell}\n\n```{.r .cell-code}\n# High Assault Areas\nassault_high <- USArrests$Assault > median(USArrests$Assault)\ncol_high <- rgb(1,0,0,.5)\ncol_low <- rgb(0,0,1,.5)\ncols <- ifelse(assault_high, col_high, col_low)\n\n# Scatterplot\n# Show High Assault Areas via 'cex=' or 'pch='\n# Could further add regression lines for each data split\nplot(Murder~UrbanPop, USArrests, pch=16, col=cols)\n\nouter_legend <- function(...) {\n  opar <- par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), \n    mar=c(0, 0, 0, 0), new=TRUE)\n  on.exit(par(opar))\n  plot(0, 0, type='n', bty='n', xaxt='n', yaxt='n')\n  legend(...)\n}\nouter_legend('topright',\n    legend=c('many assualts', 'few assaults'),\n    pch=16, col=c(col_high, col_low),\n    horiz=T, cex=1, bty='n')\n```\n\n::: {.cell-output-display}\n![](03_18_InterprettingRegression_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\nSee also <https://plotly.com/r/bubble-charts/>\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(plotly)\n# Scatter Plot\nUSArrests$ID <- rownames(USArrests)\nfig <- plot_ly(\n    USArrests, x = ~UrbanPop, y = ~Assault,\n    mode='markers',\n    type='scatter',\n    hoverinfo='text',\n    text = ~paste('<b>', ID, '</b>',\n        \"<br>Urban  :\", UrbanPop,\n        \"<br>Assault:\", Assault,\n        \"<br>Murder :\", Murder),\n    color=~Murder,\n    marker=list(\n        size=~Murder,\n        opacity=0.5,\n        showscale=T,  \n        colorbar = list(title='Murder Arrests (per 100,000)')))\nfig <- layout(fig,\n    showlegend=F,\n    title='Crime and Urbanization in America 1975',\n    xaxis = list(title = 'Percent of People in an Urban Area'),\n    yaxis = list(title = 'Assault Arrests per 100,000 People'))\nfig\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-c8abee8027e20971a0de\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-c8abee8027e20971a0de\">{\"x\":{\"visdat\":{\"26345ec06172\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"26345ec06172\",\"attrs\":{\"26345ec06172\":{\"x\":{},\"y\":{},\"mode\":\"markers\",\"hoverinfo\":\"text\",\"text\":{},\"marker\":{\"size\":{},\"opacity\":0.5,\"showscale\":true,\"colorbar\":{\"title\":\"Murder Arrests (per 100,000)\"}},\"color\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"showlegend\":false,\"title\":\"Crime and Urbanization in America 1975\",\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Percent of People in an Urban Area\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Assault Arrests per 100,000 People\"},\"hovermode\":\"closest\"},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"x\":[58,48,80,50,91,78,77,72,80,60,83,54,83,65,57,66,52,66,51,67,85,74,66,44,70,53,62,81,56,89,70,86,45,44,75,68,67,72,87,48,45,59,80,80,32,63,73,39,66,60],\"y\":[236,263,294,190,276,204,110,238,335,211,46,120,249,113,56,115,109,249,83,300,149,255,72,259,178,109,102,252,57,159,285,254,337,45,120,151,159,106,174,279,86,188,201,120,48,156,145,81,53,161],\"mode\":\"markers\",\"hoverinfo\":[\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\"],\"text\":[\"<b> Alabama <\\/b> <br>Urban  : 58 <br>Assault: 236 <br>Murder : 13.2\",\"<b> Alaska <\\/b> <br>Urban  : 48 <br>Assault: 263 <br>Murder : 10\",\"<b> Arizona <\\/b> <br>Urban  : 80 <br>Assault: 294 <br>Murder : 8.1\",\"<b> Arkansas <\\/b> <br>Urban  : 50 <br>Assault: 190 <br>Murder : 8.8\",\"<b> California <\\/b> <br>Urban  : 91 <br>Assault: 276 <br>Murder : 9\",\"<b> Colorado <\\/b> <br>Urban  : 78 <br>Assault: 204 <br>Murder : 7.9\",\"<b> Connecticut <\\/b> <br>Urban  : 77 <br>Assault: 110 <br>Murder : 3.3\",\"<b> Delaware <\\/b> <br>Urban  : 72 <br>Assault: 238 <br>Murder : 5.9\",\"<b> Florida <\\/b> <br>Urban  : 80 <br>Assault: 335 <br>Murder : 15.4\",\"<b> Georgia <\\/b> <br>Urban  : 60 <br>Assault: 211 <br>Murder : 17.4\",\"<b> Hawaii <\\/b> <br>Urban  : 83 <br>Assault: 46 <br>Murder : 5.3\",\"<b> Idaho <\\/b> <br>Urban  : 54 <br>Assault: 120 <br>Murder : 2.6\",\"<b> Illinois <\\/b> <br>Urban  : 83 <br>Assault: 249 <br>Murder : 10.4\",\"<b> Indiana <\\/b> <br>Urban  : 65 <br>Assault: 113 <br>Murder : 7.2\",\"<b> Iowa <\\/b> <br>Urban  : 57 <br>Assault: 56 <br>Murder : 2.2\",\"<b> Kansas <\\/b> <br>Urban  : 66 <br>Assault: 115 <br>Murder : 6\",\"<b> Kentucky <\\/b> <br>Urban  : 52 <br>Assault: 109 <br>Murder : 9.7\",\"<b> Louisiana <\\/b> <br>Urban  : 66 <br>Assault: 249 <br>Murder : 15.4\",\"<b> Maine <\\/b> <br>Urban  : 51 <br>Assault: 83 <br>Murder : 2.1\",\"<b> Maryland <\\/b> <br>Urban  : 67 <br>Assault: 300 <br>Murder : 11.3\",\"<b> Massachusetts <\\/b> <br>Urban  : 85 <br>Assault: 149 <br>Murder : 4.4\",\"<b> Michigan <\\/b> <br>Urban  : 74 <br>Assault: 255 <br>Murder : 12.1\",\"<b> Minnesota <\\/b> <br>Urban  : 66 <br>Assault: 72 <br>Murder : 2.7\",\"<b> Mississippi <\\/b> <br>Urban  : 44 <br>Assault: 259 <br>Murder : 16.1\",\"<b> Missouri <\\/b> <br>Urban  : 70 <br>Assault: 178 <br>Murder : 9\",\"<b> Montana <\\/b> <br>Urban  : 53 <br>Assault: 109 <br>Murder : 6\",\"<b> Nebraska <\\/b> <br>Urban  : 62 <br>Assault: 102 <br>Murder : 4.3\",\"<b> Nevada <\\/b> <br>Urban  : 81 <br>Assault: 252 <br>Murder : 12.2\",\"<b> New Hampshire <\\/b> <br>Urban  : 56 <br>Assault: 57 <br>Murder : 2.1\",\"<b> New Jersey <\\/b> <br>Urban  : 89 <br>Assault: 159 <br>Murder : 7.4\",\"<b> New Mexico <\\/b> <br>Urban  : 70 <br>Assault: 285 <br>Murder : 11.4\",\"<b> New York <\\/b> <br>Urban  : 86 <br>Assault: 254 <br>Murder : 11.1\",\"<b> North Carolina <\\/b> <br>Urban  : 45 <br>Assault: 337 <br>Murder : 13\",\"<b> North Dakota <\\/b> <br>Urban  : 44 <br>Assault: 45 <br>Murder : 0.8\",\"<b> Ohio <\\/b> <br>Urban  : 75 <br>Assault: 120 <br>Murder : 7.3\",\"<b> Oklahoma <\\/b> <br>Urban  : 68 <br>Assault: 151 <br>Murder : 6.6\",\"<b> Oregon <\\/b> <br>Urban  : 67 <br>Assault: 159 <br>Murder : 4.9\",\"<b> Pennsylvania <\\/b> <br>Urban  : 72 <br>Assault: 106 <br>Murder : 6.3\",\"<b> Rhode Island <\\/b> <br>Urban  : 87 <br>Assault: 174 <br>Murder : 3.4\",\"<b> South Carolina <\\/b> <br>Urban  : 48 <br>Assault: 279 <br>Murder : 14.4\",\"<b> South Dakota <\\/b> <br>Urban  : 45 <br>Assault: 86 <br>Murder : 3.8\",\"<b> Tennessee <\\/b> <br>Urban  : 59 <br>Assault: 188 <br>Murder : 13.2\",\"<b> Texas <\\/b> <br>Urban  : 80 <br>Assault: 201 <br>Murder : 12.7\",\"<b> Utah <\\/b> <br>Urban  : 80 <br>Assault: 120 <br>Murder : 3.2\",\"<b> Vermont <\\/b> <br>Urban  : 32 <br>Assault: 48 <br>Murder : 2.2\",\"<b> Virginia <\\/b> <br>Urban  : 63 <br>Assault: 156 <br>Murder : 8.5\",\"<b> Washington <\\/b> <br>Urban  : 73 <br>Assault: 145 <br>Murder : 4\",\"<b> West Virginia <\\/b> <br>Urban  : 39 <br>Assault: 81 <br>Murder : 5.7\",\"<b> Wisconsin <\\/b> <br>Urban  : 66 <br>Assault: 53 <br>Murder : 2.6\",\"<b> Wyoming <\\/b> <br>Urban  : 60 <br>Assault: 161 <br>Murder : 6.8\"],\"marker\":{\"colorbar\":{\"title\":\"Murder Arrests (per 100,000)\",\"ticklen\":2},\"cmin\":0.80000000000000004,\"cmax\":17.399999999999999,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666667\",\"rgba(70,19,97,1)\"],[\"0.0833333333333333\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":true,\"color\":[13.199999999999999,10,8.0999999999999996,8.8000000000000007,9,7.9000000000000004,3.2999999999999998,5.9000000000000004,15.4,17.399999999999999,5.2999999999999998,2.6000000000000001,10.4,7.2000000000000002,2.2000000000000002,6,9.6999999999999993,15.4,2.1000000000000001,11.300000000000001,4.4000000000000004,12.1,2.7000000000000002,16.100000000000001,9,6,4.2999999999999998,12.199999999999999,2.1000000000000001,7.4000000000000004,11.4,11.1,13,0.80000000000000004,7.2999999999999998,6.5999999999999996,4.9000000000000004,6.2999999999999998,3.3999999999999999,14.4,3.7999999999999998,13.199999999999999,12.699999999999999,3.2000000000000002,2.2000000000000002,8.5,4,5.7000000000000002,2.6000000000000001,6.7999999999999998],\"size\":[13.199999999999999,10,8.0999999999999996,8.8000000000000007,9,7.9000000000000004,3.2999999999999998,5.9000000000000004,15.4,17.399999999999999,5.2999999999999998,2.6000000000000001,10.4,7.2000000000000002,2.2000000000000002,6,9.6999999999999993,15.4,2.1000000000000001,11.300000000000001,4.4000000000000004,12.1,2.7000000000000002,16.100000000000001,9,6,4.2999999999999998,12.199999999999999,2.1000000000000001,7.4000000000000004,11.4,11.1,13,0.80000000000000004,7.2999999999999998,6.5999999999999996,4.9000000000000004,6.2999999999999998,3.3999999999999999,14.4,3.7999999999999998,13.199999999999999,12.699999999999999,3.2000000000000002,2.2000000000000002,8.5,4,5.7000000000000002,2.6000000000000001,6.7999999999999998],\"opacity\":0.5,\"line\":{\"colorbar\":{\"title\":\"\",\"ticklen\":2},\"cmin\":0.80000000000000004,\"cmax\":17.399999999999999,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666667\",\"rgba(70,19,97,1)\"],[\"0.0833333333333333\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":false,\"color\":[13.199999999999999,10,8.0999999999999996,8.8000000000000007,9,7.9000000000000004,3.2999999999999998,5.9000000000000004,15.4,17.399999999999999,5.2999999999999998,2.6000000000000001,10.4,7.2000000000000002,2.2000000000000002,6,9.6999999999999993,15.4,2.1000000000000001,11.300000000000001,4.4000000000000004,12.1,2.7000000000000002,16.100000000000001,9,6,4.2999999999999998,12.199999999999999,2.1000000000000001,7.4000000000000004,11.4,11.1,13,0.80000000000000004,7.2999999999999998,6.5999999999999996,4.9000000000000004,6.2999999999999998,3.3999999999999999,14.4,3.7999999999999998,13.199999999999999,12.699999999999999,3.2000000000000002,2.2000000000000002,8.5,4,5.7000000000000002,2.6000000000000001,6.7999999999999998]}},\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[32,91],\"y\":[45,337],\"type\":\"scatter\",\"mode\":\"markers\",\"opacity\":0,\"hoverinfo\":\"none\",\"showlegend\":false,\"marker\":{\"colorbar\":{\"title\":\"Murder\",\"ticklen\":2},\"cmin\":0.80000000000000004,\"cmax\":17.399999999999999,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666667\",\"rgba(70,19,97,1)\"],[\"0.0833333333333333\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":true,\"color\":[0.80000000000000004,17.399999999999999],\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n\n## Model Diagnostics\n\nThere's little sense in getting great standard errors for a terrible model. Plotting your regression object a simple and easy step to help diagnose whether your model is in some way bad. We next go through what each of these figures show.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreg <- lm(Murder~Assault+UrbanPop, data=USArrests)\npar(mfrow=c(2,2))\nplot(reg, pch=16, col=grey(0,.5))\n```\n\n::: {.cell-output-display}\n![](03_18_InterprettingRegression_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n#### **Outliers**. {-}\nThe first diagnostic plot examines outliers in terms the observed outcome $\\hat{Y}_i$ being far from its prediction $\\hat{y}_i$. You may be interested in such outliers because they can (but do not have to) unduly influence your estimates. \n\nThe third diagnostic plot examines another type of outlier, the observed explanatory variable $\\hat{X}_{i}$ is far from the others. A point has high *leverage* if the estimates change dramatically when you estimate the model without that data point.\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 40\nx <- c(25, runif(N-1,3,8))\ne <- rnorm(N,0,0.4)\ny <- 3 + 0.6*sqrt(x) + e\nplot(y~x, pch=16, col=grey(0,.5))\npoints(x[1],y[1], pch=16, col=rgb(1,0,0,.5))\n\nabline(lm(y~x), col=2, lty=2)\nabline(lm(y[-1]~x[-1]))\n```\n\n::: {.cell-output-display}\n![](03_18_InterprettingRegression_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nStandardized residuals are $r_i=\\frac{\\hat{e}_i}{s_{[i]}\\sqrt{1-h_i}}$, where $s_{[i]}$ is the root mean squared error of a regression with the $i$th observation removed and $h_i$ is the leverage of residual $\\hat{e}_{i}$. \n\n::: {.cell}\n\n```{.r .cell-code}\nwhich.max(hatvalues(reg))\nwhich.max(rstandard(reg))\n```\n:::\n\n\nThe fourth plot further assesses outliers in the explanatory variables ($X$) using *Cook's Distance*, which sums of all prediction changes when observation $i$ is removed and scales proportionally to the mean square error\n\\begin{eqnarray}\nD_{i} \n&=& \\frac{\\sum_{j} \\left( \\hat{y_j} - \\hat{y_j}_{[i]} \\right)^2 }{ p \\hat{S}^2 }\n= \\frac{\\hat{e}_{i}^2}{p \\hat{S}^2 } \\frac{h_i}{(1-h_i)^2} \\\\\n\\hat{S}^2 &=& \\frac{\\sum_{i} \\hat{e}_{i}^2 }{n-K}.\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\nwhich.max(cooks.distance(reg))\ncar::influencePlot(reg)\n```\n:::\n\n\n\nSee <https://www.r-bloggers.com/2016/06/leverage-and-influence-in-a-nutshell/> for a good interactive explanation, and <https://online.stat.psu.edu/stat462/node/87/> for detail. See [AEJ-leverage](https://www.rwi-essen.de/fileadmin/user_upload/RWI/Publikationen/I4R_Discussion_Paper_Series/032_I4R_Haddad_Kattan_Wochner-updateJune28.pdf) and [NBER-leverage](https://statmodeling.stat.columbia.edu/2025/02/28/the-r-squared-on-this-is-kinda-low-no/) for examples of leverage in economics.\n\n\n#### **Collinearity**. {-}\nThis is when one explanatory variable in a multiple linear regression model can be linearly predicted from the others with a substantial degree of accuracy. Coefficient estimates may change erratically in response to small changes in the model or the data. (In the extreme case, there are more variables than observations $K>n$ and an infinite number of solutions to a linear model.) To diagnose collinearity, we can use the *Variance Inflation Factor*: $\\hat{VIF}_{k}=\\frac{1}{1-\\hat{R}^2_k}$, where $\\hat{R}^2_k$ is the $\\hat{R}^2$ for the regression of $\\hat{X}_k$ on the other covariates $\\hat{X}_{-k}$ (a regression that does not involve the response variable $\\hat{Y}$)\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::vif(reg) \nsqrt(car::vif(reg)) > 2 # problem?\n```\n:::\n\n\n\n#### **Normality**. {-}\nThe second plot examines whether the residuals are normally distributed. Your OLS coefficient estimates do not depend on the normality of the residuals. (Good thing, because there's no reason the residuals of economic phenomena should be so well behaved.) Many hypothesis tests are, however, affected by the distribution of the residuals. For these reasons, you may be interested in assessing normality \n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\nhist(resid(reg),\n    main='Histogram of Residuals',\n    font.main=1, border=NA)\n\nqqnorm(resid(reg),\n    main=\"Normal Q-Q Plot of Residuals\",\n    font.main=1, col=grey(0,.5), pch=16)\nqqline(resid(reg), col=1, lty=2)\n\n#shapiro.test(resid(reg))\n```\n:::\n\n\nHeterskedasticity may also matters for variability estimates. This is not shown in the plot, but you can conduct a simple test\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmtest)\nlmtest::bptest(reg)\n```\n:::\n\n\n## Data Transformations\n\nTransforming variables can often improve your model fit while still estimating it via OLS. This is because OLS only requires the model to be \"linear in the parameters\". Under the assumptions of the model is correctly specified, the following table is how we can interpret the coefficients of the transformed data. (Note for small changes, $\\Delta ln(x) \\approx \\Delta x / x = \\Delta x \\% \\cdot 100$.)\n\n| *Specification* | *Regressand* | *Regressor* | *Derivative* | *Interpretation (If True)* |\n| --- | --- | --- | --- | --- |\n| linear--linear | $y$          | $x$   | $\\Delta y = \\beta_1\\cdot\\Delta x$ | Change $x$ by one unit $\\rightarrow$ change $y$ by $\\beta_1$ units.|\n| log--linear | $ln(y)$ | $x$ | $\\Delta y \\% \\cdot 100 \\approx \\beta_1 \\cdot \\Delta x$ | Change $x$ by one unit $\\rightarrow$ change $y$ by $100 \\cdot \\beta_1$ percent. |\n| linear--log | $y$ | $ln(x)$ | $\\Delta y \\approx  \\frac{\\beta_1}{100}\\cdot \\Delta x \\%$ | Change $x$ by one percent $\\rightarrow$ change $y$ by $\\frac{\\beta_1}{100}$ units |\n| log--log | $ln(y)$ | $ln(x)$ | $\\Delta y \\% \\approx \\beta_1\\cdot \\Delta x \\%$ | Change $x$ by one percent $\\rightarrow$ change $y$ by $\\beta_1$ percent|\n\nNow recall from micro theory that an additively seperable and linear production function is referred to as ``perfect substitutes''. With a linear model and untranformed data, you have implicitly modelled the different regressors $X$ as perfect substitutes. Further recall that the ''perfect substitutes'' model is a special case of the constant elasticity of substitution production function. Here, we will build on <http://dx.doi.org/10.2139/ssrn.3917397>, and consider box-cox transforming both $X$ and $y$. Specifically, apply the box-cox transform of $y$ using parameter $\\lambda$ and apply another box-cox transform to each $x$ using the same parameter $\\rho$ so that\n\\begin{eqnarray}\nY^{(\\lambda)}_{i} &=& \\sum_{k=1}^{K}\\beta_{k} X^{(\\rho)}_{ik} + \\epsilon_{i}\\\\\nY^{(\\lambda)}_{i} &=&\n\\begin{cases}\n\\lambda^{-1}\\left[ (Y_i+1)^{\\lambda}- 1\\right] & \\lambda \\neq 0 \\\\\n\\log\\left(Y_i+1\\right) &  \\lambda=0\n\\end{cases}.\\\\\nx^{(\\rho)}_{i} &=&\n\\begin{cases}\n\\rho^{-1}\\left[ (X_i)^{\\rho}- 1\\right] & \\rho \\neq 0 \\\\\n\\log\\left(X_{i}+1\\right) &  \\rho=0\n\\end{cases}.\n\\end{eqnarray}\n\nNotice that this nests:\n\n * linear-linear $(\\rho=\\lambda=1)$.\n * linear-log $(\\rho=1, \\lambda=0)$.\n * log-linear $(\\rho=0, \\lambda=1)$.\n * log-log  $(\\rho=\\lambda=0)$.\n\n\nIf $\\rho=\\lambda$, we get the CES production function. This nests the ''perfect substitutes'' linear-linear model ($\\rho=\\lambda=1$) , the ''cobb-douglas''  log-log model  ($\\rho=\\lambda=0$), and many others. We can define $\\lambda=\\rho/\\lambda'$ to be clear that this is indeed a CES-type transformation where\n\n* $\\rho \\in (-\\infty,1]$ controls the \"substitutability\" of explanatory variables. E.g., $\\rho <0$ is ''complementary''.\n* $\\lambda$ determines ''returns to scale''. E.g., $\\lambda<1$ is ''decreasing returns''.\n\n\nWe compute the mean squared error in the original scale by inverting the predictions;\n\\begin{eqnarray}\n\\hat{y}_{i} =\n\\begin{cases}\n\\left[ \\hat{y}_{i}^{(\\lambda)} \\cdot \\lambda \\right]^{1/\\lambda} -1 & \\lambda  \\neq 0 \\\\\n\\exp\\left( \\hat{y}_{i}^{(\\lambda)} \\right) -1 &  \\lambda=0\n\\end{cases}.\n\\end{eqnarray}\n\n\nIt is easiest to optimize parameters in a 2-step procedure called  `concentrated optimization'. We first solve for $\\hat{\\beta}(\\rho,\\lambda)$ and compute the mean squared error $MSE(\\rho,\\lambda)$. We then find the $(\\rho,\\lambda)$ which minimizes $MSE$.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Box-Cox Transformation Function\nbxcx <- function( xy, rho){\n    if (rho == 0L) {\n      log(xy+1)\n    } else if(rho == 1L){\n      xy\n    } else {\n      ((xy+1)^rho - 1)/rho\n    }\n}\nbxcx_inv <- function( xy, rho){\n    if (rho == 0L) {\n      exp(xy) - 1\n    } else if(rho == 1L){\n      xy\n    } else {\n     (xy * rho + 1)^(1/rho) - 1\n    }\n}\n\n# Which Variables\nreg <- lm(Murder~Assault+UrbanPop, data=USArrests)\nX <- USArrests[,c('Assault','UrbanPop')]\nY <- USArrests[,'Murder']\n\n# Simple Grid Search over potential (Rho,Lambda) \nrl_df <- expand.grid(rho=seq(-2,2,by=.5),lambda=seq(-2,2,by=.5))\n\n# Compute Mean Squared Error\n# from OLS on Transformed Data\nerrors <- apply(rl_df,1,function(rl){\n    Xr <- bxcx(X,rl[[1]])\n    Yr <- bxcx(Y,rl[[2]])\n    Datr <- cbind(Murder=Yr,Xr)\n    Regr <- lm(Murder~Assault+UrbanPop, data=Datr)\n    Predr <- bxcx_inv(predict(Regr),rl[[2]])\n    Resr  <- (Y - Predr)\n    return(Resr)\n})\nrl_df$mse <- colMeans(errors^2)\n\n# Want Small MSE and Interpretable\nlayout(matrix(1:2,ncol=2), width=c(3,1), height=c(1,1))\npar(mar=c(4,4,2,0))\nplot(lambda~rho,rl_df, cex=8, pch=15,\n    xlab=expression(rho),\n    ylab=expression(lambda),\n    col=hcl.colors(25)[cut(1/rl_df$mse,25)])\n# Which min\nrl0 <- rl_df[which.min(rl_df$mse),c('rho','lambda')]\npoints(rl0$rho, rl0$lambda, pch=0, col=1, cex=8, lwd=2)\n# Legend\nplot(c(0,2),c(0,1), type='n', axes=F,\n    xlab='',ylab='', cex.main=.8,\n    main=expression(frac(1,'Mean Square Error')))\nrasterImage(as.raster(matrix(hcl.colors(25), ncol=1)), 0, 0, 1,1)\ntext(x=1.5, y=seq(1,0,l=10), cex=.5,\n    labels=levels(cut(1/rl_df$mse,10)))\n```\n\n::: {.cell-output-display}\n![](03_18_InterprettingRegression_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=960}\n:::\n:::\n\n\nThe parameters $-1,0,1,2$ are easy to interpret and might be selected instead if there is only a small loss in fit. (In the above example, we might choose $\\lambda=0$ instead of the $\\lambda$ which minimized the mean square error). You can also plot the specific predictions to better understand the effect of data  transformation beyond mean squared error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot for Specific Comparisons\nXr <- bxcx(X,rl0[[1]])\nYr <- bxcx(Y,rl0[[2]])\nDatr <- cbind(Murder=Yr,Xr)\nRegr <- lm(Murder~Assault+UrbanPop, data=Datr)\nPredr <- bxcx_inv(predict(Regr),rl0[[2]])\n\ncols <- c(rgb(1,0,0,.5), col=rgb(0,0,1,.5))\nplot(Y, Predr, pch=16, col=cols[1], ylab='Prediction', \n    ylim=range(Y,Predr))\npoints(Y, predict(reg), pch=16, col=cols[2])\nlegend('topleft', pch=c(16), col=cols,\n    title=expression(rho~', '~lambda),\n    legend=c( paste0(rl0, collapse=', '),'1, 1') )\nabline(a=0,b=1, lty=2)\n```\n\n::: {.cell-output-display}\n![](03_18_InterprettingRegression_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nWhen explicitly transforming data according to $\\lambda$ and $\\rho$, these parameters increase the degrees of freedom by two. The default hypothesis testing procedures do not account for you trying out different transformations, and should be adjusted by the increased degrees of freedom. Specification searches deflate standard errors and are a major source for false discoveries.\n\nNote that if you are ultimately interested in the outcome $Y$, then transforming/untransforming $Y$ can introduce a bias. To understand when you might be better off sticking with an untransformed outcome variable, see the literature on \"smearing\".\n\n\n\n## More Literature\n\n\nDiagnostics\n\n* <https://book.stat420.org/model-diagnostics.html#leverage>\n* <https://socialsciences.mcmaster.ca/jfox/Books/RegressionDiagnostics/index.html>\n* <https://bookdown.org/ripberjt/labbook/diagnosing-and-addressing-problems-in-linear-regression.html>\n* Belsley, D. A., Kuh, E., and Welsch, R. E. (1980). Regression Diagnostics: Identifying influential data and sources of collinearity. Wiley. <https://doi.org/10.1002/0471725153>\n* Fox, J. D. (2020). Regression diagnostics: An introduction (2nd ed.). SAGE. <https://dx.doi.org/10.4135/9781071878651>\n\n",
    "supporting": [
      "03_18_InterprettingRegression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/htmltools-fill-0.5.9/fill.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"site_libs/plotly-binding-4.11.0/plotly.js\"></script>\n<script src=\"site_libs/typedarray-0.1/typedarray.min.js\"></script>\n<script src=\"site_libs/jquery-3.5.1/jquery.min.js\"></script>\n<link href=\"site_libs/crosstalk-1.2.2/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/crosstalk-1.2.2/js/crosstalk.min.js\"></script>\n<link href=\"site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/plotly-main-2.11.1/plotly-latest.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}