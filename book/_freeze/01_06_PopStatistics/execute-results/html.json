{
  "hash": "789d27f81a3aae81ceea21589fc55343",
  "result": {
    "engine": "knitr",
    "markdown": "## Statistical Theory\n\nWe will now dig a little deeper theoretically into the statistics we compute. When we know how the data are generated theoretically, we can often compute the theoretical value of the two most basic and often-used statistics: the mean and variance. Recall that we denote $X_{i}$ as a random variable that can take on specific values $x$ from the sample space with known probabilities. For example, $X_{i}$ is a coin that we will flip and it can land on heads ($x=1$) or tails ($x=0$).\n\n* $X_{i}$ is random variable, taking on outcomes $x$\n* $\\hat{X}_{i}$ is a realized value\n\nThe theoretical mean, $\\mu$, and variance, $\\sigma^2$, describe the population distribution $F$ generating the data. The population parameters ($\\mu, \\sigma^2$) are fixed, true values. They are often unknown and estimated with sample statistics that are calculated from data. With sample statistics, you should distinguish the particular estimate you calculated for your sample, $\\hat{M}$, and the estimator $M$ that is computed on any sample. For example, consider flipping a coin three times: $M$ corresponds to a theoretical formula you compute on coins before they are flipped and $\\hat{M}$ corresponds to a specific value after you flip the coins.\n\n| Concept      | Mean      | Variance   | What it is | Terminology                              | Alternative Terminology |\n| ------------ | --------- | ---------- |----------------------| ---------------------------------------- |----------- |\n| Estimate     | $\\hat{M}$ | $\\hat{V}$ | one number you compute for a single sample of size $n$ | empirical mean, variance  | sample mean, sample variance (dividing by $n$) |\n| Estimator    | $M$       | $V$        | a random variable, formula that computes the statistic for any sample of size $n$ | | |\n| Population   | $\\mu$     | $\\sigma^2$ | a number defined by the theoretical distribution | theoretical mean, variance | population/long-run mean, variance | \n: Statistical definitions\n\nFor concreteness, we separately analyze how the the theoretical mean and variance are computed for discrete and continuous random variables.\n\n\n## Population Parameters\n\n#### **Discrete Random Variables**. {-} \nIf the sample space is discrete, we can compute the *expected value*, or theoretical mean, as\n\\begin{eqnarray}\n\\mu = \\mathbb{E}[X_{i}] = \\sum_{x} x Prob(X_{i}=x),\n\\end{eqnarray}\nwhere $\\mathbb{E}$ is the expectation function that explicitly uses the population distribution via $Prob(X_{i}=x)$: the probability the random variable $X_{i}$ takes the particular value $x$. Similarly, we can compute the population variance as\n\\begin{eqnarray}\n\\sigma^2 = \\mathbb{V}[X_{i}] = \\sum_{x} \\left(x - \\mathbb{E}[X_{i}] \\right)^2 Prob(X_{i}=x),\n\\end{eqnarray}\nwhere $\\mathbb{V}$ is the variance function which also explicitly uses the population distribution. The population standard deviation is $\\sigma = \\sqrt{\\mathbb{V}[X_{i}]}$, which measures the spread of the distribution of individual outcomes around the theoretical mean.\n\nFor example, consider a five-sided die with equal probability of landing on each side. I.e., $X_{i}$ is a Discrete Uniform random variable with outcomes $x \\in \\{1,2,3,4,5\\}$. What is the theoretical mean? What is the variance and standard deviation?\n\\begin{eqnarray}\n\\mu &=& 1\\frac{1}{5} + 2\\frac{1}{5} + 3\\frac{1}{5} + 4\\frac{1}{5} + 5\\frac{1}{5} = 15/5 = 3\\\\\n\\mathbb{V}[X_{i}] &=& (1-3)^2\\frac{1}{5} +\n  (2 - 3)^2\\frac{1}{5} +\n  (3 - 3)^2\\frac{1}{5} +\n  (4 - 3)^2\\frac{1}{5} +\n  (5 - 3)^2\\frac{1}{5} \\\\\n  &=& 2^2 \\frac{1}{5} + 1^2\\frac{1}{5} + 0^2 \\frac{1}{5} + + 1^2\\frac{1}{5} +  2^2 \\frac{1}{5}\n  = (4 + 1 + 1 + 4)/5 = 10/5 = 2\\\\\n\\sigma &=& \\sqrt{2}\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Computerized Way to Compute Mean\nx <- c(1,2,3,4,5)\nx_probs <- c(1/5, 1/5, 1/5, 1/5, 1/5)\nX_mean <- sum(x*x_probs)\nX_mean\n## [1] 3\n\n# Computerized Way to Compute SD\nXvar <- sum((x-X_mean)^2*x_probs)\nXvar\n## [1] 2\nXsd <- sqrt(Xvar)\nXsd\n## [1] 1.414214\n\n# Verified by simulation\nXsim <- sample(x, prob=x_probs,\n          size=10000, replace=T)\nmean(Xsim)\n## [1] 3.0083\nsd(Xsim)\n## [1] 1.416133\n```\n:::\n\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nFor example, consider an unfair coin with a $3/4$ probability of heads ($x=1$) and a $1/4$ probability of tails ($x=0$) has a theoretical mean of \n\\begin{eqnarray}\n\\mathbb{E}[X_{i}] = 0\\frac{1}{4} + 1\\frac{3}{4} = \\frac{3}{4}\n\\end{eqnarray}\nand a theoretical variance of \n\\begin{eqnarray}\n\\mathbb{V}[X_{i}] = [0 - \\frac{3}{4}]^2 \\frac{1}{4} + [1 - \\frac{3}{4}]^2 \\frac{3}{4} \n= \\frac{9}{64} + \\frac{3}{64}\n= \\frac{12}{64} \n= \\frac{3}{16}. \n\\end{eqnarray}\nVerify both the mean and variance by simulation.\n\n::: {.cell}\n\n```{.r .cell-code}\n# A simulation of many coin flips\nx <- c(0,1)\nx_probs <- c(1/4, 3/4)\nX <- sample(x, 1000, prob=x_probs, replace=T)\n\nround( mean(X), 4)\n## [1] 0.753\n\nround( var(X), 4)\n## [1] 0.1862\n```\n:::\n\n:::\n\n:::{.callout-note icon=false collapse=\"true\"}\nConsider a four-sided die with outcomes $\\{1,2,3,4 \\}$ and corresponding probabilities $\\{ 1/8, 2/8, 1/8, 4/8 \\}$ . What is the mean?\n\\begin{eqnarray}\n\\mathbb{E}[X_{i}] = 1 \\frac{1}{8} + 2 \\frac{2}{8} + 3 \\frac{1}{8} + 4 \\frac{4}{8} = 3\n\\end{eqnarray}\nWhat is the variance? Verify both the mean and variance by simulation.\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nConsider an unfair coin with a $p$ probability of heads ($x=1$) and a $1-p$ probability of tails ($x=0$), where $p$ is a parameter between $0$ and $1$. The theoretical mean is\n\\begin{eqnarray}\n\\mathbb{E}[X_{i}] = 1[p] + 0[1-p] = p\n\\end{eqnarray}\nand the theoretical variance is \n\\begin{eqnarray}\n\\mathbb{V}[X_{i}] \n= [1 - p]^2 p + [0 - p]^2 [1-p] = [1 - p]^2 p + p^2 [1-p] \n= [1-p]p\\left( [1-p] + p\\right) = [1-p] p.\n\\end{eqnarray}\nSo the theoretical standard deviation is $\\sigma=\\sqrt{[1-p] p}$.\n:::\n\n\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nSuppose $X_{i}$ is a discrete random variable with this probability mass function: \n\\begin{eqnarray}\nX_{i} &=& \\begin{cases}\n-1 &  \\text{ with probability } \\frac{1}{2 \\lambda^2} \\\\\n0  & \\text{ with probability } 1-\\frac{1}{\\lambda^2} \\\\\n+1 & \\text{ with probability } \\frac{1}{2\\lambda^2} \n\\end{cases},\n\\end{eqnarray}\nwhere $\\lambda>0$ is a parameter. What is the theoretical standard deviation?\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nFor a discrete uniform random variable with the sample space $\\{1,2,3,4,5\\}$, calculate the theoretical median and IQR.\n:::\n\n\n#### **Estimation**. {-} \n\n\nSometimes, you may have a dataset of values and probability weights. Othertimes, you can calculate them yourself.\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nSuppose there is an experiment with three possible outcomes, $\\{A, B, C\\}$. It was repeated $50$ times and discovered that $A$ occurred $10$ times, $B$ occurred $13$ times, and $C$ occurred $27$ times. The estimated probability of each outcome is found via the bar plot $\\hat{p}_{A} = 10/50$, $\\hat{p}_{B} = 13/50$, $\\hat{p}_{A} = 27/50$.\nWe can also estimate the \"in\" probabilities as $\\hat{Prob}(A \\text{ or } B)=10/50+13/50=23/50$ and $\\hat{Prob}(B \\text{ or } C)=13/50+27/50=40/50$, as well as the \"out\" probability as $\\hat{Prob}(A \\text{ or } C)=13/50+27/50=37/50$.\n\nDo these calculations with the computer.\n\n::: {.cell}\n\n:::\n\n:::\n\n\n\nIn either case, you can explicitly do the computations for discrete data. Given data on unique outcome $x$, we can estimate the probability of each outcome as \n\\begin{eqnarray}\n\\hat{p}_{x}=\\sum_{i=1}^{n}\\mathbf{1}\\left( \\hat{X}_{i}=x\\right)/n.\n\\end{eqnarray}\nGiven probability estimates, we can compute the sample mean as \n\\begin{eqnarray}\n\\hat{M} &=& \\sum_{x} x \\hat{p}_{x}.\n\\end{eqnarray}\n\n\n:::{.callout-note icon=false collapse=\"true\"}\nSuppose we flipped a coin $100$ times and found that $76$ were heads and $24$ were tails. The estimated probabilities are $76/100$ for the outcome $X_{1}=1$ and $24/100$ for the outcome $X_{i}=0$. We compute the mean as $\\hat{M}=1\\times 0.76 + 0 \\times 0.24 = 0.76$. \n\nNote that we get the same number if we instead computed $\\sum_{i} \\hat{X}_{i}/100 = [\\sum_{i=1}^{24} 0/100 + \\sum_{i=25}^{100} 1/100 = [100-24]/100 = 0.76.$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute a sample estimate using probability weights\nP  <- table(X) #table of counts\np <- c(P)/length(X) #frequencies (must sum to 1)\nx <- as.numeric(names(p)) #unique values\ncbind(x,p)\n##   x     p\n## 0 0 0.247\n## 1 1 0.753\n\n# Sample Mean Estimate\nX_mean <- sum(x*p)\nX_mean\n## [1] 0.753\n\nmean(X)\n## [1] 0.753\n```\n:::\n\n:::\n\n:::{.callout-tip icon=false collapse=\"true\"}\nTry estimating the sample mean the two different ways for another random sample\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(0,1,2)\nx_probs <- c(1/3,1/3,1/3)\nX  <-  sample(x, prob=x_probs, 1000, replace=T)\n\n# First Way (Computerized)\nmean(X)\n## [1] 0.995\n\n# Second Way (Explicit Calculations)\n# start with a table of counts like the previous example\n```\n:::\n\n:::\n\nThe weighting idea generalizes to other statistics. E.g., we can also estimate variance with probability weights\n\n:::{.callout-tip icon=false collapse=\"false\"}\nProvide an example of computing a weighted variance building on this code below\n\n::: {.cell}\n\n```{.r .cell-code}\nx_diff <- (x - x_mean)^2\np <- P/sum(P)\nx_var <- sum(p * x_diff)\n```\n:::\n\n:::\n\n\n\n#### **Continuous Random Variables**. {-}\n\nMany continuous random variables are parameterized by their means and variances. For example, the exponential distribution has a rate parameter $\\lambda$, which corresponds to the theoretical mean $1/\\lambda$ and variance $1/\\lambda^2$. For another example, the two parameters of the normal distribution are $\\mu$ and $\\sigma^2$, which corresponds to the theoretical mean and variance. \n\n::: {.cell}\n\n```{.r .cell-code}\n# Exponential Random Variable\nX <- rexp(5000, 2)\nm <- mean(X)\nround(m, 2)\n## [1] 0.49\n\n# Normal Random Variable\nX <- rnorm(5000, 1, 2)\nround(mean(X), 2)\n## [1] 1\nround(var(X), 2)\n## [1] 3.98\n```\n:::\n\n\n<details>\n<summary> Advanced and Optional </summary>\n  <p>\nIf the sample space is continuous, we can compute the theoretical mean (or expected value) as\n\\begin{eqnarray}\n\\mathbb{E}[X] = \\int x f(x) d x,\n\\end{eqnarray}\nwhere $f(x)$ is the probability density for the particular value $x$. Note that this is not a probability; probabilities are areas under $f$.\n\nSimilarly, we can compute the theoretical variance as\n\\begin{eqnarray}\n\\mathbb{V}[X_{i}]= \\int \\left(x - \\mathbb{E}[X_{i}] \\right)^2 f(x) d x,\n\\end{eqnarray}\n\nFor example, consider a random variable with a continuous uniform distribution over $[-1, 1]$. In this case, $f(x)=1/[1 - (-1)]=1/2$ for each $x \\in [-1, 1]$ and \n\\begin{eqnarray}\n\\mathbb{E}[X_{i}] = \\int_{-1}^{1} \\frac{x}{2} d x = \\int_{-1}^{0} \\frac{x}{2} d x + \\int_{0}^{1} \\frac{x}{2} d x = 0\n\\end{eqnarray}\nand \n\\begin{eqnarray}\n\\mathbb{V}[X_{i}]= \\int_{-1}^{1} x^2 \\frac{1}{2} d x = \\frac{1}{2} \\frac{x^3}{3}|_{-1}^{1} = \\frac{1}{6}[1 - (-1)] = 2/6 =1/3\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- USArrests[,'Murder']\nround( mean(X), 4)\n## [1] 7.788\nround( var(X), 4)\n## [1] 18.9705\n```\n:::\n\n:::{.callout-tip icon=false collapse=\"false\"}\nYou can estimate means and variances for continuous random variables with weights, but here we have an additional approximation error due to binning by the histogram.\n\n::: {.cell}\n\n```{.r .cell-code}\n# values and probabilities\nh  <- hist(X, plot=F)\nwt <- h[['counts']]/length(X) \nxt <- h[['mids']]\n# Weighted mean\nX_mean <- sum(wt*xt)\nX_mean\n## [1] 7.8\n\n# Compare to \"mean(x)\"\n```\n:::\n\n\nTry it yourself with \n\n::: {.cell}\n\n```{.r .cell-code}\nX <- runif(2000, -1, 1)\n```\n:::\n\n:::\n\n  </p>\n</details>\n\n\n\n#### **Law of the Unconscious Statistician (LOTUS)**. {-}\n\nAs before, we will represent the random variable as $X_{i}$, which can take on values $x$ from the sample space. If $X_{i}$ is a discrete random variable (a random variable with a discrete sample space) and $g$ is a function, then\n$\\mathbb E[g(X_{i})] = \\sum_x g(x)Prob(X_{i}=x)$.\n\n:::{.callout-tip icon=false collapse=\"true\"}\nLet $X_{i}$ take values $\\{1,2,3\\}$ with\n\\begin{eqnarray}\nPr(X_{i}=1)=0.2,\\quad Prob(X_{i}=2)=0.5,\\quad Prob(X_{i}=3)=0.3.\n\\end{eqnarray}\nLet $g(x)=x^2+1$. Then $g(1)=1^2+1=2$, $g(2)=2^2+1=5$, $g(3)=3^2+1=10$. \n\nThen, by LOTUS,\n\\begin{eqnarray}\n\\mathbb E[g(X_{i})]=\\sum_x g(x)Prob(X_{i}=x)\n&=& g(1)\\cdot 0.2 + g(2)\\cdot 0.5 + g(3)\\cdot 0.3 \\\\\n&=& 2 \\cdot 0.2 + 5 \\cdot 0.5 + 10 \\cdot 0.3 \\\\\n&=& 0.4 + 2.5 + 3 = 5.9.\n\\end{eqnarray}\n\n::: {.cell}\n\n```{.r .cell-code}\nx  <- c(1,2,3)\nx_probs <- c(0.2,0.5,0.3)\ng  <- function(x) x^2 + 1\nsum(g(x) * x_probs) \n## [1] 5.9\n```\n:::\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng  <- function(x) x^2 + 1\n\n# A theoretical example\nx <- c(1,2,3,4)\nx_probs <- c(1/4, 1/4, 1/4, 1/4)\nsum(g(x) * x_probs) \n## [1] 8.5\n\n# A simulation example\nX <- sample(x, prob=x_probs, size=1000, replace=T)\nmean(g(X))\n## [1] 8.544\n```\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nIf $X_{i}$ is a continuous random variable (a random variable with a continuous sample space) and $g$ is a function, then\n$\\mathbb E[g(X_{i})] = \\int_{-\\infty}^{\\infty} g(x)f(x) dx$.\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rexp(5e5, rate = 1)           # X ~ Exp(1)\nmean(sqrt(x))                      # LOTUS Simulation\n## [1] 0.8861617\nsqrt(pi) / 2                       # Exact via LOTUS integral\n## [1] 0.8862269\n```\n:::\n\n:::\n\nNote that you have already seen the special case where $g(X_{i})=\\left(X_{i}-\\mathbb{E}[X_{i}]\\right)^2$. LOTUS also applies to continuous random variables.\n\n\n## Estimators\n\n#### **Means**. {-}\nThe LLN follows from a famous theoretical result in statistics, *Linearity of Expectations*: the expected value of a sum of random variables equals the sum of their individual expected values.  To be concrete, suppose we take $n$ random variables, each one denoted as $X_{i}$. Then, for constants $a,b,1/n$, we have\n\\begin{eqnarray}\n\\mathbb{E}[a X_{1}+ b X_{2}] &=& a \\mathbb{E}[X_{1}]+ b \\mathbb{E}[X_{2}]\\\\\n\\mathbb{E}\\left[M\\right] &=& \\mathbb{E}\\left[ \\sum_{i=1}^{n} X_{i}/n \\right] = \\sum_{i=1}^{n} \\mathbb{E}[X_{i}]/n\n\\end{eqnarray}\nAssuming each data point has identical means; $\\mathbb{E}[X_{i}]=\\mu$, the expected value of the sample average is the mean; $\\mathbb{E}\\left[M\\right] = \\sum_{i=1}^{n} \\mu/n = \\mu$.\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nFor example, consider a coin flip with Heads $X_{i}=1$ having probability $p$ and Tails $X_{i}=0$ having probability $1-p$. First notice that $\\mathbb{E}[X_{i}]=p$. Then notice we can first \n\\begin{align*}\n\\mathbb{E}[X_{1}+X_{2}] \n&= [1+1][p \\times p] + [1+0][p \\times (1-p)] + [0+1][(1-p) \\times p] + [0+0][(1-p) \\times (1-p)] &  \\text{``HH + HT + TH + TT''} \\\\\n&= [1][p \\times p] + [1][p \\times (1-p)] + [0][(1-p) \\times p] + [0][(1-p) \\times (1-p)] &  \\text{first outcomes times prob.} \\\\\n&+ [1][p \\times p] + [0][p \\times (1-p)] + [1][(1-p) \\times p] + [0][(1-p) \\times (1-p)] &\n\\text{+second outcomes times prob.} \\\\\n&= [1][p \\times p] + [1][p \\times (1-p)] + [1][p \\times p] + [1][(1-p) \\times p] & \\text{drop zeros}\\\\\n&= 1p (p + [1-p]) +  1p (p + [1-p]) = p+p & \\text{algebra}\\\\\n&= \\mathbb{E}[X_{1}] +  \\mathbb{E}[X_{2}] .\n\\end{align*}\nThe theoretical mean is $\\mathbb{E}[\\frac{X_{1}+X_{2}}{2}]=\\frac{p+p}{2}=p$.\n:::\n\n\n#### **Variances**. {-}\nAnother famous theoretical result in statistics is that if we have independent and identical data (i.e., that each random variable $X_{i}$ has the same mean $\\mu$, same variance $\\sigma^2$, and is drawn without any dependence on the previous draws), then the standard error of the sample mean decreases by $1/\\sqrt{n}$. Intuitively, this follows from thinking of the variance as a type of mean (the mean squared deviation from $\\mu$).\n\\begin{eqnarray}\n\\mathbb{V}\\left( M \\right) \n&=& \\mathbb{V}\\left( \\frac{\\sum_{i=1}^{n} X_{i}}{n} \\right) \n= \\sum_{i=1}^{n} \\mathbb{V}\\left(\\frac{X_{i}}{n}\\right)\n= \\sum_{i=1}^{n} \\frac{\\sigma^2}{n^2}\n= \\sigma^2/n\\\\\nSE\\left(M\\right) &=& \\sqrt{\\mathbb{V}\\left( M \\right) } = \\sqrt{\\sigma^2/n} = \\sigma/\\sqrt{n}.\n\\end{eqnarray}\n\n\nNote that the *standard deviation* refers variability of individual observations (in either a sample or population), and is hence different from the standard error. Nonetheless, the sample standard deviation can be used to estimate the standard error of the sample mean: we can estimate $SE\\left(M\\right)$ by replacing the population standard deviation, $\\sigma$, with the sample standard deviation, $\\hat{S}$. To be clear, see that\n\n| Concept            | What varies                 | Notation     | What it measures           |\n| ------------------ | --------------------------- | ------------ | -------------------------- |\n| Standard deviation | individual observations     | sample: $\\hat{S}$, population $\\sigma$  | variability within sample/population |\n| Standard error     | a statistic                 | $SE(M)$ | variability across samples |\n: Types of variability\n\n\nWe can then estimate $SE(M)$ based on theory: assume that the data are i.i.d and that $M$ follows a normal distribution, and then compute $\\hat{S}/\\sqrt{n}\\approx SE(M)$. \n\nAnother data-driven way is to create a bootstrap distribution for the sample mean and then compute the standard deviation of the bootstrap distribution $\\hat{SE}^{\\text{boot}} \\approx SE(M)$. The theory-driven estimate is often a little different from than the bootstrap estimate, as the theory-driven estimate is based on idealistic assumptions (i.i.d. and normality) whereas the bootstrap estimate is driven by data that are often not ideal. The theory-driven estimate does have some advantages too, but we do not go into that as we would need more advanced mathematics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_dat <- USArrests[,'Murder']\nn <- length(sample_dat)\ntheory_se <- sd(sample_dat)/sqrt(n)\n\n# Bootstrap estimates\nbootstrap_means <- vector(length=9999)\nfor(b in seq_along(bootstrap_means)){\n    dat_id <- seq(1,n)\n    boot_id <- sample(dat_id , replace=T)\n    dat_b  <- sample_dat[boot_id] # c.f. jackknife\n    mean_b <- mean(dat_b)\n    bootstrap_means[b] <-mean_b\n}\nboot_se <- sd(bootstrap_means)\n\nc(boot_se, theory_se)\n## [1] 0.6072777 0.6159621\n```\n:::\n\n\n\n#### **Shape**. {-}\nSometimes, the sampling distribution is approximately normal (according to the CLT). In this case, you can use a standard error and the normal distribution to get a confidence interval.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standard Errors\ntheory_sd <- sd(sample_dat1)/sqrt(length(sample_dat1))\n## Normal CI\ntheory_quantile <- qnorm(c(0.025, 0.975))\ntheory_ci <- mean(sample_dat1) + theory_quantile*theory_sd\n\n# compare with: boot_ci, jack_ci\n```\n:::\n\n\n\n## Further Reading\n\nProbability Theory\n\n* [Refresher] <https://www.khanacademy.org/math/statistics-probability/probability-library/basic-theoretical-probability/a/probability-the-basics>\n* <https://book.stat420.org/probability-and-statistics-in-r.html>\n* <https://bookdown.org/speegled/foundations-of-statistics/>\n* <https://math.dartmouth.edu/~prob/prob/prob.pdf>\n* <https://bookdown.org/probability/beta/discrete-random-variables.html>\n* <https://www.econometrics-with-r.org/2.1-random-variables-and-probability-distributions.html>\n* <https://probability4datascience.com/ch02.html>\n* <https://statsthinking21.github.io/statsthinking21-R-site/probability-in-r-with-lucy-king.html>\n* <https://bookdown.org/probability/statistics/>\n* <https://www.atmos.albany.edu/facstaff/timm/ATM315spring14/R/IPSUR.pdf>\n* <https://rc2e.com/probability>\n* <https://bookdown.org/probability/beta/>\n* <https://bookdown.org/a_shaker/STM1001_Topic_3/>\n* <https://bookdown.org/fsancier/bookdown-demo/>\n* <https://bookdown.org/kevin_davisross/probsim-book/>\n* <https://bookdown.org/machar1991/ITER/2-pt.html>\n* <https://www.atmos.albany.edu/facstaff/timm/ATM315spring14/R/IPSUR.pdf>\n* <https://math.dartmouth.edu/~prob/prob/prob.pdf>\n\nFor \n\n* <https://dlsun.github.io/probability/linearity.html>\n\nFor weighted statistics, see\n\n* <https://seismo.berkeley.edu/~kirchner/Toolkits/Toolkit_12.pdf>\n* <https://www.bookdown.org/rwnahhas/RMPH/survey-desc.html>\n\n",
    "supporting": [
      "01_06_PopStatistics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}