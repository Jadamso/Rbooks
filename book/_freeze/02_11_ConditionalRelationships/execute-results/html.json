{
  "hash": "04f253472a9814108c0b2f37f444328c",
  "result": {
    "engine": "knitr",
    "markdown": "\n# Conditional Relationships\n***\n\n## Conditional Distributions\n\n#### **Discrete Data**.{-}\n\nIn contrast to marginal and joint distributions, *conditional* distributions describe how the distribution of one variable changes when we restrict attention to a subgroup defined by another variable. Formally, if $\\hat{p}_{xy}$ denotes the empirical joint distribution, then the empirical conditional distribution of $Y_{i}$ given $X_{i}=x$ is\n\\begin{eqnarray}\n\\hat{p}_{y \\mid x} = \\frac{\\hat{p}_{xy}}{\\hat{p}_{x} },\n\\end{eqnarray}\nwhere $\\hat{p}_{x} = \\sum_y \\hat{p}_{xy}$.\n\n\nFor example, suppose we observe a sample of $n=13$ students with two discrete variables:\n\n* $X_{i}$ depicts years of education, taking values in $\\{12, 14\\}$\n* $Y_{i}$ depicts sex, where $y=1$ means female and $y=0$ means male\n\nAssume the count data are summarized by the following frequency table:\n\n\\begin{array}{c|cc|c}\n & y=0\\ (\\text{male}) & y=1\\ (\\text{female}) & \\text{Row total}\\\\\n\\hline\nx=12 & 4/13 & 3/13 & 7/13\\\\\nx=14 & 1/13 & 5/13 & 6/13\\\\\n\\hline\n\\text{Column total} & 5/13 & 7/13 & 1\n\\end{array}\n\nWe will compute the conditional distribution of $Y_{i}$ given $X_{i}=x$. For $x=12$, we compute\n\\begin{eqnarray}\n\\hat{p}_{y=0 \\mid x=12}\n&=& \\frac{\\hat{p}_{x=12, y=0}}{\\hat{p}_{x=12}}\n= \\frac{4/13}{7/13}\n= \\frac{4}{7} \n\\approx 0.57. \\\\\n\\hat{p}_{y=1\\mid x=12}\n&=& \\frac{\\hat p_{x=12, y=1}}{\\hat{p}_{x=12}}\n= \\frac{3/13}{7/13}\n= \\frac{3}{7}\n\\approx 0.43.\n\\end{eqnarray}\nSimilarly, for $x=14$, we compute\n\\begin{eqnarray}\n\\hat{p}_{y=0 \\mid x=14}\n&=& \\frac{\\hat{p}_{x=14, y=0}}{\\hat{p}_{x=14}}\n= \\frac{1/13}{6/13}\n= \\frac{1}{6}\n\\approx 0.17. \\\\\n\\hat{p}_{y=1\\mid x=14}\n&=& \\frac{\\hat p_{x=14, y=1}}{\\hat{p}_{x=14}}\n= \\frac{5/13}{6/13}\n= \\frac{5}{6}\n\\approx 0.83.\n\\end{eqnarray}\n\nIn this example, we say that\n\n* conditional on students having $12$ years of education, $\\approx 57\\%$ are male and $\\approx 43\\%$ are female.\n* conditional on students having $14$ years of education, $\\approx 17\\%$ are male and $\\approx 83\\%$ are female.\n\n\nWe can also compute the conditional distribution of $X_{i}$ given $Y_{i}=y$, $\\hat{p}_{x \\mid y} = \\hat{p}_{xy} / \\hat{p}_{y}$,just as we did above.\n\n:::{.callout-tip icon=false collapse=\"true\"}\nContinuing the example above, show that\n\n* among male students, $\\approx 80\\%$ have 12 years of education.\n* among female students, $\\approx 38\\%$ have 12 years of education.\n\nTry programming the results, especially if you are stuck or uncertain\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Counts implied by the table:\n# X=12: 4 male, 3 female\n# X=14: 1 male, 5 female\nX <- c(rep(12,7), rep(14,6))\nY <- c(rep(0,4), rep(1,3),\n       rep(0,1), rep(1,5))\n\ndat <- data.frame(educ=X, female=Y)\n\ntab <- table(dat)\ntab\n##     female\n## educ 0 1\n##   12 4 3\n##   14 1 5\n\n# Joint distribution\nround(prop.table(tab), 3)\n##     female\n## educ     0     1\n##   12 0.308 0.231\n##   14 0.077 0.385\n\n# Conditional distribution of Y given X\nround(prop.table(tab, 1), 3)\n##     female\n## educ     0     1\n##   12 0.571 0.429\n##   14 0.167 0.833\n\n# Conditional distribution of X given Y\nround(prop.table(tab, 2), 3)\n##     female\n## educ     0     1\n##   12 0.800 0.375\n##   14 0.200 0.625\n```\n:::\n\n\n:::\n\nConditional distributions change the unit of analysis:\n\n* $\\hat{p}_{y=1 \\mid x=14}$ answers \"what fraction of student are female within the subgroup with 14 years of education?\"\n* $\\hat{p}_{x=14 \\mid y=1}$ answers \"what fraction of students have 14 years of education within the subgroup that is female?\"\n\n#### **Simpson's Paradox**.{-}\n\nFrequency tables can be tricky, especially when using percents, so take your time making and interpreting them. Remember that a ratio can change due changes in either the numerator or the denominator.\n\nConsider the following example, which shows that $400$ men and $400$ women applied to university and that only a share were accepted in each department (English or Engineering). If you computed percents for each sex within each cell, you would find men have a higher overall acceptance rate, but women have higher acceptance rates within each department! This counterintuitive phenomena is known as Simpson's Paradox, and the example is part of a real debate about discrimination (<http://homepage.stat.uiowa.edu/~mbognar/1030/Bickel-Berkeley.pdf>). \n\nEven if women have higher admission rates within both departments, women can still have a lower overall admission rate if women disproportionately apply to the more selective department (English) and men disproportionately apply to the less selective department (Engineering). The same issue is relevant for a variety of labor market issues, such as the gender pay gap, and a great many other social issues, such as why are some countries rich and others poor.\n\n\n| Department   | Men                  | Women                 | Total                 |\n|-------------|-----------------------|-----------------------|-----------------------|\n| English     | 100 (40, **$40\\%$**)      | 350 (150, **$43\\%$**)     | 450 (190, **$42\\%$**)     |\n| Engineering | 300 (160, **$53\\%$**)     | 50 (30, **$60\\%$**)       | 350 (190, **$54\\%$**)     |\n| Total       | 400 (200, **$50\\%$**)     | 400 (180, **$45\\%$**)     | 800 (380, **$48\\%$**)     |\n\n: School Applicants (Admitted), by Sex and Department\n\n\n#### **Continuous Data**.{-}\n\nThese describe the relationship between $\\hat{Y}_{i}$ and $\\hat{X}_{i}$. We show how distribution or density of $Y$ changes according to $X$. When $X$ is continuous, as it often is, we split it into distinct bins and convert it to a factor variable. E.g., \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split Data by Urban Population above/below mean\npop_mean <- mean(USArrests[,'UrbanPop'])\npop_cut <- USArrests[,'UrbanPop']< pop_mean\nmurder_lowpop <- USArrests[pop_cut,'Murder']\nmurder_highpop <- USArrests[!pop_cut,'Murder']\ncols <- c(low=rgb(0,0,1,.75), high=rgb(1,0,0,.75))\n\n# Common Histogram \nylim <- c(0,.25)\nxbks <-  seq(min(USArrests[,'Murder'])-1, max(USArrests[,'Murder'])+1, by=1)\n\npar(mfrow=c(1,2))\nhist(murder_lowpop,\n    breaks=xbks, col=cols[1],\n    main='Urban Pop >= Mean', font.main=1,\n    xlab='Murder Arrests', freq=F,\n    border=NA, ylim=ylim)\n\nhist(murder_highpop,\n    breaks=xbks, col=cols[2],\n    main='Urban Pop < Mean', font.main=1,\n    xlab='Murder Arrests', freq=F,\n    border=NA, ylim=ylim)\n```\n\n::: {.cell-output-display}\n![](02_11_ConditionalRelationships_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nIt is sometimes it is preferable to show the ECDF instead. And you can glue various combinations together to convey more information all at once\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayout( t(c(1,2,2)))\n# Full Sample Density\nhist(USArrests[,'Murder'], \n    main='Full Sample Density', font.main=1,\n    xlab='Murder Arrests',\n    breaks=xbks, freq=F, border=NA)\n\n# Split Sample Distribution Comparison\nF_lowpop <- ecdf(murder_lowpop)\nplot(F_lowpop, col=cols[1],\n    pch=16, xlab='Murder Arrests',\n    main='Split Sample Distributions',\n    font.main=1, bty='n')\nF_highpop <- ecdf(murder_highpop)\nplot(F_highpop, add=T, col=cols[2], pch=16)\n\nlegend('bottomright', col=cols,\n    pch=16, bty='n', inset=c(0,.1),\n    title='% Urban Pop.',\n    legend=c('Low (<= Mean)','High (>= Mean)'))\n```\n\n::: {.cell-output-display}\n![](02_11_ConditionalRelationships_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple Interactive Scatter Plot\n# plot(Assault~UrbanPop, USArrests, col=grey(0,.5), pch=16,\n#    cex=USArrests[,'Murder']/diff(range(USArrests[,'Murder']))*2,\n#    main='US Murder arrests (per 100,000)')\n```\n:::\n\n\nYou can also split data into more than two groups. For more than three groups, boxplots are often more effective than histograms or ECDF's.\n\n::: {.cell}\n\n```{.r .cell-code}\n# K Groups with even spacing (not even counts)\nK <- 4\nUSArrests[,'UrbanPop_Kcut'] <- cut(USArrests[,'UrbanPop'],K)\ntable(USArrests[,'UrbanPop_Kcut'] )\n## \n## (31.9,46.8] (46.8,61.5] (61.5,76.2] (76.2,91.1] \n##           6          13          17          14\n\n# Full sample\n#boxplot(USArrests[,'Murder'], main='',\n#    xlab='All Data', ylab='Murder Arrests')\n\n# Boxplots for each group\nKcols <- hcl.colors(K,alpha=.5)\nboxplot(Murder~UrbanPop_Kcut, USArrests,\n    main='', col=Kcols, \n    varwidth=T, #show number of obs. per group\n    xlab='Urban Population', ylab='')\n```\n\n::: {.cell-output-display}\n![](02_11_ConditionalRelationships_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# 4 Groups with equal numbers of observations\n#Qcuts <- c(\n#    '0%'=min(USArrests[,'UrbanPop'])-10*.Machine[['double.eps']],\n#    quantile(USArrests[,'UrbanPop'], probs=c(.25,.5,.75,1)))\n#USArrests[,'UrbanPop']_cut <- cut(USArrests[,'UrbanPop'], Qcuts)\n#boxplot(Murder~UrbanPop_cut, USArrests, col=hcl.colors(4,alpha=.5))\n```\n:::\n\n\n\n\n\n\n\n\n## Group Differences\n\nFor mixed data, $\\hat{Y}_{i}$ is a cardinal variable and $\\hat{X}_{i}$ is a factor variable (typically unordered). For such \"grouped data\", we analyze associations via group comparisons. The basic idea is best seen in a comparison of two samples, which corresponds to an $\\hat{X}_{i}$ with two categories. For example, the heights of men and women in Canada or the homicide rates in two different American states. For another example, the wages for people with and without completing a degree.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wooldridge)\n## Group 1\nX1_id <- wage1[,'educ'] == 15\nY1 <- wage1[X1_id, 'wage']\n## Group 2\nX2_id <- wage1[,'educ'] == 16\nY2 <- wage1[X2_id, 'wage']\n\n# Initial Summary Figure\nbks <- seq(0, 24, by=1.5)\ndlim <- c(0,.2)\ncols <- c(rgb(1,0,0,.5), rgb(0,0,1,.5))\n\nhist(Y1, breaks=bks, ylim=dlim,\n     col=cols[1], xlab='Wages',\n     freq=F, border=NA, main='')\nhist(Y2, breaks=bks, ylim=dlim,\n     col=cols[2],\n     freq=F, border=NA, add=T)\nlegend('topright',\n       col=cols, pch=15,\n       legend=c('15 Years', '16 Years'),\n       title='School Completed')\n```\n\n::: {.cell-output-display}\n![](02_11_ConditionalRelationships_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nThere may be several differences between groups. Often, the first statistic we investigate is the difference in means. \n\n#### **Mean Differences**. {-}\n\nWe often want to know if the means of different sample are different. To test this hypothesis, we compute the means separately for each sample and then examine the differences term\n\\begin{eqnarray} \n\\hat{D} = \\hat{M}_{Y1} - \\hat{M}_{Y2},\n\\end{eqnarray}\nwith a null hypothesis that there is no difference in the population means.\n\n:::{.callout-note icon=false collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Differences between means\nm1 <- mean(Y1)\nm2 <- mean(Y2)\nd <- m1-m2\n    \n# Bootstrap Distribution\nbootstrap_diff <- vector(length=9999)\nfor(b in seq(bootstrap_diff) ){\n    Y1_b <- sample(Y1, replace=T)\n    Y2_b <- sample(Y2, replace=T)\n    m1_b <- mean(Y1_b)\n    m2_b <- mean(Y2_b)\n    d_b <- m1_b - m2_b\n    bootstrap_diff[b] <- d_b\n}\nhist(bootstrap_diff,\n    border=NA, font.main=1,\n    main='Difference in Means')\n\n# 2-Sided Test via Confidence Interval\nboot_ci <- quantile(bootstrap_diff, probs=c(.025, .975))\nabline(v=boot_ci, lwd=2)\nabline(v=0, lwd=2, col=2)\n```\n\n::: {.cell-output-display}\n![](02_11_ConditionalRelationships_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n:::\n\nJust as with one sample tests, we can compute a *standardized differences*, where $D$ is converted into a $t$ statistic. Note, however, that we have to compute the standard error for the difference statistic, which is a bit more complicated. However, this allows us to easily conduct one or two sided hypothesis tests using a standard normal approximation.\n\n:::{.callout-tip icon=false collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nse_hat <- sqrt(var(Y1)/n1 + var(Y2)/n2);\nt_obs <- d/se_hat\n\nt_2sample <- function(Y1, Y2){\n    # Differences between means\n    m1 <- mean(Y1)\n    m2 <- mean(Y2)\n    d <- (m1-m2)\n\n    # SE estimate\n    n1  <- length(Y1)\n    n2  <- length(Y2)\n    s1  <- var(Y1)\n    s2  <- var(Y2)\n    s   <- ((n1-1)*s1 + (n2-1)*s2)/(n1+n2-2)\n    d_se <- sqrt(s*(1/n1+1/n2))\n\n    # t stat\n    t_stat <- d/d_se\n    return(t_stat)\n}\n \ntstat <- twosam(data[,'male'], data[,'female'])\ntstat\n```\n:::\n\n:::\n\n\n#### **Quantile Differences**. {-}\nThe above procedure generalized from differences in means to other quantiles statistics like medians.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Quantile Comparison\n\n## Distribution 1\nF1 <- ecdf(Y1)\nplot(F1, col=cols[1],\n     pch=16, xlab='Wages',\n     main='Comparing Medians',\n     font.main=1, bty='n')\n## Median 1\nmed1 <- quantile(F1, probs=0.5)\nsegments(med1, 0, med1, 0.5, col=cols[1], lty=2)\nabline(h=0.5, lty=2)\n\n## Distribution 2\nF2 <- ecdf(Y2)\nplot(F2, add=TRUE, col=cols[2], pch=16)\n## Median 2\nmed2 <- quantile(F2, probs=0.5)\nsegments(med2, 0, med2, 0.5, col=cols[2], lty=2)\n\n## Legend\nlegend('bottomright',\n       col=cols, pch=15,\n       legend=c('Grade 15', 'Grade 16'),\n       title='School Completed')\n```\n\n::: {.cell-output-display}\n![](02_11_ConditionalRelationships_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-note icon=false collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap Distribution Function\nboot_quant <- function(Y1, Y2, B=9999, prob=0.5, ...){\n    bootstrap_diff <- vector(length=B)\n    for(b in seq(bootstrap_diff)){\n        Y1_b <- sample(Y1, replace=T)\n        Y2_b <- sample(Y2, replace=T)\n        q1_b <- quantile(Y1_b, probs=0.5, ...)\n        q2_b <- quantile(Y2_b, probs=0.5, ...)\n        d_b <- q1_b - q2_b\n        bootstrap_diff[b] <- d_b\n    }\n    return(bootstrap_diff)\n}\n\n# 2-Sided Test for Median Differences\n# d <- median(Y2) - median(Y1)\nboot_d <- boot_quant(Y1, Y1, B=999, prob=0.5)\nhist(boot_d, border=NA, font.main=1,\n    main='Difference in Medians')\nabline(v=quantile(boot_d, probs=c(.025, .975)), lwd=2)\nabline(v=0, lwd=2, col=2)\n```\n\n::: {.cell-output-display}\n![](02_11_ConditionalRelationships_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\n1 - ecdf(boot_d)(0)\n## [1] 0.3973974\n```\n:::\n\n:::\n\n\nIf we want to test for the differences in medians across groups with independent observations, we can also use notches in the boxplot. If the notches of two boxes do not overlap, then there is rough evidence that the difference in medians is statistically significant. The square root of the sample size is also shown as the bin width in each boxplot.^[Let each group $g$ have median $\\tilde{M}_{g}$, interquartile range $\\hat{IQR}_{g}$, observations $n_{g}$. We can compute standard deviation of the median as $\\tilde{S}_{g}= \\frac{1.25 \\hat{IQR}_{g}}{1.35 \\sqrt{n_{g}}}$. As a rough guess, the interval $\\tilde{M}_{g} \\pm 1.7 \\tilde{S}_{g}$ is the historical default and displayed as a *notch* in the boxplot. See also <https://www.tandfonline.com/doi/abs/10.1080/00031305.1978.10479236>.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(Y1, Y2,\n    col=cols,\n    notch=T,\n    varwidth=T)\n```\n\n::: {.cell-output-display}\n![](02_11_ConditionalRelationships_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nNote that bootstrap tests can perform poorly with highly unequal variances or skewed data. To see this yourself, make a simulation with skewed data and unequal variances.\n\n\nIn principle, we can also examine whether there are differences in spread or shape statistics such as `sd` and `IQR`, or `skew` and `kurtosis`. More often, however, we examine whether there are any differences in the distributions.\n\n:::{.callout-tip icon=false collapse=\"true\"}\nHere is an example to look at differences in \"spread\"\n\n::: {.cell}\n\n```{.r .cell-code}\nboot_fun <- function( fun, B=9999, ...){\n    bootstrap_diff <- vector(length=B)\n    for(b in seq(bootstrap_diff)){\n        Y1_b <- sample(Y1, replace=T)\n        Y2_b <- sample(Y2, replace=T)\n        f1_b <- fun(Y1_b, ...)\n        f2_b <- fun(Y2_b, ...)\n        d_b <- f1_b - f2_b\n        bootstrap_diff[b] <- d_b\n    }\n    return(bootstrap_diff)\n}\n\n# 2-Sided Test for SD Differences\n#d <- sd(Y2) - sd(Y1)\nboot_d <- boot_fun(sd)\nhist(boot_d, border=NA, font.main=1,\n    main='Difference in Standard Deviations')\nabline(v=quantile(boot_d, probs=c(.025, .975)), lwd=2)\nabline(v=0, lwd=2, col=2)\n1 - ecdf(boot_d)(0)\n\n\n# Try any function!\n# boot_fun( function(xs) { IQR(xs)/median(xs) } )\n```\n:::\n\n:::\n\n\n\n## Distributional Comparisons\n\nWe can also examine whether there are any differences between the entire *distributions*. We typically start by plotting the data using ECDF's or a boxplot, and then calculate a statistic for hypothesis testing. Which plot and test statistic depends on how many groups there are.\n\n#### **Two groups**.{-}\n\nOne useful visualization for two groups is to plot the quantiles against one another: a quantile-quantile plot. I.e., the first data point on the bottom left shows the first quantile for both distributions. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Wage Data (same as from before)\n#library(wooldridge)\n#Y1 <- sort( wage1[wage1[,'educ'] == 15,  'wage'])  \n#Y2 <- sort( wage1[wage1[,'educ'] == 16,  'wage'] )\n\n# Compute Quantiles\nquants <- seq(0,1,length.out=101)\nQ1 <- quantile(Y1, probs=quants)\nQ2 <- quantile(Y2, probs=quants)\n\n# Compare Distributions via Quantiles\n#ry <- range(c(Y1, Y2))\n#plot(ry, c(0,1), type='n', font.main=1,\n#    main='Distributional Comparison',\n#    xlab=\"Quantile\",\n#    ylab=\"Probability\")\n#lines(Q1, quants, col=2)\n#lines(Q2, quants, col=4)\n#legend('bottomright', col=c(2,4), lty=1,\n#\tlegend=c(\n#\t    expression(hat(F)[1]),\n#\t    expression(hat(F)[2]) \n#))\n\n# Compare Quantiles\nry <- range(c(Y1, Y2))\nplot(Q1, Q2, xlim=ry, ylim=ry,\n    xlab=expression(Q[1]),\n    ylab=expression(Q[2]),\n    main='Quantile-Quantile Plot',\n    font.main=1,\n    pch=16, col=grey(0,.25))\nabline(a=0,b=1,lty=2)\n```\n\n::: {.cell-output-display}\n![](02_11_ConditionalRelationships_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nThe starting point for hypothesis testing is the Kolmogorov-Smirnov Statistic: the maximum absolute difference between two CDF's over all sample data $y \\in \\{Y_1\\} \\cup \\{Y_2\\}$.\n\\begin{eqnarray}\n\\hat{KS} &=& \\max_{y} |\\hat{F}_{1}(y)- \\hat{F}_{2}(y)|^{p},\n\\end{eqnarray}\nwhere $p$ is an integer (typically 1). An intuitive alternative is the Cramer-von Mises Statistic: the sum of absolute differences (raised to an integer, typically 2) between two CDF's. \n\\begin{eqnarray}\n\\hat{CVM} &=& \\sum_{y} | \\hat{F}_{1}(y)- \\hat{F}_{2}(y)|^{p}.\n\\end{eqnarray}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Distributions\ny <- sort(c(Y1, Y2))\nF1 <- ecdf(Y1)(y)\nF2 <- ecdf(Y2)(y)\n\nlibrary(twosamples)\n\n# Kolmogorov-Smirnov\nKSq <- which.max(abs(F2 - F1))\nKSqv <- round(twosamples::ks_stat(Y1, Y2),2)\n\n# Cramer-von Mises Statistic (p=2)\nCVMqv <- round(twosamples::cvm_stat(Y1, Y2, power=2), 2) \n\n# Visualize Differences\nplot(range(y), c(0,1), type=\"n\", xlab='x', ylab='ECDF')\nlines(y, F1, col=cols[1], lwd=2)\nlines(y, F2, col=cols[2], lwd=2)\n\n# KS\ntitle( paste0('KS: ', KSqv), adj=0, font.main=1)\nsegments(y[KSq], F1[KSq], y[KSq], F2[KSq], lwd=1.5, col=grey(0,.75), lty=2)\n\n# CVM\ntitle( paste0('CVM: ', CVMqv), adj=1, font.main=1)\nsegments(y, F1, y, F2, lwd=.5, col=grey(0,.2))\n```\n\n::: {.cell-output-display}\n![](02_11_ConditionalRelationships_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nJust as before, you use bootstrapping for hypothesis testing.\n\n::: {.cell}\n\n```{.r .cell-code}\ntwosamples::ks_test(Y1, Y2)\n## Test Stat   P-Value \n## 0.2892157 0.0850000\n\ntwosamples::cvm_test(Y1, Y2)\n## Test Stat   P-Value \n##  2.084253  0.083000\n```\n:::\n\n\n:::{.callout-tip icon=false collapse=\"true\"}\nCompare the distribution of arrests for two different counties, each with data over time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wooldridge)\ncountymurders\n```\n:::\n\n\n:::\n\n#### **Multiple groups**. {-}\nWith multiple groups, you will want to begin with a summary figure (such as a boxplot). We can also tests the equality of all distributions (whether at least one group is different). The *Kruskal-Wallis* test examines $H_0:\\; F_1 = F_2 = \\dots = F_G$ versus $H_A:\\; \\text{at least one } F_g \\text{ differs}$, where $F_g$ is the continuous distribution of group $g=1,...G$. This test does not tell us which group is different.\n\nTo conduct the test, first denote individuals $i=1,...n$ with overall ranks $\\hat{r}_1,....\\hat{r}_{n}$. Each individual belongs to group $g=1,...G$, and each group $g$ has $n_{g}$ individuals with average rank $\\bar{r}_{g} = \\sum_{i} \\hat{r}_{i} /n_{g}$. The Kruskal Wallis statistic is \n\\begin{eqnarray}\n\\hat{KW} &=& (N-1) \\frac{\\sum_{g=1}^{G} n_{g}( \\bar{r}_{g} - \\bar{r}  )^2  }{\\sum_{i=1}^{n} ( \\hat{r}_{i} - \\bar{r}  )^2}, \n\\end{eqnarray}\nwhere  $\\bar{r} = \\frac{n+1}{2}$ is the grand mean rank.\n\nIn the special case with only two groups, $G=2$, the Kruskal Wallis test reduces to the *Mannâ€“Whitney U* test (also known as the *Wilcoxon rank-sum* test). In this case, we can write the hypotheses in terms of individual outcomes in each group, $Y_i$ in one group $Y_j$ in the other; $H_0: Prob(Y_i > Y_j)=Prob(Y_i > Y_i)$  versus $H_A: Prob(Y_i > Y_j) \\neq Prob(Y_i > Y_j)$. The corresponding test statistic is\n\\begin{eqnarray}\n\\hat{U}   &=& \\min(\\hat{U}_1, \\hat{U}_2) \\\\\n\\hat{U}_g &=& \\sum_{i\\in g}\\sum_{j\\in -g}\n           \\Bigl[\\mathbf 1( \\hat{Y}_{i} > \\hat{Y}_{j}) + \\tfrac12\\mathbf 1(\\hat{Y}_{i} = \\hat{Y}_{j})\\Bigr].\n\\end{eqnarray}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\ndata(CASchools)\nCASchools[,'stratio'] <- CASchools[,'students']/CASchools[,'teachers']\n\n# Do student/teacher ratio differ for at least 1 county?\n# Single test of multiple distributions\nkruskal.test(CASchools[,'stratio'], CASchools[,'county'])\n## \n## \tKruskal-Wallis rank sum test\n## \n## data:  CASchools[, \"stratio\"] and CASchools[, \"county\"]\n## Kruskal-Wallis chi-squared = 161.18, df = 44, p-value = 2.831e-15\n\n# Multiple pairwise tests\n# pairwise.wilcox.test(CASchools[,'stratio'], CASchools[,'county'])\n```\n:::\n\n\n## Further Reading \n\nOther Statistics \n\n* <https://cran.r-project.org/web/packages/qualvar/vignettes/wilcox1973.html>\n\n\n",
    "supporting": [
      "02_11_ConditionalRelationships_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}